  I0518 08:52:56.713440      19 e2e.go:117] Starting e2e run "5ca56008-4132-4939-9b12-133755241423" on Ginkgo node 1
  May 18 08:52:56.755: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1684399976 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May 18 08:52:57.028: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 08:52:57.030: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May 18 08:52:57.102: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May 18 08:52:57.111: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
  May 18 08:52:57.111: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  May 18 08:52:57.111: INFO: e2e test version: v1.27.1
  May 18 08:52:57.114: INFO: kube-apiserver version: v1.27.1
  May 18 08:52:57.115: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 08:52:57.123: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/18/23 08:52:57.544
  May 18 08:52:57.544: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 08:52:57.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:52:57.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:52:57.57
  STEP: Creating projection with secret that has name projected-secret-test-835c32bf-58b1-4dca-888f-9e4012b1f6af @ 05/18/23 08:52:57.574
  STEP: Creating a pod to test consume secrets @ 05/18/23 08:52:57.579
  STEP: Saw pod success @ 05/18/23 08:53:03.613
  May 18 08:53:03.618: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-secrets-1220c68b-c933-44a9-8af8-2ec50f1ec462 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 08:53:03.663
  May 18 08:53:03.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3955" for this suite. @ 05/18/23 08:53:03.684
• [6.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/18/23 08:53:03.699
  May 18 08:53:03.700: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 08:53:03.701
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:03.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:03.723
  STEP: Creating configMap with name projected-configmap-test-volume-map-85d42b4f-79dd-4b23-b69d-965b5555383c @ 05/18/23 08:53:03.726
  STEP: Creating a pod to test consume configMaps @ 05/18/23 08:53:03.731
  STEP: Saw pod success @ 05/18/23 08:53:07.757
  May 18 08:53:07.761: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-configmaps-2bc9f08e-9369-4812-a6d4-0408323f4bed container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 08:53:07.768
  May 18 08:53:07.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-519" for this suite. @ 05/18/23 08:53:07.786
• [4.096 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/18/23 08:53:07.796
  May 18 08:53:07.796: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/18/23 08:53:07.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:07.814
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:07.818
  May 18 08:53:07.822: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 08:53:14.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3966" for this suite. @ 05/18/23 08:53:14.58
• [6.801 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/18/23 08:53:14.603
  May 18 08:53:14.604: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 08:53:14.605
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:14.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:14.643
  STEP: Creating a pod to test downward api env vars @ 05/18/23 08:53:14.647
  STEP: Saw pod success @ 05/18/23 08:53:18.683
  May 18 08:53:18.688: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downward-api-b61b8492-4a8b-4ce8-87bc-b3001c918c29 container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 08:53:18.71
  May 18 08:53:18.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2495" for this suite. @ 05/18/23 08:53:18.734
• [4.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/18/23 08:53:18.749
  May 18 08:53:18.749: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 08:53:18.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:18.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:18.773
  STEP: creating a Service @ 05/18/23 08:53:18.782
  STEP: watching for the Service to be added @ 05/18/23 08:53:18.797
  May 18 08:53:18.800: INFO: Found Service test-service-7sv7j in namespace services-7629 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May 18 08:53:18.804: INFO: Service test-service-7sv7j created
  STEP: Getting /status @ 05/18/23 08:53:18.805
  May 18 08:53:18.812: INFO: Service test-service-7sv7j has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/18/23 08:53:18.812
  STEP: watching for the Service to be patched @ 05/18/23 08:53:18.82
  May 18 08:53:18.825: INFO: observed Service test-service-7sv7j in namespace services-7629 with annotations: map[] & LoadBalancer: {[]}
  May 18 08:53:18.825: INFO: Found Service test-service-7sv7j in namespace services-7629 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May 18 08:53:18.826: INFO: Service test-service-7sv7j has service status patched
  STEP: updating the ServiceStatus @ 05/18/23 08:53:18.827
  May 18 08:53:18.843: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/18/23 08:53:18.843
  May 18 08:53:18.846: INFO: Observed Service test-service-7sv7j in namespace services-7629 with annotations: map[] & Conditions: {[]}
  May 18 08:53:18.847: INFO: Observed event: &Service{ObjectMeta:{test-service-7sv7j  services-7629  b7bb03a5-ea20-4b05-a0f1-17bda59f82d8 1923359 0 2023-05-18 08:53:18 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-18 08:53:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-18 08:53:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.97.56.3,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.97.56.3],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May 18 08:53:18.847: INFO: Found Service test-service-7sv7j in namespace services-7629 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 18 08:53:18.848: INFO: Service test-service-7sv7j has service status updated
  STEP: patching the service @ 05/18/23 08:53:18.848
  STEP: watching for the Service to be patched @ 05/18/23 08:53:18.86
  May 18 08:53:18.862: INFO: observed Service test-service-7sv7j in namespace services-7629 with labels: map[test-service-static:true]
  May 18 08:53:18.862: INFO: observed Service test-service-7sv7j in namespace services-7629 with labels: map[test-service-static:true]
  May 18 08:53:18.862: INFO: observed Service test-service-7sv7j in namespace services-7629 with labels: map[test-service-static:true]
  May 18 08:53:18.863: INFO: Found Service test-service-7sv7j in namespace services-7629 with labels: map[test-service:patched test-service-static:true]
  May 18 08:53:18.863: INFO: Service test-service-7sv7j patched
  STEP: deleting the service @ 05/18/23 08:53:18.863
  STEP: watching for the Service to be deleted @ 05/18/23 08:53:18.879
  May 18 08:53:18.881: INFO: Observed event: ADDED
  May 18 08:53:18.882: INFO: Observed event: MODIFIED
  May 18 08:53:18.882: INFO: Observed event: MODIFIED
  May 18 08:53:18.883: INFO: Observed event: MODIFIED
  May 18 08:53:18.883: INFO: Found Service test-service-7sv7j in namespace services-7629 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May 18 08:53:18.884: INFO: Service test-service-7sv7j deleted
  May 18 08:53:18.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7629" for this suite. @ 05/18/23 08:53:18.89
• [0.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/18/23 08:53:18.91
  May 18 08:53:18.910: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replicaset @ 05/18/23 08:53:18.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:18.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:18.936
  STEP: Create a Replicaset @ 05/18/23 08:53:18.95
  STEP: Verify that the required pods have come up. @ 05/18/23 08:53:18.957
  May 18 08:53:18.960: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 18 08:53:23.963: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/18/23 08:53:23.964
  STEP: Getting /status @ 05/18/23 08:53:23.964
  May 18 08:53:23.971: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/18/23 08:53:23.971
  May 18 08:53:23.980: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/18/23 08:53:23.98
  May 18 08:53:23.983: INFO: Observed &ReplicaSet event: ADDED
  May 18 08:53:23.984: INFO: Observed &ReplicaSet event: MODIFIED
  May 18 08:53:23.984: INFO: Observed &ReplicaSet event: MODIFIED
  May 18 08:53:23.985: INFO: Observed &ReplicaSet event: MODIFIED
  May 18 08:53:23.985: INFO: Found replicaset test-rs in namespace replicaset-2983 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 18 08:53:23.985: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/18/23 08:53:23.986
  May 18 08:53:23.986: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 18 08:53:23.991: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/18/23 08:53:23.992
  May 18 08:53:23.994: INFO: Observed &ReplicaSet event: ADDED
  May 18 08:53:23.994: INFO: Observed &ReplicaSet event: MODIFIED
  May 18 08:53:23.994: INFO: Observed &ReplicaSet event: MODIFIED
  May 18 08:53:23.995: INFO: Observed &ReplicaSet event: MODIFIED
  May 18 08:53:23.995: INFO: Observed replicaset test-rs in namespace replicaset-2983 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 18 08:53:23.995: INFO: Observed &ReplicaSet event: MODIFIED
  May 18 08:53:23.995: INFO: Found replicaset test-rs in namespace replicaset-2983 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May 18 08:53:23.995: INFO: Replicaset test-rs has a patched status
  May 18 08:53:23.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2983" for this suite. @ 05/18/23 08:53:23.999
• [5.097 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/18/23 08:53:24.015
  May 18 08:53:24.015: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 08:53:24.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:24.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:24.037
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 08:53:24.041
  STEP: Saw pod success @ 05/18/23 08:53:28.063
  May 18 08:53:28.067: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-52f026d0-f719-414e-913d-543b35ddbaa1 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 08:53:28.073
  May 18 08:53:28.085: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1457" for this suite. @ 05/18/23 08:53:28.089
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/18/23 08:53:28.101
  May 18 08:53:28.102: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 08:53:28.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:28.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:28.121
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 08:53:28.124
  STEP: Saw pod success @ 05/18/23 08:53:32.149
  May 18 08:53:32.154: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-62b70093-e682-4175-88ff-4ff208a505bb container client-container: <nil>
  STEP: delete the pod @ 05/18/23 08:53:32.163
  May 18 08:53:32.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5518" for this suite. @ 05/18/23 08:53:32.187
• [4.095 seconds]
------------------------------
SS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/18/23 08:53:32.199
  May 18 08:53:32.200: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubelet-test @ 05/18/23 08:53:32.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:32.225
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:32.229
  May 18 08:53:36.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6190" for this suite. @ 05/18/23 08:53:36.259
• [4.066 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/18/23 08:53:36.267
  May 18 08:53:36.267: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-pred @ 05/18/23 08:53:36.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:53:36.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:53:36.29
  May 18 08:53:36.297: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 18 08:53:36.308: INFO: Waiting for terminating namespaces to be deleted...
  May 18 08:53:36.313: INFO: 
  Logging pods the apiserver thinks is on node ck-test-kube-1-27-worker before test
  May 18 08:53:36.328: INFO: calico-node-z6wl2 from kube-system started at 2023-05-12 06:51:27 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.328: INFO: 	Container calico-node ready: true, restart count 0
  May 18 08:53:36.328: INFO: kube-proxy-269wt from kube-system started at 2023-05-12 06:50:32 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.328: INFO: 	Container kube-proxy ready: true, restart count 0
  May 18 08:53:36.328: INFO: bin-false01a3f7f5-e80d-47e2-8ba6-8495e01f9754 from kubelet-test-6190 started at 2023-05-18 08:53:32 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.328: INFO: 	Container bin-false01a3f7f5-e80d-47e2-8ba6-8495e01f9754 ready: false, restart count 0
  May 18 08:53:36.328: INFO: node-exporter-7q9w4 from secloudit-monitoring started at 2023-05-12 07:19:01 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.328: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 08:53:36.328: INFO: 	Container node-exporter ready: true, restart count 0
  May 18 08:53:36.328: INFO: sonobuoy from sonobuoy started at 2023-05-18 08:52:38 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.328: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 18 08:53:36.329: INFO: sonobuoy-e2e-job-cccd1fa16dd54418 from sonobuoy started at 2023-05-18 08:52:39 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.329: INFO: 	Container e2e ready: true, restart count 0
  May 18 08:53:36.329: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 08:53:36.329: INFO: sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-8p6sc from sonobuoy started at 2023-05-18 08:52:40 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.329: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 08:53:36.329: INFO: 	Container systemd-logs ready: true, restart count 0
  May 18 08:53:36.329: INFO: 
  Logging pods the apiserver thinks is on node ck-test-kube-1-27-worker-1 before test
  May 18 08:53:36.377: INFO: argocd-application-controller-0 from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.377: INFO: 	Container argocd-application-controller ready: true, restart count 0
  May 18 08:53:36.377: INFO: argocd-applicationset-controller-57fd46cc99-vjt7t from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.377: INFO: 	Container argocd-applicationset-controller ready: true, restart count 0
  May 18 08:53:36.377: INFO: argocd-dex-server-75d8c65447-wrjw6 from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.377: INFO: 	Container dex ready: true, restart count 0
  May 18 08:53:36.377: INFO: argocd-notifications-controller-6f5f6d97cb-sdzxc from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.377: INFO: 	Container argocd-notifications-controller ready: true, restart count 0
  May 18 08:53:36.378: INFO: argocd-redis-77bf5b886-dn8z5 from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container redis ready: true, restart count 0
  May 18 08:53:36.378: INFO: argocd-repo-server-59684bc98c-dk4hd from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container argocd-repo-server ready: true, restart count 0
  May 18 08:53:36.378: INFO: argocd-server-d5bd658c-fs9dz from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container argocd-server ready: true, restart count 0
  May 18 08:53:36.378: INFO: istio-egressgateway-575466f5bb-mqw8r from istio-system started at 2023-05-18 06:47:53 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container istio-proxy ready: true, restart count 0
  May 18 08:53:36.378: INFO: istio-ingressgateway-cb9c6b49d-htxtq from istio-system started at 2023-05-18 06:47:53 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container istio-proxy ready: true, restart count 0
  May 18 08:53:36.378: INFO: istiod-6fbbf67d58-zhh74 from istio-system started at 2023-05-18 06:47:39 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container discovery ready: true, restart count 0
  May 18 08:53:36.378: INFO: calico-kube-controllers-674fff74c8-lvpc2 from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 18 08:53:36.378: INFO: calico-node-946wb from kube-system started at 2023-05-12 06:51:27 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container calico-node ready: true, restart count 0
  May 18 08:53:36.378: INFO: coredns-5d78c9869d-fchlv from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container coredns ready: true, restart count 0
  May 18 08:53:36.378: INFO: coredns-5d78c9869d-vbhw2 from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container coredns ready: true, restart count 0
  May 18 08:53:36.378: INFO: kube-proxy-s5x6t from kube-system started at 2023-05-12 06:50:32 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container kube-proxy ready: true, restart count 0
  May 18 08:53:36.378: INFO: alertmanager-main-0 from secloudit-monitoring started at 2023-05-18 07:36:19 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 08:53:36.378: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 08:53:36.378: INFO: alertmanager-main-1 from secloudit-monitoring started at 2023-05-12 07:19:12 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 08:53:36.378: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 08:53:36.378: INFO: alertmanager-main-2 from secloudit-monitoring started at 2023-05-18 07:36:20 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 08:53:36.378: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 08:53:36.378: INFO: blackbox-exporter-659b6ff548-25j5z from secloudit-monitoring started at 2023-05-12 07:17:19 +0000 UTC (3 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container blackbox-exporter ready: true, restart count 0
  May 18 08:53:36.378: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 08:53:36.378: INFO: 	Container module-configmap-reloader ready: true, restart count 0
  May 18 08:53:36.378: INFO: grafana-6cb59f497c-ssmtq from secloudit-monitoring started at 2023-05-18 07:36:15 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.378: INFO: 	Container grafana ready: true, restart count 0
  May 18 08:53:36.379: INFO: kube-state-metrics-f59465db9-v6k4r from secloudit-monitoring started at 2023-05-18 07:36:14 +0000 UTC (3 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
  May 18 08:53:36.379: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
  May 18 08:53:36.379: INFO: 	Container kube-state-metrics ready: true, restart count 0
  May 18 08:53:36.379: INFO: node-exporter-4ttx4 from secloudit-monitoring started at 2023-05-12 07:18:53 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 08:53:36.379: INFO: 	Container node-exporter ready: true, restart count 0
  May 18 08:53:36.379: INFO: prometheus-adapter-74d4cb89b8-ll246 from secloudit-monitoring started at 2023-05-12 07:18:13 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container prometheus-adapter ready: true, restart count 0
  May 18 08:53:36.379: INFO: prometheus-adapter-74d4cb89b8-z4hvp from secloudit-monitoring started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container prometheus-adapter ready: true, restart count 0
  May 18 08:53:36.379: INFO: prometheus-k8s-0 from secloudit-monitoring started at 2023-05-12 07:19:13 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 08:53:36.379: INFO: 	Container prometheus ready: true, restart count 0
  May 18 08:53:36.379: INFO: prometheus-k8s-1 from secloudit-monitoring started at 2023-05-18 07:36:19 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 08:53:36.379: INFO: 	Container prometheus ready: true, restart count 0
  May 18 08:53:36.379: INFO: prometheus-operator-59f6b8bc59-wv7ll from secloudit-monitoring started at 2023-05-18 07:36:15 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 08:53:36.379: INFO: 	Container prometheus-operator ready: true, restart count 0
  May 18 08:53:36.379: INFO: sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-xppr7 from sonobuoy started at 2023-05-18 08:52:40 +0000 UTC (2 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 08:53:36.379: INFO: 	Container systemd-logs ready: true, restart count 0
  May 18 08:53:36.379: INFO: tekton-pipelines-remote-resolvers-76f588dbf5-z6v9v from tekton-pipelines-resolvers started at 2023-05-18 07:36:15 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container controller ready: true, restart count 0
  May 18 08:53:36.379: INFO: tekton-pipelines-controller-dd857fb44-mwdzd from tekton-pipelines started at 2023-05-12 07:32:59 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container tekton-pipelines-controller ready: true, restart count 0
  May 18 08:53:36.379: INFO: tekton-pipelines-webhook-79bbb84bb4-7q2lq from tekton-pipelines started at 2023-05-12 07:32:59 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container webhook ready: true, restart count 0
  May 18 08:53:36.379: INFO: tekton-triggers-controller-99d8dc8bb-g47nr from tekton-pipelines started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container tekton-triggers-controller ready: true, restart count 0
  May 18 08:53:36.379: INFO: tekton-triggers-core-interceptors-ddf77b777-psf7x from tekton-pipelines started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container tekton-triggers-core-interceptors ready: true, restart count 0
  May 18 08:53:36.379: INFO: tekton-triggers-webhook-69569f7f45-vv4c8 from tekton-pipelines started at 2023-05-12 07:33:13 +0000 UTC (1 container statuses recorded)
  May 18 08:53:36.379: INFO: 	Container webhook ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/18/23 08:53:36.379
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/18/23 08:53:40.414
  STEP: Trying to apply a random label on the found node. @ 05/18/23 08:53:40.457
  STEP: verifying the node has the label kubernetes.io/e2e-f9a5007c-92c5-4d0a-984d-c9f5c9fef5f0 95 @ 05/18/23 08:53:40.515
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/18/23 08:53:40.521
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.190.143 on the node which pod4 resides and expect not scheduled @ 05/18/23 08:53:42.546
  STEP: removing the label kubernetes.io/e2e-f9a5007c-92c5-4d0a-984d-c9f5c9fef5f0 off the node ck-test-kube-1-27-worker @ 05/18/23 08:58:42.564
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-f9a5007c-92c5-4d0a-984d-c9f5c9fef5f0 @ 05/18/23 08:58:42.59
  May 18 08:58:42.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6896" for this suite. @ 05/18/23 08:58:42.606
• [306.349 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/18/23 08:58:42.623
  May 18 08:58:42.623: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename cronjob @ 05/18/23 08:58:42.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 08:58:42.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 08:58:42.65
  STEP: Creating a cronjob @ 05/18/23 08:58:42.655
  STEP: Ensuring more than one job is running at a time @ 05/18/23 08:58:42.663
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/18/23 09:00:00.668
  STEP: Removing cronjob @ 05/18/23 09:00:00.673
  May 18 09:00:00.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7592" for this suite. @ 05/18/23 09:00:00.686
• [78.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/18/23 09:00:00.697
  May 18 09:00:00.697: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename server-version @ 05/18/23 09:00:00.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:00.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:00.802
  STEP: Request ServerVersion @ 05/18/23 09:00:00.825
  STEP: Confirm major version @ 05/18/23 09:00:00.828
  May 18 09:00:00.828: INFO: Major version: 1
  STEP: Confirm minor version @ 05/18/23 09:00:00.828
  May 18 09:00:00.828: INFO: cleanMinorVersion: 27
  May 18 09:00:00.829: INFO: Minor version: 27
  May 18 09:00:00.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-2720" for this suite. @ 05/18/23 09:00:00.835
• [0.144 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/18/23 09:00:00.846
  May 18 09:00:00.846: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:00:00.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:00.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:00.874
  STEP: creating a collection of services @ 05/18/23 09:00:00.879
  May 18 09:00:00.879: INFO: Creating e2e-svc-a-25lxn
  May 18 09:00:00.897: INFO: Creating e2e-svc-b-hm7tw
  May 18 09:00:00.912: INFO: Creating e2e-svc-c-l62p7
  STEP: deleting service collection @ 05/18/23 09:00:00.93
  May 18 09:00:00.981: INFO: Collection of services has been deleted
  May 18 09:00:00.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1549" for this suite. @ 05/18/23 09:00:00.999
• [0.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/18/23 09:00:01.027
  May 18 09:00:01.028: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:00:01.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:01.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:01.058
  STEP: Starting the proxy @ 05/18/23 09:00:01.062
  May 18 09:00:01.063: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-7966 proxy --unix-socket=/tmp/kubectl-proxy-unix3357320649/test'
  STEP: retrieving proxy /api/ output @ 05/18/23 09:00:01.198
  May 18 09:00:01.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7966" for this suite. @ 05/18/23 09:00:01.211
• [0.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/18/23 09:00:01.221
  May 18 09:00:01.221: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename endpointslice @ 05/18/23 09:00:01.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:01.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:01.243
  May 18 09:00:03.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1644" for this suite. @ 05/18/23 09:00:03.318
• [2.103 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/18/23 09:00:03.326
  May 18 09:00:03.326: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename endpointslice @ 05/18/23 09:00:03.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:03.345
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:03.348
  STEP: getting /apis @ 05/18/23 09:00:03.353
  STEP: getting /apis/discovery.k8s.io @ 05/18/23 09:00:03.362
  STEP: getting /apis/discovery.k8s.iov1 @ 05/18/23 09:00:03.368
  STEP: creating @ 05/18/23 09:00:03.37
  STEP: getting @ 05/18/23 09:00:03.387
  STEP: listing @ 05/18/23 09:00:03.389
  STEP: watching @ 05/18/23 09:00:03.392
  May 18 09:00:03.392: INFO: starting watch
  STEP: cluster-wide listing @ 05/18/23 09:00:03.393
  STEP: cluster-wide watching @ 05/18/23 09:00:03.397
  May 18 09:00:03.397: INFO: starting watch
  STEP: patching @ 05/18/23 09:00:03.398
  STEP: updating @ 05/18/23 09:00:03.403
  May 18 09:00:03.409: INFO: waiting for watch events with expected annotations
  May 18 09:00:03.410: INFO: saw patched and updated annotations
  STEP: deleting @ 05/18/23 09:00:03.41
  STEP: deleting a collection @ 05/18/23 09:00:03.42
  May 18 09:00:03.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1772" for this suite. @ 05/18/23 09:00:03.435
• [0.115 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/18/23 09:00:03.443
  May 18 09:00:03.443: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename field-validation @ 05/18/23 09:00:03.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:03.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:03.461
  May 18 09:00:03.465: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:00:06.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8098" for this suite. @ 05/18/23 09:00:06.153
• [2.717 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/18/23 09:00:06.164
  May 18 09:00:06.164: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:00:06.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:06.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:06.199
  STEP: Creating a ResourceQuota with best effort scope @ 05/18/23 09:00:06.202
  STEP: Ensuring ResourceQuota status is calculated @ 05/18/23 09:00:06.211
  STEP: Creating a ResourceQuota with not best effort scope @ 05/18/23 09:00:08.217
  STEP: Ensuring ResourceQuota status is calculated @ 05/18/23 09:00:08.221
  STEP: Creating a best-effort pod @ 05/18/23 09:00:10.227
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/18/23 09:00:10.242
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/18/23 09:00:12.248
  STEP: Deleting the pod @ 05/18/23 09:00:14.254
  STEP: Ensuring resource quota status released the pod usage @ 05/18/23 09:00:14.272
  STEP: Creating a not best-effort pod @ 05/18/23 09:00:16.28
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/18/23 09:00:16.297
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/18/23 09:00:18.3
  STEP: Deleting the pod @ 05/18/23 09:00:20.307
  STEP: Ensuring resource quota status released the pod usage @ 05/18/23 09:00:20.328
  May 18 09:00:22.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-556" for this suite. @ 05/18/23 09:00:22.337
• [16.178 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/18/23 09:00:22.346
  May 18 09:00:22.346: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename job @ 05/18/23 09:00:22.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:22.362
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:22.367
  STEP: Creating a suspended job @ 05/18/23 09:00:22.373
  STEP: Patching the Job @ 05/18/23 09:00:22.381
  STEP: Watching for Job to be patched @ 05/18/23 09:00:22.399
  May 18 09:00:22.403: INFO: Event ADDED observed for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 18 09:00:22.403: INFO: Event MODIFIED observed for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 18 09:00:22.403: INFO: Event MODIFIED found for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/18/23 09:00:22.404
  STEP: Watching for Job to be updated @ 05/18/23 09:00:22.414
  May 18 09:00:22.417: INFO: Event MODIFIED found for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 18 09:00:22.417: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/18/23 09:00:22.417
  May 18 09:00:22.422: INFO: Job: e2e-mkfwv as labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched]
  STEP: Waiting for job to complete @ 05/18/23 09:00:22.422
  STEP: Delete a job collection with a labelselector @ 05/18/23 09:00:30.433
  STEP: Watching for Job to be deleted @ 05/18/23 09:00:30.444
  May 18 09:00:30.450: INFO: Event MODIFIED observed for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 18 09:00:30.450: INFO: Event MODIFIED observed for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 18 09:00:30.450: INFO: Event MODIFIED observed for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 18 09:00:30.451: INFO: Event MODIFIED observed for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 18 09:00:30.451: INFO: Event MODIFIED observed for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 18 09:00:30.451: INFO: Event DELETED found for Job e2e-mkfwv in namespace job-8892 with labels: map[e2e-job-label:e2e-mkfwv e2e-mkfwv:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/18/23 09:00:30.451
  May 18 09:00:30.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8892" for this suite. @ 05/18/23 09:00:30.482
• [8.152 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/18/23 09:00:30.5
  May 18 09:00:30.500: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:00:30.505
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:30.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:30.528
  STEP: Setting up server cert @ 05/18/23 09:00:30.552
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:00:30.818
  STEP: Deploying the webhook pod @ 05/18/23 09:00:30.827
  STEP: Wait for the deployment to be ready @ 05/18/23 09:00:30.839
  May 18 09:00:30.851: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/18/23 09:00:32.868
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:00:32.895
  May 18 09:00:33.896: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/18/23 09:00:33.975
  STEP: Creating a configMap that should be mutated @ 05/18/23 09:00:34.001
  STEP: Deleting the collection of validation webhooks @ 05/18/23 09:00:34.051
  STEP: Creating a configMap that should not be mutated @ 05/18/23 09:00:34.091
  May 18 09:00:34.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1402" for this suite. @ 05/18/23 09:00:34.146
  STEP: Destroying namespace "webhook-markers-7450" for this suite. @ 05/18/23 09:00:34.152
• [3.659 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/18/23 09:00:34.166
  May 18 09:00:34.166: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-webhook @ 05/18/23 09:00:34.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:34.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:34.202
  STEP: Setting up server cert @ 05/18/23 09:00:34.205
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/18/23 09:00:34.678
  STEP: Deploying the custom resource conversion webhook pod @ 05/18/23 09:00:34.684
  STEP: Wait for the deployment to be ready @ 05/18/23 09:00:34.698
  May 18 09:00:34.705: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/18/23 09:00:36.72
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:00:36.762
  May 18 09:00:37.763: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 18 09:00:37.768: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Creating a v1 custom resource @ 05/18/23 09:00:40.364
  STEP: Create a v2 custom resource @ 05/18/23 09:00:40.384
  STEP: List CRs in v1 @ 05/18/23 09:00:40.435
  STEP: List CRs in v2 @ 05/18/23 09:00:40.444
  May 18 09:00:40.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-9076" for this suite. @ 05/18/23 09:00:41.02
• [6.869 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/18/23 09:00:41.042
  May 18 09:00:41.042: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sysctl @ 05/18/23 09:00:41.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:41.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:41.076
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/18/23 09:00:41.084
  STEP: Watching for error events or started pod @ 05/18/23 09:00:41.093
  STEP: Waiting for pod completion @ 05/18/23 09:00:43.103
  STEP: Checking that the pod succeeded @ 05/18/23 09:00:45.124
  STEP: Getting logs from the pod @ 05/18/23 09:00:45.124
  STEP: Checking that the sysctl is actually updated @ 05/18/23 09:00:45.16
  May 18 09:00:45.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6388" for this suite. @ 05/18/23 09:00:45.167
• [4.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/18/23 09:00:45.18
  May 18 09:00:45.181: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:00:45.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:45.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:45.217
  STEP: Creating a pod to test downward api env vars @ 05/18/23 09:00:45.224
  STEP: Saw pod success @ 05/18/23 09:00:49.264
  May 18 09:00:49.269: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downward-api-dd929a38-8921-4a1d-b863-7ef80f8ec643 container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 09:00:49.279
  May 18 09:00:49.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4438" for this suite. @ 05/18/23 09:00:49.307
• [4.134 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/18/23 09:00:49.317
  May 18 09:00:49.317: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:00:49.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:49.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:49.344
  STEP: validating api versions @ 05/18/23 09:00:49.349
  May 18 09:00:49.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8259 api-versions'
  May 18 09:00:49.497: INFO: stderr: ""
  May 18 09:00:49.497: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nargoproj.io/v1alpha1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nextensions.istio.io/v1alpha1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nimageregistry.secloudit.io/v1\ninstall.istio.io/v1alpha1\nmetrics.k8s.io/v1beta1\nmonitoring.coreos.com/v1\nmonitoring.coreos.com/v1alpha1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.istio.io/v1alpha3\nnetworking.istio.io/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nresolution.tekton.dev/v1alpha1\nresolution.tekton.dev/v1beta1\nscheduling.k8s.io/v1\nsecurity.istio.io/v1\nsecurity.istio.io/v1beta1\nstorage.k8s.io/v1\ntekton.dev/v1\ntekton.dev/v1alpha1\ntekton.dev/v1beta1\ntelemetry.istio.io/v1alpha1\ntriggers.tekton.dev/v1alpha1\ntriggers.tekton.dev/v1beta1\nv1\nworkload.secloudit.io/v1\n"
  May 18 09:00:49.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8259" for this suite. @ 05/18/23 09:00:49.502
• [0.194 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/18/23 09:00:49.511
  May 18 09:00:49.511: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 09:00:49.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:00:49.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:00:49.533
  STEP: Creating pod busybox-b6f42419-0280-4bdb-9ddc-37736b86cdf6 in namespace container-probe-4418 @ 05/18/23 09:00:49.537
  May 18 09:00:51.559: INFO: Started pod busybox-b6f42419-0280-4bdb-9ddc-37736b86cdf6 in namespace container-probe-4418
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/18/23 09:00:51.56
  May 18 09:00:51.566: INFO: Initial restart count of pod busybox-b6f42419-0280-4bdb-9ddc-37736b86cdf6 is 0
  May 18 09:04:52.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:04:52.279
  STEP: Destroying namespace "container-probe-4418" for this suite. @ 05/18/23 09:04:52.297
• [242.821 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/18/23 09:04:52.343
  May 18 09:04:52.344: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:04:52.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:04:52.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:04:52.367
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/18/23 09:04:52.371
  STEP: Saw pod success @ 05/18/23 09:04:56.392
  May 18 09:04:56.395: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-a18ba1d1-f86e-4b7d-ae17-f0818ec42e2a container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:04:56.417
  May 18 09:04:56.430: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1934" for this suite. @ 05/18/23 09:04:56.437
• [4.102 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/18/23 09:04:56.446
  May 18 09:04:56.446: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 09:04:56.449
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:04:56.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:04:56.465
  STEP: Creating pod liveness-113e3d6c-d78d-4b2f-9d7c-e7388b546e22 in namespace container-probe-8603 @ 05/18/23 09:04:56.469
  May 18 09:04:58.488: INFO: Started pod liveness-113e3d6c-d78d-4b2f-9d7c-e7388b546e22 in namespace container-probe-8603
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/18/23 09:04:58.488
  May 18 09:04:58.492: INFO: Initial restart count of pod liveness-113e3d6c-d78d-4b2f-9d7c-e7388b546e22 is 0
  May 18 09:08:59.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:08:59.219
  STEP: Destroying namespace "container-probe-8603" for this suite. @ 05/18/23 09:08:59.244
• [242.825 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/18/23 09:08:59.32
  May 18 09:08:59.320: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:08:59.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:08:59.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:08:59.358
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:08:59.362
  STEP: Saw pod success @ 05/18/23 09:09:03.39
  May 18 09:09:03.397: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-796e62f6-1712-4d10-afdd-b3a8c1e7651e container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:09:03.43
  May 18 09:09:03.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6557" for this suite. @ 05/18/23 09:09:03.457
• [4.146 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/18/23 09:09:03.476
  May 18 09:09:03.476: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename daemonsets @ 05/18/23 09:09:03.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:03.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:03.507
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/18/23 09:09:03.533
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/18/23 09:09:03.541
  May 18 09:09:03.548: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:09:03.552: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:09:03.552: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  May 18 09:09:04.559: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:09:04.563: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:09:04.563: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  May 18 09:09:05.560: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:09:05.567: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 09:09:05.569: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/18/23 09:09:05.576
  May 18 09:09:05.599: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:09:05.608: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 09:09:05.612: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/18/23 09:09:05.613
  STEP: Deleting DaemonSet "daemon-set" @ 05/18/23 09:09:06.638
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4487, will wait for the garbage collector to delete the pods @ 05/18/23 09:09:06.639
  May 18 09:09:06.705: INFO: Deleting DaemonSet.extensions daemon-set took: 10.440325ms
  May 18 09:09:06.806: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.390665ms
  May 18 09:09:08.714: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:09:08.714: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 18 09:09:08.717: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1928298"},"items":null}

  May 18 09:09:08.724: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1928298"},"items":null}

  May 18 09:09:08.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4487" for this suite. @ 05/18/23 09:09:08.748
• [5.281 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/18/23 09:09:08.759
  May 18 09:09:08.759: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename svcaccounts @ 05/18/23 09:09:08.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:08.784
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:08.788
  May 18 09:09:08.808: INFO: created pod pod-service-account-defaultsa
  May 18 09:09:08.808: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May 18 09:09:08.816: INFO: created pod pod-service-account-mountsa
  May 18 09:09:08.816: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May 18 09:09:08.824: INFO: created pod pod-service-account-nomountsa
  May 18 09:09:08.825: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May 18 09:09:08.834: INFO: created pod pod-service-account-defaultsa-mountspec
  May 18 09:09:08.834: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May 18 09:09:08.841: INFO: created pod pod-service-account-mountsa-mountspec
  May 18 09:09:08.841: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May 18 09:09:08.846: INFO: created pod pod-service-account-nomountsa-mountspec
  May 18 09:09:08.846: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May 18 09:09:08.853: INFO: created pod pod-service-account-defaultsa-nomountspec
  May 18 09:09:08.854: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May 18 09:09:08.861: INFO: created pod pod-service-account-mountsa-nomountspec
  May 18 09:09:08.861: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May 18 09:09:08.867: INFO: created pod pod-service-account-nomountsa-nomountspec
  May 18 09:09:08.867: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May 18 09:09:08.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8017" for this suite. @ 05/18/23 09:09:08.871
• [0.123 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/18/23 09:09:08.916
  May 18 09:09:08.917: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:09:08.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:08.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:08.936
  STEP: Discovering how many secrets are in namespace by default @ 05/18/23 09:09:08.941
  STEP: Counting existing ResourceQuota @ 05/18/23 09:09:13.947
  STEP: Creating a ResourceQuota @ 05/18/23 09:09:18.953
  STEP: Ensuring resource quota status is calculated @ 05/18/23 09:09:18.96
  STEP: Creating a Secret @ 05/18/23 09:09:20.966
  STEP: Ensuring resource quota status captures secret creation @ 05/18/23 09:09:20.978
  STEP: Deleting a secret @ 05/18/23 09:09:22.982
  STEP: Ensuring resource quota status released usage @ 05/18/23 09:09:22.988
  May 18 09:09:24.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4319" for this suite. @ 05/18/23 09:09:24.998
• [16.092 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/18/23 09:09:25.009
  May 18 09:09:25.010: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename var-expansion @ 05/18/23 09:09:25.013
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:25.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:25.034
  STEP: Creating a pod to test substitution in volume subpath @ 05/18/23 09:09:25.037
  STEP: Saw pod success @ 05/18/23 09:09:29.059
  May 18 09:09:29.063: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod var-expansion-0af13ebd-0f85-4732-8a7a-0edecca9a1e7 container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 09:09:29.07
  May 18 09:09:29.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1482" for this suite. @ 05/18/23 09:09:29.087
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/18/23 09:09:29.103
  May 18 09:09:29.103: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:09:29.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:29.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:29.122
  STEP: creating Agnhost RC @ 05/18/23 09:09:29.125
  May 18 09:09:29.125: INFO: namespace kubectl-2008
  May 18 09:09:29.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2008 create -f -'
  May 18 09:09:31.193: INFO: stderr: ""
  May 18 09:09:31.193: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/18/23 09:09:31.193
  May 18 09:09:32.205: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 09:09:32.206: INFO: Found 0 / 1
  May 18 09:09:33.200: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 09:09:33.200: INFO: Found 1 / 1
  May 18 09:09:33.200: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 18 09:09:33.204: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 09:09:33.205: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 18 09:09:33.205: INFO: wait on agnhost-primary startup in kubectl-2008 
  May 18 09:09:33.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2008 logs agnhost-primary-6vzj5 agnhost-primary'
  May 18 09:09:33.344: INFO: stderr: ""
  May 18 09:09:33.344: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/18/23 09:09:33.344
  May 18 09:09:33.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2008 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May 18 09:09:33.477: INFO: stderr: ""
  May 18 09:09:33.477: INFO: stdout: "service/rm2 exposed\n"
  May 18 09:09:33.484: INFO: Service rm2 in namespace kubectl-2008 found.
  STEP: exposing service @ 05/18/23 09:09:35.494
  May 18 09:09:35.494: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2008 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May 18 09:09:35.688: INFO: stderr: ""
  May 18 09:09:35.688: INFO: stdout: "service/rm3 exposed\n"
  May 18 09:09:35.695: INFO: Service rm3 in namespace kubectl-2008 found.
  May 18 09:09:37.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2008" for this suite. @ 05/18/23 09:09:37.713
• [8.618 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/18/23 09:09:37.745
  May 18 09:09:37.746: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:09:37.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:37.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:37.779
  STEP: Creating projection with secret that has name projected-secret-test-11a3d140-1aad-491a-95f0-7321ce028e5b @ 05/18/23 09:09:37.783
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:09:37.79
  STEP: Saw pod success @ 05/18/23 09:09:41.815
  May 18 09:09:41.818: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-secrets-31615138-4e3c-4af9-a343-9ef3ddb12b11 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:09:41.827
  May 18 09:09:41.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4188" for this suite. @ 05/18/23 09:09:41.85
• [4.112 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/18/23 09:09:41.863
  May 18 09:09:41.863: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename namespaces @ 05/18/23 09:09:41.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:41.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:41.885
  STEP: Creating a test namespace @ 05/18/23 09:09:41.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:41.901
  STEP: Creating a service in the namespace @ 05/18/23 09:09:41.905
  STEP: Deleting the namespace @ 05/18/23 09:09:41.928
  STEP: Waiting for the namespace to be removed. @ 05/18/23 09:09:41.938
  STEP: Recreating the namespace @ 05/18/23 09:09:47.961
  STEP: Verifying there is no service in the namespace @ 05/18/23 09:09:47.981
  May 18 09:09:47.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2353" for this suite. @ 05/18/23 09:09:47.991
  STEP: Destroying namespace "nsdeletetest-5287" for this suite. @ 05/18/23 09:09:48.003
  May 18 09:09:48.032: INFO: Namespace nsdeletetest-5287 was already deleted
  STEP: Destroying namespace "nsdeletetest-7961" for this suite. @ 05/18/23 09:09:48.032
• [6.182 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/18/23 09:09:48.045
  May 18 09:09:48.045: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:09:48.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:48.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:48.07
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/18/23 09:09:48.077
  STEP: Saw pod success @ 05/18/23 09:09:52.101
  May 18 09:09:52.104: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-4744365b-9b39-4c47-99c6-454630322b9d container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:09:52.113
  May 18 09:09:52.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4387" for this suite. @ 05/18/23 09:09:52.137
• [4.100 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/18/23 09:09:52.15
  May 18 09:09:52.150: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename svcaccounts @ 05/18/23 09:09:52.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:52.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:52.17
  STEP: Creating ServiceAccount "e2e-sa-rl67k"  @ 05/18/23 09:09:52.175
  May 18 09:09:52.181: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-rl67k"  @ 05/18/23 09:09:52.181
  May 18 09:09:52.191: INFO: AutomountServiceAccountToken: true
  May 18 09:09:52.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7088" for this suite. @ 05/18/23 09:09:52.198
• [0.053 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/18/23 09:09:52.206
  May 18 09:09:52.206: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:09:52.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:52.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:52.224
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:09:52.227
  STEP: Saw pod success @ 05/18/23 09:09:56.247
  May 18 09:09:56.250: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-1fdcd1e9-bd30-45c5-b099-2d506c93225b container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:09:56.257
  May 18 09:09:56.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5171" for this suite. @ 05/18/23 09:09:56.277
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/18/23 09:09:56.291
  May 18 09:09:56.291: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replication-controller @ 05/18/23 09:09:56.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:56.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:56.309
  STEP: creating a ReplicationController @ 05/18/23 09:09:56.314
  STEP: waiting for RC to be added @ 05/18/23 09:09:56.318
  STEP: waiting for available Replicas @ 05/18/23 09:09:56.319
  STEP: patching ReplicationController @ 05/18/23 09:09:58.315
  STEP: waiting for RC to be modified @ 05/18/23 09:09:58.327
  STEP: patching ReplicationController status @ 05/18/23 09:09:58.327
  STEP: waiting for RC to be modified @ 05/18/23 09:09:58.333
  STEP: waiting for available Replicas @ 05/18/23 09:09:58.333
  STEP: fetching ReplicationController status @ 05/18/23 09:09:58.339
  STEP: patching ReplicationController scale @ 05/18/23 09:09:58.342
  STEP: waiting for RC to be modified @ 05/18/23 09:09:58.348
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/18/23 09:09:58.349
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/18/23 09:09:59.815
  STEP: updating ReplicationController status @ 05/18/23 09:09:59.819
  STEP: waiting for RC to be modified @ 05/18/23 09:09:59.828
  STEP: listing all ReplicationControllers @ 05/18/23 09:09:59.829
  STEP: checking that ReplicationController has expected values @ 05/18/23 09:09:59.833
  STEP: deleting ReplicationControllers by collection @ 05/18/23 09:09:59.833
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/18/23 09:09:59.855
  May 18 09:09:59.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0518 09:09:59.903138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-8950" for this suite. @ 05/18/23 09:09:59.905
• [3.618 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/18/23 09:09:59.914
  May 18 09:09:59.914: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:09:59.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:09:59.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:09:59.935
  STEP: Creating configMap with name configmap-test-upd-8e221283-e226-49b9-8dfd-e68d108224e5 @ 05/18/23 09:09:59.943
  STEP: Creating the pod @ 05/18/23 09:09:59.946
  E0518 09:10:00.902566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:01.903723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-8e221283-e226-49b9-8dfd-e68d108224e5 @ 05/18/23 09:10:01.983
  STEP: waiting to observe update in volume @ 05/18/23 09:10:01.99
  E0518 09:10:02.903547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:03.904138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:10:04.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6835" for this suite. @ 05/18/23 09:10:04.024
• [4.119 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/18/23 09:10:04.036
  May 18 09:10:04.036: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename dns @ 05/18/23 09:10:04.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:10:04.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:10:04.062
  STEP: Creating a test headless service @ 05/18/23 09:10:04.068
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6993.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6993.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 164.105.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.105.164_udp@PTR;check="$$(dig +tcp +noall +answer +search 164.105.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.105.164_tcp@PTR;sleep 1; done
   @ 05/18/23 09:10:04.096
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6993.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6993.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6993.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6993.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6993.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 164.105.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.105.164_udp@PTR;check="$$(dig +tcp +noall +answer +search 164.105.109.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.109.105.164_tcp@PTR;sleep 1; done
   @ 05/18/23 09:10:04.096
  STEP: creating a pod to probe DNS @ 05/18/23 09:10:04.096
  STEP: submitting the pod to kubernetes @ 05/18/23 09:10:04.096
  E0518 09:10:04.904274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:05.904697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:06.904965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:07.905190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 09:10:08.13
  STEP: looking for the results for each expected name from probers @ 05/18/23 09:10:08.134
  May 18 09:10:08.142: INFO: Unable to read wheezy_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:08.146: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:08.149: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:08.152: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:08.171: INFO: Unable to read jessie_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:08.174: INFO: Unable to read jessie_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:08.178: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:08.182: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:08.195: INFO: Lookups using dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d failed for: [wheezy_udp@dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_udp@dns-test-service.dns-6993.svc.cluster.local jessie_tcp@dns-test-service.dns-6993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local]

  E0518 09:10:08.906286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:09.906666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:10.907472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:11.907923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:12.908079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:10:13.204: INFO: Unable to read wheezy_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:13.210: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:13.217: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:13.222: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:13.250: INFO: Unable to read jessie_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:13.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:13.261: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:13.266: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:13.286: INFO: Lookups using dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d failed for: [wheezy_udp@dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_udp@dns-test-service.dns-6993.svc.cluster.local jessie_tcp@dns-test-service.dns-6993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local]

  E0518 09:10:13.908594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:14.909199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:15.909470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:16.909929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:17.910161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:10:18.207: INFO: Unable to read wheezy_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:18.213: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:18.218: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:18.225: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:18.251: INFO: Unable to read jessie_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:18.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:18.261: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:18.267: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:18.281: INFO: Lookups using dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d failed for: [wheezy_udp@dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_udp@dns-test-service.dns-6993.svc.cluster.local jessie_tcp@dns-test-service.dns-6993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local]

  E0518 09:10:18.910310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:19.910540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:20.910508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:21.911301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:22.911779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:10:23.201: INFO: Unable to read wheezy_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:23.205: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:23.210: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:23.215: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:23.240: INFO: Unable to read jessie_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:23.246: INFO: Unable to read jessie_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:23.250: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:23.255: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:23.272: INFO: Lookups using dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d failed for: [wheezy_udp@dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_udp@dns-test-service.dns-6993.svc.cluster.local jessie_tcp@dns-test-service.dns-6993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local]

  E0518 09:10:23.912307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:24.912314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:25.912389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:26.912999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:27.913296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:10:28.201: INFO: Unable to read wheezy_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:28.206: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:28.210: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:28.214: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:28.229: INFO: Unable to read jessie_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:28.232: INFO: Unable to read jessie_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:28.235: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:28.239: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:28.253: INFO: Lookups using dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d failed for: [wheezy_udp@dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_udp@dns-test-service.dns-6993.svc.cluster.local jessie_tcp@dns-test-service.dns-6993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local]

  E0518 09:10:28.913516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:29.913760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:30.914043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:31.914270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:32.914542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:10:33.202: INFO: Unable to read wheezy_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:33.209: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:33.219: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:33.225: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:33.251: INFO: Unable to read jessie_udp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:33.256: INFO: Unable to read jessie_tcp@dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:33.260: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:33.264: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local from pod dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d: the server could not find the requested resource (get pods dns-test-57d72115-b9d6-4f44-897b-9840bb57982d)
  May 18 09:10:33.280: INFO: Lookups using dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d failed for: [wheezy_udp@dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@dns-test-service.dns-6993.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_udp@dns-test-service.dns-6993.svc.cluster.local jessie_tcp@dns-test-service.dns-6993.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6993.svc.cluster.local]

  E0518 09:10:33.915651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:34.916643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:35.917312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:36.918094      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:37.925231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:10:38.274: INFO: DNS probes using dns-6993/dns-test-57d72115-b9d6-4f44-897b-9840bb57982d succeeded

  May 18 09:10:38.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:10:38.282
  STEP: deleting the test service @ 05/18/23 09:10:38.329
  STEP: deleting the test headless service @ 05/18/23 09:10:38.356
  STEP: Destroying namespace "dns-6993" for this suite. @ 05/18/23 09:10:38.385
• [34.360 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/18/23 09:10:38.402
  May 18 09:10:38.402: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:10:38.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:10:38.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:10:38.432
  STEP: Setting up server cert @ 05/18/23 09:10:38.465
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:10:38.921
  E0518 09:10:38.925678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook pod @ 05/18/23 09:10:38.928
  STEP: Wait for the deployment to be ready @ 05/18/23 09:10:38.941
  May 18 09:10:38.961: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:10:39.926256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:40.927674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:10:40.983
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:10:41.009
  E0518 09:10:41.928420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:10:42.010: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/18/23 09:10:42.016
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/18/23 09:10:42.02
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/18/23 09:10:42.021
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/18/23 09:10:42.021
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/18/23 09:10:42.024
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/18/23 09:10:42.024
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/18/23 09:10:42.027
  May 18 09:10:42.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-860" for this suite. @ 05/18/23 09:10:42.097
  STEP: Destroying namespace "webhook-markers-9816" for this suite. @ 05/18/23 09:10:42.112
• [3.718 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/18/23 09:10:42.125
  May 18 09:10:42.126: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/18/23 09:10:42.13
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:10:42.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:10:42.161
  May 18 09:10:42.166: INFO: Waiting up to 1m0s for all nodes to be ready
  E0518 09:10:42.928619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:43.929088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:44.929203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:45.929764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:46.930229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:47.933280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:48.934411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:49.934615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:50.934926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:51.936008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:52.936218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:53.936492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:54.936547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:55.936785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:56.937075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:57.941739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:58.942475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:10:59.942852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:00.943585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:01.944603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:02.945355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:03.945514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:04.945578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:05.945743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:06.947282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:07.947361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:08.947685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:09.947884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:10.949035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:11.949097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:12.949353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:13.949757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:14.950332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:15.950123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:16.951026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:17.951693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:18.952159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:19.952375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:20.953139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:21.953322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:22.954162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:23.954689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:24.955860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:25.956329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:26.957398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:27.958619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:28.959727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:29.959856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:30.960921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:31.962124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:32.962691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:33.963583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:34.963920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:35.964606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:36.964742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:37.966951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:38.967146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:39.967573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:40.968174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:41.969481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:11:42.227: INFO: Waiting for terminating namespaces to be deleted...
  May 18 09:11:42.235: INFO: Starting informer...
  STEP: Starting pods... @ 05/18/23 09:11:42.235
  May 18 09:11:42.456: INFO: Pod1 is running on ck-test-kube-1-27-worker. Tainting Node
  E0518 09:11:42.970211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:43.970821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:11:44.680: INFO: Pod2 is running on ck-test-kube-1-27-worker. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/18/23 09:11:44.681
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/18/23 09:11:44.697
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/18/23 09:11:44.706
  E0518 09:11:44.971007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:45.971478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:46.971644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:47.977906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:48.972281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:49.973103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:11:50.867: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0518 09:11:50.974120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:51.975815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:52.975782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:53.976185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:54.976439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:55.976803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:56.981503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:57.982193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:58.982561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:11:59.982904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:00.983625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:01.984613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:02.985540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:03.985356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:04.985954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:05.986189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:06.986271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:07.987180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:08.987185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:09.987879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:10.983: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May 18 09:12:10.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0518 09:12:10.988539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/18/23 09:12:11.035
  STEP: Destroying namespace "taint-multiple-pods-5893" for this suite. @ 05/18/23 09:12:11.044
• [88.929 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/18/23 09:12:11.057
  May 18 09:12:11.057: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename field-validation @ 05/18/23 09:12:11.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:11.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:11.093
  May 18 09:12:11.098: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:12:11.990010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:12.989839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0518 09:12:13.782894      19 warnings.go:70] unknown field "alpha"
  W0518 09:12:13.782947      19 warnings.go:70] unknown field "beta"
  W0518 09:12:13.782959      19 warnings.go:70] unknown field "delta"
  W0518 09:12:13.782970      19 warnings.go:70] unknown field "epsilon"
  W0518 09:12:13.782981      19 warnings.go:70] unknown field "gamma"
  May 18 09:12:13.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5114" for this suite. @ 05/18/23 09:12:13.822
• [2.777 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/18/23 09:12:13.843
  May 18 09:12:13.844: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-pred @ 05/18/23 09:12:13.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:13.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:13.887
  May 18 09:12:13.894: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 18 09:12:13.908: INFO: Waiting for terminating namespaces to be deleted...
  May 18 09:12:13.914: INFO: 
  Logging pods the apiserver thinks is on node ck-test-kube-1-27-worker before test
  May 18 09:12:13.932: INFO: calico-node-z6wl2 from kube-system started at 2023-05-12 06:51:27 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.932: INFO: 	Container calico-node ready: true, restart count 0
  May 18 09:12:13.933: INFO: kube-proxy-269wt from kube-system started at 2023-05-12 06:50:32 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.934: INFO: 	Container kube-proxy ready: true, restart count 0
  May 18 09:12:13.935: INFO: node-exporter-7q9w4 from secloudit-monitoring started at 2023-05-12 07:19:01 +0000 UTC (2 container statuses recorded)
  May 18 09:12:13.935: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:12:13.936: INFO: 	Container node-exporter ready: true, restart count 0
  May 18 09:12:13.937: INFO: sonobuoy from sonobuoy started at 2023-05-18 08:52:38 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.938: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 18 09:12:13.939: INFO: sonobuoy-e2e-job-cccd1fa16dd54418 from sonobuoy started at 2023-05-18 08:52:39 +0000 UTC (2 container statuses recorded)
  May 18 09:12:13.939: INFO: 	Container e2e ready: true, restart count 0
  May 18 09:12:13.942: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:12:13.942: INFO: sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-8p6sc from sonobuoy started at 2023-05-18 08:52:40 +0000 UTC (2 container statuses recorded)
  May 18 09:12:13.943: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:12:13.944: INFO: 	Container systemd-logs ready: true, restart count 0
  May 18 09:12:13.944: INFO: 
  Logging pods the apiserver thinks is on node ck-test-kube-1-27-worker-1 before test
  May 18 09:12:13.983: INFO: argocd-application-controller-0 from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.983: INFO: 	Container argocd-application-controller ready: true, restart count 0
  May 18 09:12:13.983: INFO: argocd-applicationset-controller-57fd46cc99-vjt7t from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.984: INFO: 	Container argocd-applicationset-controller ready: true, restart count 0
  May 18 09:12:13.984: INFO: argocd-dex-server-75d8c65447-wrjw6 from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.985: INFO: 	Container dex ready: true, restart count 0
  May 18 09:12:13.985: INFO: argocd-notifications-controller-6f5f6d97cb-sdzxc from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.985: INFO: 	Container argocd-notifications-controller ready: true, restart count 0
  May 18 09:12:13.986: INFO: argocd-redis-77bf5b886-dn8z5 from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.986: INFO: 	Container redis ready: true, restart count 0
  May 18 09:12:13.986: INFO: argocd-repo-server-59684bc98c-dk4hd from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.987: INFO: 	Container argocd-repo-server ready: true, restart count 0
  May 18 09:12:13.987: INFO: argocd-server-d5bd658c-fs9dz from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.987: INFO: 	Container argocd-server ready: true, restart count 0
  May 18 09:12:13.988: INFO: istio-egressgateway-575466f5bb-mqw8r from istio-system started at 2023-05-18 06:47:53 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.988: INFO: 	Container istio-proxy ready: true, restart count 0
  May 18 09:12:13.988: INFO: istio-ingressgateway-cb9c6b49d-htxtq from istio-system started at 2023-05-18 06:47:53 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.989: INFO: 	Container istio-proxy ready: true, restart count 0
  May 18 09:12:13.989: INFO: istiod-6fbbf67d58-zhh74 from istio-system started at 2023-05-18 06:47:39 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.989: INFO: 	Container discovery ready: true, restart count 0
  May 18 09:12:13.990: INFO: calico-kube-controllers-674fff74c8-lvpc2 from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.990: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 18 09:12:13.990: INFO: calico-node-946wb from kube-system started at 2023-05-12 06:51:27 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.990: INFO: 	Container calico-node ready: true, restart count 0
  May 18 09:12:13.991: INFO: coredns-5d78c9869d-fchlv from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  E0518 09:12:13.990317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:13.991: INFO: 	Container coredns ready: true, restart count 0
  May 18 09:12:13.992: INFO: coredns-5d78c9869d-vbhw2 from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.993: INFO: 	Container coredns ready: true, restart count 0
  May 18 09:12:13.993: INFO: kube-proxy-s5x6t from kube-system started at 2023-05-12 06:50:32 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.993: INFO: 	Container kube-proxy ready: true, restart count 0
  May 18 09:12:13.994: INFO: alertmanager-main-0 from secloudit-monitoring started at 2023-05-18 07:36:19 +0000 UTC (2 container statuses recorded)
  May 18 09:12:13.994: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:12:13.994: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:12:13.995: INFO: alertmanager-main-1 from secloudit-monitoring started at 2023-05-12 07:19:12 +0000 UTC (2 container statuses recorded)
  May 18 09:12:13.995: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:12:13.995: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:12:13.996: INFO: alertmanager-main-2 from secloudit-monitoring started at 2023-05-18 07:36:20 +0000 UTC (2 container statuses recorded)
  May 18 09:12:13.996: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:12:13.996: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:12:13.997: INFO: blackbox-exporter-659b6ff548-25j5z from secloudit-monitoring started at 2023-05-12 07:17:19 +0000 UTC (3 container statuses recorded)
  May 18 09:12:13.997: INFO: 	Container blackbox-exporter ready: true, restart count 0
  May 18 09:12:13.997: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:12:13.997: INFO: 	Container module-configmap-reloader ready: true, restart count 0
  May 18 09:12:13.998: INFO: grafana-6cb59f497c-ssmtq from secloudit-monitoring started at 2023-05-18 07:36:15 +0000 UTC (1 container statuses recorded)
  May 18 09:12:13.998: INFO: 	Container grafana ready: true, restart count 0
  May 18 09:12:13.998: INFO: kube-state-metrics-f59465db9-v6k4r from secloudit-monitoring started at 2023-05-18 07:36:14 +0000 UTC (3 container statuses recorded)
  May 18 09:12:14.000: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
  May 18 09:12:14.001: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
  May 18 09:12:14.001: INFO: 	Container kube-state-metrics ready: true, restart count 0
  May 18 09:12:14.002: INFO: node-exporter-4ttx4 from secloudit-monitoring started at 2023-05-12 07:18:53 +0000 UTC (2 container statuses recorded)
  May 18 09:12:14.002: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:12:14.003: INFO: 	Container node-exporter ready: true, restart count 0
  May 18 09:12:14.003: INFO: prometheus-adapter-74d4cb89b8-ll246 from secloudit-monitoring started at 2023-05-12 07:18:13 +0000 UTC (1 container statuses recorded)
  May 18 09:12:14.003: INFO: 	Container prometheus-adapter ready: true, restart count 0
  May 18 09:12:14.004: INFO: prometheus-adapter-74d4cb89b8-z4hvp from secloudit-monitoring started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:12:14.004: INFO: 	Container prometheus-adapter ready: true, restart count 0
  May 18 09:12:14.004: INFO: prometheus-k8s-0 from secloudit-monitoring started at 2023-05-12 07:19:13 +0000 UTC (2 container statuses recorded)
  May 18 09:12:14.004: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:12:14.004: INFO: 	Container prometheus ready: true, restart count 0
  May 18 09:12:14.004: INFO: prometheus-k8s-1 from secloudit-monitoring started at 2023-05-18 07:36:19 +0000 UTC (2 container statuses recorded)
  May 18 09:12:14.004: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:12:14.004: INFO: 	Container prometheus ready: true, restart count 0
  May 18 09:12:14.004: INFO: prometheus-operator-59f6b8bc59-wv7ll from secloudit-monitoring started at 2023-05-18 07:36:15 +0000 UTC (2 container statuses recorded)
  May 18 09:12:14.004: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:12:14.005: INFO: 	Container prometheus-operator ready: true, restart count 0
  May 18 09:12:14.005: INFO: sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-xppr7 from sonobuoy started at 2023-05-18 08:52:40 +0000 UTC (2 container statuses recorded)
  May 18 09:12:14.005: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:12:14.005: INFO: 	Container systemd-logs ready: true, restart count 0
  May 18 09:12:14.005: INFO: tekton-pipelines-remote-resolvers-76f588dbf5-z6v9v from tekton-pipelines-resolvers started at 2023-05-18 07:36:15 +0000 UTC (1 container statuses recorded)
  May 18 09:12:14.005: INFO: 	Container controller ready: true, restart count 0
  May 18 09:12:14.005: INFO: tekton-pipelines-controller-dd857fb44-mwdzd from tekton-pipelines started at 2023-05-12 07:32:59 +0000 UTC (1 container statuses recorded)
  May 18 09:12:14.005: INFO: 	Container tekton-pipelines-controller ready: true, restart count 0
  May 18 09:12:14.005: INFO: tekton-pipelines-webhook-79bbb84bb4-7q2lq from tekton-pipelines started at 2023-05-12 07:32:59 +0000 UTC (1 container statuses recorded)
  May 18 09:12:14.005: INFO: 	Container webhook ready: true, restart count 0
  May 18 09:12:14.005: INFO: tekton-triggers-controller-99d8dc8bb-g47nr from tekton-pipelines started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:12:14.006: INFO: 	Container tekton-triggers-controller ready: true, restart count 0
  May 18 09:12:14.006: INFO: tekton-triggers-core-interceptors-ddf77b777-psf7x from tekton-pipelines started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:12:14.006: INFO: 	Container tekton-triggers-core-interceptors ready: true, restart count 0
  May 18 09:12:14.006: INFO: tekton-triggers-webhook-69569f7f45-vv4c8 from tekton-pipelines started at 2023-05-12 07:33:13 +0000 UTC (1 container statuses recorded)
  May 18 09:12:14.006: INFO: 	Container webhook ready: true, restart count 0
  STEP: verifying the node has the label node ck-test-kube-1-27-worker @ 05/18/23 09:12:14.061
  STEP: verifying the node has the label node ck-test-kube-1-27-worker-1 @ 05/18/23 09:12:14.081
  May 18 09:12:14.113: INFO: Pod argocd-application-controller-0 requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod argocd-applicationset-controller-57fd46cc99-vjt7t requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod argocd-dex-server-75d8c65447-wrjw6 requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod argocd-notifications-controller-6f5f6d97cb-sdzxc requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod argocd-redis-77bf5b886-dn8z5 requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod argocd-repo-server-59684bc98c-dk4hd requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod argocd-server-d5bd658c-fs9dz requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod istio-egressgateway-575466f5bb-mqw8r requesting resource cpu=10m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod istio-ingressgateway-cb9c6b49d-htxtq requesting resource cpu=10m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod istiod-6fbbf67d58-zhh74 requesting resource cpu=10m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod calico-kube-controllers-674fff74c8-lvpc2 requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod calico-node-946wb requesting resource cpu=250m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.113: INFO: Pod calico-node-z6wl2 requesting resource cpu=250m on Node ck-test-kube-1-27-worker
  May 18 09:12:14.113: INFO: Pod coredns-5d78c9869d-fchlv requesting resource cpu=100m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod coredns-5d78c9869d-vbhw2 requesting resource cpu=100m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod kube-proxy-269wt requesting resource cpu=0m on Node ck-test-kube-1-27-worker
  May 18 09:12:14.114: INFO: Pod kube-proxy-s5x6t requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod alertmanager-main-0 requesting resource cpu=104m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod alertmanager-main-1 requesting resource cpu=104m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod alertmanager-main-2 requesting resource cpu=104m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod blackbox-exporter-659b6ff548-25j5z requesting resource cpu=30m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod grafana-6cb59f497c-ssmtq requesting resource cpu=100m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod kube-state-metrics-f59465db9-v6k4r requesting resource cpu=40m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod node-exporter-4ttx4 requesting resource cpu=112m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod node-exporter-7q9w4 requesting resource cpu=112m on Node ck-test-kube-1-27-worker
  May 18 09:12:14.114: INFO: Pod prometheus-adapter-74d4cb89b8-ll246 requesting resource cpu=102m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod prometheus-adapter-74d4cb89b8-z4hvp requesting resource cpu=102m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod prometheus-k8s-0 requesting resource cpu=100m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod prometheus-k8s-1 requesting resource cpu=100m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod prometheus-operator-59f6b8bc59-wv7ll requesting resource cpu=110m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.114: INFO: Pod sonobuoy requesting resource cpu=0m on Node ck-test-kube-1-27-worker
  May 18 09:12:14.115: INFO: Pod sonobuoy-e2e-job-cccd1fa16dd54418 requesting resource cpu=0m on Node ck-test-kube-1-27-worker
  May 18 09:12:14.115: INFO: Pod sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-8p6sc requesting resource cpu=0m on Node ck-test-kube-1-27-worker
  May 18 09:12:14.115: INFO: Pod sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-xppr7 requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.117: INFO: Pod tekton-pipelines-remote-resolvers-76f588dbf5-z6v9v requesting resource cpu=100m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.117: INFO: Pod tekton-pipelines-controller-dd857fb44-mwdzd requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.118: INFO: Pod tekton-pipelines-webhook-79bbb84bb4-7q2lq requesting resource cpu=100m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.118: INFO: Pod tekton-triggers-controller-99d8dc8bb-g47nr requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.119: INFO: Pod tekton-triggers-core-interceptors-ddf77b777-psf7x requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  May 18 09:12:14.119: INFO: Pod tekton-triggers-webhook-69569f7f45-vv4c8 requesting resource cpu=0m on Node ck-test-kube-1-27-worker-1
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/18/23 09:12:14.12
  May 18 09:12:14.120: INFO: Creating a pod which consumes cpu=1146m on Node ck-test-kube-1-27-worker
  May 18 09:12:14.131: INFO: Creating a pod which consumes cpu=148m on Node ck-test-kube-1-27-worker-1
  E0518 09:12:14.994904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:15.993253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/18/23 09:12:16.155
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-1a4dc162-03af-4a4f-9c8f-b51bb80b8c01.176031e83c557062], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5372/filler-pod-1a4dc162-03af-4a4f-9c8f-b51bb80b8c01 to ck-test-kube-1-27-worker] @ 05/18/23 09:12:16.163
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-1a4dc162-03af-4a4f-9c8f-b51bb80b8c01.176031e86b7034a4], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/18/23 09:12:16.165
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-1a4dc162-03af-4a4f-9c8f-b51bb80b8c01.176031e86c77e787], Reason = [Created], Message = [Created container filler-pod-1a4dc162-03af-4a4f-9c8f-b51bb80b8c01] @ 05/18/23 09:12:16.166
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-1a4dc162-03af-4a4f-9c8f-b51bb80b8c01.176031e87258f845], Reason = [Started], Message = [Started container filler-pod-1a4dc162-03af-4a4f-9c8f-b51bb80b8c01] @ 05/18/23 09:12:16.166
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fa526c2d-ce07-44b8-810b-2317fe8b5468.176031e83c897df4], Reason = [Scheduled], Message = [Successfully assigned sched-pred-5372/filler-pod-fa526c2d-ce07-44b8-810b-2317fe8b5468 to ck-test-kube-1-27-worker-1] @ 05/18/23 09:12:16.167
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fa526c2d-ce07-44b8-810b-2317fe8b5468.176031e87f90269c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/18/23 09:12:16.167
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fa526c2d-ce07-44b8-810b-2317fe8b5468.176031e8829d23f4], Reason = [Created], Message = [Created container filler-pod-fa526c2d-ce07-44b8-810b-2317fe8b5468] @ 05/18/23 09:12:16.168
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-fa526c2d-ce07-44b8-810b-2317fe8b5468.176031e89728c313], Reason = [Started], Message = [Started container filler-pod-fa526c2d-ce07-44b8-810b-2317fe8b5468] @ 05/18/23 09:12:16.168
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.176031e8b564f415], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 Insufficient cpu. preemption: 0/3 nodes are available: 1 Preemption is not helpful for scheduling, 2 No preemption victims found for incoming pod..] @ 05/18/23 09:12:16.183
  E0518 09:12:16.994240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node ck-test-kube-1-27-worker @ 05/18/23 09:12:17.179
  STEP: verifying the node doesn't have the label node @ 05/18/23 09:12:17.198
  STEP: removing the label node off the node ck-test-kube-1-27-worker-1 @ 05/18/23 09:12:17.202
  STEP: verifying the node doesn't have the label node @ 05/18/23 09:12:17.214
  May 18 09:12:17.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5372" for this suite. @ 05/18/23 09:12:17.223
• [3.387 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/18/23 09:12:17.237
  May 18 09:12:17.238: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:12:17.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:17.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:17.257
  STEP: Creating projection with secret that has name projected-secret-test-map-dffbca20-4362-4606-9ec8-8f535ae0960d @ 05/18/23 09:12:17.259
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:12:17.262
  E0518 09:12:17.994511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:18.995584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:19.996034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:20.997910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:12:21.286
  May 18 09:12:21.291: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-secrets-f06628a9-b775-438f-ab02-8fa0f775b098 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:12:21.32
  May 18 09:12:21.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-371" for this suite. @ 05/18/23 09:12:21.347
• [4.118 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/18/23 09:12:21.362
  May 18 09:12:21.362: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replication-controller @ 05/18/23 09:12:21.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:21.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:21.391
  STEP: Given a ReplicationController is created @ 05/18/23 09:12:21.396
  STEP: When the matched label of one of its pods change @ 05/18/23 09:12:21.405
  May 18 09:12:21.408: INFO: Pod name pod-release: Found 0 pods out of 1
  E0518 09:12:21.997032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:22.997158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:23.997740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:24.997944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:25.998426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:26.416: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/18/23 09:12:26.429
  E0518 09:12:26.998613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:27.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9485" for this suite. @ 05/18/23 09:12:27.448
• [6.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/18/23 09:12:27.467
  May 18 09:12:27.467: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename deployment @ 05/18/23 09:12:27.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:27.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:27.494
  STEP: creating a Deployment @ 05/18/23 09:12:27.508
  STEP: waiting for Deployment to be created @ 05/18/23 09:12:27.514
  STEP: waiting for all Replicas to be Ready @ 05/18/23 09:12:27.517
  May 18 09:12:27.519: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 18 09:12:27.519: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 18 09:12:27.529: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 18 09:12:27.530: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 18 09:12:27.544: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 18 09:12:27.545: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 18 09:12:27.585: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 18 09:12:27.585: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0518 09:12:27.999573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:28.936: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 18 09:12:28.937: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0518 09:12:29.000650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:29.056: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/18/23 09:12:29.057
  W0518 09:12:29.064215      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 18 09:12:29.066: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/18/23 09:12:29.066
  May 18 09:12:29.068: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0
  May 18 09:12:29.068: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0
  May 18 09:12:29.068: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0
  May 18 09:12:29.068: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0
  May 18 09:12:29.069: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0
  May 18 09:12:29.069: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0
  May 18 09:12:29.069: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0
  May 18 09:12:29.069: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 0
  May 18 09:12:29.069: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  May 18 09:12:29.069: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  May 18 09:12:29.070: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.070: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.070: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.070: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.078: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.078: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.106: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.107: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.117: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.117: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:29.121: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  May 18 09:12:29.122: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  E0518 09:12:30.000712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:30.066: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:30.066: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:30.104: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  STEP: listing Deployments @ 05/18/23 09:12:30.104
  May 18 09:12:30.111: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/18/23 09:12:30.111
  May 18 09:12:30.128: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/18/23 09:12:30.128
  May 18 09:12:30.136: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 18 09:12:30.138: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 18 09:12:30.168: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 18 09:12:30.178: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 18 09:12:30.191: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0518 09:12:31.000881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:31.990: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0518 09:12:32.001064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:32.079: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  May 18 09:12:32.113: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 18 09:12:32.120: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0518 09:12:33.001307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:34.001851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:34.040: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/18/23 09:12:34.086
  STEP: fetching the DeploymentStatus @ 05/18/23 09:12:34.097
  May 18 09:12:34.107: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  May 18 09:12:34.108: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  May 18 09:12:34.108: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  May 18 09:12:34.108: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  May 18 09:12:34.109: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 1
  May 18 09:12:34.109: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:34.109: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 3
  May 18 09:12:34.109: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:34.109: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 2
  May 18 09:12:34.109: INFO: observed Deployment test-deployment in namespace deployment-7829 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/18/23 09:12:34.109
  May 18 09:12:34.124: INFO: observed event type MODIFIED
  May 18 09:12:34.124: INFO: observed event type MODIFIED
  May 18 09:12:34.125: INFO: observed event type MODIFIED
  May 18 09:12:34.125: INFO: observed event type MODIFIED
  May 18 09:12:34.125: INFO: observed event type MODIFIED
  May 18 09:12:34.126: INFO: observed event type MODIFIED
  May 18 09:12:34.126: INFO: observed event type MODIFIED
  May 18 09:12:34.127: INFO: observed event type MODIFIED
  May 18 09:12:34.127: INFO: observed event type MODIFIED
  May 18 09:12:34.128: INFO: observed event type MODIFIED
  May 18 09:12:34.128: INFO: observed event type MODIFIED
  May 18 09:12:34.133: INFO: Log out all the ReplicaSets if there is no deployment created
  May 18 09:12:34.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-7829" for this suite. @ 05/18/23 09:12:34.146
• [6.714 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/18/23 09:12:34.183
  May 18 09:12:34.183: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:12:34.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:34.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:34.214
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/18/23 09:12:34.217
  E0518 09:12:35.001999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:36.002374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:37.005366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:38.005214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:12:38.242
  May 18 09:12:38.246: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-71dd069f-6c97-443b-93c5-203994c0ee06 container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:12:38.253
  May 18 09:12:38.266: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3257" for this suite. @ 05/18/23 09:12:38.271
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/18/23 09:12:38.285
  May 18 09:12:38.285: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 09:12:38.288
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:38.303
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:38.308
  STEP: creating the pod @ 05/18/23 09:12:38.312
  STEP: submitting the pod to kubernetes @ 05/18/23 09:12:38.312
  E0518 09:12:39.006782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:40.006956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 05/18/23 09:12:40.332
  STEP: updating the pod @ 05/18/23 09:12:40.336
  May 18 09:12:40.854: INFO: Successfully updated pod "pod-update-d2a65508-e56d-4253-9b76-9980fcc4d8e4"
  STEP: verifying the updated pod is in kubernetes @ 05/18/23 09:12:40.86
  May 18 09:12:40.865: INFO: Pod update OK
  May 18 09:12:40.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3134" for this suite. @ 05/18/23 09:12:40.874
• [2.597 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/18/23 09:12:40.897
  May 18 09:12:40.898: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename limitrange @ 05/18/23 09:12:40.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:40.916
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:40.921
  STEP: Creating a LimitRange @ 05/18/23 09:12:40.927
  STEP: Setting up watch @ 05/18/23 09:12:40.927
  E0518 09:12:41.006913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Submitting a LimitRange @ 05/18/23 09:12:41.031
  STEP: Verifying LimitRange creation was observed @ 05/18/23 09:12:41.039
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/18/23 09:12:41.04
  May 18 09:12:41.044: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 18 09:12:41.044: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/18/23 09:12:41.045
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/18/23 09:12:41.052
  May 18 09:12:41.056: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 18 09:12:41.056: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/18/23 09:12:41.056
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/18/23 09:12:41.064
  May 18 09:12:41.069: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May 18 09:12:41.070: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/18/23 09:12:41.071
  STEP: Failing to create a Pod with more than max resources @ 05/18/23 09:12:41.076
  STEP: Updating a LimitRange @ 05/18/23 09:12:41.08
  STEP: Verifying LimitRange updating is effective @ 05/18/23 09:12:41.085
  E0518 09:12:42.007305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:43.008553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 05/18/23 09:12:43.094
  STEP: Failing to create a Pod with more than max resources @ 05/18/23 09:12:43.102
  STEP: Deleting a LimitRange @ 05/18/23 09:12:43.109
  STEP: Verifying the LimitRange was deleted @ 05/18/23 09:12:43.125
  E0518 09:12:44.008474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:45.009162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:46.013046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:47.013240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:48.014299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:48.131: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/18/23 09:12:48.132
  May 18 09:12:48.142: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6290" for this suite. @ 05/18/23 09:12:48.151
• [7.265 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/18/23 09:12:48.166
  May 18 09:12:48.167: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename dns @ 05/18/23 09:12:48.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:12:48.214
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:12:48.219
  STEP: Creating a test headless service @ 05/18/23 09:12:48.225
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-8018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-8018.svc.cluster.local;sleep 1; done
   @ 05/18/23 09:12:48.232
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-8018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-8018.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-8018.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-8018.svc.cluster.local;sleep 1; done
   @ 05/18/23 09:12:48.232
  STEP: creating a pod to probe DNS @ 05/18/23 09:12:48.232
  STEP: submitting the pod to kubernetes @ 05/18/23 09:12:48.232
  E0518 09:12:49.014594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:50.015472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 09:12:50.279
  STEP: looking for the results for each expected name from probers @ 05/18/23 09:12:50.284
  May 18 09:12:50.293: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:50.298: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:50.303: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:50.309: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:50.314: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:50.320: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:50.325: INFO: Unable to read jessie_udp@dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:50.328: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:50.329: INFO: Lookups using dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local wheezy_udp@dns-test-service-2.dns-8018.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-8018.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_udp@dns-test-service-2.dns-8018.svc.cluster.local jessie_tcp@dns-test-service-2.dns-8018.svc.cluster.local]

  E0518 09:12:51.015667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:52.016152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:53.016128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:54.016294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:55.016353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:12:55.335: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:55.340: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:55.353: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:55.357: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:12:55.363: INFO: Lookups using dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local]

  E0518 09:12:56.017238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:57.017260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:58.017415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:12:59.018088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:00.017941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:00.336: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:00.341: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:00.359: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:00.369: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:00.380: INFO: Lookups using dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local]

  E0518 09:13:01.018095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:02.019534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:03.019895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:04.020521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:05.020618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:05.338: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:05.352: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:05.364: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:05.372: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:05.381: INFO: Lookups using dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local]

  E0518 09:13:06.021198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:07.020998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:08.024032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:09.025259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:10.025198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:10.337: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:10.344: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:10.361: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:10.366: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:10.380: INFO: Lookups using dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local]

  E0518 09:13:11.025539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:12.026180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:13.027290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:14.027660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:15.028435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:15.336: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:15.345: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:15.359: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:15.364: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local from pod dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733: the server could not find the requested resource (get pods dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733)
  May 18 09:13:15.375: INFO: Lookups using dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-8018.svc.cluster.local]

  E0518 09:13:16.028600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:17.028797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:18.029032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:19.029529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:20.029453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:20.367: INFO: DNS probes using dns-8018/dns-test-63d06630-e141-4d66-91d2-ac2c12d6a733 succeeded

  May 18 09:13:20.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:13:20.374
  STEP: deleting the test headless service @ 05/18/23 09:13:20.401
  STEP: Destroying namespace "dns-8018" for this suite. @ 05/18/23 09:13:20.421
• [32.266 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/18/23 09:13:20.445
  May 18 09:13:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename disruption @ 05/18/23 09:13:20.456
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:13:20.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:13:20.501
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:13:20.511
  STEP: Updating PodDisruptionBudget status @ 05/18/23 09:13:20.518
  STEP: Waiting for all pods to be running @ 05/18/23 09:13:20.525
  May 18 09:13:20.530: INFO: running pods: 0 < 1
  E0518 09:13:21.030018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:22.030178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/18/23 09:13:22.535
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:13:22.558
  STEP: Patching PodDisruptionBudget status @ 05/18/23 09:13:22.566
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:13:22.576
  May 18 09:13:22.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5015" for this suite. @ 05/18/23 09:13:22.585
• [2.145 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/18/23 09:13:22.597
  May 18 09:13:22.597: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename conformance-tests @ 05/18/23 09:13:22.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:13:22.611
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:13:22.614
  STEP: Getting node addresses @ 05/18/23 09:13:22.619
  May 18 09:13:22.619: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May 18 09:13:22.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-2063" for this suite. @ 05/18/23 09:13:22.63
• [0.037 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/18/23 09:13:22.638
  May 18 09:13:22.639: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:13:22.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:13:22.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:13:22.658
  STEP: Setting up server cert @ 05/18/23 09:13:22.679
  E0518 09:13:23.030425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:13:23.168
  STEP: Deploying the webhook pod @ 05/18/23 09:13:23.179
  STEP: Wait for the deployment to be ready @ 05/18/23 09:13:23.196
  May 18 09:13:23.201: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0518 09:13:24.030593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:25.030959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:13:25.212
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:13:25.226
  E0518 09:13:26.030984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:26.227: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/18/23 09:13:26.232
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/18/23 09:13:26.232
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/18/23 09:13:26.253
  E0518 09:13:27.031108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/18/23 09:13:27.264
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/18/23 09:13:27.264
  E0518 09:13:28.031272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 05/18/23 09:13:28.295
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/18/23 09:13:28.295
  E0518 09:13:29.032357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:30.032798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:31.033237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:32.034145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:33.034929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/18/23 09:13:33.335
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/18/23 09:13:33.336
  E0518 09:13:34.035688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:35.036316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:36.036820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:37.037682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:38.039387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:38.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2959" for this suite. @ 05/18/23 09:13:38.454
  STEP: Destroying namespace "webhook-markers-6533" for this suite. @ 05/18/23 09:13:38.487
• [15.857 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/18/23 09:13:38.497
  May 18 09:13:38.499: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:13:38.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:13:38.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:13:38.528
  May 18 09:13:38.532: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-913 create -f -'
  E0518 09:13:39.040017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:40.039735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:40.946: INFO: stderr: ""
  May 18 09:13:40.946: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May 18 09:13:40.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-913 create -f -'
  E0518 09:13:41.039997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:41.545: INFO: stderr: ""
  May 18 09:13:41.545: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/18/23 09:13:41.545
  E0518 09:13:42.040173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:42.552: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 09:13:42.552: INFO: Found 1 / 1
  May 18 09:13:42.552: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 18 09:13:42.556: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 09:13:42.556: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 18 09:13:42.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-913 describe pod agnhost-primary-2c57r'
  May 18 09:13:42.715: INFO: stderr: ""
  May 18 09:13:42.715: INFO: stdout: "Name:             agnhost-primary-2c57r\nNamespace:        kubectl-913\nPriority:         0\nService Account:  default\nNode:             ck-test-kube-1-27-worker/192.168.190.143\nStart Time:       Thu, 18 May 2023 09:13:40 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 2f1a469c25e5e2d5b1926f3905110c78024e9e7881cfd73048ffbffac797bd25\n                  cni.projectcalico.org/podIP: 172.16.161.73/32\n                  cni.projectcalico.org/podIPs: 172.16.161.73/32\nStatus:           Running\nIP:               172.16.161.73\nIPs:\n  IP:           172.16.161.73\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://843922321c9aa6b73b9c6bc3c8ad1e9faee184fbedd74fcc4400d4bb21f7bd67\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 18 May 2023 09:13:41 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-f5x5f (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-f5x5f:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-913/agnhost-primary-2c57r to ck-test-kube-1-27-worker\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  May 18 09:13:42.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-913 describe rc agnhost-primary'
  May 18 09:13:42.855: INFO: stderr: ""
  May 18 09:13:42.855: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-913\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-2c57r\n"
  May 18 09:13:42.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-913 describe service agnhost-primary'
  May 18 09:13:42.996: INFO: stderr: ""
  May 18 09:13:42.996: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-913\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.111.14.210\nIPs:               10.111.14.210\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         172.16.161.73:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May 18 09:13:43.002: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-913 describe node ck-test-kube-1-27-master'
  E0518 09:13:43.041000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:43.175: INFO: stderr: ""
  May 18 09:13:43.176: INFO: stdout: "Name:               ck-test-kube-1-27-master\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ck-test-kube-1-27-master\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 192.168.190.140/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 172.16.243.128\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 12 May 2023 06:49:22 +0000\nTaints:             node-role.kubernetes.io/control-plane:NoSchedule\nUnschedulable:      false\nLease:\n  HolderIdentity:  ck-test-kube-1-27-master\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 18 May 2023 09:13:42 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 12 May 2023 06:51:57 +0000   Fri, 12 May 2023 06:51:57 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 18 May 2023 09:12:51 +0000   Fri, 12 May 2023 06:49:22 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 18 May 2023 09:12:51 +0000   Fri, 12 May 2023 06:49:22 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 18 May 2023 09:12:51 +0000   Fri, 12 May 2023 06:49:22 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 18 May 2023 09:12:51 +0000   Fri, 12 May 2023 06:51:48 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.190.140\n  Hostname:    ck-test-kube-1-27-master\nCapacity:\n  cpu:                4\n  ephemeral-storage:  101430960Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16393460Ki\n  pods:               110\nAllocatable:\n  cpu:                4\n  ephemeral-storage:  93478772582\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             16291060Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 02d6b42068b64a079e1551636927092e\n  System UUID:                ba01a140-933a-4ffc-a8af-a9058581fcec\n  Boot ID:                    adf0ced9-dd7b-4879-8e1c-94c4148d56da\n  Kernel Version:             5.4.0-136-generic\n  OS Image:                   Ubuntu 20.04.5 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.20\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-4phq9                                          250m (6%)     0 (0%)      0 (0%)           0 (0%)         6d2h\n  kube-system                 etcd-ck-test-kube-1-27-master                              100m (2%)     0 (0%)      100Mi (0%)       0 (0%)         6d2h\n  kube-system                 kube-apiserver-ck-test-kube-1-27-master                    250m (6%)     0 (0%)      0 (0%)           0 (0%)         6d2h\n  kube-system                 kube-controller-manager-ck-test-kube-1-27-master           200m (5%)     0 (0%)      0 (0%)           0 (0%)         6d2h\n  kube-system                 kube-proxy-bzcx2                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         6d2h\n  kube-system                 kube-scheduler-ck-test-kube-1-27-master                    100m (2%)     0 (0%)      0 (0%)           0 (0%)         6d2h\n  secloudit-monitoring        node-exporter-f8hnd                                        112m (2%)     270m (6%)   200Mi (1%)       220Mi (1%)     6d2h\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-dcfsj    0 (0%)        0 (0%)      0 (0%)           0 (0%)         21m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests     Limits\n  --------           --------     ------\n  cpu                1012m (25%)  270m (6%)\n  memory             300Mi (1%)   220Mi (1%)\n  ephemeral-storage  0 (0%)       0 (0%)\n  hugepages-1Gi      0 (0%)       0 (0%)\n  hugepages-2Mi      0 (0%)       0 (0%)\nEvents:              <none>\n"
  May 18 09:13:43.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-913 describe namespace kubectl-913'
  May 18 09:13:43.328: INFO: stderr: ""
  May 18 09:13:43.328: INFO: stdout: "Name:         kubectl-913\nLabels:       e2e-framework=kubectl\n              e2e-run=5ca56008-4132-4939-9b12-133755241423\n              kubernetes.io/metadata.name=kubectl-913\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May 18 09:13:43.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-913" for this suite. @ 05/18/23 09:13:43.334
• [4.844 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/18/23 09:13:43.349
  May 18 09:13:43.349: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-pred @ 05/18/23 09:13:43.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:13:43.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:13:43.374
  May 18 09:13:43.379: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 18 09:13:43.389: INFO: Waiting for terminating namespaces to be deleted...
  May 18 09:13:43.394: INFO: 
  Logging pods the apiserver thinks is on node ck-test-kube-1-27-worker before test
  May 18 09:13:43.409: INFO: calico-node-z6wl2 from kube-system started at 2023-05-12 06:51:27 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.409: INFO: 	Container calico-node ready: true, restart count 0
  May 18 09:13:43.409: INFO: kube-proxy-269wt from kube-system started at 2023-05-12 06:50:32 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.409: INFO: 	Container kube-proxy ready: true, restart count 0
  May 18 09:13:43.409: INFO: agnhost-primary-2c57r from kubectl-913 started at 2023-05-18 09:13:40 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.409: INFO: 	Container agnhost-primary ready: true, restart count 0
  May 18 09:13:43.409: INFO: node-exporter-7q9w4 from secloudit-monitoring started at 2023-05-12 07:19:01 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.409: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:13:43.409: INFO: 	Container node-exporter ready: true, restart count 0
  May 18 09:13:43.409: INFO: sonobuoy from sonobuoy started at 2023-05-18 08:52:38 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.409: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 18 09:13:43.409: INFO: sonobuoy-e2e-job-cccd1fa16dd54418 from sonobuoy started at 2023-05-18 08:52:39 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.409: INFO: 	Container e2e ready: true, restart count 0
  May 18 09:13:43.409: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:13:43.409: INFO: sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-8p6sc from sonobuoy started at 2023-05-18 08:52:40 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.409: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:13:43.409: INFO: 	Container systemd-logs ready: true, restart count 0
  May 18 09:13:43.409: INFO: 
  Logging pods the apiserver thinks is on node ck-test-kube-1-27-worker-1 before test
  May 18 09:13:43.433: INFO: argocd-application-controller-0 from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.433: INFO: 	Container argocd-application-controller ready: true, restart count 0
  May 18 09:13:43.434: INFO: argocd-applicationset-controller-57fd46cc99-vjt7t from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.434: INFO: 	Container argocd-applicationset-controller ready: true, restart count 0
  May 18 09:13:43.434: INFO: argocd-dex-server-75d8c65447-wrjw6 from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.435: INFO: 	Container dex ready: true, restart count 0
  May 18 09:13:43.435: INFO: argocd-notifications-controller-6f5f6d97cb-sdzxc from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.436: INFO: 	Container argocd-notifications-controller ready: true, restart count 0
  May 18 09:13:43.436: INFO: argocd-redis-77bf5b886-dn8z5 from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.437: INFO: 	Container redis ready: true, restart count 0
  May 18 09:13:43.437: INFO: argocd-repo-server-59684bc98c-dk4hd from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.437: INFO: 	Container argocd-repo-server ready: true, restart count 0
  May 18 09:13:43.438: INFO: argocd-server-d5bd658c-fs9dz from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.438: INFO: 	Container argocd-server ready: true, restart count 0
  May 18 09:13:43.439: INFO: istio-egressgateway-575466f5bb-mqw8r from istio-system started at 2023-05-18 06:47:53 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.439: INFO: 	Container istio-proxy ready: true, restart count 0
  May 18 09:13:43.440: INFO: istio-ingressgateway-cb9c6b49d-htxtq from istio-system started at 2023-05-18 06:47:53 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.440: INFO: 	Container istio-proxy ready: true, restart count 0
  May 18 09:13:43.440: INFO: istiod-6fbbf67d58-zhh74 from istio-system started at 2023-05-18 06:47:39 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.441: INFO: 	Container discovery ready: true, restart count 0
  May 18 09:13:43.442: INFO: calico-kube-controllers-674fff74c8-lvpc2 from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.442: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 18 09:13:43.442: INFO: calico-node-946wb from kube-system started at 2023-05-12 06:51:27 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.443: INFO: 	Container calico-node ready: true, restart count 0
  May 18 09:13:43.443: INFO: coredns-5d78c9869d-fchlv from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.443: INFO: 	Container coredns ready: true, restart count 0
  May 18 09:13:43.443: INFO: coredns-5d78c9869d-vbhw2 from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.444: INFO: 	Container coredns ready: true, restart count 0
  May 18 09:13:43.444: INFO: kube-proxy-s5x6t from kube-system started at 2023-05-12 06:50:32 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.444: INFO: 	Container kube-proxy ready: true, restart count 0
  May 18 09:13:43.444: INFO: alertmanager-main-0 from secloudit-monitoring started at 2023-05-18 07:36:19 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.445: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:13:43.445: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:13:43.445: INFO: alertmanager-main-1 from secloudit-monitoring started at 2023-05-12 07:19:12 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.445: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:13:43.445: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:13:43.445: INFO: alertmanager-main-2 from secloudit-monitoring started at 2023-05-18 07:36:20 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.445: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:13:43.445: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:13:43.445: INFO: blackbox-exporter-659b6ff548-25j5z from secloudit-monitoring started at 2023-05-12 07:17:19 +0000 UTC (3 container statuses recorded)
  May 18 09:13:43.446: INFO: 	Container blackbox-exporter ready: true, restart count 0
  May 18 09:13:43.446: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:13:43.446: INFO: 	Container module-configmap-reloader ready: true, restart count 0
  May 18 09:13:43.446: INFO: grafana-6cb59f497c-ssmtq from secloudit-monitoring started at 2023-05-18 07:36:15 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.446: INFO: 	Container grafana ready: true, restart count 0
  May 18 09:13:43.447: INFO: kube-state-metrics-f59465db9-v6k4r from secloudit-monitoring started at 2023-05-18 07:36:14 +0000 UTC (3 container statuses recorded)
  May 18 09:13:43.447: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
  May 18 09:13:43.447: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
  May 18 09:13:43.447: INFO: 	Container kube-state-metrics ready: true, restart count 0
  May 18 09:13:43.447: INFO: node-exporter-4ttx4 from secloudit-monitoring started at 2023-05-12 07:18:53 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.448: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:13:43.448: INFO: 	Container node-exporter ready: true, restart count 0
  May 18 09:13:43.448: INFO: prometheus-adapter-74d4cb89b8-ll246 from secloudit-monitoring started at 2023-05-12 07:18:13 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.448: INFO: 	Container prometheus-adapter ready: true, restart count 0
  May 18 09:13:43.449: INFO: prometheus-adapter-74d4cb89b8-z4hvp from secloudit-monitoring started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.449: INFO: 	Container prometheus-adapter ready: true, restart count 0
  May 18 09:13:43.449: INFO: prometheus-k8s-0 from secloudit-monitoring started at 2023-05-12 07:19:13 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.449: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:13:43.449: INFO: 	Container prometheus ready: true, restart count 0
  May 18 09:13:43.449: INFO: prometheus-k8s-1 from secloudit-monitoring started at 2023-05-18 07:36:19 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.450: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:13:43.450: INFO: 	Container prometheus ready: true, restart count 0
  May 18 09:13:43.450: INFO: prometheus-operator-59f6b8bc59-wv7ll from secloudit-monitoring started at 2023-05-18 07:36:15 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.450: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:13:43.450: INFO: 	Container prometheus-operator ready: true, restart count 0
  May 18 09:13:43.451: INFO: sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-xppr7 from sonobuoy started at 2023-05-18 08:52:40 +0000 UTC (2 container statuses recorded)
  May 18 09:13:43.451: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:13:43.451: INFO: 	Container systemd-logs ready: true, restart count 0
  May 18 09:13:43.451: INFO: tekton-pipelines-remote-resolvers-76f588dbf5-z6v9v from tekton-pipelines-resolvers started at 2023-05-18 07:36:15 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.451: INFO: 	Container controller ready: true, restart count 0
  May 18 09:13:43.452: INFO: tekton-pipelines-controller-dd857fb44-mwdzd from tekton-pipelines started at 2023-05-12 07:32:59 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.452: INFO: 	Container tekton-pipelines-controller ready: true, restart count 0
  May 18 09:13:43.452: INFO: tekton-pipelines-webhook-79bbb84bb4-7q2lq from tekton-pipelines started at 2023-05-12 07:32:59 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.452: INFO: 	Container webhook ready: true, restart count 0
  May 18 09:13:43.452: INFO: tekton-triggers-controller-99d8dc8bb-g47nr from tekton-pipelines started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.453: INFO: 	Container tekton-triggers-controller ready: true, restart count 0
  May 18 09:13:43.453: INFO: tekton-triggers-core-interceptors-ddf77b777-psf7x from tekton-pipelines started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.453: INFO: 	Container tekton-triggers-core-interceptors ready: true, restart count 0
  May 18 09:13:43.454: INFO: tekton-triggers-webhook-69569f7f45-vv4c8 from tekton-pipelines started at 2023-05-12 07:33:13 +0000 UTC (1 container statuses recorded)
  May 18 09:13:43.454: INFO: 	Container webhook ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/18/23 09:13:43.454
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.176031fd0b9c2afa], Reason = [FailedScheduling], Message = [0/3 nodes are available: 1 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 05/18/23 09:13:43.534
  E0518 09:13:44.042194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:13:44.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4634" for this suite. @ 05/18/23 09:13:44.537
• [1.195 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/18/23 09:13:44.546
  May 18 09:13:44.546: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename tables @ 05/18/23 09:13:44.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:13:44.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:13:44.567
  May 18 09:13:44.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-6164" for this suite. @ 05/18/23 09:13:44.579
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/18/23 09:13:44.596
  May 18 09:13:44.596: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:13:44.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:13:44.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:13:44.617
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:13:44.621
  E0518 09:13:45.042451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:46.043592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:47.043588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:48.043690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:13:48.641
  May 18 09:13:48.645: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-57425bc9-6472-47fc-b23c-1ae542e9899a container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:13:48.653
  May 18 09:13:48.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2456" for this suite. @ 05/18/23 09:13:48.672
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/18/23 09:13:48.686
  May 18 09:13:48.687: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pod-network-test @ 05/18/23 09:13:48.688
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:13:48.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:13:48.709
  STEP: Performing setup for networking test in namespace pod-network-test-6996 @ 05/18/23 09:13:48.713
  STEP: creating a selector @ 05/18/23 09:13:48.714
  STEP: Creating the service pods in kubernetes @ 05/18/23 09:13:48.714
  May 18 09:13:48.714: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0518 09:13:49.044782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:50.045485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:51.045922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:52.046131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:53.047230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:54.047637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:55.047566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:56.047962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:57.049101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:58.049278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:13:59.050142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:00.050293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:01.051186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:02.051968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:03.052265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:04.053058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:05.053982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:06.055194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:07.055789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:08.056072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:09.056611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:10.056963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/18/23 09:14:10.817
  E0518 09:14:11.057081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:12.057898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:12.868: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 18 09:14:12.869: INFO: Going to poll 172.16.161.94 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May 18 09:14:12.874: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.161.94 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6996 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:14:12.874: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:14:12.878: INFO: ExecWithOptions: Clientset creation
  May 18 09:14:12.878: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6996/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.161.94+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0518 09:14:13.058708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:14.030: INFO: Found all 1 expected endpoints: [netserver-0]
  May 18 09:14:14.030: INFO: Going to poll 172.16.78.112 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May 18 09:14:14.039: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 172.16.78.112 8081 | grep -v '^\s*$'] Namespace:pod-network-test-6996 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:14:14.039: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:14:14.041: INFO: ExecWithOptions: Clientset creation
  May 18 09:14:14.042: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6996/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+172.16.78.112+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0518 09:14:14.059821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:15.060175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:15.166: INFO: Found all 1 expected endpoints: [netserver-1]
  May 18 09:14:15.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6996" for this suite. @ 05/18/23 09:14:15.177
• [26.508 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/18/23 09:14:15.208
  May 18 09:14:15.208: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:14:15.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:14:15.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:14:15.238
  STEP: creating service in namespace services-8167 @ 05/18/23 09:14:15.242
  STEP: creating service affinity-nodeport in namespace services-8167 @ 05/18/23 09:14:15.242
  STEP: creating replication controller affinity-nodeport in namespace services-8167 @ 05/18/23 09:14:15.317
  I0518 09:14:15.338588      19 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-8167, replica count: 3
  E0518 09:14:16.060279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:17.062074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:18.061478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:14:18.391339      19 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 09:14:18.406: INFO: Creating new exec pod
  E0518 09:14:19.062266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:20.062624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:21.062512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:21.441: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-8167 exec execpod-affinityw926j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May 18 09:14:21.782: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport 80\n+ echo hostName\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May 18 09:14:21.782: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:14:21.782: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-8167 exec execpod-affinityw926j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.218.32 80'
  May 18 09:14:21.972: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.218.32 80\nConnection to 10.103.218.32 80 port [tcp/http] succeeded!\n"
  May 18 09:14:21.972: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:14:21.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-8167 exec execpod-affinityw926j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.143 31218'
  E0518 09:14:22.062960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:22.176: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.190.143 31218\nConnection to 192.168.190.143 31218 port [tcp/*] succeeded!\n"
  May 18 09:14:22.177: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:14:22.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-8167 exec execpod-affinityw926j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.135 31218'
  May 18 09:14:22.392: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.190.135 31218\nConnection to 192.168.190.135 31218 port [tcp/*] succeeded!\n"
  May 18 09:14:22.392: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:14:22.393: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-8167 exec execpod-affinityw926j -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.190.143:31218/ ; done'
  May 18 09:14:22.717: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:31218/\n"
  May 18 09:14:22.717: INFO: stdout: "\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp\naffinity-nodeport-55klp"
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Received response from host: affinity-nodeport-55klp
  May 18 09:14:22.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:14:22.722: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-8167, will wait for the garbage collector to delete the pods @ 05/18/23 09:14:22.737
  May 18 09:14:22.812: INFO: Deleting ReplicationController affinity-nodeport took: 11.779298ms
  May 18 09:14:22.913: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.663589ms
  E0518 09:14:23.063286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:24.063730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-8167" for this suite. @ 05/18/23 09:14:25.05
• [9.854 seconds]
------------------------------
  E0518 09:14:25.064164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/18/23 09:14:25.069
  May 18 09:14:25.069: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 09:14:25.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:14:25.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:14:25.095
  STEP: set up a multi version CRD @ 05/18/23 09:14:25.101
  May 18 09:14:25.103: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:14:26.065321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:27.065614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:28.078523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:29.078854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:30.079195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 05/18/23 09:14:30.933
  STEP: check the new version name is served @ 05/18/23 09:14:30.96
  E0518 09:14:31.079433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:32.080065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:33.080233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 05/18/23 09:14:33.377
  E0518 09:14:34.081005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/18/23 09:14:34.52
  E0518 09:14:35.081411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:36.081499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:37.081521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:38.083005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:39.083387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:39.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2279" for this suite. @ 05/18/23 09:14:39.75
• [14.688 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/18/23 09:14:39.768
  May 18 09:14:39.768: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:14:39.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:14:39.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:14:39.791
  STEP: Creating the pod @ 05/18/23 09:14:39.795
  E0518 09:14:40.084183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:41.084635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:42.084493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:43.085693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:44.085793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:44.358: INFO: Successfully updated pod "labelsupdatefe1d79d1-a409-4dc9-b9d3-50cb0f4bd9ff"
  E0518 09:14:45.086749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:46.087324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:46.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4473" for this suite. @ 05/18/23 09:14:46.389
• [6.629 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/18/23 09:14:46.403
  May 18 09:14:46.403: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-runtime @ 05/18/23 09:14:46.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:14:46.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:14:46.436
  STEP: create the container @ 05/18/23 09:14:46.44
  W0518 09:14:46.449851      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/18/23 09:14:46.45
  E0518 09:14:47.088082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:48.088439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:49.089040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/18/23 09:14:49.474
  STEP: the container should be terminated @ 05/18/23 09:14:49.479
  STEP: the termination message should be set @ 05/18/23 09:14:49.48
  May 18 09:14:49.480: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/18/23 09:14:49.48
  May 18 09:14:49.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8269" for this suite. @ 05/18/23 09:14:49.501
• [3.106 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/18/23 09:14:49.51
  May 18 09:14:49.510: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename statefulset @ 05/18/23 09:14:49.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:14:49.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:14:49.54
  STEP: Creating service test in namespace statefulset-9065 @ 05/18/23 09:14:49.545
  STEP: Looking for a node to schedule stateful set and pod @ 05/18/23 09:14:49.55
  STEP: Creating pod with conflicting port in namespace statefulset-9065 @ 05/18/23 09:14:49.558
  STEP: Waiting until pod test-pod will start running in namespace statefulset-9065 @ 05/18/23 09:14:49.569
  E0518 09:14:50.089170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:51.089787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-9065 @ 05/18/23 09:14:51.579
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9065 @ 05/18/23 09:14:51.59
  May 18 09:14:51.623: INFO: Observed stateful pod in namespace: statefulset-9065, name: ss-0, uid: a4434a30-47a9-46b7-af3a-fe5cad40ca96, status phase: Pending. Waiting for statefulset controller to delete.
  May 18 09:14:51.635: INFO: Observed stateful pod in namespace: statefulset-9065, name: ss-0, uid: a4434a30-47a9-46b7-af3a-fe5cad40ca96, status phase: Failed. Waiting for statefulset controller to delete.
  May 18 09:14:51.645: INFO: Observed stateful pod in namespace: statefulset-9065, name: ss-0, uid: a4434a30-47a9-46b7-af3a-fe5cad40ca96, status phase: Failed. Waiting for statefulset controller to delete.
  May 18 09:14:51.649: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9065
  STEP: Removing pod with conflicting port in namespace statefulset-9065 @ 05/18/23 09:14:51.649
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9065 and will be in running state @ 05/18/23 09:14:51.702
  E0518 09:14:52.089967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:53.089984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:14:53.711: INFO: Deleting all statefulset in ns statefulset-9065
  May 18 09:14:53.714: INFO: Scaling statefulset ss to 0
  E0518 09:14:54.090528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:55.090536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:56.091348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:57.091492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:58.091572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:14:59.091974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:00.091913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:01.092065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:02.093164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:03.093295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:15:03.738: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:15:03.742: INFO: Deleting statefulset ss
  May 18 09:15:03.759: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9065" for this suite. @ 05/18/23 09:15:03.767
• [14.280 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/18/23 09:15:03.794
  May 18 09:15:03.794: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubelet-test @ 05/18/23 09:15:03.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:15:03.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:15:03.826
  May 18 09:15:03.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4893" for this suite. @ 05/18/23 09:15:03.873
• [0.086 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/18/23 09:15:03.883
  May 18 09:15:03.883: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/18/23 09:15:03.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:15:03.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:15:03.906
  STEP: getting /apis @ 05/18/23 09:15:03.91
  STEP: getting /apis/storage.k8s.io @ 05/18/23 09:15:03.921
  STEP: getting /apis/storage.k8s.io/v1 @ 05/18/23 09:15:03.923
  STEP: creating @ 05/18/23 09:15:03.925
  STEP: watching @ 05/18/23 09:15:03.94
  May 18 09:15:03.940: INFO: starting watch
  STEP: getting @ 05/18/23 09:15:03.956
  STEP: listing in namespace @ 05/18/23 09:15:03.959
  STEP: listing across namespaces @ 05/18/23 09:15:03.962
  STEP: patching @ 05/18/23 09:15:03.966
  STEP: updating @ 05/18/23 09:15:03.972
  May 18 09:15:03.979: INFO: waiting for watch events with expected annotations in namespace
  May 18 09:15:03.979: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/18/23 09:15:03.979
  STEP: deleting a collection @ 05/18/23 09:15:03.992
  May 18 09:15:04.006: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-1763" for this suite. @ 05/18/23 09:15:04.012
• [0.139 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/18/23 09:15:04.024
  May 18 09:15:04.025: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename deployment @ 05/18/23 09:15:04.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:15:04.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:15:04.051
  STEP: creating a Deployment @ 05/18/23 09:15:04.062
  May 18 09:15:04.062: INFO: Creating simple deployment test-deployment-56lbm
  May 18 09:15:04.072: INFO: new replicaset for deployment "test-deployment-56lbm" is yet to be created
  E0518 09:15:04.094256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:05.094912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 05/18/23 09:15:06.09
  E0518 09:15:06.095525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:15:06.097: INFO: Deployment test-deployment-56lbm has Conditions: [{Available True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-56lbm-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/18/23 09:15:06.097
  May 18 09:15:06.113: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 15, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 15, 5, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 15, 5, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 15, 4, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-56lbm-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/18/23 09:15:06.113
  May 18 09:15:06.118: INFO: Observed &Deployment event: ADDED
  May 18 09:15:06.118: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-56lbm-5994cf9475"}
  May 18 09:15:06.119: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.119: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-56lbm-5994cf9475"}
  May 18 09:15:06.119: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 18 09:15:06.120: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.120: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 18 09:15:06.120: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-56lbm-5994cf9475" is progressing.}
  May 18 09:15:06.120: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.121: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 18 09:15:06.121: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-56lbm-5994cf9475" has successfully progressed.}
  May 18 09:15:06.121: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.122: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 18 09:15:06.123: INFO: Observed Deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-56lbm-5994cf9475" has successfully progressed.}
  May 18 09:15:06.124: INFO: Found Deployment test-deployment-56lbm in namespace deployment-4999 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 18 09:15:06.125: INFO: Deployment test-deployment-56lbm has an updated status
  STEP: patching the Statefulset Status @ 05/18/23 09:15:06.126
  May 18 09:15:06.127: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 18 09:15:06.138: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/18/23 09:15:06.139
  May 18 09:15:06.144: INFO: Observed &Deployment event: ADDED
  May 18 09:15:06.144: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-56lbm-5994cf9475"}
  May 18 09:15:06.145: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.146: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-56lbm-5994cf9475"}
  May 18 09:15:06.146: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 18 09:15:06.146: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.147: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 18 09:15:06.147: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:04 +0000 UTC 2023-05-18 09:15:04 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-56lbm-5994cf9475" is progressing.}
  May 18 09:15:06.147: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.147: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 18 09:15:06.147: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-56lbm-5994cf9475" has successfully progressed.}
  May 18 09:15:06.148: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.149: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:05 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 18 09:15:06.149: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-18 09:15:05 +0000 UTC 2023-05-18 09:15:04 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-56lbm-5994cf9475" has successfully progressed.}
  May 18 09:15:06.149: INFO: Observed deployment test-deployment-56lbm in namespace deployment-4999 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 18 09:15:06.149: INFO: Observed &Deployment event: MODIFIED
  May 18 09:15:06.149: INFO: Found deployment test-deployment-56lbm in namespace deployment-4999 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May 18 09:15:06.150: INFO: Deployment test-deployment-56lbm has a patched status
  May 18 09:15:06.154: INFO: Deployment "test-deployment-56lbm":
  &Deployment{ObjectMeta:{test-deployment-56lbm  deployment-4999  a9e86aa6-94ed-4e76-b68b-b1eaa72d929a 1931889 1 2023-05-18 09:15:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-18 09:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-18 09:15:06 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-18 09:15:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00437b558 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-56lbm-5994cf9475",LastUpdateTime:2023-05-18 09:15:06 +0000 UTC,LastTransitionTime:2023-05-18 09:15:06 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 18 09:15:06.162: INFO: New ReplicaSet "test-deployment-56lbm-5994cf9475" of Deployment "test-deployment-56lbm":
  &ReplicaSet{ObjectMeta:{test-deployment-56lbm-5994cf9475  deployment-4999  50fac4c9-a8fc-4b72-a1f7-1d3d7d5fa6c8 1931884 1 2023-05-18 09:15:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-56lbm a9e86aa6-94ed-4e76-b68b-b1eaa72d929a 0xc0046b30b7 0xc0046b30b8}] [] [{kube-controller-manager Update apps/v1 2023-05-18 09:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a9e86aa6-94ed-4e76-b68b-b1eaa72d929a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:15:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0046b3168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:15:06.168: INFO: Pod "test-deployment-56lbm-5994cf9475-htqpb" is available:
  &Pod{ObjectMeta:{test-deployment-56lbm-5994cf9475-htqpb test-deployment-56lbm-5994cf9475- deployment-4999  d8e2fc5b-6ff0-49a2-8683-c77b64320081 1931883 0 2023-05-18 09:15:04 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:79a35e02394d5294e037fbfb75fbc2fc53b56d3e1c7506575a9469aada963761 cni.projectcalico.org/podIP:172.16.161.75/32 cni.projectcalico.org/podIPs:172.16.161.75/32] [{apps/v1 ReplicaSet test-deployment-56lbm-5994cf9475 50fac4c9-a8fc-4b72-a1f7-1d3d7d5fa6c8 0xc0046b3547 0xc0046b3548}] [] [{calico Update v1 2023-05-18 09:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-18 09:15:04 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50fac4c9-a8fc-4b72-a1f7-1d3d7d5fa6c8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-18 09:15:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.75\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9qcb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9qcb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:15:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:15:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:15:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:15:04 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.75,StartTime:2023-05-18 09:15:04 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:15:05 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://206c4da77be597b31be20e19d1da176a6296aacc88c7b597f7e391b350c843db,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.75,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:15:06.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4999" for this suite. @ 05/18/23 09:15:06.177
• [2.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/18/23 09:15:06.19
  May 18 09:15:06.190: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename security-context @ 05/18/23 09:15:06.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:15:06.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:15:06.219
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/18/23 09:15:06.225
  E0518 09:15:07.096310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:08.096464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:09.097061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:10.097289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:15:10.251
  May 18 09:15:10.255: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod security-context-6d60bb48-f75d-4568-80f0-f02596ffc286 container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:15:10.269
  May 18 09:15:10.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-8341" for this suite. @ 05/18/23 09:15:10.293
• [4.112 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/18/23 09:15:10.305
  May 18 09:15:10.305: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename events @ 05/18/23 09:15:10.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:15:10.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:15:10.324
  STEP: creating a test event @ 05/18/23 09:15:10.328
  STEP: listing all events in all namespaces @ 05/18/23 09:15:10.332
  STEP: patching the test event @ 05/18/23 09:15:10.335
  STEP: fetching the test event @ 05/18/23 09:15:10.34
  STEP: updating the test event @ 05/18/23 09:15:10.342
  STEP: getting the test event @ 05/18/23 09:15:10.349
  STEP: deleting the test event @ 05/18/23 09:15:10.355
  STEP: listing all events in all namespaces @ 05/18/23 09:15:10.361
  May 18 09:15:10.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8555" for this suite. @ 05/18/23 09:15:10.37
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/18/23 09:15:10.379
  May 18 09:15:10.379: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:15:10.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:15:10.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:15:10.398
  STEP: Setting up server cert @ 05/18/23 09:15:10.422
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:15:10.927
  STEP: Deploying the webhook pod @ 05/18/23 09:15:10.934
  STEP: Wait for the deployment to be ready @ 05/18/23 09:15:10.945
  May 18 09:15:10.952: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:15:11.098237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:12.099427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:15:12.961: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 15, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 15, 10, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 15, 10, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 15, 10, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:15:13.099571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:14.099778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:15:14.967
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:15:14.991
  E0518 09:15:15.100319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:15:15.991: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/18/23 09:15:15.996
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/18/23 09:15:16.022
  STEP: Creating a dummy validating-webhook-configuration object @ 05/18/23 09:15:16.049
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/18/23 09:15:16.063
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/18/23 09:15:16.075
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/18/23 09:15:16.088
  May 18 09:15:16.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0518 09:15:16.100395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-874" for this suite. @ 05/18/23 09:15:16.19
  STEP: Destroying namespace "webhook-markers-3035" for this suite. @ 05/18/23 09:15:16.208
• [5.838 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/18/23 09:15:16.237
  May 18 09:15:16.237: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/18/23 09:15:16.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:15:16.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:15:16.258
  E0518 09:15:17.100580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:18.100786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:15:18.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/18/23 09:15:18.301
  STEP: Cleaning up the configmap @ 05/18/23 09:15:18.31
  STEP: Cleaning up the pod @ 05/18/23 09:15:18.318
  STEP: Destroying namespace "emptydir-wrapper-1396" for this suite. @ 05/18/23 09:15:18.334
• [2.104 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/18/23 09:15:18.345
  May 18 09:15:18.345: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename statefulset @ 05/18/23 09:15:18.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:15:18.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:15:18.396
  STEP: Creating service test in namespace statefulset-3548 @ 05/18/23 09:15:18.403
  STEP: Creating a new StatefulSet @ 05/18/23 09:15:18.411
  May 18 09:15:18.424: INFO: Found 0 stateful pods, waiting for 3
  E0518 09:15:19.101485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:20.101655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:21.101825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:22.101919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:23.102082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:24.103006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:25.103265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:26.103991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:27.104280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:28.104562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:15:28.440: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:15:28.440: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:15:28.441: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/18/23 09:15:28.45
  May 18 09:15:28.470: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/18/23 09:15:28.47
  E0518 09:15:29.105069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:30.105357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:31.105547      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:32.106752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:33.107266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:34.108227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:35.108752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:36.109249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:37.110661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:38.110749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/18/23 09:15:38.489
  STEP: Performing a canary update @ 05/18/23 09:15:38.49
  May 18 09:15:38.513: INFO: Updating stateful set ss2
  May 18 09:15:38.523: INFO: Waiting for Pod statefulset-3548/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0518 09:15:39.110944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:40.111071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:41.111267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:42.112087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:43.112358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:44.112525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:45.112679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:46.113306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:47.113986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:48.114159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/18/23 09:15:48.535
  May 18 09:15:48.603: INFO: Found 2 stateful pods, waiting for 3
  E0518 09:15:49.115170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:50.115203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:51.115976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:52.116234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:53.117770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:54.117919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:55.118059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:56.118251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:57.119118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:15:58.119252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:15:58.611: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:15:58.611: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:15:58.611: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/18/23 09:15:58.62
  May 18 09:15:58.639: INFO: Updating stateful set ss2
  May 18 09:15:58.649: INFO: Waiting for Pod statefulset-3548/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0518 09:15:59.119760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:00.120247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:01.120495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:02.121368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:03.121511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:04.122506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:05.123660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:06.125037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:07.125444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:08.125697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:16:08.691: INFO: Updating stateful set ss2
  May 18 09:16:08.731: INFO: Waiting for StatefulSet statefulset-3548/ss2 to complete update
  May 18 09:16:08.731: INFO: Waiting for Pod statefulset-3548/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0518 09:16:09.127100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:10.127636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:11.128229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:12.129014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:13.129196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:14.129731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:15.129956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:16.130588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:17.131430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:18.131538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:16:18.742: INFO: Deleting all statefulset in ns statefulset-3548
  May 18 09:16:18.745: INFO: Scaling statefulset ss2 to 0
  E0518 09:16:19.132670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:20.133084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:21.133679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:22.134164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:23.134830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:24.135132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:25.135838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:26.136179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:27.137277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:28.137374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:16:28.766: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:16:28.769: INFO: Deleting statefulset ss2
  May 18 09:16:28.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3548" for this suite. @ 05/18/23 09:16:28.791
• [70.452 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/18/23 09:16:28.798
  May 18 09:16:28.798: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:16:28.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:16:28.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:16:28.825
  STEP: creating a secret @ 05/18/23 09:16:28.83
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/18/23 09:16:28.836
  STEP: patching the secret @ 05/18/23 09:16:28.843
  STEP: deleting the secret using a LabelSelector @ 05/18/23 09:16:28.854
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/18/23 09:16:28.864
  May 18 09:16:28.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1858" for this suite. @ 05/18/23 09:16:28.879
• [0.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/18/23 09:16:28.901
  May 18 09:16:28.901: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename statefulset @ 05/18/23 09:16:28.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:16:28.925
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:16:28.93
  STEP: Creating service test in namespace statefulset-7217 @ 05/18/23 09:16:28.933
  May 18 09:16:28.957: INFO: Found 0 stateful pods, waiting for 1
  E0518 09:16:29.137814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:30.138689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:31.139526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:32.140588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:33.141473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:34.141877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:35.142387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:36.142940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:37.143565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:38.144293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:16:38.965: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/18/23 09:16:38.974
  W0518 09:16:38.985596      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 18 09:16:39.000: INFO: Found 1 stateful pods, waiting for 2
  E0518 09:16:39.145207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:40.146397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:41.146342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:42.146953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:43.147462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:44.147716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:45.148089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:46.148330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:47.149268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:48.149351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:16:49.009: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:16:49.009: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/18/23 09:16:49.017
  STEP: Delete all of the StatefulSets @ 05/18/23 09:16:49.022
  STEP: Verify that StatefulSets have been deleted @ 05/18/23 09:16:49.03
  May 18 09:16:49.035: INFO: Deleting all statefulset in ns statefulset-7217
  May 18 09:16:49.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7217" for this suite. @ 05/18/23 09:16:49.058
• [20.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/18/23 09:16:49.077
  May 18 09:16:49.078: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:16:49.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:16:49.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:16:49.114
  STEP: Counting existing ResourceQuota @ 05/18/23 09:16:49.119
  E0518 09:16:49.149671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:50.150217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:51.150549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:52.150871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:53.151492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/18/23 09:16:54.122
  STEP: Ensuring resource quota status is calculated @ 05/18/23 09:16:54.127
  E0518 09:16:54.151735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:16:55.151894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:16:56.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4975" for this suite. @ 05/18/23 09:16:56.136
• [7.064 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/18/23 09:16:56.143
  May 18 09:16:56.143: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename svc-latency @ 05/18/23 09:16:56.144
  E0518 09:16:56.152103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:16:56.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:16:56.163
  May 18 09:16:56.167: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-665 @ 05/18/23 09:16:56.168
  I0518 09:16:56.174508      19 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-665, replica count: 1
  E0518 09:16:57.153201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:16:57.226574      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0518 09:16:58.154475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:16:58.227187      19 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 09:16:58.341: INFO: Created: latency-svc-sfnkz
  May 18 09:16:58.346: INFO: Got endpoints: latency-svc-sfnkz [18.160526ms]
  May 18 09:16:58.367: INFO: Created: latency-svc-lt22d
  May 18 09:16:58.369: INFO: Got endpoints: latency-svc-lt22d [23.245966ms]
  May 18 09:16:58.374: INFO: Created: latency-svc-w2fh8
  May 18 09:16:58.385: INFO: Got endpoints: latency-svc-w2fh8 [37.736719ms]
  May 18 09:16:58.387: INFO: Created: latency-svc-58r2d
  May 18 09:16:58.395: INFO: Got endpoints: latency-svc-58r2d [48.729157ms]
  May 18 09:16:58.402: INFO: Created: latency-svc-9zvr2
  May 18 09:16:58.410: INFO: Got endpoints: latency-svc-9zvr2 [62.691638ms]
  May 18 09:16:58.411: INFO: Created: latency-svc-bjqg4
  May 18 09:16:58.418: INFO: Got endpoints: latency-svc-bjqg4 [70.848728ms]
  May 18 09:16:58.426: INFO: Created: latency-svc-p92f9
  May 18 09:16:58.434: INFO: Got endpoints: latency-svc-p92f9 [87.03732ms]
  May 18 09:16:58.446: INFO: Created: latency-svc-nc8xn
  May 18 09:16:58.456: INFO: Got endpoints: latency-svc-nc8xn [106.325053ms]
  May 18 09:16:58.466: INFO: Created: latency-svc-m78hk
  May 18 09:16:58.471: INFO: Got endpoints: latency-svc-m78hk [121.785259ms]
  May 18 09:16:58.490: INFO: Created: latency-svc-964d6
  May 18 09:16:58.497: INFO: Got endpoints: latency-svc-964d6 [147.704048ms]
  May 18 09:16:58.500: INFO: Created: latency-svc-bs8fr
  May 18 09:16:58.515: INFO: Got endpoints: latency-svc-bs8fr [165.751478ms]
  May 18 09:16:58.522: INFO: Created: latency-svc-tthlt
  May 18 09:16:58.529: INFO: Got endpoints: latency-svc-tthlt [179.144977ms]
  May 18 09:16:58.531: INFO: Created: latency-svc-6lzzq
  May 18 09:16:58.538: INFO: Got endpoints: latency-svc-6lzzq [188.864205ms]
  May 18 09:16:58.545: INFO: Created: latency-svc-zld9m
  May 18 09:16:58.549: INFO: Got endpoints: latency-svc-zld9m [199.379623ms]
  May 18 09:16:58.550: INFO: Created: latency-svc-lzbgf
  May 18 09:16:58.559: INFO: Got endpoints: latency-svc-lzbgf [208.917401ms]
  May 18 09:16:58.561: INFO: Created: latency-svc-lllhg
  May 18 09:16:58.567: INFO: Got endpoints: latency-svc-lllhg [217.64985ms]
  May 18 09:16:58.571: INFO: Created: latency-svc-g8qcj
  May 18 09:16:58.576: INFO: Got endpoints: latency-svc-g8qcj [206.039498ms]
  May 18 09:16:58.582: INFO: Created: latency-svc-852f7
  May 18 09:16:58.583: INFO: Got endpoints: latency-svc-852f7 [197.551763ms]
  May 18 09:16:58.590: INFO: Created: latency-svc-4zhll
  May 18 09:16:58.597: INFO: Got endpoints: latency-svc-4zhll [200.625098ms]
  May 18 09:16:58.599: INFO: Created: latency-svc-zxrxd
  May 18 09:16:58.606: INFO: Got endpoints: latency-svc-zxrxd [194.607823ms]
  May 18 09:16:58.607: INFO: Created: latency-svc-f9lsw
  May 18 09:16:58.615: INFO: Created: latency-svc-f72k2
  May 18 09:16:58.616: INFO: Got endpoints: latency-svc-f9lsw [196.239472ms]
  May 18 09:16:58.619: INFO: Got endpoints: latency-svc-f72k2 [184.188227ms]
  May 18 09:16:58.625: INFO: Created: latency-svc-rwkct
  May 18 09:16:58.636: INFO: Got endpoints: latency-svc-rwkct [179.127085ms]
  May 18 09:16:58.639: INFO: Created: latency-svc-gm28s
  May 18 09:16:58.643: INFO: Got endpoints: latency-svc-gm28s [151.386541ms]
  May 18 09:16:58.651: INFO: Created: latency-svc-wmxql
  May 18 09:16:58.655: INFO: Got endpoints: latency-svc-wmxql [157.687682ms]
  May 18 09:16:58.765: INFO: Created: latency-svc-55xgb
  May 18 09:16:58.765: INFO: Created: latency-svc-8hg95
  May 18 09:16:58.769: INFO: Created: latency-svc-77jx6
  May 18 09:16:58.786: INFO: Created: latency-svc-bcb8s
  May 18 09:16:58.788: INFO: Created: latency-svc-mfpxx
  May 18 09:16:58.791: INFO: Created: latency-svc-4tn49
  May 18 09:16:58.791: INFO: Created: latency-svc-hvkv5
  May 18 09:16:58.790: INFO: Created: latency-svc-jlknf
  May 18 09:16:58.793: INFO: Created: latency-svc-xtj6l
  May 18 09:16:58.794: INFO: Got endpoints: latency-svc-77jx6 [218.533842ms]
  May 18 09:16:58.787: INFO: Created: latency-svc-kr6q9
  May 18 09:16:58.795: INFO: Created: latency-svc-xkr4t
  May 18 09:16:58.793: INFO: Created: latency-svc-nsv2h
  May 18 09:16:58.789: INFO: Created: latency-svc-dgk29
  May 18 09:16:58.789: INFO: Created: latency-svc-tvrm6
  May 18 09:16:58.795: INFO: Created: latency-svc-shfbm
  May 18 09:16:58.800: INFO: Got endpoints: latency-svc-shfbm [250.393846ms]
  May 18 09:16:58.797: INFO: Got endpoints: latency-svc-4tn49 [238.238995ms]
  May 18 09:16:58.797: INFO: Got endpoints: latency-svc-xtj6l [200.405776ms]
  May 18 09:16:58.797: INFO: Got endpoints: latency-svc-hvkv5 [258.691199ms]
  May 18 09:16:58.802: INFO: Got endpoints: latency-svc-jlknf [195.399008ms]
  May 18 09:16:58.802: INFO: Got endpoints: latency-svc-xkr4t [166.285939ms]
  May 18 09:16:58.797: INFO: Got endpoints: latency-svc-55xgb [229.433612ms]
  May 18 09:16:58.797: INFO: Got endpoints: latency-svc-8hg95 [178.499968ms]
  May 18 09:16:58.800: INFO: Got endpoints: latency-svc-tvrm6 [184.003011ms]
  May 18 09:16:58.807: INFO: Got endpoints: latency-svc-nsv2h [291.566315ms]
  May 18 09:16:58.809: INFO: Got endpoints: latency-svc-dgk29 [165.454738ms]
  May 18 09:16:58.809: INFO: Got endpoints: latency-svc-mfpxx [153.223359ms]
  May 18 09:16:58.810: INFO: Got endpoints: latency-svc-bcb8s [280.82371ms]
  May 18 09:16:58.815: INFO: Created: latency-svc-66p26
  May 18 09:16:58.827: INFO: Created: latency-svc-lmgb4
  May 18 09:16:58.833: INFO: Created: latency-svc-8gxsr
  May 18 09:16:58.842: INFO: Created: latency-svc-dpbht
  May 18 09:16:58.849: INFO: Got endpoints: latency-svc-kr6q9 [265.68751ms]
  May 18 09:16:58.854: INFO: Created: latency-svc-bpt4h
  May 18 09:16:58.868: INFO: Created: latency-svc-4bjrg
  May 18 09:16:58.869: INFO: Created: latency-svc-m95m8
  May 18 09:16:58.879: INFO: Created: latency-svc-t77sq
  May 18 09:16:58.890: INFO: Created: latency-svc-jvnfh
  May 18 09:16:58.901: INFO: Got endpoints: latency-svc-66p26 [104.880414ms]
  May 18 09:16:58.906: INFO: Created: latency-svc-tl5vl
  May 18 09:16:58.915: INFO: Created: latency-svc-5dvhq
  May 18 09:16:58.923: INFO: Created: latency-svc-nxt9l
  May 18 09:16:58.928: INFO: Created: latency-svc-scfxf
  May 18 09:16:58.936: INFO: Created: latency-svc-6gvzq
  May 18 09:16:58.941: INFO: Created: latency-svc-66rvh
  May 18 09:16:58.955: INFO: Got endpoints: latency-svc-lmgb4 [154.621896ms]
  May 18 09:16:58.956: INFO: Created: latency-svc-c7n2p
  May 18 09:16:58.969: INFO: Created: latency-svc-q2fg8
  May 18 09:16:58.998: INFO: Got endpoints: latency-svc-8gxsr [196.982163ms]
  May 18 09:16:59.008: INFO: Created: latency-svc-jlxf4
  May 18 09:16:59.046: INFO: Got endpoints: latency-svc-dpbht [242.998965ms]
  May 18 09:16:59.063: INFO: Created: latency-svc-bc75h
  May 18 09:16:59.095: INFO: Got endpoints: latency-svc-bpt4h [293.058713ms]
  May 18 09:16:59.109: INFO: Created: latency-svc-zg4hd
  May 18 09:16:59.149: INFO: Got endpoints: latency-svc-4bjrg [347.605437ms]
  E0518 09:16:59.154852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:16:59.168: INFO: Created: latency-svc-tswnp
  May 18 09:16:59.195: INFO: Got endpoints: latency-svc-m95m8 [391.610906ms]
  May 18 09:16:59.221: INFO: Created: latency-svc-gvptt
  May 18 09:16:59.249: INFO: Got endpoints: latency-svc-t77sq [447.031523ms]
  May 18 09:16:59.266: INFO: Created: latency-svc-bb7gg
  May 18 09:16:59.296: INFO: Got endpoints: latency-svc-jvnfh [495.476221ms]
  May 18 09:16:59.315: INFO: Created: latency-svc-7hsdp
  May 18 09:16:59.347: INFO: Got endpoints: latency-svc-tl5vl [544.139778ms]
  May 18 09:16:59.375: INFO: Created: latency-svc-8fkd4
  May 18 09:16:59.400: INFO: Got endpoints: latency-svc-5dvhq [592.886336ms]
  May 18 09:16:59.420: INFO: Created: latency-svc-wx5jc
  May 18 09:16:59.448: INFO: Got endpoints: latency-svc-nxt9l [638.346708ms]
  May 18 09:16:59.463: INFO: Created: latency-svc-rghd7
  May 18 09:16:59.498: INFO: Got endpoints: latency-svc-scfxf [688.773239ms]
  May 18 09:16:59.514: INFO: Created: latency-svc-hvc95
  May 18 09:16:59.546: INFO: Got endpoints: latency-svc-6gvzq [734.645303ms]
  May 18 09:16:59.569: INFO: Created: latency-svc-8q4qw
  May 18 09:16:59.598: INFO: Got endpoints: latency-svc-66rvh [748.854753ms]
  May 18 09:16:59.612: INFO: Created: latency-svc-kmffk
  May 18 09:16:59.646: INFO: Got endpoints: latency-svc-c7n2p [744.741341ms]
  May 18 09:16:59.665: INFO: Created: latency-svc-v2x4m
  May 18 09:16:59.696: INFO: Got endpoints: latency-svc-q2fg8 [741.01739ms]
  May 18 09:16:59.709: INFO: Created: latency-svc-hfhhf
  May 18 09:16:59.746: INFO: Got endpoints: latency-svc-jlxf4 [747.819381ms]
  May 18 09:16:59.758: INFO: Created: latency-svc-wbfb4
  May 18 09:16:59.796: INFO: Got endpoints: latency-svc-bc75h [749.35049ms]
  May 18 09:16:59.809: INFO: Created: latency-svc-fn652
  May 18 09:16:59.845: INFO: Got endpoints: latency-svc-zg4hd [749.11196ms]
  May 18 09:16:59.861: INFO: Created: latency-svc-7p5dc
  May 18 09:16:59.895: INFO: Got endpoints: latency-svc-tswnp [745.95238ms]
  May 18 09:16:59.909: INFO: Created: latency-svc-f5gzd
  May 18 09:16:59.946: INFO: Got endpoints: latency-svc-gvptt [750.966882ms]
  May 18 09:16:59.959: INFO: Created: latency-svc-885lv
  May 18 09:17:00.003: INFO: Got endpoints: latency-svc-bb7gg [753.550223ms]
  May 18 09:17:00.020: INFO: Created: latency-svc-g7c58
  May 18 09:17:00.049: INFO: Got endpoints: latency-svc-7hsdp [753.139292ms]
  May 18 09:17:00.068: INFO: Created: latency-svc-8qp86
  May 18 09:17:00.098: INFO: Got endpoints: latency-svc-8fkd4 [750.920462ms]
  May 18 09:17:00.117: INFO: Created: latency-svc-4jfjc
  May 18 09:17:00.148: INFO: Got endpoints: latency-svc-wx5jc [747.599043ms]
  E0518 09:17:00.155967      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:17:00.163: INFO: Created: latency-svc-98bb4
  May 18 09:17:00.196: INFO: Got endpoints: latency-svc-rghd7 [748.367394ms]
  May 18 09:17:00.215: INFO: Created: latency-svc-rxhfp
  May 18 09:17:00.246: INFO: Got endpoints: latency-svc-hvc95 [747.806699ms]
  May 18 09:17:00.268: INFO: Created: latency-svc-9nn8p
  May 18 09:17:00.297: INFO: Got endpoints: latency-svc-8q4qw [751.91334ms]
  May 18 09:17:00.317: INFO: Created: latency-svc-rprsc
  May 18 09:17:00.354: INFO: Got endpoints: latency-svc-kmffk [755.698485ms]
  May 18 09:17:00.381: INFO: Created: latency-svc-xvbjc
  May 18 09:17:00.396: INFO: Got endpoints: latency-svc-v2x4m [750.348423ms]
  May 18 09:17:00.412: INFO: Created: latency-svc-h2km4
  May 18 09:17:00.448: INFO: Got endpoints: latency-svc-hfhhf [752.200147ms]
  May 18 09:17:00.465: INFO: Created: latency-svc-kngrt
  May 18 09:17:00.498: INFO: Got endpoints: latency-svc-wbfb4 [751.893568ms]
  May 18 09:17:00.512: INFO: Created: latency-svc-4dm5g
  May 18 09:17:00.547: INFO: Got endpoints: latency-svc-fn652 [750.616728ms]
  May 18 09:17:00.567: INFO: Created: latency-svc-gmz8z
  May 18 09:17:00.599: INFO: Got endpoints: latency-svc-7p5dc [754.732394ms]
  May 18 09:17:00.618: INFO: Created: latency-svc-lx2tx
  May 18 09:17:00.647: INFO: Got endpoints: latency-svc-f5gzd [751.164533ms]
  May 18 09:17:00.664: INFO: Created: latency-svc-wdzcp
  May 18 09:17:00.700: INFO: Got endpoints: latency-svc-885lv [753.086345ms]
  May 18 09:17:00.724: INFO: Created: latency-svc-2nxhm
  May 18 09:17:00.747: INFO: Got endpoints: latency-svc-g7c58 [743.311804ms]
  May 18 09:17:00.762: INFO: Created: latency-svc-sxrvd
  May 18 09:17:00.799: INFO: Got endpoints: latency-svc-8qp86 [749.785992ms]
  May 18 09:17:00.828: INFO: Created: latency-svc-xtxpm
  May 18 09:17:00.846: INFO: Got endpoints: latency-svc-4jfjc [747.427525ms]
  May 18 09:17:00.861: INFO: Created: latency-svc-z8tns
  May 18 09:17:00.898: INFO: Got endpoints: latency-svc-98bb4 [749.038341ms]
  May 18 09:17:00.914: INFO: Created: latency-svc-kvzxn
  May 18 09:17:00.955: INFO: Got endpoints: latency-svc-rxhfp [758.756262ms]
  May 18 09:17:00.977: INFO: Created: latency-svc-tgnnw
  May 18 09:17:00.996: INFO: Got endpoints: latency-svc-9nn8p [749.101732ms]
  May 18 09:17:01.018: INFO: Created: latency-svc-7m9ml
  May 18 09:17:01.050: INFO: Got endpoints: latency-svc-rprsc [752.503992ms]
  May 18 09:17:01.066: INFO: Created: latency-svc-mdkqc
  May 18 09:17:01.099: INFO: Got endpoints: latency-svc-xvbjc [744.05694ms]
  May 18 09:17:01.113: INFO: Created: latency-svc-cbfjx
  May 18 09:17:01.148: INFO: Got endpoints: latency-svc-h2km4 [750.825122ms]
  E0518 09:17:01.156508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:17:01.180: INFO: Created: latency-svc-qsrbp
  May 18 09:17:01.198: INFO: Got endpoints: latency-svc-kngrt [748.897888ms]
  May 18 09:17:01.214: INFO: Created: latency-svc-pscn2
  May 18 09:17:01.249: INFO: Got endpoints: latency-svc-4dm5g [751.012681ms]
  May 18 09:17:01.268: INFO: Created: latency-svc-mztkh
  May 18 09:17:01.296: INFO: Got endpoints: latency-svc-gmz8z [749.117102ms]
  May 18 09:17:01.313: INFO: Created: latency-svc-jvb9p
  May 18 09:17:01.345: INFO: Got endpoints: latency-svc-lx2tx [745.834964ms]
  May 18 09:17:01.366: INFO: Created: latency-svc-nfc95
  May 18 09:17:01.395: INFO: Got endpoints: latency-svc-wdzcp [748.757489ms]
  May 18 09:17:01.410: INFO: Created: latency-svc-cw472
  May 18 09:17:01.446: INFO: Got endpoints: latency-svc-2nxhm [745.094158ms]
  May 18 09:17:01.460: INFO: Created: latency-svc-2q2lq
  May 18 09:17:01.494: INFO: Got endpoints: latency-svc-sxrvd [747.577387ms]
  May 18 09:17:01.506: INFO: Created: latency-svc-7j7cq
  May 18 09:17:01.548: INFO: Got endpoints: latency-svc-xtxpm [747.751754ms]
  May 18 09:17:01.561: INFO: Created: latency-svc-8llvw
  May 18 09:17:01.597: INFO: Got endpoints: latency-svc-z8tns [751.09475ms]
  May 18 09:17:01.611: INFO: Created: latency-svc-6spz4
  May 18 09:17:01.645: INFO: Got endpoints: latency-svc-kvzxn [746.693295ms]
  May 18 09:17:01.659: INFO: Created: latency-svc-lq7l4
  May 18 09:17:01.696: INFO: Got endpoints: latency-svc-tgnnw [739.494353ms]
  May 18 09:17:01.711: INFO: Created: latency-svc-whntj
  May 18 09:17:01.745: INFO: Got endpoints: latency-svc-7m9ml [747.653724ms]
  May 18 09:17:01.764: INFO: Created: latency-svc-z5h4w
  May 18 09:17:01.814: INFO: Got endpoints: latency-svc-mdkqc [763.624838ms]
  May 18 09:17:01.901: INFO: Got endpoints: latency-svc-cbfjx [802.206716ms]
  May 18 09:17:01.932: INFO: Got endpoints: latency-svc-qsrbp [783.844623ms]
  May 18 09:17:01.951: INFO: Got endpoints: latency-svc-pscn2 [751.768524ms]
  May 18 09:17:01.957: INFO: Created: latency-svc-nhmql
  May 18 09:17:01.988: INFO: Created: latency-svc-6kpsj
  May 18 09:17:02.001: INFO: Created: latency-svc-qn898
  May 18 09:17:02.002: INFO: Got endpoints: latency-svc-mztkh [751.987501ms]
  May 18 09:17:02.008: INFO: Created: latency-svc-gcfvq
  May 18 09:17:02.017: INFO: Created: latency-svc-vf7hg
  May 18 09:17:02.045: INFO: Got endpoints: latency-svc-jvb9p [747.859167ms]
  May 18 09:17:02.057: INFO: Created: latency-svc-s9bg4
  May 18 09:17:02.095: INFO: Got endpoints: latency-svc-nfc95 [747.161818ms]
  May 18 09:17:02.107: INFO: Created: latency-svc-7flzt
  May 18 09:17:02.145: INFO: Got endpoints: latency-svc-cw472 [748.057083ms]
  E0518 09:17:02.157203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:17:02.159: INFO: Created: latency-svc-hr49r
  May 18 09:17:02.196: INFO: Got endpoints: latency-svc-2q2lq [748.50827ms]
  May 18 09:17:02.211: INFO: Created: latency-svc-4hdxg
  May 18 09:17:02.247: INFO: Got endpoints: latency-svc-7j7cq [753.223332ms]
  May 18 09:17:02.261: INFO: Created: latency-svc-sqpkt
  May 18 09:17:02.296: INFO: Got endpoints: latency-svc-8llvw [747.333001ms]
  May 18 09:17:02.311: INFO: Created: latency-svc-9ghvj
  May 18 09:17:02.346: INFO: Got endpoints: latency-svc-6spz4 [748.323528ms]
  May 18 09:17:02.364: INFO: Created: latency-svc-dsr8n
  May 18 09:17:02.396: INFO: Got endpoints: latency-svc-lq7l4 [751.005275ms]
  May 18 09:17:02.410: INFO: Created: latency-svc-4xvx8
  May 18 09:17:02.445: INFO: Got endpoints: latency-svc-whntj [749.294224ms]
  May 18 09:17:02.457: INFO: Created: latency-svc-2cwql
  May 18 09:17:02.499: INFO: Got endpoints: latency-svc-z5h4w [752.699633ms]
  May 18 09:17:02.512: INFO: Created: latency-svc-tmlch
  May 18 09:17:02.546: INFO: Got endpoints: latency-svc-nhmql [731.735921ms]
  May 18 09:17:02.562: INFO: Created: latency-svc-rhhqw
  May 18 09:17:02.596: INFO: Got endpoints: latency-svc-6kpsj [694.565122ms]
  May 18 09:17:02.610: INFO: Created: latency-svc-mcltm
  May 18 09:17:02.646: INFO: Got endpoints: latency-svc-qn898 [714.055279ms]
  May 18 09:17:02.662: INFO: Created: latency-svc-sqb8b
  May 18 09:17:02.696: INFO: Got endpoints: latency-svc-gcfvq [745.368354ms]
  May 18 09:17:02.709: INFO: Created: latency-svc-824sb
  May 18 09:17:02.747: INFO: Got endpoints: latency-svc-vf7hg [744.097145ms]
  May 18 09:17:02.764: INFO: Created: latency-svc-pqcgf
  May 18 09:17:02.799: INFO: Got endpoints: latency-svc-s9bg4 [753.679569ms]
  May 18 09:17:02.819: INFO: Created: latency-svc-tvgdq
  May 18 09:17:02.849: INFO: Got endpoints: latency-svc-7flzt [754.077625ms]
  May 18 09:17:02.870: INFO: Created: latency-svc-w8nwb
  May 18 09:17:02.898: INFO: Got endpoints: latency-svc-hr49r [751.419592ms]
  May 18 09:17:02.921: INFO: Created: latency-svc-dfc9m
  May 18 09:17:02.953: INFO: Got endpoints: latency-svc-4hdxg [756.85293ms]
  May 18 09:17:02.974: INFO: Created: latency-svc-nkgl2
  May 18 09:17:03.000: INFO: Got endpoints: latency-svc-sqpkt [752.047908ms]
  May 18 09:17:03.024: INFO: Created: latency-svc-v7qwp
  May 18 09:17:03.048: INFO: Got endpoints: latency-svc-9ghvj [750.554766ms]
  May 18 09:17:03.072: INFO: Created: latency-svc-zbs8j
  May 18 09:17:03.101: INFO: Got endpoints: latency-svc-dsr8n [754.28248ms]
  May 18 09:17:03.146: INFO: Created: latency-svc-xxjwl
  May 18 09:17:03.149: INFO: Got endpoints: latency-svc-4xvx8 [751.493629ms]
  E0518 09:17:03.156992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:17:03.166: INFO: Created: latency-svc-2g4pm
  May 18 09:17:03.196: INFO: Got endpoints: latency-svc-2cwql [751.149036ms]
  May 18 09:17:03.213: INFO: Created: latency-svc-x9dfr
  May 18 09:17:03.250: INFO: Got endpoints: latency-svc-tmlch [751.138599ms]
  May 18 09:17:03.266: INFO: Created: latency-svc-jl4vv
  May 18 09:17:03.296: INFO: Got endpoints: latency-svc-rhhqw [749.894396ms]
  May 18 09:17:03.313: INFO: Created: latency-svc-p5wl2
  May 18 09:17:03.348: INFO: Got endpoints: latency-svc-mcltm [752.525316ms]
  May 18 09:17:03.363: INFO: Created: latency-svc-sg75p
  May 18 09:17:03.398: INFO: Got endpoints: latency-svc-sqb8b [751.413372ms]
  May 18 09:17:03.413: INFO: Created: latency-svc-7zdtl
  May 18 09:17:03.447: INFO: Got endpoints: latency-svc-824sb [750.679782ms]
  May 18 09:17:03.464: INFO: Created: latency-svc-d66bl
  May 18 09:17:03.498: INFO: Got endpoints: latency-svc-pqcgf [750.606706ms]
  May 18 09:17:03.513: INFO: Created: latency-svc-gbxx4
  May 18 09:17:03.551: INFO: Got endpoints: latency-svc-tvgdq [752.148079ms]
  May 18 09:17:03.572: INFO: Created: latency-svc-lw8jd
  May 18 09:17:03.598: INFO: Got endpoints: latency-svc-w8nwb [749.563576ms]
  May 18 09:17:03.615: INFO: Created: latency-svc-bjcgw
  May 18 09:17:03.648: INFO: Got endpoints: latency-svc-dfc9m [749.11647ms]
  May 18 09:17:03.668: INFO: Created: latency-svc-s9b6j
  May 18 09:17:03.697: INFO: Got endpoints: latency-svc-nkgl2 [743.176265ms]
  May 18 09:17:03.713: INFO: Created: latency-svc-2xswp
  May 18 09:17:03.750: INFO: Got endpoints: latency-svc-v7qwp [750.769461ms]
  May 18 09:17:03.773: INFO: Created: latency-svc-7fjnf
  May 18 09:17:03.800: INFO: Got endpoints: latency-svc-zbs8j [751.438539ms]
  May 18 09:17:03.818: INFO: Created: latency-svc-8g4xk
  May 18 09:17:03.849: INFO: Got endpoints: latency-svc-xxjwl [747.268376ms]
  May 18 09:17:03.865: INFO: Created: latency-svc-zkmkg
  May 18 09:17:03.901: INFO: Got endpoints: latency-svc-2g4pm [751.510286ms]
  May 18 09:17:03.923: INFO: Created: latency-svc-4nw89
  May 18 09:17:03.954: INFO: Got endpoints: latency-svc-x9dfr [757.059463ms]
  May 18 09:17:03.970: INFO: Created: latency-svc-898cr
  May 18 09:17:04.000: INFO: Got endpoints: latency-svc-jl4vv [749.937915ms]
  May 18 09:17:04.017: INFO: Created: latency-svc-6s5p9
  May 18 09:17:04.047: INFO: Got endpoints: latency-svc-p5wl2 [749.598426ms]
  May 18 09:17:04.064: INFO: Created: latency-svc-t777v
  May 18 09:17:04.096: INFO: Got endpoints: latency-svc-sg75p [747.254781ms]
  May 18 09:17:04.114: INFO: Created: latency-svc-xtfq8
  May 18 09:17:04.151: INFO: Got endpoints: latency-svc-7zdtl [751.620276ms]
  E0518 09:17:04.158337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:17:04.169: INFO: Created: latency-svc-dmntm
  May 18 09:17:04.197: INFO: Got endpoints: latency-svc-d66bl [748.064663ms]
  May 18 09:17:04.212: INFO: Created: latency-svc-kfsnk
  May 18 09:17:04.250: INFO: Got endpoints: latency-svc-gbxx4 [750.828011ms]
  May 18 09:17:04.269: INFO: Created: latency-svc-qrxbr
  May 18 09:17:04.297: INFO: Got endpoints: latency-svc-lw8jd [744.189402ms]
  May 18 09:17:04.312: INFO: Created: latency-svc-gjj5c
  May 18 09:17:04.347: INFO: Got endpoints: latency-svc-bjcgw [747.39977ms]
  May 18 09:17:04.365: INFO: Created: latency-svc-th8f6
  May 18 09:17:04.405: INFO: Got endpoints: latency-svc-s9b6j [756.331097ms]
  May 18 09:17:04.423: INFO: Created: latency-svc-b2hf4
  May 18 09:17:04.446: INFO: Got endpoints: latency-svc-2xswp [747.041666ms]
  May 18 09:17:04.461: INFO: Created: latency-svc-jvvqb
  May 18 09:17:04.497: INFO: Got endpoints: latency-svc-7fjnf [745.326358ms]
  May 18 09:17:04.514: INFO: Created: latency-svc-pk6nn
  May 18 09:17:04.550: INFO: Got endpoints: latency-svc-8g4xk [749.865804ms]
  May 18 09:17:04.581: INFO: Created: latency-svc-z24lb
  May 18 09:17:04.596: INFO: Got endpoints: latency-svc-zkmkg [746.961784ms]
  May 18 09:17:04.613: INFO: Created: latency-svc-28f84
  May 18 09:17:04.646: INFO: Got endpoints: latency-svc-4nw89 [745.146734ms]
  May 18 09:17:04.666: INFO: Created: latency-svc-9jm2n
  May 18 09:17:04.697: INFO: Got endpoints: latency-svc-898cr [742.491241ms]
  May 18 09:17:04.714: INFO: Created: latency-svc-2brw9
  May 18 09:17:04.751: INFO: Got endpoints: latency-svc-6s5p9 [750.478635ms]
  May 18 09:17:04.774: INFO: Created: latency-svc-hnbjj
  May 18 09:17:04.800: INFO: Got endpoints: latency-svc-t777v [751.678849ms]
  May 18 09:17:04.819: INFO: Created: latency-svc-l26lt
  May 18 09:17:04.847: INFO: Got endpoints: latency-svc-xtfq8 [751.232275ms]
  May 18 09:17:04.863: INFO: Created: latency-svc-k2ths
  May 18 09:17:04.900: INFO: Got endpoints: latency-svc-dmntm [748.795208ms]
  May 18 09:17:04.917: INFO: Created: latency-svc-mpvlc
  May 18 09:17:04.947: INFO: Got endpoints: latency-svc-kfsnk [748.937194ms]
  May 18 09:17:04.962: INFO: Created: latency-svc-s48pg
  May 18 09:17:04.996: INFO: Got endpoints: latency-svc-qrxbr [745.375185ms]
  May 18 09:17:05.014: INFO: Created: latency-svc-psv8m
  May 18 09:17:05.048: INFO: Got endpoints: latency-svc-gjj5c [750.439919ms]
  May 18 09:17:05.067: INFO: Created: latency-svc-hntws
  May 18 09:17:05.097: INFO: Got endpoints: latency-svc-th8f6 [749.881687ms]
  May 18 09:17:05.116: INFO: Created: latency-svc-fq7nj
  May 18 09:17:05.150: INFO: Got endpoints: latency-svc-b2hf4 [744.692451ms]
  E0518 09:17:05.158419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:17:05.167: INFO: Created: latency-svc-24wk2
  May 18 09:17:05.198: INFO: Got endpoints: latency-svc-jvvqb [751.641678ms]
  May 18 09:17:05.213: INFO: Created: latency-svc-m7s5j
  May 18 09:17:05.247: INFO: Got endpoints: latency-svc-pk6nn [750.395498ms]
  May 18 09:17:05.262: INFO: Created: latency-svc-m75k5
  May 18 09:17:05.297: INFO: Got endpoints: latency-svc-z24lb [746.732038ms]
  May 18 09:17:05.315: INFO: Created: latency-svc-4d4gm
  May 18 09:17:05.349: INFO: Got endpoints: latency-svc-28f84 [752.057185ms]
  May 18 09:17:05.363: INFO: Created: latency-svc-hxgz5
  May 18 09:17:05.399: INFO: Got endpoints: latency-svc-9jm2n [751.164739ms]
  May 18 09:17:05.415: INFO: Created: latency-svc-crhj9
  May 18 09:17:05.451: INFO: Got endpoints: latency-svc-2brw9 [753.98442ms]
  May 18 09:17:05.466: INFO: Created: latency-svc-ss5m4
  May 18 09:17:05.500: INFO: Got endpoints: latency-svc-hnbjj [748.338975ms]
  May 18 09:17:05.516: INFO: Created: latency-svc-tmcft
  May 18 09:17:05.547: INFO: Got endpoints: latency-svc-l26lt [746.161503ms]
  May 18 09:17:05.561: INFO: Created: latency-svc-4hbwt
  May 18 09:17:05.595: INFO: Got endpoints: latency-svc-k2ths [747.991466ms]
  May 18 09:17:05.610: INFO: Created: latency-svc-qtg6z
  May 18 09:17:05.647: INFO: Got endpoints: latency-svc-mpvlc [746.334283ms]
  May 18 09:17:05.667: INFO: Created: latency-svc-58c2x
  May 18 09:17:05.697: INFO: Got endpoints: latency-svc-s48pg [748.914966ms]
  May 18 09:17:05.711: INFO: Created: latency-svc-fw569
  May 18 09:17:05.747: INFO: Got endpoints: latency-svc-psv8m [749.470666ms]
  May 18 09:17:05.759: INFO: Created: latency-svc-xxh47
  May 18 09:17:05.796: INFO: Got endpoints: latency-svc-hntws [746.795043ms]
  May 18 09:17:05.811: INFO: Created: latency-svc-bw68v
  May 18 09:17:05.847: INFO: Got endpoints: latency-svc-fq7nj [748.361729ms]
  May 18 09:17:05.865: INFO: Created: latency-svc-fs9xf
  May 18 09:17:05.897: INFO: Got endpoints: latency-svc-24wk2 [747.572419ms]
  May 18 09:17:05.915: INFO: Created: latency-svc-8nbwx
  May 18 09:17:05.947: INFO: Got endpoints: latency-svc-m7s5j [749.037615ms]
  May 18 09:17:05.965: INFO: Created: latency-svc-xbwf8
  May 18 09:17:05.996: INFO: Got endpoints: latency-svc-m75k5 [748.935099ms]
  May 18 09:17:06.012: INFO: Created: latency-svc-kzzds
  May 18 09:17:06.055: INFO: Got endpoints: latency-svc-4d4gm [758.147431ms]
  May 18 09:17:06.076: INFO: Created: latency-svc-ddpzg
  May 18 09:17:06.101: INFO: Got endpoints: latency-svc-hxgz5 [751.077864ms]
  May 18 09:17:06.126: INFO: Created: latency-svc-2x7lw
  May 18 09:17:06.148: INFO: Got endpoints: latency-svc-crhj9 [747.441167ms]
  E0518 09:17:06.159376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:17:06.166: INFO: Created: latency-svc-x4qw7
  May 18 09:17:06.197: INFO: Got endpoints: latency-svc-ss5m4 [746.076075ms]
  May 18 09:17:06.248: INFO: Got endpoints: latency-svc-tmcft [746.718765ms]
  May 18 09:17:06.297: INFO: Got endpoints: latency-svc-4hbwt [749.916071ms]
  May 18 09:17:06.352: INFO: Got endpoints: latency-svc-qtg6z [756.1309ms]
  May 18 09:17:06.401: INFO: Got endpoints: latency-svc-58c2x [754.234452ms]
  May 18 09:17:06.448: INFO: Got endpoints: latency-svc-fw569 [750.889569ms]
  May 18 09:17:06.498: INFO: Got endpoints: latency-svc-xxh47 [749.890933ms]
  May 18 09:17:06.548: INFO: Got endpoints: latency-svc-bw68v [750.563633ms]
  May 18 09:17:06.598: INFO: Got endpoints: latency-svc-fs9xf [750.427572ms]
  May 18 09:17:06.648: INFO: Got endpoints: latency-svc-8nbwx [747.68469ms]
  May 18 09:17:06.705: INFO: Got endpoints: latency-svc-xbwf8 [756.498618ms]
  May 18 09:17:06.751: INFO: Got endpoints: latency-svc-kzzds [754.577105ms]
  May 18 09:17:06.797: INFO: Got endpoints: latency-svc-ddpzg [740.994256ms]
  May 18 09:17:06.850: INFO: Got endpoints: latency-svc-2x7lw [748.588837ms]
  May 18 09:17:06.899: INFO: Got endpoints: latency-svc-x4qw7 [750.174011ms]
  May 18 09:17:06.900: INFO: Latencies: [23.245966ms 37.736719ms 48.729157ms 62.691638ms 70.848728ms 87.03732ms 104.880414ms 106.325053ms 121.785259ms 147.704048ms 151.386541ms 153.223359ms 154.621896ms 157.687682ms 165.454738ms 165.751478ms 166.285939ms 178.499968ms 179.127085ms 179.144977ms 184.003011ms 184.188227ms 188.864205ms 194.607823ms 195.399008ms 196.239472ms 196.982163ms 197.551763ms 199.379623ms 200.405776ms 200.625098ms 206.039498ms 208.917401ms 217.64985ms 218.533842ms 229.433612ms 238.238995ms 242.998965ms 250.393846ms 258.691199ms 265.68751ms 280.82371ms 291.566315ms 293.058713ms 347.605437ms 391.610906ms 447.031523ms 495.476221ms 544.139778ms 592.886336ms 638.346708ms 688.773239ms 694.565122ms 714.055279ms 731.735921ms 734.645303ms 739.494353ms 740.994256ms 741.01739ms 742.491241ms 743.176265ms 743.311804ms 744.05694ms 744.097145ms 744.189402ms 744.692451ms 744.741341ms 745.094158ms 745.146734ms 745.326358ms 745.368354ms 745.375185ms 745.834964ms 745.95238ms 746.076075ms 746.161503ms 746.334283ms 746.693295ms 746.718765ms 746.732038ms 746.795043ms 746.961784ms 747.041666ms 747.161818ms 747.254781ms 747.268376ms 747.333001ms 747.39977ms 747.427525ms 747.441167ms 747.572419ms 747.577387ms 747.599043ms 747.653724ms 747.68469ms 747.751754ms 747.806699ms 747.819381ms 747.859167ms 747.991466ms 748.057083ms 748.064663ms 748.323528ms 748.338975ms 748.361729ms 748.367394ms 748.50827ms 748.588837ms 748.757489ms 748.795208ms 748.854753ms 748.897888ms 748.914966ms 748.935099ms 748.937194ms 749.037615ms 749.038341ms 749.101732ms 749.11196ms 749.11647ms 749.117102ms 749.294224ms 749.35049ms 749.470666ms 749.563576ms 749.598426ms 749.785992ms 749.865804ms 749.881687ms 749.890933ms 749.894396ms 749.916071ms 749.937915ms 750.174011ms 750.348423ms 750.395498ms 750.427572ms 750.439919ms 750.478635ms 750.554766ms 750.563633ms 750.606706ms 750.616728ms 750.679782ms 750.769461ms 750.825122ms 750.828011ms 750.889569ms 750.920462ms 750.966882ms 751.005275ms 751.012681ms 751.077864ms 751.09475ms 751.138599ms 751.149036ms 751.164533ms 751.164739ms 751.232275ms 751.413372ms 751.419592ms 751.438539ms 751.493629ms 751.510286ms 751.620276ms 751.641678ms 751.678849ms 751.768524ms 751.893568ms 751.91334ms 751.987501ms 752.047908ms 752.057185ms 752.148079ms 752.200147ms 752.503992ms 752.525316ms 752.699633ms 753.086345ms 753.139292ms 753.223332ms 753.550223ms 753.679569ms 753.98442ms 754.077625ms 754.234452ms 754.28248ms 754.577105ms 754.732394ms 755.698485ms 756.1309ms 756.331097ms 756.498618ms 756.85293ms 757.059463ms 758.147431ms 758.756262ms 763.624838ms 783.844623ms 802.206716ms]
  May 18 09:17:06.901: INFO: 50 %ile: 748.057083ms
  May 18 09:17:06.901: INFO: 90 %ile: 753.223332ms
  May 18 09:17:06.901: INFO: 99 %ile: 783.844623ms
  May 18 09:17:06.901: INFO: Total sample count: 200
  May 18 09:17:06.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-665" for this suite. @ 05/18/23 09:17:06.914
• [10.779 seconds]
------------------------------
SS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/18/23 09:17:06.923
  May 18 09:17:06.923: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:17:06.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:17:06.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:17:06.954
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:17:06.962
  E0518 09:17:07.160077      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:08.160526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:09.161260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:10.161419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:17:10.994
  May 18 09:17:10.999: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-c953fc0f-0547-4b3b-b87f-6d91eee1fd97 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:17:11.033
  May 18 09:17:11.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8153" for this suite. @ 05/18/23 09:17:11.066
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/18/23 09:17:11.092
  May 18 09:17:11.092: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-runtime @ 05/18/23 09:17:11.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:17:11.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:17:11.121
  STEP: create the container @ 05/18/23 09:17:11.126
  W0518 09:17:11.137769      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/18/23 09:17:11.138
  E0518 09:17:11.161852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:12.163681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:13.164809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/18/23 09:17:14.161
  STEP: the container should be terminated @ 05/18/23 09:17:14.164
  STEP: the termination message should be set @ 05/18/23 09:17:14.165
  May 18 09:17:14.165: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  E0518 09:17:14.165947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the container @ 05/18/23 09:17:14.167
  May 18 09:17:14.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-65" for this suite. @ 05/18/23 09:17:14.189
• [3.107 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/18/23 09:17:14.207
  May 18 09:17:14.207: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-preemption @ 05/18/23 09:17:14.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:17:14.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:17:14.233
  May 18 09:17:14.254: INFO: Waiting up to 1m0s for all nodes to be ready
  E0518 09:17:15.166000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:16.169051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:17.169181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:18.169456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:19.169739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:20.170178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:21.170724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:22.171301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:23.171425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:24.172464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:25.173205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:26.173541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:27.174257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:28.175142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:29.175134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:30.176040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:31.176378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:32.176825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:33.177470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:34.177531      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:35.177780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:36.177974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:37.178454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:38.178994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:39.179972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:40.180528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:41.180760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:42.180833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:43.181051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:44.181542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:45.182239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:46.182830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:47.183220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:48.183238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:49.183519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:50.183745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:51.183903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:52.185162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:53.185216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:54.186636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:55.185735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:56.185810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:57.186888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:58.187061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:17:59.187690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:00.187861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:01.188252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:02.189073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:03.189446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:04.189843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:05.189867      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:06.190147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:07.191348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:08.191770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:09.191940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:10.192653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:11.193196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:12.193578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:13.194144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:14.193788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:18:14.313: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/18/23 09:18:14.317
  May 18 09:18:14.317: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/18/23 09:18:14.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:18:14.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:18:14.341
  May 18 09:18:14.360: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May 18 09:18:14.364: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May 18 09:18:14.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:18:14.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-1230" for this suite. @ 05/18/23 09:18:14.434
  STEP: Destroying namespace "sched-preemption-3993" for this suite. @ 05/18/23 09:18:14.442
• [60.241 seconds]
------------------------------
S
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/18/23 09:18:14.448
  May 18 09:18:14.448: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename cronjob @ 05/18/23 09:18:14.45
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:18:14.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:18:14.468
  STEP: Creating a ReplaceConcurrent cronjob @ 05/18/23 09:18:14.473
  STEP: Ensuring a job is scheduled @ 05/18/23 09:18:14.479
  E0518 09:18:15.194350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:16.194621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:17.194914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:18.195134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:19.195362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:20.195424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:21.196053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:22.197066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:23.197429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:24.197632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:25.198041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:26.198615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:27.199223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:28.199991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:29.201025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:30.201813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:31.202034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:32.202157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:33.202342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:34.202653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:35.203000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:36.203628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:37.204074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:38.204544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:39.204653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:40.204832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:41.205859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:42.206148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:43.206615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:44.207250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:45.207544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:46.207846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:47.208064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:48.209316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:49.210942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:50.210744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:51.212799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:52.212149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:53.212202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:54.212748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:55.212913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:56.213693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:57.214279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:58.214806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:18:59.215749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:00.216307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/18/23 09:19:00.486
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/18/23 09:19:00.505
  STEP: Ensuring the job is replaced with a new one @ 05/18/23 09:19:00.509
  E0518 09:19:01.217145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:02.217875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:03.218325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:04.218642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:05.219080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:06.219452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:07.219707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:08.219805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:09.220607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:10.220949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:11.221841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:12.222134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:13.224737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:14.222740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:15.223030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:16.223537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:17.223882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:18.224290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:19.224527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:20.224545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:21.224898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:22.225074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:23.225928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:24.226667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:25.226986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:26.227515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:27.227970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:28.228352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:29.229521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:30.230225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:31.231002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:32.231306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:33.231909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:34.231682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:35.231915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:36.232729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:37.232515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:38.232764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:39.232899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:40.233441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:41.233815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:42.233914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:43.234141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:44.234187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:45.234637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:46.235513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:47.236578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:48.237135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:49.237289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:50.237899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:51.238347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:52.238408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:53.238770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:54.239098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:55.239699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:56.240207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:57.240362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:58.241056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:19:59.241180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:00.241266      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/18/23 09:20:00.514
  May 18 09:20:00.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2875" for this suite. @ 05/18/23 09:20:00.53
• [106.094 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/18/23 09:20:00.543
  May 18 09:20:00.543: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:20:00.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:20:00.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:20:00.572
  May 18 09:20:00.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-7090 version'
  May 18 09:20:00.722: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May 18 09:20:00.722: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May 18 09:20:00.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7090" for this suite. @ 05/18/23 09:20:00.728
• [0.194 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/18/23 09:20:00.747
  May 18 09:20:00.748: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 09:20:00.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:20:00.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:20:00.769
  STEP: Creating pod test-webserver-a3413b84-bcbc-498a-b8b0-1c4a3fdd1291 in namespace container-probe-61 @ 05/18/23 09:20:00.773
  E0518 09:20:01.247002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:02.245341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:20:02.791: INFO: Started pod test-webserver-a3413b84-bcbc-498a-b8b0-1c4a3fdd1291 in namespace container-probe-61
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/18/23 09:20:02.792
  May 18 09:20:02.796: INFO: Initial restart count of pod test-webserver-a3413b84-bcbc-498a-b8b0-1c4a3fdd1291 is 0
  E0518 09:20:03.245689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:04.246437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:05.247402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:06.247761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:07.248100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:08.248487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:09.249033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:10.249248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:11.249759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:12.250459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:13.251026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:14.251832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:15.252044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:16.252376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:17.253057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:18.253580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:19.253929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:20.254055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:21.254388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:22.255068      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:23.256055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:24.256552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:25.257594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:26.258442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:27.258919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:28.259338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:29.259732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:30.260781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:31.261924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:32.262875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:33.263774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:34.264916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:35.265798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:36.266225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:37.267151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:38.267510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:39.268327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:40.268463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:41.269037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:42.269083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:43.269371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:44.269912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:45.270890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:46.270840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:47.271312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:48.271497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:49.271464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:50.271787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:51.272299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:52.273098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:53.274383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:54.274612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:55.275081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:56.275651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:57.276560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:58.276733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:20:59.277262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:00.277396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:01.277587      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:02.277791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:03.278038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:04.278230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:05.279187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:06.279425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:07.279625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:08.279781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:09.280454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:10.281038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:11.281158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:12.281394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:13.282003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:14.282379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:15.282929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:16.283803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:17.284520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:18.284751      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:19.285407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:20.286223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:21.286875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:22.287422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:23.287921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:24.288270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:25.289103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:26.289570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:27.290607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:28.290965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:29.291468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:30.291605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:31.292135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:32.293162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:33.294211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:34.294300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:35.295379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:36.295702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:37.295692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:38.296791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:39.297386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:40.297804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:41.298478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:42.299349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:43.299481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:44.300974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:45.301018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:46.305285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:47.305738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:48.305939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:49.306235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:50.306654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:51.307169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:52.308245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:53.309238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:54.309450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:55.310213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:56.310549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:57.310725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:58.310853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:21:59.311412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:00.311780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:01.311632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:02.312090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:03.312479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:04.312471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:05.313406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:06.313729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:07.314432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:08.314801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:09.315121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:10.315382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:11.316501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:12.318156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:13.317283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:14.317395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:15.318313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:16.319220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:17.319784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:18.320615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:19.321557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:20.321946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:21.322483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:22.322946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:23.323851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:24.324177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:25.324417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:26.324693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:27.325501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:28.325869      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:29.326427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:30.326750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:31.327176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:32.327640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:33.328835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:34.329259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:35.329706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:36.330027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:37.331081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:38.331435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:39.332294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:40.332680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:41.333376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:42.334252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:43.335021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:44.335381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:45.336329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:46.337096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:47.337838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:48.337999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:49.338269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:50.338405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:51.339386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:52.339574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:53.339690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:54.340653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:55.341527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:56.341883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:57.342734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:58.343395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:22:59.343936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:00.344427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:01.344503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:02.345625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:03.346816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:04.347059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:05.347818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:06.349254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:07.349717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:08.350829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:09.351088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:10.352028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:11.352428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:12.352821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:13.354798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:14.355551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:15.355722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:16.356619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:17.356737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:18.356808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:19.357232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:20.357609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:21.358193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:22.359291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:23.359959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:24.360421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:25.360759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:26.361042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:27.361315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:28.361697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:29.362312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:30.362452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:31.362658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:32.362929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:33.363179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:34.363390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:35.364062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:36.364769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:37.364806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:38.365286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:39.366049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:40.365966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:41.366605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:42.367253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:43.367637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:44.368152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:45.369932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:46.370410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:47.371049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:48.370965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:49.371081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:50.371243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:51.371452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:52.371588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:53.372075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:54.372292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:55.372277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:56.372822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:57.372500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:58.372764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:23:59.372989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:00.373322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:01.373968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:02.373959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:03.374439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:24:03.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:24:03.514
  STEP: Destroying namespace "container-probe-61" for this suite. @ 05/18/23 09:24:03.531
• [242.792 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/18/23 09:24:03.552
  May 18 09:24:03.552: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replication-controller @ 05/18/23 09:24:03.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:24:03.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:24:03.616
  STEP: Creating ReplicationController "e2e-rc-2p5c7" @ 05/18/23 09:24:03.625
  May 18 09:24:03.636: INFO: Get Replication Controller "e2e-rc-2p5c7" to confirm replicas
  E0518 09:24:04.374637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:24:04.641: INFO: Get Replication Controller "e2e-rc-2p5c7" to confirm replicas
  May 18 09:24:04.645: INFO: Found 1 replicas for "e2e-rc-2p5c7" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-2p5c7" @ 05/18/23 09:24:04.645
  STEP: Updating a scale subresource @ 05/18/23 09:24:04.65
  STEP: Verifying replicas where modified for replication controller "e2e-rc-2p5c7" @ 05/18/23 09:24:04.658
  May 18 09:24:04.658: INFO: Get Replication Controller "e2e-rc-2p5c7" to confirm replicas
  E0518 09:24:05.374973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:24:05.666: INFO: Get Replication Controller "e2e-rc-2p5c7" to confirm replicas
  May 18 09:24:05.672: INFO: Found 2 replicas for "e2e-rc-2p5c7" replication controller
  May 18 09:24:05.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-573" for this suite. @ 05/18/23 09:24:05.681
• [2.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/18/23 09:24:05.69
  May 18 09:24:05.690: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:24:05.691
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:24:05.706
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:24:05.71
  STEP: starting the proxy server @ 05/18/23 09:24:05.714
  May 18 09:24:05.714: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-4276 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/18/23 09:24:05.825
  May 18 09:24:05.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4276" for this suite. @ 05/18/23 09:24:05.849
• [0.166 seconds]
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/18/23 09:24:05.857
  May 18 09:24:05.857: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename deployment @ 05/18/23 09:24:05.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:24:05.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:24:05.879
  May 18 09:24:05.890: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0518 09:24:06.377093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:07.377290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:08.377413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:09.377645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:10.377859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:24:10.896: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/18/23 09:24:10.897
  May 18 09:24:10.897: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/18/23 09:24:10.913
  May 18 09:24:10.923: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-794  b20b6566-968a-4fbc-b680-bc74bf663295 1936801 1 2023-05-18 09:24:10 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-05-18 09:24:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003a612f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  May 18 09:24:10.927: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  May 18 09:24:10.927: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  May 18 09:24:10.927: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-794  01303844-0d5b-462e-9e27-1f023e158f64 1936804 1 2023-05-18 09:24:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment b20b6566-968a-4fbc-b680-bc74bf663295 0xc001e71667 0xc001e71668}] [] [{e2e.test Update apps/v1 2023-05-18 09:24:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:24:07 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-18 09:24:10 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"b20b6566-968a-4fbc-b680-bc74bf663295\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003052008 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:24:10.931: INFO: Pod "test-cleanup-controller-sd7wx" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-sd7wx test-cleanup-controller- deployment-794  1e8be763-8970-4328-93f7-fc9fa168c615 1936765 0 2023-05-18 09:24:05 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[cni.projectcalico.org/containerID:89ea2c403301d2cb5015d9768862b2f7f4a5b395ac564c15837b38b744340221 cni.projectcalico.org/podIP:172.16.161.72/32 cni.projectcalico.org/podIPs:172.16.161.72/32] [{apps/v1 ReplicaSet test-cleanup-controller 01303844-0d5b-462e-9e27-1f023e158f64 0xc003a61657 0xc003a61658}] [] [{kube-controller-manager Update v1 2023-05-18 09:24:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"01303844-0d5b-462e-9e27-1f023e158f64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:24:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:24:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.72\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-swwz9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-swwz9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:24:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:24:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:24:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.72,StartTime:2023-05-18 09:24:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:24:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://2cd632fef55981f29cff943a97aa12dd3bce49e02a0153e303d0125231dfcb1f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.72,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:24:10.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-794" for this suite. @ 05/18/23 09:24:10.936
• [5.093 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/18/23 09:24:10.95
  May 18 09:24:10.950: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:24:10.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:24:10.983
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:24:10.986
  STEP: Creating the pod @ 05/18/23 09:24:10.99
  E0518 09:24:11.378234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:12.378386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:13.380124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:24:13.555: INFO: Successfully updated pod "labelsupdate513ca110-29df-4289-859d-39fc99c3f119"
  E0518 09:24:14.379020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:15.379347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:16.384770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:17.380333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:24:17.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9259" for this suite. @ 05/18/23 09:24:17.608
• [6.664 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/18/23 09:24:17.634
  May 18 09:24:17.634: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:24:17.638
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:24:17.659
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:24:17.664
  STEP: creating service in namespace services-7463 @ 05/18/23 09:24:17.67
  STEP: creating service affinity-clusterip in namespace services-7463 @ 05/18/23 09:24:17.671
  STEP: creating replication controller affinity-clusterip in namespace services-7463 @ 05/18/23 09:24:17.694
  I0518 09:24:17.703679      19 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-7463, replica count: 3
  E0518 09:24:18.389390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:19.382168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:20.382308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:24:20.755614      19 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 09:24:20.763: INFO: Creating new exec pod
  E0518 09:24:21.384789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:22.384725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:23.384635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:24:23.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7463 exec execpod-affinityq2p2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May 18 09:24:24.062: INFO: stderr: "+ nc -v -t -w 2 affinity-clusterip 80\n+ echo hostName\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May 18 09:24:24.062: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:24:24.062: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7463 exec execpod-affinityq2p2z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.21.40 80'
  May 18 09:24:24.282: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.21.40 80\nConnection to 10.107.21.40 80 port [tcp/http] succeeded!\n"
  May 18 09:24:24.282: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:24:24.282: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7463 exec execpod-affinityq2p2z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.21.40:80/ ; done'
  E0518 09:24:24.385099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:24:24.594: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.21.40:80/\n"
  May 18 09:24:24.594: INFO: stdout: "\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl\naffinity-clusterip-rdfvl"
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Received response from host: affinity-clusterip-rdfvl
  May 18 09:24:24.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:24:24.602: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-7463, will wait for the garbage collector to delete the pods @ 05/18/23 09:24:24.614
  May 18 09:24:24.688: INFO: Deleting ReplicationController affinity-clusterip took: 9.40342ms
  May 18 09:24:24.789: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.283867ms
  E0518 09:24:25.385744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:26.386251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:27.386519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7463" for this suite. @ 05/18/23 09:24:27.41
• [9.785 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/18/23 09:24:27.43
  May 18 09:24:27.430: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-watch @ 05/18/23 09:24:27.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:24:27.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:24:27.451
  May 18 09:24:27.455: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:24:28.387072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:29.387762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 05/18/23 09:24:30.009
  May 18 09:24:30.014: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-18T09:24:30Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-18T09:24:30Z]] name:name1 resourceVersion:1937100 uid:b5c41027-b3dc-4780-9c09-69185308205c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0518 09:24:30.388115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:31.388270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:32.389223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:33.389225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:34.389468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:35.389990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:36.390194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:37.390405      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:38.390591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:39.390656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 05/18/23 09:24:40.015
  May 18 09:24:40.026: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-18T09:24:40Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-18T09:24:40Z]] name:name2 resourceVersion:1937170 uid:8deb0602-86c4-4016-a181-8e8dce061d95] num:map[num1:9223372036854775807 num2:1000000]]}
  E0518 09:24:40.390897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:41.391544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:42.393110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:43.393356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:44.393147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:45.394469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:46.394795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:47.395428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:48.395837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:49.396184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 05/18/23 09:24:50.027
  May 18 09:24:50.040: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-18T09:24:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-18T09:24:50Z]] name:name1 resourceVersion:1937214 uid:b5c41027-b3dc-4780-9c09-69185308205c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0518 09:24:50.397107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:51.397169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:52.397300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:53.397634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:54.397723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:55.397905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:56.398020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:57.398310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:58.398459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:24:59.398810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 05/18/23 09:25:00.043
  May 18 09:25:00.053: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-18T09:24:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-18T09:25:00Z]] name:name2 resourceVersion:1937255 uid:8deb0602-86c4-4016-a181-8e8dce061d95] num:map[num1:9223372036854775807 num2:1000000]]}
  E0518 09:25:00.399038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:01.399212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:02.400451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:03.400460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:04.401557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:05.402114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:06.403039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:07.403440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:08.403750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:09.404035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 05/18/23 09:25:10.053
  May 18 09:25:10.066: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-18T09:24:30Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-18T09:24:50Z]] name:name1 resourceVersion:1937296 uid:b5c41027-b3dc-4780-9c09-69185308205c] num:map[num1:9223372036854775807 num2:1000000]]}
  E0518 09:25:10.405106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:11.405986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:12.406303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:13.407970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:14.408629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:15.409124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:16.409525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:17.409949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:18.410629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:19.410962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 05/18/23 09:25:20.066
  May 18 09:25:20.080: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-18T09:24:40Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-18T09:25:00Z]] name:name2 resourceVersion:1937341 uid:8deb0602-86c4-4016-a181-8e8dce061d95] num:map[num1:9223372036854775807 num2:1000000]]}
  E0518 09:25:20.411674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:21.411619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:22.411752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:23.411826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:24.411984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:25.412241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:26.412552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:27.412705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:28.413023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:29.413276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:30.413875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:25:30.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-3910" for this suite. @ 05/18/23 09:25:30.606
• [63.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/18/23 09:25:30.628
  May 18 09:25:30.628: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:25:30.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:25:30.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:25:30.659
  STEP: Creating configMap with name configmap-test-volume-dcd9d46c-35b7-49c9-aa68-c9c7b612f4c0 @ 05/18/23 09:25:30.663
  STEP: Creating a pod to test consume configMaps @ 05/18/23 09:25:30.669
  E0518 09:25:31.420206      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:32.421036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:33.420665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:34.421039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:25:34.702
  May 18 09:25:34.707: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-a45d74ce-9838-43b6-ad30-ff04213020db container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 09:25:34.719
  May 18 09:25:34.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8536" for this suite. @ 05/18/23 09:25:34.744
• [4.124 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/18/23 09:25:34.753
  May 18 09:25:34.753: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-preemption @ 05/18/23 09:25:34.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:25:34.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:25:34.78
  May 18 09:25:34.796: INFO: Waiting up to 1m0s for all nodes to be ready
  E0518 09:25:35.421752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:36.421877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:37.422313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:38.422425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:39.423163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:40.423457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:41.423926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:42.425152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:43.425208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:44.425397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:45.426333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:46.426876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:47.427708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:48.428212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:49.428780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:50.429045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:51.429360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:52.429420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:53.430165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:54.430552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:55.431304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:56.431585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:57.431846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:58.432089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:25:59.432657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:00.434136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:01.434958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:02.435339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:03.435575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:04.437227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:05.436895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:06.437741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:07.438707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:08.439108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:09.439165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:10.439381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:11.439377      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:12.439483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:13.440290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:14.441148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:15.442216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:16.442858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:17.442960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:18.443235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:19.443886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:20.444048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:21.444192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:22.444816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:23.445003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:24.445174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:25.445581      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:26.446208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:27.446350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:28.446627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:29.446777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:30.447484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:31.447715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:32.448493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:33.448925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:34.449110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:26:34.881: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/18/23 09:26:34.885
  May 18 09:26:34.916: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 18 09:26:34.931: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 18 09:26:34.965: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 18 09:26:34.974: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/18/23 09:26:34.974
  E0518 09:26:35.450072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:36.450325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:37.450359      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:38.450920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/18/23 09:26:39.005
  E0518 09:26:39.451596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:40.452072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:41.453391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:42.454360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:26:43.097: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-8567" for this suite. @ 05/18/23 09:26:43.168
• [68.426 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/18/23 09:26:43.202
  May 18 09:26:43.202: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename dns @ 05/18/23 09:26:43.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:26:43.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:26:43.237
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3532.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-3532.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/18/23 09:26:43.242
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-3532.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-3532.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/18/23 09:26:43.242
  STEP: creating a pod to probe /etc/hosts @ 05/18/23 09:26:43.243
  STEP: submitting the pod to kubernetes @ 05/18/23 09:26:43.243
  E0518 09:26:43.454922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:44.455941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 09:26:45.268
  STEP: looking for the results for each expected name from probers @ 05/18/23 09:26:45.272
  May 18 09:26:45.293: INFO: DNS probes using dns-3532/dns-test-1a556aea-547c-4c9e-8f68-cf1d1ebb7f66 succeeded

  May 18 09:26:45.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:26:45.299
  STEP: Destroying namespace "dns-3532" for this suite. @ 05/18/23 09:26:45.315
• [2.126 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/18/23 09:26:45.348
  May 18 09:26:45.351: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:26:45.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:26:45.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:26:45.373
  STEP: Counting existing ResourceQuota @ 05/18/23 09:26:45.377
  E0518 09:26:45.456189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:46.457191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:47.457767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:48.458620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:49.459653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/18/23 09:26:50.38
  STEP: Ensuring resource quota status is calculated @ 05/18/23 09:26:50.385
  E0518 09:26:50.460411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:51.461297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 05/18/23 09:26:52.39
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/18/23 09:26:52.406
  E0518 09:26:52.462391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:53.462830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/18/23 09:26:54.412
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/18/23 09:26:54.416
  STEP: Ensuring a pod cannot update its resource requirements @ 05/18/23 09:26:54.419
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/18/23 09:26:54.423
  E0518 09:26:54.463931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:55.463926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/18/23 09:26:56.429
  STEP: Ensuring resource quota status released the pod usage @ 05/18/23 09:26:56.443
  E0518 09:26:56.464675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:26:57.465005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:26:58.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5373" for this suite. @ 05/18/23 09:26:58.453
• [13.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/18/23 09:26:58.464
  May 18 09:26:58.464: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:26:58.465438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Building a namespace api object, basename gc @ 05/18/23 09:26:58.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:26:58.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:26:58.488
  STEP: create the rc1 @ 05/18/23 09:26:58.499
  STEP: create the rc2 @ 05/18/23 09:26:58.505
  E0518 09:26:59.465681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:00.466210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:01.469095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:02.471371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:03.474021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:04.474502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/18/23 09:27:04.521
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/18/23 09:27:05.107
  STEP: wait for the rc to be deleted @ 05/18/23 09:27:05.126
  E0518 09:27:05.475189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:06.508694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:07.511184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:08.511304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:09.511733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:10.146: INFO: 74 pods remaining
  May 18 09:27:10.146: INFO: 74 pods has nil DeletionTimestamp
  May 18 09:27:10.147: INFO: 
  E0518 09:27:10.512635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:11.512969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:12.513981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:13.514996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:14.515903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/18/23 09:27:15.151
  May 18 09:27:15.330: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 18 09:27:15.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-26lzq" in namespace "gc-6422"
  May 18 09:27:15.343: INFO: Deleting pod "simpletest-rc-to-be-deleted-292hl" in namespace "gc-6422"
  May 18 09:27:15.364: INFO: Deleting pod "simpletest-rc-to-be-deleted-2gqvn" in namespace "gc-6422"
  May 18 09:27:15.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-2j4s7" in namespace "gc-6422"
  May 18 09:27:15.395: INFO: Deleting pod "simpletest-rc-to-be-deleted-2plrr" in namespace "gc-6422"
  May 18 09:27:15.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-2zvxz" in namespace "gc-6422"
  May 18 09:27:15.417: INFO: Deleting pod "simpletest-rc-to-be-deleted-44zbh" in namespace "gc-6422"
  May 18 09:27:15.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kfg5" in namespace "gc-6422"
  May 18 09:27:15.437: INFO: Deleting pod "simpletest-rc-to-be-deleted-4r7lm" in namespace "gc-6422"
  May 18 09:27:15.473: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mr65" in namespace "gc-6422"
  May 18 09:27:15.503: INFO: Deleting pod "simpletest-rc-to-be-deleted-5scnz" in namespace "gc-6422"
  May 18 09:27:15.523: INFO: Deleting pod "simpletest-rc-to-be-deleted-5wq9t" in namespace "gc-6422"
  E0518 09:27:15.524446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:15.541: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gf8b" in namespace "gc-6422"
  May 18 09:27:15.562: INFO: Deleting pod "simpletest-rc-to-be-deleted-6gz5p" in namespace "gc-6422"
  May 18 09:27:15.572: INFO: Deleting pod "simpletest-rc-to-be-deleted-6rs7r" in namespace "gc-6422"
  May 18 09:27:15.583: INFO: Deleting pod "simpletest-rc-to-be-deleted-6zcgz" in namespace "gc-6422"
  May 18 09:27:15.605: INFO: Deleting pod "simpletest-rc-to-be-deleted-72pc5" in namespace "gc-6422"
  May 18 09:27:15.664: INFO: Deleting pod "simpletest-rc-to-be-deleted-7bmpr" in namespace "gc-6422"
  May 18 09:27:15.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-7lqfs" in namespace "gc-6422"
  May 18 09:27:15.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m79f" in namespace "gc-6422"
  May 18 09:27:15.724: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qmxb" in namespace "gc-6422"
  May 18 09:27:15.792: INFO: Deleting pod "simpletest-rc-to-be-deleted-82xjt" in namespace "gc-6422"
  May 18 09:27:15.806: INFO: Deleting pod "simpletest-rc-to-be-deleted-87bmg" in namespace "gc-6422"
  May 18 09:27:15.817: INFO: Deleting pod "simpletest-rc-to-be-deleted-8ps7z" in namespace "gc-6422"
  May 18 09:27:15.830: INFO: Deleting pod "simpletest-rc-to-be-deleted-994mm" in namespace "gc-6422"
  May 18 09:27:15.869: INFO: Deleting pod "simpletest-rc-to-be-deleted-99wxr" in namespace "gc-6422"
  May 18 09:27:15.897: INFO: Deleting pod "simpletest-rc-to-be-deleted-9n89h" in namespace "gc-6422"
  May 18 09:27:15.906: INFO: Deleting pod "simpletest-rc-to-be-deleted-9p4rm" in namespace "gc-6422"
  May 18 09:27:15.959: INFO: Deleting pod "simpletest-rc-to-be-deleted-9qdk7" in namespace "gc-6422"
  May 18 09:27:16.035: INFO: Deleting pod "simpletest-rc-to-be-deleted-9w9dn" in namespace "gc-6422"
  May 18 09:27:16.084: INFO: Deleting pod "simpletest-rc-to-be-deleted-bfzpf" in namespace "gc-6422"
  May 18 09:27:16.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-bmhrh" in namespace "gc-6422"
  May 18 09:27:16.169: INFO: Deleting pod "simpletest-rc-to-be-deleted-c24r7" in namespace "gc-6422"
  May 18 09:27:16.182: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5877" in namespace "gc-6422"
  May 18 09:27:16.199: INFO: Deleting pod "simpletest-rc-to-be-deleted-cgd97" in namespace "gc-6422"
  May 18 09:27:16.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-cx75s" in namespace "gc-6422"
  May 18 09:27:16.235: INFO: Deleting pod "simpletest-rc-to-be-deleted-czqst" in namespace "gc-6422"
  May 18 09:27:16.249: INFO: Deleting pod "simpletest-rc-to-be-deleted-d5gh7" in namespace "gc-6422"
  May 18 09:27:16.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-d6cdk" in namespace "gc-6422"
  May 18 09:27:16.284: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgbs8" in namespace "gc-6422"
  May 18 09:27:16.296: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsz4z" in namespace "gc-6422"
  May 18 09:27:16.307: INFO: Deleting pod "simpletest-rc-to-be-deleted-dzn55" in namespace "gc-6422"
  May 18 09:27:16.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-fg8jn" in namespace "gc-6422"
  May 18 09:27:16.330: INFO: Deleting pod "simpletest-rc-to-be-deleted-flmck" in namespace "gc-6422"
  May 18 09:27:16.349: INFO: Deleting pod "simpletest-rc-to-be-deleted-fvptm" in namespace "gc-6422"
  May 18 09:27:16.363: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx5hb" in namespace "gc-6422"
  May 18 09:27:16.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-g54dk" in namespace "gc-6422"
  May 18 09:27:16.410: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5hzw" in namespace "gc-6422"
  May 18 09:27:16.420: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggc2s" in namespace "gc-6422"
  May 18 09:27:16.432: INFO: Deleting pod "simpletest-rc-to-be-deleted-gt59d" in namespace "gc-6422"
  May 18 09:27:16.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6422" for this suite. @ 05/18/23 09:27:16.454
• [18.001 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/18/23 09:27:16.47
  May 18 09:27:16.470: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 09:27:16.479
  E0518 09:27:16.525087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:27:16.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:27:16.55
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/18/23 09:27:16.575
  May 18 09:27:16.576: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:27:17.553693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:18.558804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:19.543755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:20.544829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:21.546844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:22.547486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:23.548415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:24.549164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:25.549256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:26.549365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/18/23 09:27:26.998
  May 18 09:27:27.000: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:27:27.549798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:28.550110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:29.551517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:30.321: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:27:30.552013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:31.552413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:32.552927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:33.553686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:34.554850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:35.555543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:36.556302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:37.557129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:38.558190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:39.558472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:40.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-352" for this suite. @ 05/18/23 09:27:40.06
• [23.600 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/18/23 09:27:40.072
  May 18 09:27:40.072: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:27:40.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:27:40.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:27:40.102
  STEP: Creating configMap with name projected-configmap-test-volume-af144f5e-06a5-44b3-a73c-1ec146f462a1 @ 05/18/23 09:27:40.107
  STEP: Creating a pod to test consume configMaps @ 05/18/23 09:27:40.113
  E0518 09:27:40.559159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:41.559288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:42.560110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:43.560654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:27:44.139
  May 18 09:27:44.142: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-configmaps-68af1774-2ee2-49c5-8c20-e2fa589a0cc3 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:27:44.168
  May 18 09:27:44.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2403" for this suite. @ 05/18/23 09:27:44.189
• [4.124 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/18/23 09:27:44.198
  May 18 09:27:44.198: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:27:44.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:27:44.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:27:44.223
  STEP: Creating a ResourceQuota @ 05/18/23 09:27:44.226
  STEP: Getting a ResourceQuota @ 05/18/23 09:27:44.232
  STEP: Updating a ResourceQuota @ 05/18/23 09:27:44.239
  STEP: Verifying a ResourceQuota was modified @ 05/18/23 09:27:44.246
  STEP: Deleting a ResourceQuota @ 05/18/23 09:27:44.25
  STEP: Verifying the deleted ResourceQuota @ 05/18/23 09:27:44.257
  May 18 09:27:44.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7595" for this suite. @ 05/18/23 09:27:44.268
• [0.079 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/18/23 09:27:44.277
  May 18 09:27:44.277: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:27:44.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:27:44.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:27:44.298
  STEP: creating service multi-endpoint-test in namespace services-2901 @ 05/18/23 09:27:44.301
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2901 to expose endpoints map[] @ 05/18/23 09:27:44.318
  May 18 09:27:44.324: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0518 09:27:44.561182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:45.334: INFO: successfully validated that service multi-endpoint-test in namespace services-2901 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-2901 @ 05/18/23 09:27:45.334
  E0518 09:27:45.566378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:46.567032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2901 to expose endpoints map[pod1:[100]] @ 05/18/23 09:27:47.352
  May 18 09:27:47.366: INFO: successfully validated that service multi-endpoint-test in namespace services-2901 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-2901 @ 05/18/23 09:27:47.366
  E0518 09:27:47.567606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:48.568774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:49.569628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:50.570487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2901 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/18/23 09:27:51.392
  May 18 09:27:51.402: INFO: successfully validated that service multi-endpoint-test in namespace services-2901 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/18/23 09:27:51.403
  May 18 09:27:51.403: INFO: Creating new exec pod
  E0518 09:27:51.570262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:52.571008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:53.571819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:54.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-2901 exec execpodptsdw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  E0518 09:27:54.572030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:54.655: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May 18 09:27:54.655: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:27:54.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-2901 exec execpodptsdw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.78.233 80'
  May 18 09:27:54.849: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.97.78.233 80\nConnection to 10.97.78.233 80 port [tcp/http] succeeded!\n"
  May 18 09:27:54.850: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:27:54.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-2901 exec execpodptsdw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May 18 09:27:55.015: INFO: stderr: "+ nc -v -t -w 2 multi-endpoint-test 81\n+ echo hostName\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May 18 09:27:55.015: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:27:55.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-2901 exec execpodptsdw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.97.78.233 81'
  May 18 09:27:55.255: INFO: stderr: "+ nc -v -t -w 2 10.97.78.233 81\n+ echo hostName\nConnection to 10.97.78.233 81 port [tcp/*] succeeded!\n"
  May 18 09:27:55.255: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-2901 @ 05/18/23 09:27:55.255
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2901 to expose endpoints map[pod2:[101]] @ 05/18/23 09:27:55.266
  May 18 09:27:55.313: INFO: successfully validated that service multi-endpoint-test in namespace services-2901 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-2901 @ 05/18/23 09:27:55.314
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-2901 to expose endpoints map[] @ 05/18/23 09:27:55.34
  May 18 09:27:55.357: INFO: successfully validated that service multi-endpoint-test in namespace services-2901 exposes endpoints map[]
  May 18 09:27:55.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2901" for this suite. @ 05/18/23 09:27:55.374
• [11.103 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/18/23 09:27:55.382
  May 18 09:27:55.382: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 09:27:55.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:27:55.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:27:55.406
  STEP: creating a Pod with a static label @ 05/18/23 09:27:55.418
  STEP: watching for Pod to be ready @ 05/18/23 09:27:55.425
  May 18 09:27:55.427: INFO: observed Pod pod-test in namespace pods-2817 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May 18 09:27:55.430: INFO: observed Pod pod-test in namespace pods-2817 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC  }]
  May 18 09:27:55.442: INFO: observed Pod pod-test in namespace pods-2817 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC  }]
  E0518 09:27:55.572807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:55.997: INFO: observed Pod pod-test in namespace pods-2817 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC  }]
  May 18 09:27:56.440: INFO: Found Pod pod-test in namespace pods-2817 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:56 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:56 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:27:55 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/18/23 09:27:56.453
  STEP: getting the Pod and ensuring that it's patched @ 05/18/23 09:27:56.479
  STEP: replacing the Pod's status Ready condition to False @ 05/18/23 09:27:56.482
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/18/23 09:27:56.509
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/18/23 09:27:56.509
  STEP: watching for the Pod to be deleted @ 05/18/23 09:27:56.52
  May 18 09:27:56.544: INFO: observed event type MODIFIED
  E0518 09:27:56.572902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:27:57.573250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:58.438: INFO: observed event type MODIFIED
  E0518 09:27:58.574172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:27:58.771: INFO: observed event type MODIFIED
  May 18 09:27:58.907: INFO: observed event type MODIFIED
  May 18 09:27:59.449: INFO: observed event type MODIFIED
  May 18 09:27:59.464: INFO: observed event type MODIFIED
  May 18 09:27:59.478: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2817" for this suite. @ 05/18/23 09:27:59.483
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/18/23 09:27:59.499
  May 18 09:27:59.499: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/18/23 09:27:59.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:27:59.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:27:59.535
  STEP: Setting up the test @ 05/18/23 09:27:59.538
  STEP: Creating hostNetwork=false pod @ 05/18/23 09:27:59.539
  E0518 09:27:59.577125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:00.579360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 05/18/23 09:28:01.561
  E0518 09:28:01.580391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:02.581075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:03.581959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 05/18/23 09:28:03.587
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/18/23 09:28:03.587
  May 18 09:28:03.587: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:03.587: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:03.590: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:03.590: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 18 09:28:03.731: INFO: Exec stderr: ""
  May 18 09:28:03.731: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:03.731: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:03.732: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:03.732: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 18 09:28:03.840: INFO: Exec stderr: ""
  May 18 09:28:03.841: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:03.841: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:03.842: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:03.842: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 18 09:28:03.964: INFO: Exec stderr: ""
  May 18 09:28:03.964: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:03.965: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:03.967: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:03.967: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 18 09:28:04.064: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/18/23 09:28:04.064
  May 18 09:28:04.065: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:04.065: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:04.065: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:04.065: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 18 09:28:04.163: INFO: Exec stderr: ""
  May 18 09:28:04.163: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:04.163: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:04.163: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:04.163: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 18 09:28:04.280: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/18/23 09:28:04.282
  May 18 09:28:04.282: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:04.282: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:04.287: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:04.288: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 18 09:28:04.407: INFO: Exec stderr: ""
  May 18 09:28:04.408: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:04.408: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:04.410: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:04.410: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 18 09:28:04.540: INFO: Exec stderr: ""
  May 18 09:28:04.541: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:04.541: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:04.542: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:04.542: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0518 09:28:04.582942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:28:04.656: INFO: Exec stderr: ""
  May 18 09:28:04.657: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-1806 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:28:04.657: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:28:04.659: INFO: ExecWithOptions: Clientset creation
  May 18 09:28:04.660: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-1806/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 18 09:28:04.758: INFO: Exec stderr: ""
  May 18 09:28:04.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-1806" for this suite. @ 05/18/23 09:28:04.765
• [5.273 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/18/23 09:28:04.778
  May 18 09:28:04.778: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename namespaces @ 05/18/23 09:28:04.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:28:04.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:28:04.804
  STEP: Creating a test namespace @ 05/18/23 09:28:04.81
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:28:04.828
  STEP: Creating a pod in the namespace @ 05/18/23 09:28:04.831
  STEP: Waiting for the pod to have running status @ 05/18/23 09:28:04.842
  E0518 09:28:05.583166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:06.583924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 05/18/23 09:28:06.852
  STEP: Waiting for the namespace to be removed. @ 05/18/23 09:28:06.86
  E0518 09:28:07.584155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:08.584106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:09.584557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:10.584787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:11.585092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:12.585300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:13.585731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:14.586075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:15.586847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:16.587859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:17.588221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 05/18/23 09:28:17.867
  STEP: Verifying there are no pods in the namespace @ 05/18/23 09:28:17.889
  May 18 09:28:17.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9332" for this suite. @ 05/18/23 09:28:17.9
  STEP: Destroying namespace "nsdeletetest-6375" for this suite. @ 05/18/23 09:28:17.91
  May 18 09:28:17.920: INFO: Namespace nsdeletetest-6375 was already deleted
  STEP: Destroying namespace "nsdeletetest-5873" for this suite. @ 05/18/23 09:28:17.921
• [13.149 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/18/23 09:28:17.929
  May 18 09:28:17.929: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:28:17.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:28:17.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:28:17.982
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:28:17.986
  E0518 09:28:18.589599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:19.590258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:20.590361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:21.590663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:28:22.038
  May 18 09:28:22.042: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-7c48354a-8bd2-429b-99da-33c252aa2bd9 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:28:22.052
  May 18 09:28:22.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6393" for this suite. @ 05/18/23 09:28:22.077
• [4.157 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/18/23 09:28:22.09
  May 18 09:28:22.090: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:28:22.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:28:22.107
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:28:22.112
  STEP: Creating configMap with name configmap-test-upd-454e5557-017f-400e-955f-9160acd017d4 @ 05/18/23 09:28:22.122
  STEP: Creating the pod @ 05/18/23 09:28:22.128
  E0518 09:28:22.590920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:23.592250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 05/18/23 09:28:24.151
  STEP: Waiting for pod with binary data @ 05/18/23 09:28:24.157
  May 18 09:28:24.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-478" for this suite. @ 05/18/23 09:28:24.17
• [2.088 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/18/23 09:28:24.183
  May 18 09:28:24.184: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:28:24.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:28:24.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:28:24.212
  STEP: Creating configMap with name configmap-test-volume-map-510ad2ff-3414-47fc-ad74-e0b3efec5b00 @ 05/18/23 09:28:24.216
  STEP: Creating a pod to test consume configMaps @ 05/18/23 09:28:24.222
  E0518 09:28:24.593093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:25.593310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:26.593826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:27.594360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:28:28.247
  May 18 09:28:28.252: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-b9e7b1e7-4382-4228-859e-5f57361200b5 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 09:28:28.261
  May 18 09:28:28.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8276" for this suite. @ 05/18/23 09:28:28.296
• [4.119 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/18/23 09:28:28.307
  May 18 09:28:28.307: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename statefulset @ 05/18/23 09:28:28.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:28:28.329
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:28:28.336
  STEP: Creating service test in namespace statefulset-9505 @ 05/18/23 09:28:28.341
  STEP: Creating a new StatefulSet @ 05/18/23 09:28:28.347
  May 18 09:28:28.361: INFO: Found 0 stateful pods, waiting for 3
  E0518 09:28:28.594880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:29.595887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:30.595939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:31.596798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:32.597065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:33.597155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:34.597606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:35.598470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:36.598681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:37.598938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:28:38.370: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:28:38.370: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:28:38.370: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:28:38.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-9505 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0518 09:28:38.599873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:28:38.647: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:28:38.647: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:28:38.647: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0518 09:28:39.599788      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:40.600067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:41.600746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:42.601129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:43.601331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:44.601605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:45.602515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:46.602694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:47.603006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:48.603438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/18/23 09:28:48.666
  May 18 09:28:48.692: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/18/23 09:28:48.692
  E0518 09:28:49.603659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:50.604729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:51.605468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:52.605662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:53.605886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:54.605968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:55.606223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:56.606382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:57.607482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:28:58.607959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 05/18/23 09:28:58.712
  May 18 09:28:58.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-9505 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 18 09:28:59.045: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 18 09:28:59.045: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:28:59.045: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0518 09:28:59.608664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:00.609614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:01.610065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:02.610066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:03.610304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:04.613140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:05.614135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:06.614613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:07.614891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:08.615117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 05/18/23 09:29:09.074
  May 18 09:29:09.075: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-9505 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 18 09:29:09.339: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:29:09.339: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:29:09.339: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0518 09:29:09.615836      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:10.616125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:11.616895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:12.617227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:13.617349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:14.617512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:15.617632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:16.617864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:17.618132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:18.618350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:29:19.381: INFO: Updating stateful set ss2
  E0518 09:29:19.619072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:20.619082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:21.619306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:22.619565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:23.619678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:24.620225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:25.620313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:26.620804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:27.621464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:28.621386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 05/18/23 09:29:29.404
  May 18 09:29:29.409: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-9505 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0518 09:29:29.622760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:29:29.739: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 18 09:29:29.739: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:29:29.739: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0518 09:29:30.623053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:31.623268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:32.623551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:33.624039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:34.624741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:35.625258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:36.625732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:37.625617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:38.625738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:39.625878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:29:39.763: INFO: Waiting for StatefulSet statefulset-9505/ss2 to complete update
  E0518 09:29:40.627862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:41.626639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:42.626481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:43.626582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:44.626901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:45.627182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:46.627376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:47.627590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:48.627685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:49.627937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:29:49.774: INFO: Deleting all statefulset in ns statefulset-9505
  May 18 09:29:49.778: INFO: Scaling statefulset ss2 to 0
  E0518 09:29:50.628459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:51.629142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:52.629290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:53.630168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:54.630175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:55.630408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:56.630425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:57.631080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:58.632603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:29:59.632756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:29:59.801: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:29:59.805: INFO: Deleting statefulset ss2
  May 18 09:29:59.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9505" for this suite. @ 05/18/23 09:29:59.838
• [91.538 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/18/23 09:29:59.859
  May 18 09:29:59.859: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename containers @ 05/18/23 09:29:59.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:29:59.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:29:59.894
  STEP: Creating a pod to test override command @ 05/18/23 09:29:59.9
  E0518 09:30:00.633324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:01.633638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:02.633558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:03.633912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:30:03.926
  May 18 09:30:03.930: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod client-containers-4bfcbe3b-83ad-4dc0-809b-313ad9d707ba container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 09:30:03.954
  May 18 09:30:03.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2151" for this suite. @ 05/18/23 09:30:03.973
• [4.122 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/18/23 09:30:03.982
  May 18 09:30:03.982: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename disruption @ 05/18/23 09:30:03.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:30:03.999
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:30:04.003
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/18/23 09:30:04.006
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:30:04.012
  E0518 09:30:04.634147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:05.634657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/18/23 09:30:06.039
  STEP: Waiting for all pods to be running @ 05/18/23 09:30:06.039
  May 18 09:30:06.044: INFO: pods: 0 < 3
  E0518 09:30:06.635006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:07.635114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:30:08.060: INFO: running pods: 1 < 3
  E0518 09:30:08.635390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:09.636251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/18/23 09:30:10.052
  STEP: Updating the pdb to allow a pod to be evicted @ 05/18/23 09:30:10.067
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:30:10.077
  E0518 09:30:10.637178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:11.637617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/18/23 09:30:12.086
  STEP: Waiting for all pods to be running @ 05/18/23 09:30:12.086
  STEP: Waiting for the pdb to observed all healthy pods @ 05/18/23 09:30:12.094
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/18/23 09:30:12.131
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:30:12.2
  STEP: Waiting for all pods to be running @ 05/18/23 09:30:12.215
  May 18 09:30:12.225: INFO: running pods: 2 < 3
  E0518 09:30:12.638754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:13.639166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/18/23 09:30:14.23
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/18/23 09:30:14.246
  STEP: Waiting for the pdb to be deleted @ 05/18/23 09:30:14.251
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/18/23 09:30:14.254
  STEP: Waiting for all pods to be running @ 05/18/23 09:30:14.254
  May 18 09:30:14.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8544" for this suite. @ 05/18/23 09:30:14.299
• [10.330 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/18/23 09:30:14.312
  May 18 09:30:14.312: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:30:14.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:30:14.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:30:14.36
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7019 @ 05/18/23 09:30:14.364
  STEP: changing the ExternalName service to type=NodePort @ 05/18/23 09:30:14.371
  STEP: creating replication controller externalname-service in namespace services-7019 @ 05/18/23 09:30:14.389
  I0518 09:30:14.398744      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7019, replica count: 2
  E0518 09:30:14.639553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:15.641044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:16.641715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:30:17.449460      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 09:30:17.449: INFO: Creating new exec pod
  E0518 09:30:17.642219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:18.643171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:19.657101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:30:20.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7019 exec execpodb55lb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0518 09:30:20.657499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:30:20.808: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 18 09:30:20.808: INFO: stdout: ""
  E0518 09:30:21.656754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:30:21.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7019 exec execpodb55lb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 18 09:30:22.020: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 18 09:30:22.020: INFO: stdout: ""
  E0518 09:30:22.656946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:30:22.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7019 exec execpodb55lb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 18 09:30:23.037: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 18 09:30:23.037: INFO: stdout: "externalname-service-vfrrk"
  May 18 09:30:23.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7019 exec execpodb55lb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.106.182.93 80'
  May 18 09:30:23.236: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.106.182.93 80\nConnection to 10.106.182.93 80 port [tcp/http] succeeded!\n"
  May 18 09:30:23.236: INFO: stdout: "externalname-service-vfrrk"
  May 18 09:30:23.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7019 exec execpodb55lb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.143 32466'
  May 18 09:30:23.423: INFO: stderr: "+ nc -v -t -w 2 192.168.190.143 32466\nConnection to 192.168.190.143 32466 port [tcp/*] succeeded!\n+ echo hostName\n"
  May 18 09:30:23.423: INFO: stdout: "externalname-service-sqbmp"
  May 18 09:30:23.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-7019 exec execpodb55lb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.135 32466'
  May 18 09:30:23.636: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.190.135 32466\nConnection to 192.168.190.135 32466 port [tcp/*] succeeded!\n"
  May 18 09:30:23.636: INFO: stdout: "externalname-service-sqbmp"
  May 18 09:30:23.637: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:30:23.643: INFO: Cleaning up the ExternalName to NodePort test service
  E0518 09:30:23.657037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-7019" for this suite. @ 05/18/23 09:30:23.669
• [9.361 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/18/23 09:30:23.677
  May 18 09:30:23.677: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename var-expansion @ 05/18/23 09:30:23.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:30:23.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:30:23.7
  STEP: Creating a pod to test env composition @ 05/18/23 09:30:23.703
  E0518 09:30:24.657210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:25.657412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:26.658193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:27.658319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:30:27.726
  May 18 09:30:27.733: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod var-expansion-2dabe358-40ae-49e7-8fae-9339a180964d container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 09:30:27.751
  May 18 09:30:27.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7141" for this suite. @ 05/18/23 09:30:27.781
• [4.113 seconds]
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/18/23 09:30:27.791
  May 18 09:30:27.791: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename disruption @ 05/18/23 09:30:27.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:30:27.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:30:27.819
  STEP: creating the pdb @ 05/18/23 09:30:27.825
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:30:27.836
  E0518 09:30:28.658470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:29.658934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 05/18/23 09:30:29.845
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:30:29.853
  E0518 09:30:30.659142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:31.659585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 05/18/23 09:30:31.863
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:30:31.877
  E0518 09:30:32.660609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:33.661143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 05/18/23 09:30:33.894
  May 18 09:30:33.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9251" for this suite. @ 05/18/23 09:30:33.906
• [6.126 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/18/23 09:30:33.917
  May 18 09:30:33.917: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename watch @ 05/18/23 09:30:33.92
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:30:33.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:30:33.945
  STEP: creating a new configmap @ 05/18/23 09:30:33.95
  STEP: modifying the configmap once @ 05/18/23 09:30:33.957
  STEP: modifying the configmap a second time @ 05/18/23 09:30:33.967
  STEP: deleting the configmap @ 05/18/23 09:30:33.989
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/18/23 09:30:34
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/18/23 09:30:34.005
  May 18 09:30:34.005: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5429  94e8d1e3-0057-44cb-bb08-2456150bd56b 1941997 0 2023-05-18 09:30:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-18 09:30:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 09:30:34.006: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-5429  94e8d1e3-0057-44cb-bb08-2456150bd56b 1941998 0 2023-05-18 09:30:33 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-18 09:30:33 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 09:30:34.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5429" for this suite. @ 05/18/23 09:30:34.013
• [0.102 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/18/23 09:30:34.019
  May 18 09:30:34.019: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:30:34.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:30:34.04
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:30:34.043
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-894 @ 05/18/23 09:30:34.049
  STEP: changing the ExternalName service to type=ClusterIP @ 05/18/23 09:30:34.054
  STEP: creating replication controller externalname-service in namespace services-894 @ 05/18/23 09:30:34.086
  I0518 09:30:34.094861      19 runners.go:194] Created replication controller with name: externalname-service, namespace: services-894, replica count: 2
  E0518 09:30:34.662306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:35.662417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:36.663623      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:30:37.148196      19 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 09:30:37.148: INFO: Creating new exec pod
  E0518 09:30:37.663702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:38.664123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:39.664995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:30:40.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-894 exec execpodh6hp4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 18 09:30:40.432: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 18 09:30:40.432: INFO: stdout: "externalname-service-lv6gv"
  May 18 09:30:40.432: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-894 exec execpodh6hp4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.222.191 80'
  May 18 09:30:40.629: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.222.191 80\nConnection to 10.111.222.191 80 port [tcp/http] succeeded!\n"
  May 18 09:30:40.629: INFO: stdout: "externalname-service-lv6gv"
  May 18 09:30:40.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:30:40.634: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-894" for this suite. @ 05/18/23 09:30:40.661
  E0518 09:30:40.665574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [6.649 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/18/23 09:30:40.67
  May 18 09:30:40.670: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:30:40.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:30:40.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:30:40.695
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:30:40.699
  E0518 09:30:41.666333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:42.666594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:43.669379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:44.670132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:30:44.723
  May 18 09:30:44.729: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-bee6590a-657c-4c2a-9b4b-71f86eb39a85 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:30:44.737
  May 18 09:30:44.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4635" for this suite. @ 05/18/23 09:30:44.757
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/18/23 09:30:44.768
  May 18 09:30:44.768: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 09:30:44.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:30:44.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:30:44.796
  STEP: Creating pod test-grpc-c3ac61d7-e02d-49be-aec2-d89cc1e77e8f in namespace container-probe-6473 @ 05/18/23 09:30:44.8
  E0518 09:30:45.670131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:46.670516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:30:46.819: INFO: Started pod test-grpc-c3ac61d7-e02d-49be-aec2-d89cc1e77e8f in namespace container-probe-6473
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/18/23 09:30:46.82
  May 18 09:30:46.824: INFO: Initial restart count of pod test-grpc-c3ac61d7-e02d-49be-aec2-d89cc1e77e8f is 0
  E0518 09:30:47.670525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:48.670675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:49.670710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:50.671156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:51.671767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:52.671676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:53.671789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:54.672021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:55.672135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:56.672792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:57.673112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:58.673347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:30:59.673517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:00.673889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:01.674443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:02.674694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:03.675391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:04.675746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:05.676302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:06.676919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:07.677280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:08.677418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:09.677570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:10.677808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:11.678100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:12.678350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:13.678530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:14.679303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:15.679122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:16.680242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:17.680411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:18.681166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:19.681586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:20.682283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:21.682034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:22.682695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:23.683378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:24.684095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:25.684392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:26.685344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:27.686262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:28.687060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:29.687787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:30.687960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:31.688141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:32.688383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:33.688811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:34.689072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:35.689394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:36.689317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:37.689614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:38.690555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:39.691527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:40.692299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:41.692517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:42.693110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:43.693396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:44.693873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:45.694072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:46.694848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:47.695599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:48.695724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:49.696011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:50.697787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:31:51.010: INFO: Restart count of pod container-probe-6473/test-grpc-c3ac61d7-e02d-49be-aec2-d89cc1e77e8f is now 1 (1m4.184465322s elapsed)
  May 18 09:31:51.010: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:31:51.018
  STEP: Destroying namespace "container-probe-6473" for this suite. @ 05/18/23 09:31:51.044
• [66.322 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/18/23 09:31:51.094
  May 18 09:31:51.094: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename security-context-test @ 05/18/23 09:31:51.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:31:51.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:31:51.123
  E0518 09:31:51.698363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:52.700932      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:53.701880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:54.701951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:31:55.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5080" for this suite. @ 05/18/23 09:31:55.168
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/18/23 09:31:55.176
  May 18 09:31:55.176: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:31:55.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:31:55.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:31:55.194
  STEP: Creating a pod to test downward api env vars @ 05/18/23 09:31:55.197
  E0518 09:31:55.702966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:56.703499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:57.704445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:31:58.706358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:31:59.219
  May 18 09:31:59.223: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downward-api-09388ae2-f335-4d03-9cc2-6844ff3729aa container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 09:31:59.236
  May 18 09:31:59.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4288" for this suite. @ 05/18/23 09:31:59.255
• [4.087 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/18/23 09:31:59.263
  May 18 09:31:59.263: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:31:59.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:31:59.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:31:59.281
  STEP: Creating configMap with name configmap-test-volume-map-f123559c-e60d-46cb-87a1-4fbda659f7b0 @ 05/18/23 09:31:59.284
  STEP: Creating a pod to test consume configMaps @ 05/18/23 09:31:59.287
  E0518 09:31:59.706656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:00.706700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:01.707017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:02.707489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:32:03.309
  May 18 09:32:03.313: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-80375b87-59ec-490a-be9b-268c47cc677b container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 09:32:03.319
  May 18 09:32:03.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6461" for this suite. @ 05/18/23 09:32:03.336
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/18/23 09:32:03.349
  May 18 09:32:03.349: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:32:03.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:32:03.372
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:32:03.375
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:32:03.378
  E0518 09:32:03.708289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:04.709827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:05.709839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:06.710166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:32:07.401
  May 18 09:32:07.405: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-d3a113a3-7b51-4702-bc0c-ca0d2c79ddcd container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:32:07.414
  May 18 09:32:07.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5013" for this suite. @ 05/18/23 09:32:07.438
• [4.097 seconds]
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/18/23 09:32:07.447
  May 18 09:32:07.447: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:32:07.451
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:32:07.479
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:32:07.485
  May 18 09:32:07.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3578" for this suite. @ 05/18/23 09:32:07.537
• [0.101 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/18/23 09:32:07.554
  May 18 09:32:07.555: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename proxy @ 05/18/23 09:32:07.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:32:07.573
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:32:07.577
  May 18 09:32:07.582: INFO: Creating pod...
  E0518 09:32:07.711052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:08.711256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:09.599: INFO: Creating service...
  May 18 09:32:09.616: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/pods/agnhost/proxy?method=DELETE
  May 18 09:32:09.630: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 18 09:32:09.630: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/pods/agnhost/proxy?method=OPTIONS
  May 18 09:32:09.634: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 18 09:32:09.635: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/pods/agnhost/proxy?method=PATCH
  May 18 09:32:09.639: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 18 09:32:09.640: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/pods/agnhost/proxy?method=POST
  May 18 09:32:09.644: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 18 09:32:09.644: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/pods/agnhost/proxy?method=PUT
  May 18 09:32:09.649: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 18 09:32:09.649: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/services/e2e-proxy-test-service/proxy?method=DELETE
  May 18 09:32:09.656: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 18 09:32:09.656: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May 18 09:32:09.660: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 18 09:32:09.660: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/services/e2e-proxy-test-service/proxy?method=PATCH
  May 18 09:32:09.665: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 18 09:32:09.665: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/services/e2e-proxy-test-service/proxy?method=POST
  May 18 09:32:09.670: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 18 09:32:09.670: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/services/e2e-proxy-test-service/proxy?method=PUT
  May 18 09:32:09.678: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 18 09:32:09.679: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/pods/agnhost/proxy?method=GET
  May 18 09:32:09.683: INFO: http.Client request:GET StatusCode:301
  May 18 09:32:09.684: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/services/e2e-proxy-test-service/proxy?method=GET
  May 18 09:32:09.688: INFO: http.Client request:GET StatusCode:301
  May 18 09:32:09.688: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/pods/agnhost/proxy?method=HEAD
  May 18 09:32:09.691: INFO: http.Client request:HEAD StatusCode:301
  May 18 09:32:09.692: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-60/services/e2e-proxy-test-service/proxy?method=HEAD
  May 18 09:32:09.696: INFO: http.Client request:HEAD StatusCode:301
  May 18 09:32:09.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-60" for this suite. @ 05/18/23 09:32:09.703
  E0518 09:32:09.711319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [2.157 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/18/23 09:32:09.718
  May 18 09:32:09.719: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename statefulset @ 05/18/23 09:32:09.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:32:09.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:32:09.74
  STEP: Creating service test in namespace statefulset-4399 @ 05/18/23 09:32:09.744
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/18/23 09:32:09.751
  STEP: Creating stateful set ss in namespace statefulset-4399 @ 05/18/23 09:32:09.756
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4399 @ 05/18/23 09:32:09.762
  May 18 09:32:09.765: INFO: Found 0 stateful pods, waiting for 1
  E0518 09:32:10.712439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:11.712526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:12.712915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:13.713766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:14.713871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:15.714624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:16.715200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:17.715149      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:18.715216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:19.715317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:19.772: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/18/23 09:32:19.772
  May 18 09:32:19.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-4399 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 18 09:32:20.009: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:32:20.009: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:32:20.009: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 18 09:32:20.015: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0518 09:32:20.716349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:21.717012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:22.717231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:23.718170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:24.718952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:25.719429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:26.719938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:27.720808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:28.737583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:29.724186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:30.024: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 18 09:32:30.024: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:32:30.048: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999047s
  E0518 09:32:30.725468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:31.056: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.994973931s
  E0518 09:32:31.725740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:32.062: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.986158675s
  E0518 09:32:32.727155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:33.068: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.980516032s
  E0518 09:32:33.727634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:34.072: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.974985882s
  E0518 09:32:34.728412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:35.077: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.970389714s
  E0518 09:32:35.728516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:36.084: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.964897498s
  E0518 09:32:36.728820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:37.089: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.957800991s
  E0518 09:32:37.729026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:38.094: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.952488234s
  E0518 09:32:38.729607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:39.102: INFO: Verifying statefulset ss doesn't scale past 1 for another 947.114517ms
  E0518 09:32:39.729885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4399 @ 05/18/23 09:32:40.103
  May 18 09:32:40.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-4399 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 18 09:32:40.380: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 18 09:32:40.381: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:32:40.381: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 18 09:32:40.386: INFO: Found 1 stateful pods, waiting for 3
  E0518 09:32:40.730710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:41.731548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:42.731668      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:43.731769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:44.731964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:45.732894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:46.733823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:47.734295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:48.735080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:49.735997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:50.393: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:32:50.393: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:32:50.393: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/18/23 09:32:50.393
  STEP: Scale down will halt with unhealthy stateful pod @ 05/18/23 09:32:50.393
  May 18 09:32:50.404: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-4399 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 18 09:32:50.712: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:32:50.712: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:32:50.712: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 18 09:32:50.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-4399 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0518 09:32:50.736271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:32:50.965: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:32:50.966: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:32:50.966: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 18 09:32:50.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-4399 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 18 09:32:51.230: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:32:51.230: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:32:51.230: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 18 09:32:51.230: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:32:51.233: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  E0518 09:32:51.736971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:52.737980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:53.738321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:54.738721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:55.739023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:56.739216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:57.739626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:58.739731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:32:59.739966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:00.741999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:01.246: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 18 09:33:01.246: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 18 09:33:01.246: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 18 09:33:01.270: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999908s
  E0518 09:33:01.742455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:02.277: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.994090085s
  E0518 09:33:02.743429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:03.284: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986862828s
  E0518 09:33:03.744463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:04.293: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.980088763s
  E0518 09:33:04.745304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:05.303: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.970172942s
  E0518 09:33:05.745421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:06.309: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.961861781s
  E0518 09:33:06.746162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:07.317: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.954486038s
  E0518 09:33:07.747906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:08.324: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.947528287s
  E0518 09:33:08.747837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:09.330: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.940265514s
  E0518 09:33:09.749155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:10.336: INFO: Verifying statefulset ss doesn't scale past 3 for another 934.526457ms
  E0518 09:33:10.748910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4399 @ 05/18/23 09:33:11.337
  May 18 09:33:11.346: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-4399 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 18 09:33:11.561: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 18 09:33:11.561: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:33:11.561: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 18 09:33:11.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-4399 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0518 09:33:11.750096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:11.809: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 18 09:33:11.809: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:33:11.809: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 18 09:33:11.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-4399 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 18 09:33:12.015: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 18 09:33:12.015: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:33:12.015: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 18 09:33:12.015: INFO: Scaling statefulset ss to 0
  E0518 09:33:12.750509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:13.751308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:14.751493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:15.751857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:16.752338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:17.752440      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:18.753117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:19.753596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:20.753700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:21.756169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/18/23 09:33:22.031
  May 18 09:33:22.031: INFO: Deleting all statefulset in ns statefulset-4399
  May 18 09:33:22.035: INFO: Scaling statefulset ss to 0
  May 18 09:33:22.048: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:33:22.052: INFO: Deleting statefulset ss
  May 18 09:33:22.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4399" for this suite. @ 05/18/23 09:33:22.075
• [72.363 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/18/23 09:33:22.087
  May 18 09:33:22.088: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:33:22.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:33:22.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:33:22.116
  STEP: Creating secret with name secret-test-cb7c6efc-1301-42c0-a05d-5d27274320b9 @ 05/18/23 09:33:22.12
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:33:22.125
  E0518 09:33:22.756345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:23.756561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:24.757545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:25.757933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:33:26.15
  May 18 09:33:26.154: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-secrets-0535eb4c-4363-47b4-91ca-30e8df211ade container secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:33:26.162
  May 18 09:33:26.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5499" for this suite. @ 05/18/23 09:33:26.195
• [4.121 seconds]
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/18/23 09:33:26.209
  May 18 09:33:26.209: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename init-container @ 05/18/23 09:33:26.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:33:26.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:33:26.233
  STEP: creating the pod @ 05/18/23 09:33:26.24
  May 18 09:33:26.241: INFO: PodSpec: initContainers in spec.initContainers
  E0518 09:33:26.757966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:27.758125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:28.758163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:29.758523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:30.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9271" for this suite. @ 05/18/23 09:33:30.204
• [4.005 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/18/23 09:33:30.226
  May 18 09:33:30.227: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:33:30.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:33:30.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:33:30.257
  STEP: Creating configMap with name projected-configmap-test-volume-e8690f57-0f24-41f0-ad64-79008fc1da89 @ 05/18/23 09:33:30.264
  STEP: Creating a pod to test consume configMaps @ 05/18/23 09:33:30.271
  E0518 09:33:30.758849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:31.759856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:32.760227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:33.761669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:33:34.303
  May 18 09:33:34.311: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-configmaps-5f2010cd-50b2-40fd-8685-7d1b644b60b7 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 09:33:34.319
  May 18 09:33:34.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5863" for this suite. @ 05/18/23 09:33:34.346
• [4.127 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/18/23 09:33:34.364
  May 18 09:33:34.364: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:33:34.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:33:34.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:33:34.393
  STEP: Setting up server cert @ 05/18/23 09:33:34.429
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:33:34.691
  STEP: Deploying the webhook pod @ 05/18/23 09:33:34.706
  STEP: Wait for the deployment to be ready @ 05/18/23 09:33:34.72
  May 18 09:33:34.743: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:33:34.762220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:35.762802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:36.762997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:33:36.763
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:33:36.782
  E0518 09:33:37.763183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:37.783: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 18 09:33:37.789: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3667-crds.webhook.example.com via the AdmissionRegistration API @ 05/18/23 09:33:38.307
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/18/23 09:33:38.335
  E0518 09:33:38.763430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:39.764233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:33:40.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0518 09:33:40.765508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-8938" for this suite. @ 05/18/23 09:33:41.043
  STEP: Destroying namespace "webhook-markers-3679" for this suite. @ 05/18/23 09:33:41.052
• [6.696 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/18/23 09:33:41.067
  May 18 09:33:41.067: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:33:41.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:33:41.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:33:41.101
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:33:41.104
  E0518 09:33:41.765700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:42.766254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:43.766196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:44.766773      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:33:45.13
  May 18 09:33:45.134: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-8a5dfb29-5b27-406c-a9cb-73e3776e3bd7 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:33:45.143
  May 18 09:33:45.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5793" for this suite. @ 05/18/23 09:33:45.162
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/18/23 09:33:45.175
  May 18 09:33:45.175: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:33:45.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:33:45.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:33:45.198
  STEP: Creating configMap with name cm-test-opt-del-82dad02c-87dc-4eb4-8795-ec56aeb09db6 @ 05/18/23 09:33:45.207
  STEP: Creating configMap with name cm-test-opt-upd-ef55f4a3-5471-4b10-9f3f-214053c3d278 @ 05/18/23 09:33:45.213
  STEP: Creating the pod @ 05/18/23 09:33:45.216
  E0518 09:33:45.766868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:46.767767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:47.767880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:48.768352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-82dad02c-87dc-4eb4-8795-ec56aeb09db6 @ 05/18/23 09:33:49.267
  STEP: Updating configmap cm-test-opt-upd-ef55f4a3-5471-4b10-9f3f-214053c3d278 @ 05/18/23 09:33:49.273
  STEP: Creating configMap with name cm-test-opt-create-e33aa31c-2bd6-4a70-b761-72b9859c29b3 @ 05/18/23 09:33:49.279
  STEP: waiting to observe update in volume @ 05/18/23 09:33:49.283
  E0518 09:33:49.769331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:50.769842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:51.770653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:52.771013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:53.771703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:54.772331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:55.773425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:56.774170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:57.774382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:58.774546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:33:59.774903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:00.775453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:01.777817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:02.776893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:03.777354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:04.778035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:05.777946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:06.778723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:07.779853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:08.780066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:09.780388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:10.780999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:11.782117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:12.782311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:13.782345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:14.782510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:15.782992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:16.783175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:17.784051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:18.784701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:19.785655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:20.786356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:21.786812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:22.786993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:23.787034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:24.787308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:25.787893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:26.788264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:27.789118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:28.789159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:29.789882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:30.790551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:31.791248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:32.792126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:33.792428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:34.792787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:35.793690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:36.794574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:37.795338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:38.796062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:39.797029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:40.797753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:41.797641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:42.799326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:43.799596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:44.799899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:45.800585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:46.801131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:47.801331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:48.801758      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:49.802502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:50.803387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:51.804233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:52.804514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:53.804925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:54.805108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:55.805263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:56.806140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:57.807035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:58.807459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:34:59.808310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:00.808576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:01.809096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:02.809264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:03.809981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:04.810637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:05.811546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:06.812678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:07.813304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:07.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-428" for this suite. @ 05/18/23 09:35:07.863
• [82.701 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/18/23 09:35:07.878
  May 18 09:35:07.878: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename aggregator @ 05/18/23 09:35:07.889
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:35:07.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:35:07.931
  May 18 09:35:07.946: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Registering the sample API server. @ 05/18/23 09:35:07.948
  May 18 09:35:08.303: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May 18 09:35:08.333: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  E0518 09:35:08.814131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:09.814260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:10.386: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:10.814939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:11.815999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:12.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:12.816416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:13.816656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:14.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:14.816647      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:15.817087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:16.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:16.817305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:17.818229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:18.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:18.819421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:19.820539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:20.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:20.821142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:21.822174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:22.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:22.823120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:23.824187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:24.395: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:24.825022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:25.825191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:26.392: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:26.825818      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:27.826504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:28.391: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:28.826373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:29.826775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:30.393: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 8, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:35:30.827496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:31.828000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:32.511: INFO: Waited 114.959045ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/18/23 09:35:32.574
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/18/23 09:35:32.578
  STEP: List APIServices @ 05/18/23 09:35:32.586
  May 18 09:35:32.598: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/18/23 09:35:32.598
  May 18 09:35:32.610: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/18/23 09:35:32.61
  May 18 09:35:32.621: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 18, 9, 35, 32, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/18/23 09:35:32.622
  May 18 09:35:32.627: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-18 09:35:32 +0000 UTC Passed all checks passed}
  May 18 09:35:32.628: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 18 09:35:32.629: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/18/23 09:35:32.629
  May 18 09:35:32.641: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1033568396" @ 05/18/23 09:35:32.641
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/18/23 09:35:32.666
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/18/23 09:35:32.675
  STEP: Patch APIService Status @ 05/18/23 09:35:32.679
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/18/23 09:35:32.687
  May 18 09:35:32.693: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-18 09:35:32 +0000 UTC Passed all checks passed}
  May 18 09:35:32.693: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 18 09:35:32.693: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May 18 09:35:32.694: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/18/23 09:35:32.694
  STEP: Confirm that the generated APIService has been deleted @ 05/18/23 09:35:32.708
  May 18 09:35:32.709: INFO: Requesting list of APIServices to confirm quantity
  May 18 09:35:32.716: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May 18 09:35:32.716: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  E0518 09:35:32.829110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:32.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-8949" for this suite. @ 05/18/23 09:35:32.907
• [25.036 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/18/23 09:35:32.915
  May 18 09:35:32.915: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename var-expansion @ 05/18/23 09:35:32.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:35:32.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:35:32.933
  STEP: Creating a pod to test substitution in container's args @ 05/18/23 09:35:32.937
  E0518 09:35:33.829410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:34.829798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:35.829810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:36.830073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:35:36.956
  May 18 09:35:36.959: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod var-expansion-ec5f7b76-da22-4f02-99cf-e13b840b81f7 container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 09:35:36.967
  May 18 09:35:36.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3339" for this suite. @ 05/18/23 09:35:36.986
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/18/23 09:35:36.996
  May 18 09:35:36.996: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename statefulset @ 05/18/23 09:35:36.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:35:37.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:35:37.017
  STEP: Creating service test in namespace statefulset-1955 @ 05/18/23 09:35:37.02
  STEP: Creating statefulset ss in namespace statefulset-1955 @ 05/18/23 09:35:37.027
  May 18 09:35:37.040: INFO: Found 0 stateful pods, waiting for 1
  E0518 09:35:37.832353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:38.832717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:39.832940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:40.833276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:41.833491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:42.834355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:43.834432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:44.834697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:45.835005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:46.835155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:47.049: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/18/23 09:35:47.061
  STEP: updating a scale subresource @ 05/18/23 09:35:47.066
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/18/23 09:35:47.078
  STEP: Patch a scale subresource @ 05/18/23 09:35:47.084
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/18/23 09:35:47.093
  May 18 09:35:47.101: INFO: Deleting all statefulset in ns statefulset-1955
  May 18 09:35:47.107: INFO: Scaling statefulset ss to 0
  E0518 09:35:47.836246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:48.836772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:49.837134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:50.837381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:51.837433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:52.838469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:53.839031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:54.840985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:55.839839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:56.839949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:35:57.133: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:35:57.137: INFO: Deleting statefulset ss
  May 18 09:35:57.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1955" for this suite. @ 05/18/23 09:35:57.156
• [20.167 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/18/23 09:35:57.17
  May 18 09:35:57.170: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:35:57.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:35:57.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:35:57.191
  May 18 09:35:57.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5364" for this suite. @ 05/18/23 09:35:57.236
• [0.071 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/18/23 09:35:57.243
  May 18 09:35:57.243: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename dns @ 05/18/23 09:35:57.245
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:35:57.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:35:57.266
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/18/23 09:35:57.274
  May 18 09:35:57.281: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-1156  043b3583-9063-476c-8109-85e4019ce73e 1944483 0 2023-05-18 09:35:57 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-18 09:35:57 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4l8wb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4l8wb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0518 09:35:57.841054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:35:58.842341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/18/23 09:35:59.294
  May 18 09:35:59.295: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-1156 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:35:59.295: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:35:59.296: INFO: ExecWithOptions: Clientset creation
  May 18 09:35:59.297: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1156/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/18/23 09:35:59.463
  May 18 09:35:59.463: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-1156 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:35:59.463: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:35:59.464: INFO: ExecWithOptions: Clientset creation
  May 18 09:35:59.464: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-1156/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 18 09:35:59.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:35:59.595: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-1156" for this suite. @ 05/18/23 09:35:59.609
• [2.372 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/18/23 09:35:59.638
  May 18 09:35:59.639: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename ingressclass @ 05/18/23 09:35:59.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:35:59.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:35:59.661
  STEP: getting /apis @ 05/18/23 09:35:59.665
  STEP: getting /apis/networking.k8s.io @ 05/18/23 09:35:59.672
  STEP: getting /apis/networking.k8s.iov1 @ 05/18/23 09:35:59.677
  STEP: creating @ 05/18/23 09:35:59.679
  STEP: getting @ 05/18/23 09:35:59.691
  STEP: listing @ 05/18/23 09:35:59.693
  STEP: watching @ 05/18/23 09:35:59.696
  May 18 09:35:59.696: INFO: starting watch
  STEP: patching @ 05/18/23 09:35:59.698
  STEP: updating @ 05/18/23 09:35:59.702
  May 18 09:35:59.706: INFO: waiting for watch events with expected annotations
  May 18 09:35:59.706: INFO: saw patched and updated annotations
  STEP: deleting @ 05/18/23 09:35:59.707
  STEP: deleting a collection @ 05/18/23 09:35:59.715
  May 18 09:35:59.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-7232" for this suite. @ 05/18/23 09:35:59.731
• [0.101 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/18/23 09:35:59.74
  May 18 09:35:59.740: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:35:59.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:35:59.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:35:59.811
  E0518 09:35:59.841589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:00.842210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:01.842906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:02.843541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:03.844516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:04.845321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:05.846130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:06.846504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:07.847342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:08.848215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:09.849017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:10.849828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:11.849970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:12.850771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:13.851309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:14.851703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:15.852143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 05/18/23 09:36:16.822
  E0518 09:36:16.853121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:17.853420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:18.854111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:19.854540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:20.855162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/18/23 09:36:21.828
  STEP: Ensuring resource quota status is calculated @ 05/18/23 09:36:21.835
  E0518 09:36:21.856009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:22.856268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 05/18/23 09:36:23.841
  E0518 09:36:23.856732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status captures configMap creation @ 05/18/23 09:36:23.856
  E0518 09:36:24.857089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:25.857234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 05/18/23 09:36:25.864
  STEP: Ensuring resource quota status released usage @ 05/18/23 09:36:25.871
  E0518 09:36:26.857775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:27.859915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:27.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4946" for this suite. @ 05/18/23 09:36:27.902
• [28.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/18/23 09:36:27.922
  May 18 09:36:27.922: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:36:27.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:36:27.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:36:27.966
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/18/23 09:36:27.971
  May 18 09:36:27.971: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-3880 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 18 09:36:28.162: INFO: stderr: ""
  May 18 09:36:28.162: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/18/23 09:36:28.162
  E0518 09:36:28.859846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:29.860618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:30.860830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:31.861435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:32.861645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/18/23 09:36:33.214
  May 18 09:36:33.214: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-3880 get pod e2e-test-httpd-pod -o json'
  May 18 09:36:33.370: INFO: stderr: ""
  May 18 09:36:33.370: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"8aa02d5965615a960d29ff51c43b5e11133ef1f16874416a2d977a24ea8eebb0\",\n            \"cni.projectcalico.org/podIP\": \"172.16.161.88/32\",\n            \"cni.projectcalico.org/podIPs\": \"172.16.161.88/32\"\n        },\n        \"creationTimestamp\": \"2023-05-18T09:36:28Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3880\",\n        \"resourceVersion\": \"1944714\",\n        \"uid\": \"9280ae89-5fdb-4df0-8693-e55565ea9bf2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-r9j7j\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ck-test-kube-1-27-worker\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-r9j7j\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-18T09:36:28Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-18T09:36:29Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-18T09:36:29Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-18T09:36:28Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://7866548185fea711232f09a0dd2cdff51c1c5b5f8aece1e0ed0b3e9309d9f3c9\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-18T09:36:29Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.190.143\",\n        \"phase\": \"Running\",\n        \"podIP\": \"172.16.161.88\",\n        \"podIPs\": [\n            {\n                \"ip\": \"172.16.161.88\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-18T09:36:28Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/18/23 09:36:33.37
  May 18 09:36:33.370: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-3880 replace -f -'
  E0518 09:36:33.862509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:34.328: INFO: stderr: ""
  May 18 09:36:34.328: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/18/23 09:36:34.328
  May 18 09:36:34.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-3880 delete pods e2e-test-httpd-pod'
  E0518 09:36:34.862693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:35.862978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:36.009: INFO: stderr: ""
  May 18 09:36:36.009: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 18 09:36:36.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3880" for this suite. @ 05/18/23 09:36:36.018
• [8.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/18/23 09:36:36.039
  May 18 09:36:36.040: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:36:36.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:36:36.068
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:36:36.073
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9447 @ 05/18/23 09:36:36.077
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/18/23 09:36:36.091
  STEP: creating service externalsvc in namespace services-9447 @ 05/18/23 09:36:36.092
  STEP: creating replication controller externalsvc in namespace services-9447 @ 05/18/23 09:36:36.106
  I0518 09:36:36.114279      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9447, replica count: 2
  E0518 09:36:36.863363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:37.863678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:38.863871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:36:39.169995      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/18/23 09:36:39.176
  May 18 09:36:39.201: INFO: Creating new exec pod
  E0518 09:36:39.864230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:40.864897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:41.224: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-9447 exec execpodjksf4 -- /bin/sh -x -c nslookup clusterip-service.services-9447.svc.cluster.local'
  May 18 09:36:41.460: INFO: stderr: "+ nslookup clusterip-service.services-9447.svc.cluster.local\n"
  May 18 09:36:41.460: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-9447.svc.cluster.local\tcanonical name = externalsvc.services-9447.svc.cluster.local.\nName:\texternalsvc.services-9447.svc.cluster.local\nAddress: 10.101.202.32\n\n"
  May 18 09:36:41.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9447, will wait for the garbage collector to delete the pods @ 05/18/23 09:36:41.465
  May 18 09:36:41.525: INFO: Deleting ReplicationController externalsvc took: 6.270181ms
  May 18 09:36:41.626: INFO: Terminating ReplicationController externalsvc pods took: 100.341989ms
  E0518 09:36:41.872501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:42.873434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:43.752: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-9447" for this suite. @ 05/18/23 09:36:43.773
• [7.748 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/18/23 09:36:43.799
  May 18 09:36:43.800: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubelet-test @ 05/18/23 09:36:43.803
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:36:43.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:36:43.834
  E0518 09:36:43.877156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:44.877626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:45.878109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:45.878: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7295" for this suite. @ 05/18/23 09:36:45.884
• [2.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/18/23 09:36:45.901
  May 18 09:36:45.901: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename var-expansion @ 05/18/23 09:36:45.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:36:45.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:36:45.921
  STEP: Creating a pod to test substitution in container's command @ 05/18/23 09:36:45.925
  E0518 09:36:46.878526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:47.878806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:48.881591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:49.883023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:36:49.953
  May 18 09:36:49.956: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod var-expansion-4e710ea1-a410-46a6-837a-c51aff6ae05c container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 09:36:49.964
  May 18 09:36:49.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8308" for this suite. @ 05/18/23 09:36:49.983
• [4.089 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/18/23 09:36:49.992
  May 18 09:36:49.992: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename deployment @ 05/18/23 09:36:49.994
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:36:50.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:36:50.015
  May 18 09:36:50.019: INFO: Creating deployment "test-recreate-deployment"
  May 18 09:36:50.024: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May 18 09:36:50.031: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0518 09:36:50.883651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:51.884549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:52.043: INFO: Waiting deployment "test-recreate-deployment" to complete
  May 18 09:36:52.049: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May 18 09:36:52.068: INFO: Updating deployment test-recreate-deployment
  May 18 09:36:52.068: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May 18 09:36:52.159: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3722  b468c34a-77a8-4f01-8f85-75d3269a206f 1945054 2 2023-05-18 09:36:50 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-18 09:36:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:36:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a17d898 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-18 09:36:52 +0000 UTC,LastTransitionTime:2023-05-18 09:36:52 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-18 09:36:52 +0000 UTC,LastTransitionTime:2023-05-18 09:36:50 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May 18 09:36:52.163: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-3722  d2f62e90-1b82-4dcd-83ac-b99108506c0b 1945053 1 2023-05-18 09:36:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment b468c34a-77a8-4f01-8f85-75d3269a206f 0xc00a17dc57 0xc00a17dc58}] [] [{kube-controller-manager Update apps/v1 2023-05-18 09:36:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b468c34a-77a8-4f01-8f85-75d3269a206f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:36:52 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a17dcf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:36:52.163: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May 18 09:36:52.163: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-3722  ae633daf-53ea-4ddc-b310-dc449b319632 1945043 2 2023-05-18 09:36:50 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment b468c34a-77a8-4f01-8f85-75d3269a206f 0xc00a17dd67 0xc00a17dd68}] [] [{kube-controller-manager Update apps/v1 2023-05-18 09:36:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b468c34a-77a8-4f01-8f85-75d3269a206f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:36:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00a17de18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:36:52.167: INFO: Pod "test-recreate-deployment-54757ffd6c-2j75n" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-2j75n test-recreate-deployment-54757ffd6c- deployment-3722  8de34dec-a96f-4977-8674-e243e3b4bd75 1945055 0 2023-05-18 09:36:52 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c d2f62e90-1b82-4dcd-83ac-b99108506c0b 0xc004d962a7 0xc004d962a8}] [] [{kube-controller-manager Update v1 2023-05-18 09:36:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d2f62e90-1b82-4dcd-83ac-b99108506c0b\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-18 09:36:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mz99w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mz99w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:36:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:36:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:36:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:36:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:,StartTime:2023-05-18 09:36:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:36:52.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3722" for this suite. @ 05/18/23 09:36:52.175
• [2.190 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/18/23 09:36:52.185
  May 18 09:36:52.186: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 09:36:52.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:36:52.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:36:52.205
  May 18 09:36:52.209: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:36:52.884350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:53.884463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:54.885395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/18/23 09:36:55.237
  May 18 09:36:55.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-5571 --namespace=crd-publish-openapi-5571 create -f -'
  E0518 09:36:55.885624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:56.874: INFO: stderr: ""
  May 18 09:36:56.874: INFO: stdout: "e2e-test-crd-publish-openapi-8538-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 18 09:36:56.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-5571 --namespace=crd-publish-openapi-5571 delete e2e-test-crd-publish-openapi-8538-crds test-cr'
  E0518 09:36:56.885766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:56.984: INFO: stderr: ""
  May 18 09:36:56.985: INFO: stdout: "e2e-test-crd-publish-openapi-8538-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May 18 09:36:56.985: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-5571 --namespace=crd-publish-openapi-5571 apply -f -'
  May 18 09:36:57.482: INFO: stderr: ""
  May 18 09:36:57.482: INFO: stdout: "e2e-test-crd-publish-openapi-8538-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 18 09:36:57.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-5571 --namespace=crd-publish-openapi-5571 delete e2e-test-crd-publish-openapi-8538-crds test-cr'
  May 18 09:36:57.581: INFO: stderr: ""
  May 18 09:36:57.581: INFO: stdout: "e2e-test-crd-publish-openapi-8538-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/18/23 09:36:57.582
  May 18 09:36:57.582: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-5571 explain e2e-test-crd-publish-openapi-8538-crds'
  E0518 09:36:57.886074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:58.086: INFO: stderr: ""
  May 18 09:36:58.086: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-8538-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0518 09:36:58.886577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:36:59.890363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:36:59.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5571" for this suite. @ 05/18/23 09:36:59.971
• [7.791 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/18/23 09:36:59.983
  May 18 09:36:59.983: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename runtimeclass @ 05/18/23 09:36:59.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:36:59.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:37:00.001
  STEP: getting /apis @ 05/18/23 09:37:00.005
  STEP: getting /apis/node.k8s.io @ 05/18/23 09:37:00.012
  STEP: getting /apis/node.k8s.io/v1 @ 05/18/23 09:37:00.013
  STEP: creating @ 05/18/23 09:37:00.015
  STEP: watching @ 05/18/23 09:37:00.027
  May 18 09:37:00.028: INFO: starting watch
  STEP: getting @ 05/18/23 09:37:00.033
  STEP: listing @ 05/18/23 09:37:00.035
  STEP: patching @ 05/18/23 09:37:00.038
  STEP: updating @ 05/18/23 09:37:00.043
  May 18 09:37:00.049: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/18/23 09:37:00.049
  STEP: deleting a collection @ 05/18/23 09:37:00.059
  May 18 09:37:00.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3388" for this suite. @ 05/18/23 09:37:00.074
• [0.097 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/18/23 09:37:00.081
  May 18 09:37:00.081: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 09:37:00.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:37:00.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:37:00.104
  STEP: set up a multi version CRD @ 05/18/23 09:37:00.107
  May 18 09:37:00.108: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:37:00.891221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:01.892223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:02.892711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:03.893543      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:04.894505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:05.895086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 05/18/23 09:37:06.786
  STEP: check the unserved version gets removed @ 05/18/23 09:37:06.808
  E0518 09:37:06.896098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:07.898369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:08.898519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/18/23 09:37:09.367
  E0518 09:37:09.899455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:10.900776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:11.901247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:12.901948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:13.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8179" for this suite. @ 05/18/23 09:37:13.628
• [13.557 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/18/23 09:37:13.642
  May 18 09:37:13.642: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 09:37:13.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:37:13.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:37:13.663
  May 18 09:37:13.669: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:37:13.902403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:14.903338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/18/23 09:37:15.591
  May 18 09:37:15.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 create -f -'
  E0518 09:37:15.904686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:16.904877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:17.421: INFO: stderr: ""
  May 18 09:37:17.421: INFO: stdout: "e2e-test-crd-publish-openapi-5212-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 18 09:37:17.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 delete e2e-test-crd-publish-openapi-5212-crds test-foo'
  May 18 09:37:17.553: INFO: stderr: ""
  May 18 09:37:17.553: INFO: stdout: "e2e-test-crd-publish-openapi-5212-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May 18 09:37:17.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 apply -f -'
  E0518 09:37:17.905472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:18.024: INFO: stderr: ""
  May 18 09:37:18.024: INFO: stdout: "e2e-test-crd-publish-openapi-5212-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 18 09:37:18.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 delete e2e-test-crd-publish-openapi-5212-crds test-foo'
  May 18 09:37:18.152: INFO: stderr: ""
  May 18 09:37:18.152: INFO: stdout: "e2e-test-crd-publish-openapi-5212-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/18/23 09:37:18.152
  May 18 09:37:18.152: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 create -f -'
  May 18 09:37:18.616: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/18/23 09:37:18.616
  May 18 09:37:18.617: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 create -f -'
  E0518 09:37:18.905894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:19.063: INFO: rc: 1
  May 18 09:37:19.064: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 apply -f -'
  May 18 09:37:19.572: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/18/23 09:37:19.572
  May 18 09:37:19.572: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 create -f -'
  E0518 09:37:19.906845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:20.088: INFO: rc: 1
  May 18 09:37:20.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 --namespace=crd-publish-openapi-2083 apply -f -'
  May 18 09:37:20.541: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/18/23 09:37:20.541
  May 18 09:37:20.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 explain e2e-test-crd-publish-openapi-5212-crds'
  E0518 09:37:20.906914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:21.009: INFO: stderr: ""
  May 18 09:37:21.009: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-5212-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/18/23 09:37:21.01
  May 18 09:37:21.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 explain e2e-test-crd-publish-openapi-5212-crds.metadata'
  May 18 09:37:21.554: INFO: stderr: ""
  May 18 09:37:21.554: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-5212-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May 18 09:37:21.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 explain e2e-test-crd-publish-openapi-5212-crds.spec'
  E0518 09:37:21.907227      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:21.948: INFO: stderr: ""
  May 18 09:37:21.948: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-5212-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May 18 09:37:21.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 explain e2e-test-crd-publish-openapi-5212-crds.spec.bars'
  May 18 09:37:22.380: INFO: stderr: ""
  May 18 09:37:22.380: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-5212-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/18/23 09:37:22.381
  May 18 09:37:22.381: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-2083 explain e2e-test-crd-publish-openapi-5212-crds.spec.bars2'
  May 18 09:37:22.839: INFO: rc: 1
  E0518 09:37:22.907526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:23.909129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:24.909669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:25.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2083" for this suite. @ 05/18/23 09:37:25.801
• [12.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/18/23 09:37:25.819
  May 18 09:37:25.820: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename dns @ 05/18/23 09:37:25.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:37:25.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:37:25.839
  STEP: Creating a test externalName service @ 05/18/23 09:37:25.842
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7187.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7187.svc.cluster.local; sleep 1; done
   @ 05/18/23 09:37:25.848
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7187.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7187.svc.cluster.local; sleep 1; done
   @ 05/18/23 09:37:25.848
  STEP: creating a pod to probe DNS @ 05/18/23 09:37:25.848
  STEP: submitting the pod to kubernetes @ 05/18/23 09:37:25.849
  E0518 09:37:25.910139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:26.910257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 09:37:27.874
  STEP: looking for the results for each expected name from probers @ 05/18/23 09:37:27.879
  May 18 09:37:27.909: INFO: DNS probes using dns-test-bb1e4e68-74ce-4406-8b01-c6007415f066 succeeded

  STEP: changing the externalName to bar.example.com @ 05/18/23 09:37:27.909
  E0518 09:37:27.922617      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7187.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-7187.svc.cluster.local; sleep 1; done
   @ 05/18/23 09:37:27.933
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7187.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-7187.svc.cluster.local; sleep 1; done
   @ 05/18/23 09:37:27.933
  STEP: creating a second pod to probe DNS @ 05/18/23 09:37:27.933
  STEP: submitting the pod to kubernetes @ 05/18/23 09:37:27.934
  E0518 09:37:28.913252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:29.914146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 09:37:30
  STEP: looking for the results for each expected name from probers @ 05/18/23 09:37:30.006
  May 18 09:37:30.017: INFO: DNS probes using dns-test-c7fa0d68-363a-4499-bec1-9628288e2965 succeeded

  STEP: changing the service to type=ClusterIP @ 05/18/23 09:37:30.018
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7187.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-7187.svc.cluster.local; sleep 1; done
   @ 05/18/23 09:37:30.037
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-7187.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-7187.svc.cluster.local; sleep 1; done
   @ 05/18/23 09:37:30.037
  STEP: creating a third pod to probe DNS @ 05/18/23 09:37:30.037
  STEP: submitting the pod to kubernetes @ 05/18/23 09:37:30.042
  E0518 09:37:30.913649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:31.914764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:32.915487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:33.915786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 09:37:34.075
  STEP: looking for the results for each expected name from probers @ 05/18/23 09:37:34.081
  May 18 09:37:34.095: INFO: DNS probes using dns-test-c1c11496-f54d-40c7-9978-9c0fa6746267 succeeded

  May 18 09:37:34.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:37:34.103
  STEP: deleting the pod @ 05/18/23 09:37:34.128
  STEP: deleting the pod @ 05/18/23 09:37:34.163
  STEP: deleting the test externalName service @ 05/18/23 09:37:34.217
  STEP: Destroying namespace "dns-7187" for this suite. @ 05/18/23 09:37:34.242
• [8.429 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/18/23 09:37:34.257
  May 18 09:37:34.257: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename statefulset @ 05/18/23 09:37:34.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:37:34.292
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:37:34.296
  STEP: Creating service test in namespace statefulset-1183 @ 05/18/23 09:37:34.3
  STEP: Creating stateful set ss in namespace statefulset-1183 @ 05/18/23 09:37:34.306
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1183 @ 05/18/23 09:37:34.315
  May 18 09:37:34.319: INFO: Found 0 stateful pods, waiting for 1
  E0518 09:37:34.916684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:35.916633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:36.917776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:37.921234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:38.921605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:39.922116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:40.922343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:41.923014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:42.923343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:43.923503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:44.326: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/18/23 09:37:44.326
  May 18 09:37:44.329: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-1183 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 18 09:37:44.597: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:37:44.597: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:37:44.597: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 18 09:37:44.603: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0518 09:37:44.924692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:45.925490      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:46.926160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:47.929171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:48.929357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:49.929671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:50.930058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:51.930707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:52.931114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:37:53.931436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:54.608: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 18 09:37:54.609: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:37:54.625: INFO: POD   NODE                      PHASE    GRACE  CONDITIONS
  May 18 09:37:54.625: INFO: ss-0  ck-test-kube-1-27-worker  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:34 +0000 UTC  }]
  May 18 09:37:54.626: INFO: 
  May 18 09:37:54.626: INFO: StatefulSet ss has not reached scale 3, at 1
  E0518 09:37:54.932468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:55.634: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993398165s
  E0518 09:37:55.933342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:56.641: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.986165379s
  E0518 09:37:56.933832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:57.646: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.979391571s
  E0518 09:37:57.934502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:58.653: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.974167807s
  E0518 09:37:58.935347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:37:59.658: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.967338952s
  E0518 09:37:59.936176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:00.668: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.962363201s
  E0518 09:38:00.936808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:01.676: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.95120568s
  E0518 09:38:01.937850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:02.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.944165342s
  E0518 09:38:02.938292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:03.689: INFO: Verifying statefulset ss doesn't scale past 3 for another 937.629695ms
  E0518 09:38:03.939211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1183 @ 05/18/23 09:38:04.69
  May 18 09:38:04.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-1183 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0518 09:38:04.940407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:05.006: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 18 09:38:05.006: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:38:05.006: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 18 09:38:05.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-1183 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 18 09:38:05.230: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 18 09:38:05.230: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:38:05.230: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 18 09:38:05.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-1183 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 18 09:38:05.446: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 18 09:38:05.446: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 18 09:38:05.446: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 18 09:38:05.451: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0518 09:38:05.940258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:06.941179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:07.948097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:08.948604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:09.948727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:10.948964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:11.949195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:12.949384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:13.949501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:14.950174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:15.460: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:38:15.460: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 18 09:38:15.460: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/18/23 09:38:15.46
  May 18 09:38:15.467: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-1183 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 18 09:38:15.759: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:38:15.759: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:38:15.759: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 18 09:38:15.759: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-1183 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0518 09:38:15.951369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:16.036: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:38:16.036: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:38:16.036: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 18 09:38:16.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=statefulset-1183 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 18 09:38:16.295: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 18 09:38:16.295: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 18 09:38:16.295: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 18 09:38:16.295: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:38:16.300: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0518 09:38:16.952340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:17.957039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:18.957159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:19.957444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:20.957484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:21.957998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:22.958802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:23.959216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:24.959219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:25.959392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:26.309: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 18 09:38:26.309: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 18 09:38:26.309: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 18 09:38:26.321: INFO: POD   NODE                        PHASE    GRACE  CONDITIONS
  May 18 09:38:26.322: INFO: ss-0  ck-test-kube-1-27-worker    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:34 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:34 +0000 UTC  }]
  May 18 09:38:26.322: INFO: ss-1  ck-test-kube-1-27-worker-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:54 +0000 UTC  }]
  May 18 09:38:26.322: INFO: ss-2  ck-test-kube-1-27-worker    Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:54 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:54 +0000 UTC  }]
  May 18 09:38:26.322: INFO: 
  May 18 09:38:26.322: INFO: StatefulSet ss has not reached scale 0, at 3
  E0518 09:38:26.961254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:27.329: INFO: POD   NODE                        PHASE      GRACE  CONDITIONS
  May 18 09:38:27.330: INFO: ss-0  ck-test-kube-1-27-worker    Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:34 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:34 +0000 UTC  }]
  May 18 09:38:27.330: INFO: ss-1  ck-test-kube-1-27-worker-1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:54 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:54 +0000 UTC  }]
  May 18 09:38:27.331: INFO: ss-2  ck-test-kube-1-27-worker    Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:54 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:38:16 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-18 09:37:54 +0000 UTC  }]
  May 18 09:38:27.331: INFO: 
  May 18 09:38:27.331: INFO: StatefulSet ss has not reached scale 0, at 3
  E0518 09:38:27.969307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:28.336: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.986019414s
  E0518 09:38:28.969354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:29.343: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.979521121s
  E0518 09:38:29.969660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:30.351: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.974443548s
  E0518 09:38:30.970519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:31.356: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.966765847s
  E0518 09:38:31.971610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:32.362: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.960970882s
  E0518 09:38:32.971738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:33.367: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.954478985s
  E0518 09:38:33.972895      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:34.373: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.949278046s
  E0518 09:38:34.973835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:35.378: INFO: Verifying statefulset ss doesn't scale past 0 for another 943.136091ms
  E0518 09:38:35.975193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1183 @ 05/18/23 09:38:36.379
  May 18 09:38:36.386: INFO: Scaling statefulset ss to 0
  May 18 09:38:36.402: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:38:36.407: INFO: Deleting all statefulset in ns statefulset-1183
  May 18 09:38:36.412: INFO: Scaling statefulset ss to 0
  May 18 09:38:36.429: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 09:38:36.433: INFO: Deleting statefulset ss
  May 18 09:38:36.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1183" for this suite. @ 05/18/23 09:38:36.457
• [62.213 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/18/23 09:38:36.473
  May 18 09:38:36.473: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:38:36.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:38:36.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:38:36.501
  STEP: Setting up server cert @ 05/18/23 09:38:36.531
  E0518 09:38:36.974429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:38:37.182
  STEP: Deploying the webhook pod @ 05/18/23 09:38:37.191
  STEP: Wait for the deployment to be ready @ 05/18/23 09:38:37.206
  May 18 09:38:37.216: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:38:37.978024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:38.978820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:39.232: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 9, 38, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 38, 37, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 38, 37, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 38, 37, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:38:39.979134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:40.979217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:38:41.238
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:38:41.26
  E0518 09:38:41.980261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:42.262: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/18/23 09:38:42.269
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/18/23 09:38:42.303
  STEP: Creating a configMap that should not be mutated @ 05/18/23 09:38:42.314
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/18/23 09:38:42.328
  STEP: Creating a configMap that should be mutated @ 05/18/23 09:38:42.338
  May 18 09:38:42.373: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5513" for this suite. @ 05/18/23 09:38:42.436
  STEP: Destroying namespace "webhook-markers-4632" for this suite. @ 05/18/23 09:38:42.442
• [6.000 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/18/23 09:38:42.475
  May 18 09:38:42.475: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/18/23 09:38:42.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:38:42.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:38:42.508
  May 18 09:38:42.511: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:38:42.980819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:43.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-206" for this suite. @ 05/18/23 09:38:43.105
• [0.639 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/18/23 09:38:43.119
  May 18 09:38:43.120: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:38:43.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:38:43.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:38:43.155
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/18/23 09:38:43.161
  E0518 09:38:43.981910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:44.982537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:45.983654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:46.984754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:38:47.194
  May 18 09:38:47.197: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-64e19336-2bc5-4853-80d9-2bd6ec5da24c container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:38:47.229
  May 18 09:38:47.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8134" for this suite. @ 05/18/23 09:38:47.257
• [4.149 seconds]
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/18/23 09:38:47.269
  May 18 09:38:47.269: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:38:47.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:38:47.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:38:47.297
  STEP: creating secret secrets-2218/secret-test-50f6d0d5-d364-4a8b-9cc2-598b558dfa2b @ 05/18/23 09:38:47.301
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:38:47.307
  E0518 09:38:47.985626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:48.986024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:49.986114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:50.986709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:38:51.333
  May 18 09:38:51.340: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-755be993-fab7-4eb4-b8bf-668f3ebaa744 container env-test: <nil>
  STEP: delete the pod @ 05/18/23 09:38:51.351
  May 18 09:38:51.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2218" for this suite. @ 05/18/23 09:38:51.377
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/18/23 09:38:51.385
  May 18 09:38:51.385: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename discovery @ 05/18/23 09:38:51.387
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:38:51.403
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:38:51.407
  STEP: Setting up server cert @ 05/18/23 09:38:51.413
  May 18 09:38:51.884: INFO: Checking APIGroup: apiregistration.k8s.io
  May 18 09:38:51.886: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May 18 09:38:51.886: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May 18 09:38:51.886: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May 18 09:38:51.886: INFO: Checking APIGroup: apps
  May 18 09:38:51.887: INFO: PreferredVersion.GroupVersion: apps/v1
  May 18 09:38:51.887: INFO: Versions found [{apps/v1 v1}]
  May 18 09:38:51.887: INFO: apps/v1 matches apps/v1
  May 18 09:38:51.887: INFO: Checking APIGroup: events.k8s.io
  May 18 09:38:51.889: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May 18 09:38:51.889: INFO: Versions found [{events.k8s.io/v1 v1}]
  May 18 09:38:51.889: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May 18 09:38:51.889: INFO: Checking APIGroup: authentication.k8s.io
  May 18 09:38:51.891: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May 18 09:38:51.891: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May 18 09:38:51.891: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May 18 09:38:51.891: INFO: Checking APIGroup: authorization.k8s.io
  May 18 09:38:51.893: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May 18 09:38:51.893: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May 18 09:38:51.893: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May 18 09:38:51.893: INFO: Checking APIGroup: autoscaling
  May 18 09:38:51.895: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May 18 09:38:51.895: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May 18 09:38:51.895: INFO: autoscaling/v2 matches autoscaling/v2
  May 18 09:38:51.895: INFO: Checking APIGroup: batch
  May 18 09:38:51.896: INFO: PreferredVersion.GroupVersion: batch/v1
  May 18 09:38:51.896: INFO: Versions found [{batch/v1 v1}]
  May 18 09:38:51.896: INFO: batch/v1 matches batch/v1
  May 18 09:38:51.896: INFO: Checking APIGroup: certificates.k8s.io
  May 18 09:38:51.898: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May 18 09:38:51.898: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May 18 09:38:51.898: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May 18 09:38:51.898: INFO: Checking APIGroup: networking.k8s.io
  May 18 09:38:51.900: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May 18 09:38:51.900: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May 18 09:38:51.900: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May 18 09:38:51.900: INFO: Checking APIGroup: policy
  May 18 09:38:51.901: INFO: PreferredVersion.GroupVersion: policy/v1
  May 18 09:38:51.901: INFO: Versions found [{policy/v1 v1}]
  May 18 09:38:51.901: INFO: policy/v1 matches policy/v1
  May 18 09:38:51.901: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May 18 09:38:51.903: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May 18 09:38:51.903: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May 18 09:38:51.903: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May 18 09:38:51.903: INFO: Checking APIGroup: storage.k8s.io
  May 18 09:38:51.904: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May 18 09:38:51.904: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May 18 09:38:51.904: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May 18 09:38:51.905: INFO: Checking APIGroup: admissionregistration.k8s.io
  May 18 09:38:51.906: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May 18 09:38:51.906: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May 18 09:38:51.906: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May 18 09:38:51.906: INFO: Checking APIGroup: apiextensions.k8s.io
  May 18 09:38:51.908: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May 18 09:38:51.908: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May 18 09:38:51.908: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May 18 09:38:51.908: INFO: Checking APIGroup: scheduling.k8s.io
  May 18 09:38:51.909: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May 18 09:38:51.909: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May 18 09:38:51.909: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May 18 09:38:51.909: INFO: Checking APIGroup: coordination.k8s.io
  May 18 09:38:51.910: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May 18 09:38:51.911: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May 18 09:38:51.911: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May 18 09:38:51.911: INFO: Checking APIGroup: node.k8s.io
  May 18 09:38:51.912: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May 18 09:38:51.912: INFO: Versions found [{node.k8s.io/v1 v1}]
  May 18 09:38:51.912: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May 18 09:38:51.912: INFO: Checking APIGroup: discovery.k8s.io
  May 18 09:38:51.913: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May 18 09:38:51.913: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May 18 09:38:51.913: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May 18 09:38:51.913: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May 18 09:38:51.915: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May 18 09:38:51.915: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May 18 09:38:51.915: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May 18 09:38:51.915: INFO: Checking APIGroup: crd.projectcalico.org
  May 18 09:38:51.916: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  May 18 09:38:51.916: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  May 18 09:38:51.916: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  May 18 09:38:51.916: INFO: Checking APIGroup: imageregistry.secloudit.io
  May 18 09:38:51.917: INFO: PreferredVersion.GroupVersion: imageregistry.secloudit.io/v1
  May 18 09:38:51.917: INFO: Versions found [{imageregistry.secloudit.io/v1 v1}]
  May 18 09:38:51.917: INFO: imageregistry.secloudit.io/v1 matches imageregistry.secloudit.io/v1
  May 18 09:38:51.917: INFO: Checking APIGroup: monitoring.coreos.com
  May 18 09:38:51.918: INFO: PreferredVersion.GroupVersion: monitoring.coreos.com/v1
  May 18 09:38:51.918: INFO: Versions found [{monitoring.coreos.com/v1 v1} {monitoring.coreos.com/v1alpha1 v1alpha1}]
  May 18 09:38:51.918: INFO: monitoring.coreos.com/v1 matches monitoring.coreos.com/v1
  May 18 09:38:51.918: INFO: Checking APIGroup: security.istio.io
  May 18 09:38:51.920: INFO: PreferredVersion.GroupVersion: security.istio.io/v1
  May 18 09:38:51.920: INFO: Versions found [{security.istio.io/v1 v1} {security.istio.io/v1beta1 v1beta1}]
  May 18 09:38:51.920: INFO: security.istio.io/v1 matches security.istio.io/v1
  May 18 09:38:51.920: INFO: Checking APIGroup: tekton.dev
  May 18 09:38:51.921: INFO: PreferredVersion.GroupVersion: tekton.dev/v1
  May 18 09:38:51.921: INFO: Versions found [{tekton.dev/v1 v1} {tekton.dev/v1beta1 v1beta1} {tekton.dev/v1alpha1 v1alpha1}]
  May 18 09:38:51.921: INFO: tekton.dev/v1 matches tekton.dev/v1
  May 18 09:38:51.921: INFO: Checking APIGroup: workload.secloudit.io
  May 18 09:38:51.922: INFO: PreferredVersion.GroupVersion: workload.secloudit.io/v1
  May 18 09:38:51.922: INFO: Versions found [{workload.secloudit.io/v1 v1}]
  May 18 09:38:51.922: INFO: workload.secloudit.io/v1 matches workload.secloudit.io/v1
  May 18 09:38:51.922: INFO: Checking APIGroup: argoproj.io
  May 18 09:38:51.924: INFO: PreferredVersion.GroupVersion: argoproj.io/v1alpha1
  May 18 09:38:51.924: INFO: Versions found [{argoproj.io/v1alpha1 v1alpha1}]
  May 18 09:38:51.924: INFO: argoproj.io/v1alpha1 matches argoproj.io/v1alpha1
  May 18 09:38:51.924: INFO: Checking APIGroup: extensions.istio.io
  May 18 09:38:51.926: INFO: PreferredVersion.GroupVersion: extensions.istio.io/v1alpha1
  May 18 09:38:51.926: INFO: Versions found [{extensions.istio.io/v1alpha1 v1alpha1}]
  May 18 09:38:51.927: INFO: extensions.istio.io/v1alpha1 matches extensions.istio.io/v1alpha1
  May 18 09:38:51.927: INFO: Checking APIGroup: install.istio.io
  May 18 09:38:51.929: INFO: PreferredVersion.GroupVersion: install.istio.io/v1alpha1
  May 18 09:38:51.929: INFO: Versions found [{install.istio.io/v1alpha1 v1alpha1}]
  May 18 09:38:51.929: INFO: install.istio.io/v1alpha1 matches install.istio.io/v1alpha1
  May 18 09:38:51.929: INFO: Checking APIGroup: resolution.tekton.dev
  May 18 09:38:51.931: INFO: PreferredVersion.GroupVersion: resolution.tekton.dev/v1beta1
  May 18 09:38:51.931: INFO: Versions found [{resolution.tekton.dev/v1beta1 v1beta1} {resolution.tekton.dev/v1alpha1 v1alpha1}]
  May 18 09:38:51.932: INFO: resolution.tekton.dev/v1beta1 matches resolution.tekton.dev/v1beta1
  May 18 09:38:51.932: INFO: Checking APIGroup: telemetry.istio.io
  May 18 09:38:51.933: INFO: PreferredVersion.GroupVersion: telemetry.istio.io/v1alpha1
  May 18 09:38:51.933: INFO: Versions found [{telemetry.istio.io/v1alpha1 v1alpha1}]
  May 18 09:38:51.934: INFO: telemetry.istio.io/v1alpha1 matches telemetry.istio.io/v1alpha1
  May 18 09:38:51.934: INFO: Checking APIGroup: triggers.tekton.dev
  May 18 09:38:51.935: INFO: PreferredVersion.GroupVersion: triggers.tekton.dev/v1beta1
  May 18 09:38:51.936: INFO: Versions found [{triggers.tekton.dev/v1beta1 v1beta1} {triggers.tekton.dev/v1alpha1 v1alpha1}]
  May 18 09:38:51.937: INFO: triggers.tekton.dev/v1beta1 matches triggers.tekton.dev/v1beta1
  May 18 09:38:51.937: INFO: Checking APIGroup: networking.istio.io
  May 18 09:38:51.939: INFO: PreferredVersion.GroupVersion: networking.istio.io/v1beta1
  May 18 09:38:51.940: INFO: Versions found [{networking.istio.io/v1beta1 v1beta1} {networking.istio.io/v1alpha3 v1alpha3}]
  May 18 09:38:51.940: INFO: networking.istio.io/v1beta1 matches networking.istio.io/v1beta1
  May 18 09:38:51.940: INFO: Checking APIGroup: metrics.k8s.io
  May 18 09:38:51.942: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  May 18 09:38:51.943: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  May 18 09:38:51.944: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  May 18 09:38:51.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-4241" for this suite. @ 05/18/23 09:38:51.954
• [0.583 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/18/23 09:38:51.972
  May 18 09:38:51.972: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sysctl @ 05/18/23 09:38:51.973
  E0518 09:38:51.986933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:38:51.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:38:52.017
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/18/23 09:38:52.023
  May 18 09:38:52.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-1331" for this suite. @ 05/18/23 09:38:52.037
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/18/23 09:38:52.067
  May 18 09:38:52.067: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename gc @ 05/18/23 09:38:52.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:38:52.108
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:38:52.113
  STEP: create the rc @ 05/18/23 09:38:52.122
  W0518 09:38:52.136203      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0518 09:38:52.988062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:53.989662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:54.993123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:55.993156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:56.994114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:38:57.994655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/18/23 09:38:58.14
  STEP: wait for the rc to be deleted @ 05/18/23 09:38:58.147
  E0518 09:38:58.994834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:38:59.162: INFO: 80 pods remaining
  May 18 09:38:59.162: INFO: 80 pods has nil DeletionTimestamp
  May 18 09:38:59.162: INFO: 
  E0518 09:38:59.995829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:00.173: INFO: 70 pods remaining
  May 18 09:39:00.173: INFO: 70 pods has nil DeletionTimestamp
  May 18 09:39:00.173: INFO: 
  E0518 09:39:00.997070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:01.171: INFO: 60 pods remaining
  May 18 09:39:01.171: INFO: 60 pods has nil DeletionTimestamp
  May 18 09:39:01.171: INFO: 
  E0518 09:39:01.997881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:02.161: INFO: 40 pods remaining
  May 18 09:39:02.161: INFO: 40 pods has nil DeletionTimestamp
  May 18 09:39:02.161: INFO: 
  E0518 09:39:03.000574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:03.164: INFO: 31 pods remaining
  May 18 09:39:03.164: INFO: 30 pods has nil DeletionTimestamp
  May 18 09:39:03.164: INFO: 
  E0518 09:39:04.001563      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:04.157: INFO: 20 pods remaining
  May 18 09:39:04.157: INFO: 20 pods has nil DeletionTimestamp
  May 18 09:39:04.157: INFO: 
  E0518 09:39:05.002360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/18/23 09:39:05.154
  May 18 09:39:05.344: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 18 09:39:05.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1055" for this suite. @ 05/18/23 09:39:05.348
• [13.289 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/18/23 09:39:05.357
  May 18 09:39:05.357: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename deployment @ 05/18/23 09:39:05.359
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:39:05.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:39:05.384
  May 18 09:39:05.387: INFO: Creating deployment "webserver-deployment"
  May 18 09:39:05.391: INFO: Waiting for observed generation 1
  E0518 09:39:06.002864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:07.003779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:07.397: INFO: Waiting for all required pods to come up
  May 18 09:39:07.402: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/18/23 09:39:07.402
  E0518 09:39:08.004486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:09.005278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:10.006293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:11.006483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:12.007190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:13.008893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:14.009387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:15.010132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:16.010852      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:17.013424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:18.015489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:19.021031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:20.022844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:21.024194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:22.024441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:23.025573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:23.464: INFO: Waiting for deployment "webserver-deployment" to complete
  May 18 09:39:23.471: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May 18 09:39:23.508: INFO: Updating deployment webserver-deployment
  May 18 09:39:23.508: INFO: Waiting for observed generation 2
  E0518 09:39:24.029410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:25.029385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:25.534: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May 18 09:39:25.543: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May 18 09:39:25.546: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 18 09:39:25.555: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May 18 09:39:25.555: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May 18 09:39:25.559: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 18 09:39:25.566: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May 18 09:39:25.566: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May 18 09:39:25.578: INFO: Updating deployment webserver-deployment
  May 18 09:39:25.579: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May 18 09:39:25.586: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May 18 09:39:25.599: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  May 18 09:39:25.622: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-2016  b1f7f8e9-9fce-461a-9142-4be8a91a040b 1948022 3 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00b0860c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-18 09:39:23 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-18 09:39:25 +0000 UTC,LastTransitionTime:2023-05-18 09:39:25 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  May 18 09:39:25.647: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-2016  162354ef-ec47-4622-adaf-ed1100b3f55a 1948021 3 2023-05-18 09:39:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment b1f7f8e9-9fce-461a-9142-4be8a91a040b 0xc00af5d427 0xc00af5d428}] [] [{kube-controller-manager Update apps/v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1f7f8e9-9fce-461a-9142-4be8a91a040b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00af5d4c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:39:25.647: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May 18 09:39:25.647: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-2016  f0482875-c83c-48d3-a306-252aa3097c15 1948018 3 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment b1f7f8e9-9fce-461a-9142-4be8a91a040b 0xc00af5d337 0xc00af5d338}] [] [{kube-controller-manager Update apps/v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b1f7f8e9-9fce-461a-9142-4be8a91a040b\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00af5d3c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:39:25.680: INFO: Pod "webserver-deployment-67bd4bf6dc-5rdx8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5rdx8 webserver-deployment-67bd4bf6dc- deployment-2016  0794bb8f-5d59-4d7b-a6f9-7e74316bdabe 1947712 0 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:c8fa6f998c42223767542cb5f429c3e6e2c3d5e8fd7e2fe1ccb4f41362c6dca4 cni.projectcalico.org/podIP:172.16.161.121/32 cni.projectcalico.org/podIPs:172.16.161.121/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00af5d9c7 0xc00af5d9c8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:39:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-78sbp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-78sbp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.121,StartTime:2023-05-18 09:39:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:39:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://d662084dfed34fda37db7e11e4bbf27459b339ac099ce5a8df0bb596889075a5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.121,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.683: INFO: Pod "webserver-deployment-67bd4bf6dc-64vx2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-64vx2 webserver-deployment-67bd4bf6dc- deployment-2016  41b50e05-3711-42a9-a4f2-05df154f4e4f 1947719 0 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:c9888bf65436b2585d1043b94c89fb7abd2a2a5943fc6dfb14092bf67b5851e1 cni.projectcalico.org/podIP:172.16.161.80/32 cni.projectcalico.org/podIPs:172.16.161.80/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00af5dbe7 0xc00af5dbe8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:39:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.80\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vdfw6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vdfw6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.80,StartTime:2023-05-18 09:39:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:39:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e1e2e6b3b52ed2762b343b5726c05a4a6210a9df952f4a2bba1b014b8149405b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.80,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.684: INFO: Pod "webserver-deployment-67bd4bf6dc-8rxwr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8rxwr webserver-deployment-67bd4bf6dc- deployment-2016  391b5721-7d06-4a8c-928c-73ccfb6cf232 1947878 0 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:18b68f94a2086b78181258be43d1ea20ddebaa2674b5413ed692bc6c96029441 cni.projectcalico.org/podIP:172.16.78.131/32 cni.projectcalico.org/podIPs:172.16.78.131/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00af5de07 0xc00af5de08}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.78.131\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rck68,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rck68,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.135,PodIP:172.16.78.131,StartTime:2023-05-18 09:39:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:39:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://38fc994d885c4a34b5da8511b44cb14b4e7174859c496af7333d566a0f34fc48,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.78.131,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.696: INFO: Pod "webserver-deployment-67bd4bf6dc-flqcp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-flqcp webserver-deployment-67bd4bf6dc- deployment-2016  709fa4d0-7c0d-4d72-adac-67927a6707e8 1947724 0 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:2757d71149613eb13ea1667b4d96a67cd888a4a2b4300e9d3a6bd465d7e20364 cni.projectcalico.org/podIP:172.16.161.105/32 cni.projectcalico.org/podIPs:172.16.161.105/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00b07c007 0xc00b07c008}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:39:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:39:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.105\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-89r9x,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-89r9x,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.105,StartTime:2023-05-18 09:39:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:39:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://37111765994b560606017ee182bceecfde606ad1f8472f368a9eaecda9dc1c8c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.105,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.697: INFO: Pod "webserver-deployment-67bd4bf6dc-frm9k" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-frm9k webserver-deployment-67bd4bf6dc- deployment-2016  85918724-b939-4563-bb2e-561d14b333d9 1947833 0 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:e00ed65f083b1888808149e24979e185b37df51ba37f72af3abf8f57b0e27fe9 cni.projectcalico.org/podIP:172.16.78.190/32 cni.projectcalico.org/podIPs:172.16.78.190/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00b07c227 0xc00b07c228}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:39:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:39:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.78.190\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-98xdv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-98xdv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.135,PodIP:172.16.78.190,StartTime:2023-05-18 09:39:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:39:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://ef743814b156468e7fcb0bc23a0e00515870da3c291d49d2c51ef7321bd8d02a,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.78.190,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.697: INFO: Pod "webserver-deployment-67bd4bf6dc-k6pl8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-k6pl8 webserver-deployment-67bd4bf6dc- deployment-2016  0a32d640-5027-4394-a37c-1509bac92358 1947839 0 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:8b9f3bae22fb499633659793b6133260ad9c81925a3823719f309699bae05a85 cni.projectcalico.org/podIP:172.16.78.130/32 cni.projectcalico.org/podIPs:172.16.78.130/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00b07c447 0xc00b07c448}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:39:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.78.130\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-krgr7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-krgr7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.135,PodIP:172.16.78.130,StartTime:2023-05-18 09:39:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:39:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7fb5adf706e61e4211e3f14d7c6afad07156c277ac7b6e1b10b4d2a552ce9b65,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.78.130,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.697: INFO: Pod "webserver-deployment-67bd4bf6dc-k6trf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-k6trf webserver-deployment-67bd4bf6dc- deployment-2016  6225cef4-1d62-47dd-a246-069301ad2138 1948025 0 2023-05-18 09:39:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00b07c647 0xc00b07c648}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5xhhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5xhhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.698: INFO: Pod "webserver-deployment-67bd4bf6dc-l6qzt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-l6qzt webserver-deployment-67bd4bf6dc- deployment-2016  1a078ca5-352e-4f86-8ca8-00f75eacfade 1948031 0 2023-05-18 09:39:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00b07c790 0xc00b07c791}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vf9z5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vf9z5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:25 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:,StartTime:2023-05-18 09:39:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.698: INFO: Pod "webserver-deployment-67bd4bf6dc-q2s9k" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-q2s9k webserver-deployment-67bd4bf6dc- deployment-2016  a132454d-b7db-4327-8860-5ba059e6b2d7 1948026 0 2023-05-18 09:39:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00b07c947 0xc00b07c948}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mj6wd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mj6wd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.699: INFO: Pod "webserver-deployment-67bd4bf6dc-rncjj" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-rncjj webserver-deployment-67bd4bf6dc- deployment-2016  53505c66-d597-4a38-b640-ab1b30bae83c 1947826 0 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:810c52c70df4b9f3323845b2164b538b595f36dddffea01a43b778f889ae0ad5 cni.projectcalico.org/podIP:172.16.78.191/32 cni.projectcalico.org/podIPs:172.16.78.191/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00b07cab0 0xc00b07cab1}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:39:21 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.78.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bqs4c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bqs4c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:21 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.135,PodIP:172.16.78.191,StartTime:2023-05-18 09:39:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:39:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://0360b004d1aaf7ec1c6c090d07a415d8a7e6dbb76f658b0fb373f761d2da3c32,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.78.191,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.699: INFO: Pod "webserver-deployment-67bd4bf6dc-xt5gl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xt5gl webserver-deployment-67bd4bf6dc- deployment-2016  e2e8b678-6024-43e9-bb98-96c329e8adfb 1947690 0 2023-05-18 09:39:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:7f0604e24fc3b5ed5c20f170ea3ce3e3f112b192fcf6e4baafe77c07a20627da cni.projectcalico.org/podIP:172.16.161.98/32 cni.projectcalico.org/podIPs:172.16.161.98/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc f0482875-c83c-48d3-a306-252aa3097c15 0xc00b07ccc7 0xc00b07ccc8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0482875-c83c-48d3-a306-252aa3097c15\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:39:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.98\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fdtz8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fdtz8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.98,StartTime:2023-05-18 09:39:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:39:16 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8818e4f94bce6311676c54c0e0d614566689a6dd35a79ebfa2aeff607123641d,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.98,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.700: INFO: Pod "webserver-deployment-7b75d79cf5-6cjms" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6cjms webserver-deployment-7b75d79cf5- deployment-2016  c669b477-6b1f-40aa-b01c-514c52594c60 1947973 0 2023-05-18 09:39:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:383b88a3bfd258698c1f5a761a620d2b64129f5a3b803724698d1b10c9242cbe cni.projectcalico.org/podIP:172.16.161.89/32 cni.projectcalico.org/podIPs:172.16.161.89/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 162354ef-ec47-4622-adaf-ed1100b3f55a 0xc00b07cee7 0xc00b07cee8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"162354ef-ec47-4622-adaf-ed1100b3f55a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-18 09:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n2jtr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n2jtr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:,StartTime:2023-05-18 09:39:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.701: INFO: Pod "webserver-deployment-7b75d79cf5-9snc7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-9snc7 webserver-deployment-7b75d79cf5- deployment-2016  4d66299d-aa69-4c19-af24-3e55f7456863 1947949 0 2023-05-18 09:39:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 162354ef-ec47-4622-adaf-ed1100b3f55a 0xc00b07d0e7 0xc00b07d0e8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"162354ef-ec47-4622-adaf-ed1100b3f55a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-42wr5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-42wr5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.135,PodIP:,StartTime:2023-05-18 09:39:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.701: INFO: Pod "webserver-deployment-7b75d79cf5-cjtwq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-cjtwq webserver-deployment-7b75d79cf5- deployment-2016  de4fabdb-064e-41e6-9c69-109016c18d91 1947956 0 2023-05-18 09:39:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:41b9cff36e7e0103f73ba3e3655ee7d2e49edd83ff2e007500a34c2494c833ed cni.projectcalico.org/podIP:172.16.161.97/32 cni.projectcalico.org/podIPs:172.16.161.97/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 162354ef-ec47-4622-adaf-ed1100b3f55a 0xc00b07d2e7 0xc00b07d2e8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"162354ef-ec47-4622-adaf-ed1100b3f55a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-18 09:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlph6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlph6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:,StartTime:2023-05-18 09:39:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.702: INFO: Pod "webserver-deployment-7b75d79cf5-jfzpj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-jfzpj webserver-deployment-7b75d79cf5- deployment-2016  7dae5dba-150c-418e-a464-4be39a3775dd 1947920 0 2023-05-18 09:39:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 162354ef-ec47-4622-adaf-ed1100b3f55a 0xc00b07d4e7 0xc00b07d4e8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"162354ef-ec47-4622-adaf-ed1100b3f55a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rdmdb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rdmdb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.135,PodIP:,StartTime:2023-05-18 09:39:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.702: INFO: Pod "webserver-deployment-7b75d79cf5-k7d6q" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-k7d6q webserver-deployment-7b75d79cf5- deployment-2016  30b97091-a1ea-42e0-8d1d-6e96d1a0d702 1947961 0 2023-05-18 09:39:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:0b453156491c2792a8e2ce371a727698e1d1eb990459c6afa6d31e951758ade1 cni.projectcalico.org/podIP:172.16.161.71/32 cni.projectcalico.org/podIPs:172.16.161.71/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 162354ef-ec47-4622-adaf-ed1100b3f55a 0xc00b07d6e7 0xc00b07d6e8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"162354ef-ec47-4622-adaf-ed1100b3f55a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-18 09:39:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-18 09:39:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j2ktl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j2ktl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:,StartTime:2023-05-18 09:39:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.702: INFO: Pod "webserver-deployment-7b75d79cf5-nhrxq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-nhrxq webserver-deployment-7b75d79cf5- deployment-2016  630a6d75-d4b1-43b9-b80d-e732a21702f8 1948030 0 2023-05-18 09:39:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 162354ef-ec47-4622-adaf-ed1100b3f55a 0xc00b07d8e7 0xc00b07d8e8}] [] [{kube-controller-manager Update v1 2023-05-18 09:39:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"162354ef-ec47-4622-adaf-ed1100b3f55a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rhl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rhl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:39:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:39:25.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2016" for this suite. @ 05/18/23 09:39:25.719
• [20.387 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/18/23 09:39:25.751
  May 18 09:39:25.752: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:39:25.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:39:25.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:39:25.787
  STEP: Creating projection with secret that has name projected-secret-test-0da37db1-aa31-40ab-91ca-796f1a8fe925 @ 05/18/23 09:39:25.791
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:39:25.796
  E0518 09:39:26.029695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:27.029800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:28.029854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:29.030503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:39:29.825
  May 18 09:39:29.833: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-secrets-7a9faca6-6fd2-43f5-966f-b77bed0044ed container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:39:29.841
  May 18 09:39:29.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7774" for this suite. @ 05/18/23 09:39:29.861
• [4.113 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/18/23 09:39:29.872
  May 18 09:39:29.873: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:39:29.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:39:29.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:39:29.891
  STEP: Creating configMap with name configmap-test-volume-61c8dc8d-4064-458c-8ab2-b7bda0124cf3 @ 05/18/23 09:39:29.938
  STEP: Creating a pod to test consume configMaps @ 05/18/23 09:39:29.942
  E0518 09:39:30.031555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:31.033674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:32.034309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:33.038842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:39:33.966
  May 18 09:39:33.974: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-8c304777-3c15-44f2-b157-b756ea9bada5 container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:39:33.989
  May 18 09:39:34.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5740" for this suite. @ 05/18/23 09:39:34.009
• [4.144 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/18/23 09:39:34.019
  May 18 09:39:34.020: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replication-controller @ 05/18/23 09:39:34.022
  E0518 09:39:34.034553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:39:34.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:39:34.045
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/18/23 09:39:34.049
  E0518 09:39:35.035692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:36.036782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 05/18/23 09:39:36.066
  STEP: Then the orphan pod is adopted @ 05/18/23 09:39:36.071
  E0518 09:39:37.037688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:39:37.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3078" for this suite. @ 05/18/23 09:39:37.089
• [3.078 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/18/23 09:39:37.1
  May 18 09:39:37.101: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename job @ 05/18/23 09:39:37.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:39:37.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:39:37.132
  STEP: Creating a job @ 05/18/23 09:39:37.137
  STEP: Ensuring active pods == parallelism @ 05/18/23 09:39:37.146
  E0518 09:39:38.063050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:39.038705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 05/18/23 09:39:39.151
  STEP: deleting Job.batch foo in namespace job-3052, will wait for the garbage collector to delete the pods @ 05/18/23 09:39:39.152
  May 18 09:39:39.213: INFO: Deleting Job.batch foo took: 6.206814ms
  May 18 09:39:39.314: INFO: Terminating Job.batch foo pods took: 100.785948ms
  E0518 09:39:40.039381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:41.040008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:42.041212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:43.041456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:44.042513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:45.043090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:46.044217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:47.045356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:48.046240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:49.046671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:50.047180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:51.047915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:52.048526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:53.049263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:54.049702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:55.050279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:56.050147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:57.050847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:58.051339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:39:59.052202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:00.053345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:01.054250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:02.054775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:03.055322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:04.055724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:05.056038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:06.056413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:07.056660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:08.057452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:09.057554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:10.057695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:11.057630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:12.058431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 05/18/23 09:40:12.215
  May 18 09:40:12.220: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3052" for this suite. @ 05/18/23 09:40:12.226
• [35.145 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/18/23 09:40:12.246
  May 18 09:40:12.246: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename field-validation @ 05/18/23 09:40:12.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:12.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:12.274
  May 18 09:40:12.278: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:40:13.059309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:14.059439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0518 09:40:14.840543      19 warnings.go:70] unknown field "alpha"
  W0518 09:40:14.840742      19 warnings.go:70] unknown field "beta"
  W0518 09:40:14.840753      19 warnings.go:70] unknown field "delta"
  W0518 09:40:14.840761      19 warnings.go:70] unknown field "epsilon"
  W0518 09:40:14.840768      19 warnings.go:70] unknown field "gamma"
  May 18 09:40:14.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1041" for this suite. @ 05/18/23 09:40:14.878
• [2.641 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/18/23 09:40:14.894
  May 18 09:40:14.894: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/18/23 09:40:14.896
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:14.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:14.922
  STEP: fetching the /apis discovery document @ 05/18/23 09:40:14.927
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/18/23 09:40:14.929
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/18/23 09:40:14.93
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/18/23 09:40:14.93
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/18/23 09:40:14.932
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/18/23 09:40:14.933
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/18/23 09:40:14.935
  May 18 09:40:14.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4875" for this suite. @ 05/18/23 09:40:14.942
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/18/23 09:40:14.958
  May 18 09:40:14.958: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:40:14.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:14.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:14.981
  STEP: Counting existing ResourceQuota @ 05/18/23 09:40:14.986
  E0518 09:40:15.060096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:16.061190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:17.062129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:18.062544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:19.063596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/18/23 09:40:19.992
  STEP: Ensuring resource quota status is calculated @ 05/18/23 09:40:19.998
  E0518 09:40:20.064549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:21.065166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 05/18/23 09:40:22.004
  STEP: Ensuring resource quota status captures replicaset creation @ 05/18/23 09:40:22.019
  E0518 09:40:22.065713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:23.066184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 05/18/23 09:40:24.028
  STEP: Ensuring resource quota status released usage @ 05/18/23 09:40:24.036
  E0518 09:40:24.066301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:25.066536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:40:26.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3954" for this suite. @ 05/18/23 09:40:26.048
• [11.106 seconds]
------------------------------
SSSSSS  E0518 09:40:26.066855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/18/23 09:40:26.072
  May 18 09:40:26.072: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename runtimeclass @ 05/18/23 09:40:26.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:26.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:26.097
  E0518 09:40:27.067235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:28.067914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:40:28.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6598" for this suite. @ 05/18/23 09:40:28.147
• [2.080 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/18/23 09:40:28.153
  May 18 09:40:28.153: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:40:28.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:28.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:28.174
  STEP: Counting existing ResourceQuota @ 05/18/23 09:40:28.176
  E0518 09:40:29.068441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:30.069098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:31.070162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:32.071060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:33.071244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/18/23 09:40:33.181
  STEP: Ensuring resource quota status is calculated @ 05/18/23 09:40:33.189
  E0518 09:40:34.071514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:35.071624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 05/18/23 09:40:35.194
  STEP: Creating a NodePort Service @ 05/18/23 09:40:35.219
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/18/23 09:40:35.25
  STEP: Ensuring resource quota status captures service creation @ 05/18/23 09:40:35.275
  E0518 09:40:36.072189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:37.072225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 05/18/23 09:40:37.28
  STEP: Ensuring resource quota status released usage @ 05/18/23 09:40:37.334
  E0518 09:40:38.074006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:39.073470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:40:39.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7194" for this suite. @ 05/18/23 09:40:39.343
• [11.196 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/18/23 09:40:39.35
  May 18 09:40:39.350: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:40:39.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:39.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:39.378
  May 18 09:40:39.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8395" for this suite. @ 05/18/23 09:40:39.388
• [0.043 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/18/23 09:40:39.395
  May 18 09:40:39.395: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 09:40:39.396
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:39.409
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:39.412
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/18/23 09:40:39.414
  May 18 09:40:39.415: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:40:40.074018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:41.075064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:42.076195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:40:42.301: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:40:43.076506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:44.077219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:45.078020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:46.078650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:47.078837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:48.079537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:49.080450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:50.088629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:51.089140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:40:51.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1352" for this suite. @ 05/18/23 09:40:51.593
• [12.210 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/18/23 09:40:51.61
  May 18 09:40:51.610: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-pred @ 05/18/23 09:40:51.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:51.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:51.646
  May 18 09:40:51.652: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 18 09:40:51.665: INFO: Waiting for terminating namespaces to be deleted...
  May 18 09:40:51.670: INFO: 
  Logging pods the apiserver thinks is on node ck-test-kube-1-27-worker before test
  May 18 09:40:51.686: INFO: calico-node-z6wl2 from kube-system started at 2023-05-12 06:51:27 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.687: INFO: 	Container calico-node ready: true, restart count 0
  May 18 09:40:51.688: INFO: kube-proxy-269wt from kube-system started at 2023-05-12 06:50:32 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.688: INFO: 	Container kube-proxy ready: true, restart count 0
  May 18 09:40:51.689: INFO: node-exporter-7q9w4 from secloudit-monitoring started at 2023-05-12 07:19:01 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.690: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:40:51.691: INFO: 	Container node-exporter ready: true, restart count 0
  May 18 09:40:51.692: INFO: sonobuoy from sonobuoy started at 2023-05-18 08:52:38 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.693: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 18 09:40:51.694: INFO: sonobuoy-e2e-job-cccd1fa16dd54418 from sonobuoy started at 2023-05-18 08:52:39 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.695: INFO: 	Container e2e ready: true, restart count 0
  May 18 09:40:51.696: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:40:51.696: INFO: sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-8p6sc from sonobuoy started at 2023-05-18 08:52:40 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.697: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:40:51.697: INFO: 	Container systemd-logs ready: true, restart count 0
  May 18 09:40:51.697: INFO: 
  Logging pods the apiserver thinks is on node ck-test-kube-1-27-worker-1 before test
  May 18 09:40:51.727: INFO: argocd-application-controller-0 from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.728: INFO: 	Container argocd-application-controller ready: true, restart count 0
  May 18 09:40:51.729: INFO: argocd-applicationset-controller-57fd46cc99-vjt7t from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.730: INFO: 	Container argocd-applicationset-controller ready: true, restart count 0
  May 18 09:40:51.730: INFO: argocd-dex-server-75d8c65447-wrjw6 from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.731: INFO: 	Container dex ready: true, restart count 0
  May 18 09:40:51.732: INFO: argocd-notifications-controller-6f5f6d97cb-sdzxc from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.733: INFO: 	Container argocd-notifications-controller ready: true, restart count 0
  May 18 09:40:51.733: INFO: argocd-redis-77bf5b886-dn8z5 from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.734: INFO: 	Container redis ready: true, restart count 0
  May 18 09:40:51.735: INFO: argocd-repo-server-59684bc98c-dk4hd from argocd started at 2023-05-18 06:54:00 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.736: INFO: 	Container argocd-repo-server ready: true, restart count 0
  May 18 09:40:51.737: INFO: argocd-server-d5bd658c-fs9dz from argocd started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.737: INFO: 	Container argocd-server ready: true, restart count 0
  May 18 09:40:51.737: INFO: istio-egressgateway-575466f5bb-mqw8r from istio-system started at 2023-05-18 06:47:53 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.737: INFO: 	Container istio-proxy ready: true, restart count 0
  May 18 09:40:51.737: INFO: istio-ingressgateway-cb9c6b49d-htxtq from istio-system started at 2023-05-18 06:47:53 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.738: INFO: 	Container istio-proxy ready: true, restart count 0
  May 18 09:40:51.738: INFO: istiod-6fbbf67d58-zhh74 from istio-system started at 2023-05-18 06:47:39 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.738: INFO: 	Container discovery ready: true, restart count 0
  May 18 09:40:51.738: INFO: calico-kube-controllers-674fff74c8-lvpc2 from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.739: INFO: 	Container calico-kube-controllers ready: true, restart count 0
  May 18 09:40:51.739: INFO: calico-node-946wb from kube-system started at 2023-05-12 06:51:27 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.739: INFO: 	Container calico-node ready: true, restart count 0
  May 18 09:40:51.739: INFO: coredns-5d78c9869d-fchlv from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.740: INFO: 	Container coredns ready: true, restart count 0
  May 18 09:40:51.740: INFO: coredns-5d78c9869d-vbhw2 from kube-system started at 2023-05-12 06:51:45 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.740: INFO: 	Container coredns ready: true, restart count 0
  May 18 09:40:51.740: INFO: kube-proxy-s5x6t from kube-system started at 2023-05-12 06:50:32 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.741: INFO: 	Container kube-proxy ready: true, restart count 0
  May 18 09:40:51.741: INFO: alertmanager-main-0 from secloudit-monitoring started at 2023-05-18 07:36:19 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.741: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:40:51.741: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:40:51.742: INFO: alertmanager-main-1 from secloudit-monitoring started at 2023-05-12 07:19:12 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.742: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:40:51.742: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:40:51.742: INFO: alertmanager-main-2 from secloudit-monitoring started at 2023-05-18 07:36:20 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.743: INFO: 	Container alertmanager ready: true, restart count 0
  May 18 09:40:51.743: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:40:51.743: INFO: blackbox-exporter-659b6ff548-25j5z from secloudit-monitoring started at 2023-05-12 07:17:19 +0000 UTC (3 container statuses recorded)
  May 18 09:40:51.744: INFO: 	Container blackbox-exporter ready: true, restart count 0
  May 18 09:40:51.744: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:40:51.744: INFO: 	Container module-configmap-reloader ready: true, restart count 0
  May 18 09:40:51.744: INFO: grafana-6cb59f497c-ssmtq from secloudit-monitoring started at 2023-05-18 07:36:15 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.744: INFO: 	Container grafana ready: true, restart count 0
  May 18 09:40:51.745: INFO: kube-state-metrics-f59465db9-v6k4r from secloudit-monitoring started at 2023-05-18 07:36:14 +0000 UTC (3 container statuses recorded)
  May 18 09:40:51.745: INFO: 	Container kube-rbac-proxy-main ready: true, restart count 0
  May 18 09:40:51.745: INFO: 	Container kube-rbac-proxy-self ready: true, restart count 0
  May 18 09:40:51.746: INFO: 	Container kube-state-metrics ready: true, restart count 0
  May 18 09:40:51.746: INFO: node-exporter-4ttx4 from secloudit-monitoring started at 2023-05-12 07:18:53 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.746: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:40:51.746: INFO: 	Container node-exporter ready: true, restart count 0
  May 18 09:40:51.747: INFO: prometheus-adapter-74d4cb89b8-ll246 from secloudit-monitoring started at 2023-05-12 07:18:13 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.747: INFO: 	Container prometheus-adapter ready: true, restart count 0
  May 18 09:40:51.747: INFO: prometheus-adapter-74d4cb89b8-z4hvp from secloudit-monitoring started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container prometheus-adapter ready: true, restart count 0
  May 18 09:40:51.748: INFO: prometheus-k8s-0 from secloudit-monitoring started at 2023-05-12 07:19:13 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:40:51.748: INFO: 	Container prometheus ready: true, restart count 0
  May 18 09:40:51.748: INFO: prometheus-k8s-1 from secloudit-monitoring started at 2023-05-18 07:36:19 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container config-reloader ready: true, restart count 0
  May 18 09:40:51.748: INFO: 	Container prometheus ready: true, restart count 0
  May 18 09:40:51.748: INFO: prometheus-operator-59f6b8bc59-wv7ll from secloudit-monitoring started at 2023-05-18 07:36:15 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container kube-rbac-proxy ready: true, restart count 0
  May 18 09:40:51.748: INFO: 	Container prometheus-operator ready: true, restart count 0
  May 18 09:40:51.748: INFO: sonobuoy-systemd-logs-daemon-set-df72c9795b714e81-xppr7 from sonobuoy started at 2023-05-18 08:52:40 +0000 UTC (2 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 18 09:40:51.748: INFO: 	Container systemd-logs ready: true, restart count 0
  May 18 09:40:51.748: INFO: tekton-pipelines-remote-resolvers-76f588dbf5-z6v9v from tekton-pipelines-resolvers started at 2023-05-18 07:36:15 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container controller ready: true, restart count 0
  May 18 09:40:51.748: INFO: tekton-pipelines-controller-dd857fb44-mwdzd from tekton-pipelines started at 2023-05-12 07:32:59 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container tekton-pipelines-controller ready: true, restart count 0
  May 18 09:40:51.748: INFO: tekton-pipelines-webhook-79bbb84bb4-7q2lq from tekton-pipelines started at 2023-05-12 07:32:59 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container webhook ready: true, restart count 0
  May 18 09:40:51.748: INFO: tekton-triggers-controller-99d8dc8bb-g47nr from tekton-pipelines started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container tekton-triggers-controller ready: true, restart count 0
  May 18 09:40:51.748: INFO: tekton-triggers-core-interceptors-ddf77b777-psf7x from tekton-pipelines started at 2023-05-18 07:36:14 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container tekton-triggers-core-interceptors ready: true, restart count 0
  May 18 09:40:51.748: INFO: tekton-triggers-webhook-69569f7f45-vv4c8 from tekton-pipelines started at 2023-05-12 07:33:13 +0000 UTC (1 container statuses recorded)
  May 18 09:40:51.748: INFO: 	Container webhook ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/18/23 09:40:51.748
  E0518 09:40:52.089920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:53.090090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/18/23 09:40:53.77
  STEP: Trying to apply a random label on the found node. @ 05/18/23 09:40:53.785
  STEP: verifying the node has the label kubernetes.io/e2e-19f54d7a-a0a8-4121-acd8-a6ad6717becc 42 @ 05/18/23 09:40:53.803
  STEP: Trying to relaunch the pod, now with labels. @ 05/18/23 09:40:53.809
  E0518 09:40:54.090951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:55.091504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-19f54d7a-a0a8-4121-acd8-a6ad6717becc off the node ck-test-kube-1-27-worker @ 05/18/23 09:40:55.834
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-19f54d7a-a0a8-4121-acd8-a6ad6717becc @ 05/18/23 09:40:55.85
  May 18 09:40:55.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-8385" for this suite. @ 05/18/23 09:40:55.861
• [4.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/18/23 09:40:55.87
  May 18 09:40:55.870: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:40:55.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:40:55.889
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:40:55.893
  STEP: Setting up server cert @ 05/18/23 09:40:55.917
  E0518 09:40:56.092332      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:40:56.551
  STEP: Deploying the webhook pod @ 05/18/23 09:40:56.559
  STEP: Wait for the deployment to be ready @ 05/18/23 09:40:56.568
  May 18 09:40:56.580: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:40:57.093250      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:40:58.093410      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:40:58.594
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:40:58.608
  E0518 09:40:59.094127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:40:59.609: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/18/23 09:40:59.615
  STEP: create a pod that should be denied by the webhook @ 05/18/23 09:40:59.635
  STEP: create a pod that causes the webhook to hang @ 05/18/23 09:40:59.653
  E0518 09:41:00.094976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:01.095864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:02.097272      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:03.096762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:04.097339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:05.097454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:06.098165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:07.098999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:08.099165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:09.099812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 05/18/23 09:41:09.663
  STEP: create a configmap that should be admitted by the webhook @ 05/18/23 09:41:09.683
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/18/23 09:41:09.698
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/18/23 09:41:09.712
  STEP: create a namespace that bypass the webhook @ 05/18/23 09:41:09.72
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/18/23 09:41:09.745
  May 18 09:41:09.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3137" for this suite. @ 05/18/23 09:41:09.842
  STEP: Destroying namespace "webhook-markers-4123" for this suite. @ 05/18/23 09:41:09.866
  STEP: Destroying namespace "exempted-namespace-5911" for this suite. @ 05/18/23 09:41:09.874
• [14.009 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/18/23 09:41:09.882
  May 18 09:41:09.882: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename limitrange @ 05/18/23 09:41:09.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:09.905
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:09.908
  STEP: Creating LimitRange "e2e-limitrange-lmhb7" in namespace "limitrange-7970" @ 05/18/23 09:41:09.91
  STEP: Creating another limitRange in another namespace @ 05/18/23 09:41:09.916
  May 18 09:41:09.929: INFO: Namespace "e2e-limitrange-lmhb7-20" created
  May 18 09:41:09.930: INFO: Creating LimitRange "e2e-limitrange-lmhb7" in namespace "e2e-limitrange-lmhb7-20"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-lmhb7" @ 05/18/23 09:41:09.934
  May 18 09:41:09.936: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-lmhb7" in "limitrange-7970" namespace @ 05/18/23 09:41:09.936
  May 18 09:41:09.942: INFO: LimitRange "e2e-limitrange-lmhb7" has been patched
  STEP: Delete LimitRange "e2e-limitrange-lmhb7" by Collection with labelSelector: "e2e-limitrange-lmhb7=patched" @ 05/18/23 09:41:09.943
  STEP: Confirm that the limitRange "e2e-limitrange-lmhb7" has been deleted @ 05/18/23 09:41:09.949
  May 18 09:41:09.949: INFO: Requesting list of LimitRange to confirm quantity
  May 18 09:41:09.951: INFO: Found 0 LimitRange with label "e2e-limitrange-lmhb7=patched"
  May 18 09:41:09.951: INFO: LimitRange "e2e-limitrange-lmhb7" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-lmhb7" @ 05/18/23 09:41:09.951
  May 18 09:41:09.956: INFO: Found 1 limitRange
  May 18 09:41:09.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7970" for this suite. @ 05/18/23 09:41:09.961
  STEP: Destroying namespace "e2e-limitrange-lmhb7-20" for this suite. @ 05/18/23 09:41:09.968
• [0.093 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/18/23 09:41:09.977
  May 18 09:41:09.977: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/18/23 09:41:09.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:09.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:09.993
  STEP: creating a target pod @ 05/18/23 09:41:09.996
  E0518 09:41:10.100404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:11.100784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/18/23 09:41:12.015
  E0518 09:41:12.105210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:13.106001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:14.107075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:15.107630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/18/23 09:41:16.048
  May 18 09:41:16.048: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-8604 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:41:16.048: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:41:16.049: INFO: ExecWithOptions: Clientset creation
  May 18 09:41:16.049: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-8604/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  E0518 09:41:16.107699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:16.203: INFO: Exec stderr: ""
  May 18 09:41:16.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-8604" for this suite. @ 05/18/23 09:41:16.234
• [6.266 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/18/23 09:41:16.248
  May 18 09:41:16.248: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replication-controller @ 05/18/23 09:41:16.25
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:16.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:16.272
  May 18 09:41:16.278: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0518 09:41:17.108140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/18/23 09:41:17.293
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/18/23 09:41:17.299
  E0518 09:41:18.109562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/18/23 09:41:18.306
  May 18 09:41:18.313: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/18/23 09:41:18.313
  May 18 09:41:18.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4654" for this suite. @ 05/18/23 09:41:18.321
• [2.079 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/18/23 09:41:18.328
  May 18 09:41:18.328: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replicaset @ 05/18/23 09:41:18.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:18.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:18.349
  STEP: Create a ReplicaSet @ 05/18/23 09:41:18.351
  STEP: Verify that the required pods have come up @ 05/18/23 09:41:18.356
  May 18 09:41:18.358: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0518 09:41:19.109781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:20.109803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:21.109977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:22.110165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:23.110321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:23.364: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/18/23 09:41:23.364
  May 18 09:41:23.369: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/18/23 09:41:23.369
  STEP: DeleteCollection of the ReplicaSets @ 05/18/23 09:41:23.381
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/18/23 09:41:23.405
  May 18 09:41:23.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-932" for this suite. @ 05/18/23 09:41:23.428
• [5.127 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/18/23 09:41:23.456
  May 18 09:41:23.456: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:41:23.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:23.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:23.515
  STEP: Creating secret with name secret-test-map-075d99ee-8e14-4679-9a1e-9a7f1ccd3f4c @ 05/18/23 09:41:23.524
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:41:23.536
  E0518 09:41:24.111705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:25.112330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:26.112005      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:27.112434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:41:27.6
  May 18 09:41:27.605: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-secrets-c350d41c-7d9e-4080-b7a3-676f6d4cf9c9 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:41:27.617
  May 18 09:41:27.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6205" for this suite. @ 05/18/23 09:41:27.646
• [4.198 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/18/23 09:41:27.655
  May 18 09:41:27.655: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 09:41:27.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:27.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:27.685
  STEP: creating the pod @ 05/18/23 09:41:27.69
  STEP: submitting the pod to kubernetes @ 05/18/23 09:41:27.69
  W0518 09:41:27.699951      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0518 09:41:28.112550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:29.113114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 05/18/23 09:41:29.715
  STEP: updating the pod @ 05/18/23 09:41:29.721
  E0518 09:41:30.113396      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:30.243: INFO: Successfully updated pod "pod-update-activedeadlineseconds-ec69d797-3b0e-4a7d-bab0-069c03e42a1a"
  E0518 09:41:31.113667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:32.114330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:33.114904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:34.114771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:34.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1013" for this suite. @ 05/18/23 09:41:34.265
• [6.620 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/18/23 09:41:34.289
  May 18 09:41:34.289: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:41:34.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:34.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:34.322
  STEP: creating a replication controller @ 05/18/23 09:41:34.327
  May 18 09:41:34.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 create -f -'
  E0518 09:41:35.115284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:36.115235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:36.240: INFO: stderr: ""
  May 18 09:41:36.240: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/18/23 09:41:36.24
  May 18 09:41:36.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 18 09:41:36.385: INFO: stderr: ""
  May 18 09:41:36.385: INFO: stdout: "update-demo-nautilus-nrsss update-demo-nautilus-v2r2v "
  May 18 09:41:36.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-nrsss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 18 09:41:36.486: INFO: stderr: ""
  May 18 09:41:36.486: INFO: stdout: ""
  May 18 09:41:36.486: INFO: update-demo-nautilus-nrsss is created but not running
  E0518 09:41:37.116143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:38.116351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:39.116716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:40.117309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:41.118131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:41.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 18 09:41:41.649: INFO: stderr: ""
  May 18 09:41:41.649: INFO: stdout: "update-demo-nautilus-nrsss update-demo-nautilus-v2r2v "
  May 18 09:41:41.649: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-nrsss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 18 09:41:41.779: INFO: stderr: ""
  May 18 09:41:41.779: INFO: stdout: "true"
  May 18 09:41:41.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-nrsss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 18 09:41:41.889: INFO: stderr: ""
  May 18 09:41:41.889: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 18 09:41:41.889: INFO: validating pod update-demo-nautilus-nrsss
  May 18 09:41:41.897: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 18 09:41:41.897: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 18 09:41:41.897: INFO: update-demo-nautilus-nrsss is verified up and running
  May 18 09:41:41.897: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-v2r2v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 18 09:41:42.009: INFO: stderr: ""
  May 18 09:41:42.009: INFO: stdout: "true"
  May 18 09:41:42.010: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-v2r2v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0518 09:41:42.119523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:42.131: INFO: stderr: ""
  May 18 09:41:42.131: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 18 09:41:42.131: INFO: validating pod update-demo-nautilus-v2r2v
  May 18 09:41:42.137: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 18 09:41:42.137: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 18 09:41:42.137: INFO: update-demo-nautilus-v2r2v is verified up and running
  STEP: scaling down the replication controller @ 05/18/23 09:41:42.137
  May 18 09:41:42.144: INFO: scanned /root for discovery docs: <nil>
  May 18 09:41:42.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0518 09:41:43.120056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:43.294: INFO: stderr: ""
  May 18 09:41:43.294: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/18/23 09:41:43.294
  May 18 09:41:43.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 18 09:41:43.442: INFO: stderr: ""
  May 18 09:41:43.442: INFO: stdout: "update-demo-nautilus-nrsss update-demo-nautilus-v2r2v "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 05/18/23 09:41:43.442
  E0518 09:41:44.121036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:45.121161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:46.121639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:47.121785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:48.122744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:48.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 18 09:41:48.623: INFO: stderr: ""
  May 18 09:41:48.623: INFO: stdout: "update-demo-nautilus-nrsss "
  May 18 09:41:48.624: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-nrsss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 18 09:41:48.743: INFO: stderr: ""
  May 18 09:41:48.743: INFO: stdout: "true"
  May 18 09:41:48.743: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-nrsss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 18 09:41:48.844: INFO: stderr: ""
  May 18 09:41:48.844: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 18 09:41:48.844: INFO: validating pod update-demo-nautilus-nrsss
  May 18 09:41:48.853: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 18 09:41:48.854: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 18 09:41:48.854: INFO: update-demo-nautilus-nrsss is verified up and running
  STEP: scaling up the replication controller @ 05/18/23 09:41:48.854
  May 18 09:41:48.857: INFO: scanned /root for discovery docs: <nil>
  May 18 09:41:48.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0518 09:41:49.123621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:49.996: INFO: stderr: ""
  May 18 09:41:49.997: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/18/23 09:41:49.997
  May 18 09:41:49.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 18 09:41:50.108: INFO: stderr: ""
  May 18 09:41:50.108: INFO: stdout: "update-demo-nautilus-jlptk update-demo-nautilus-nrsss "
  May 18 09:41:50.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-jlptk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0518 09:41:50.124239      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:50.247: INFO: stderr: ""
  May 18 09:41:50.247: INFO: stdout: ""
  May 18 09:41:50.247: INFO: update-demo-nautilus-jlptk is created but not running
  E0518 09:41:51.124171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:52.125119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:53.125283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:54.125385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:41:55.125600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:55.248: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 18 09:41:55.397: INFO: stderr: ""
  May 18 09:41:55.397: INFO: stdout: "update-demo-nautilus-jlptk update-demo-nautilus-nrsss "
  May 18 09:41:55.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-jlptk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 18 09:41:55.524: INFO: stderr: ""
  May 18 09:41:55.524: INFO: stdout: "true"
  May 18 09:41:55.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-jlptk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 18 09:41:55.637: INFO: stderr: ""
  May 18 09:41:55.637: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 18 09:41:55.637: INFO: validating pod update-demo-nautilus-jlptk
  May 18 09:41:55.642: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 18 09:41:55.643: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 18 09:41:55.643: INFO: update-demo-nautilus-jlptk is verified up and running
  May 18 09:41:55.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-nrsss -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 18 09:41:55.752: INFO: stderr: ""
  May 18 09:41:55.752: INFO: stdout: "true"
  May 18 09:41:55.752: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods update-demo-nautilus-nrsss -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 18 09:41:55.871: INFO: stderr: ""
  May 18 09:41:55.871: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 18 09:41:55.871: INFO: validating pod update-demo-nautilus-nrsss
  E0518 09:41:56.126286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:41:56.428: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 18 09:41:56.428: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 18 09:41:56.428: INFO: update-demo-nautilus-nrsss is verified up and running
  STEP: using delete to clean up resources @ 05/18/23 09:41:56.428
  May 18 09:41:56.429: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 delete --grace-period=0 --force -f -'
  May 18 09:41:56.573: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 09:41:56.573: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 18 09:41:56.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get rc,svc -l name=update-demo --no-headers'
  May 18 09:41:56.767: INFO: stderr: "No resources found in kubectl-2211 namespace.\n"
  May 18 09:41:56.767: INFO: stdout: ""
  May 18 09:41:56.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-2211 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 18 09:41:56.903: INFO: stderr: ""
  May 18 09:41:56.903: INFO: stdout: ""
  May 18 09:41:56.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2211" for this suite. @ 05/18/23 09:41:56.909
• [22.633 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/18/23 09:41:56.921
  May 18 09:41:56.921: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename namespaces @ 05/18/23 09:41:56.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:56.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:56.95
  STEP: Updating Namespace "namespaces-8627" @ 05/18/23 09:41:56.958
  May 18 09:41:56.981: INFO: Namespace "namespaces-8627" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"5ca56008-4132-4939-9b12-133755241423", "kubernetes.io/metadata.name":"namespaces-8627", "namespaces-8627":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May 18 09:41:56.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8627" for this suite. @ 05/18/23 09:41:56.995
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/18/23 09:41:57.015
  May 18 09:41:57.016: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename proxy @ 05/18/23 09:41:57.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:41:57.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:41:57.038
  STEP: starting an echo server on multiple ports @ 05/18/23 09:41:57.061
  STEP: creating replication controller proxy-service-vwjj6 in namespace proxy-411 @ 05/18/23 09:41:57.061
  I0518 09:41:57.069546      19 runners.go:194] Created replication controller with name: proxy-service-vwjj6, namespace: proxy-411, replica count: 1
  E0518 09:41:57.127128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:41:58.121467      19 runners.go:194] proxy-service-vwjj6 Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0518 09:41:58.127902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:41:59.121875      19 runners.go:194] proxy-service-vwjj6 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  E0518 09:41:59.128295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:42:00.122778      19 runners.go:194] proxy-service-vwjj6 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 09:42:00.127: INFO: setup took 3.086569394s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/18/23 09:42:00.127
  E0518 09:42:00.128400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:42:00.137: INFO: (0) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.926527ms)
  May 18 09:42:00.137: INFO: (0) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.279013ms)
  May 18 09:42:00.137: INFO: (0) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.349863ms)
  May 18 09:42:00.145: INFO: (0) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 16.005047ms)
  May 18 09:42:00.145: INFO: (0) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 16.876632ms)
  May 18 09:42:00.145: INFO: (0) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 16.54647ms)
  May 18 09:42:00.145: INFO: (0) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 15.917412ms)
  May 18 09:42:00.145: INFO: (0) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 16.047722ms)
  May 18 09:42:00.149: INFO: (0) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 20.496237ms)
  May 18 09:42:00.149: INFO: (0) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 19.865842ms)
  May 18 09:42:00.149: INFO: (0) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 20.129063ms)
  May 18 09:42:00.149: INFO: (0) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 20.05186ms)
  May 18 09:42:00.150: INFO: (0) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 21.480282ms)
  May 18 09:42:00.150: INFO: (0) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 21.259339ms)
  May 18 09:42:00.150: INFO: (0) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 21.861544ms)
  May 18 09:42:00.152: INFO: (0) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 23.058878ms)
  May 18 09:42:00.156: INFO: (1) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 4.575414ms)
  May 18 09:42:00.156: INFO: (1) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 4.591988ms)
  May 18 09:42:00.158: INFO: (1) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 6.364277ms)
  May 18 09:42:00.159: INFO: (1) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 6.667876ms)
  May 18 09:42:00.159: INFO: (1) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 6.880388ms)
  May 18 09:42:00.161: INFO: (1) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 8.81941ms)
  May 18 09:42:00.163: INFO: (1) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 10.348042ms)
  May 18 09:42:00.163: INFO: (1) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 10.502374ms)
  May 18 09:42:00.163: INFO: (1) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 11.162897ms)
  May 18 09:42:00.163: INFO: (1) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 11.448658ms)
  May 18 09:42:00.163: INFO: (1) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 11.088968ms)
  May 18 09:42:00.163: INFO: (1) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 11.257579ms)
  May 18 09:42:00.163: INFO: (1) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 10.895269ms)
  May 18 09:42:00.164: INFO: (1) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 11.387931ms)
  May 18 09:42:00.164: INFO: (1) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 11.278397ms)
  May 18 09:42:00.164: INFO: (1) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 11.440483ms)
  May 18 09:42:00.177: INFO: (2) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 10.937952ms)
  May 18 09:42:00.177: INFO: (2) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 11.708712ms)
  May 18 09:42:00.177: INFO: (2) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 11.578605ms)
  May 18 09:42:00.177: INFO: (2) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 11.114185ms)
  May 18 09:42:00.177: INFO: (2) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 11.389027ms)
  May 18 09:42:00.178: INFO: (2) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 11.357208ms)
  May 18 09:42:00.180: INFO: (2) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 14.389675ms)
  May 18 09:42:00.184: INFO: (2) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 18.069519ms)
  May 18 09:42:00.185: INFO: (2) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 18.281051ms)
  May 18 09:42:00.185: INFO: (2) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 18.869949ms)
  May 18 09:42:00.186: INFO: (2) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 19.618278ms)
  May 18 09:42:00.186: INFO: (2) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 19.974192ms)
  May 18 09:42:00.186: INFO: (2) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 20.233299ms)
  May 18 09:42:00.186: INFO: (2) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 19.536294ms)
  May 18 09:42:00.186: INFO: (2) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 20.019974ms)
  May 18 09:42:00.186: INFO: (2) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 20.488154ms)
  May 18 09:42:00.193: INFO: (3) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 5.369276ms)
  May 18 09:42:00.194: INFO: (3) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 5.805804ms)
  May 18 09:42:00.195: INFO: (3) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 7.335538ms)
  May 18 09:42:00.195: INFO: (3) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 6.732708ms)
  May 18 09:42:00.195: INFO: (3) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 7.633298ms)
  May 18 09:42:00.196: INFO: (3) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.137238ms)
  May 18 09:42:00.196: INFO: (3) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 7.901539ms)
  May 18 09:42:00.198: INFO: (3) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 9.429185ms)
  May 18 09:42:00.198: INFO: (3) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 9.505441ms)
  May 18 09:42:00.198: INFO: (3) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 9.82254ms)
  May 18 09:42:00.199: INFO: (3) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 10.34162ms)
  May 18 09:42:00.199: INFO: (3) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 11.297352ms)
  May 18 09:42:00.199: INFO: (3) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 10.23108ms)
  May 18 09:42:00.199: INFO: (3) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 10.977623ms)
  May 18 09:42:00.199: INFO: (3) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 11.271652ms)
  May 18 09:42:00.199: INFO: (3) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 11.97399ms)
  May 18 09:42:00.208: INFO: (4) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 8.398595ms)
  May 18 09:42:00.209: INFO: (4) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 8.696087ms)
  May 18 09:42:00.209: INFO: (4) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 9.31816ms)
  May 18 09:42:00.211: INFO: (4) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 10.525059ms)
  May 18 09:42:00.211: INFO: (4) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 10.713096ms)
  May 18 09:42:00.211: INFO: (4) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 11.123365ms)
  May 18 09:42:00.212: INFO: (4) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 11.61812ms)
  May 18 09:42:00.212: INFO: (4) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 12.033223ms)
  May 18 09:42:00.212: INFO: (4) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 10.998813ms)
  May 18 09:42:00.212: INFO: (4) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 10.978879ms)
  May 18 09:42:00.212: INFO: (4) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 11.60699ms)
  May 18 09:42:00.214: INFO: (4) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 12.726413ms)
  May 18 09:42:00.214: INFO: (4) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 12.745395ms)
  May 18 09:42:00.214: INFO: (4) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 13.039707ms)
  May 18 09:42:00.214: INFO: (4) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 13.629527ms)
  May 18 09:42:00.214: INFO: (4) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 13.367105ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 9.514116ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 10.447983ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 9.467378ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 10.789078ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 10.504319ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 11.160939ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 11.004959ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 10.955169ms)
  May 18 09:42:00.226: INFO: (5) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 11.148881ms)
  May 18 09:42:00.227: INFO: (5) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 11.125295ms)
  May 18 09:42:00.227: INFO: (5) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 11.144195ms)
  May 18 09:42:00.227: INFO: (5) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 12.009875ms)
  May 18 09:42:00.228: INFO: (5) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 12.634437ms)
  May 18 09:42:00.228: INFO: (5) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 12.757396ms)
  May 18 09:42:00.228: INFO: (5) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 13.225113ms)
  May 18 09:42:00.228: INFO: (5) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 12.694367ms)
  May 18 09:42:00.238: INFO: (6) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 7.643113ms)
  May 18 09:42:00.240: INFO: (6) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 11.094731ms)
  May 18 09:42:00.240: INFO: (6) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 10.744708ms)
  May 18 09:42:00.240: INFO: (6) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 9.971696ms)
  May 18 09:42:00.240: INFO: (6) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 11.710818ms)
  May 18 09:42:00.241: INFO: (6) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 10.77818ms)
  May 18 09:42:00.241: INFO: (6) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 10.065695ms)
  May 18 09:42:00.241: INFO: (6) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 10.550066ms)
  May 18 09:42:00.241: INFO: (6) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 12.377331ms)
  May 18 09:42:00.241: INFO: (6) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 10.584331ms)
  May 18 09:42:00.242: INFO: (6) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 13.111666ms)
  May 18 09:42:00.242: INFO: (6) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 12.3368ms)
  May 18 09:42:00.242: INFO: (6) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 12.843647ms)
  May 18 09:42:00.242: INFO: (6) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 11.002994ms)
  May 18 09:42:00.242: INFO: (6) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 12.013022ms)
  May 18 09:42:00.242: INFO: (6) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 12.461185ms)
  May 18 09:42:00.247: INFO: (7) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 5.076376ms)
  May 18 09:42:00.248: INFO: (7) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 5.606943ms)
  May 18 09:42:00.250: INFO: (7) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 7.471596ms)
  May 18 09:42:00.251: INFO: (7) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 8.003322ms)
  May 18 09:42:00.251: INFO: (7) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 8.437973ms)
  May 18 09:42:00.252: INFO: (7) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 9.980413ms)
  May 18 09:42:00.252: INFO: (7) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 9.700738ms)
  May 18 09:42:00.252: INFO: (7) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 9.629804ms)
  May 18 09:42:00.252: INFO: (7) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 9.802079ms)
  May 18 09:42:00.253: INFO: (7) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 9.966604ms)
  May 18 09:42:00.253: INFO: (7) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 10.54627ms)
  May 18 09:42:00.253: INFO: (7) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 10.620026ms)
  May 18 09:42:00.253: INFO: (7) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 10.726803ms)
  May 18 09:42:00.253: INFO: (7) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 11.307696ms)
  May 18 09:42:00.253: INFO: (7) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 11.039001ms)
  May 18 09:42:00.253: INFO: (7) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 10.339137ms)
  May 18 09:42:00.262: INFO: (8) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 7.634349ms)
  May 18 09:42:00.262: INFO: (8) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.090084ms)
  May 18 09:42:00.262: INFO: (8) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.338415ms)
  May 18 09:42:00.262: INFO: (8) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 7.699762ms)
  May 18 09:42:00.262: INFO: (8) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.393837ms)
  May 18 09:42:00.263: INFO: (8) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 7.821261ms)
  May 18 09:42:00.263: INFO: (8) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 8.136541ms)
  May 18 09:42:00.263: INFO: (8) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.327642ms)
  May 18 09:42:00.263: INFO: (8) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 8.852984ms)
  May 18 09:42:00.263: INFO: (8) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 9.030738ms)
  May 18 09:42:00.264: INFO: (8) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 9.697391ms)
  May 18 09:42:00.265: INFO: (8) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 10.376309ms)
  May 18 09:42:00.265: INFO: (8) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 10.575781ms)
  May 18 09:42:00.265: INFO: (8) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 10.643521ms)
  May 18 09:42:00.265: INFO: (8) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 10.840953ms)
  May 18 09:42:00.265: INFO: (8) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 11.018076ms)
  May 18 09:42:00.273: INFO: (9) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 6.843613ms)
  May 18 09:42:00.274: INFO: (9) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 7.392765ms)
  May 18 09:42:00.274: INFO: (9) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 7.435703ms)
  May 18 09:42:00.274: INFO: (9) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.337562ms)
  May 18 09:42:00.276: INFO: (9) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 9.67344ms)
  May 18 09:42:00.276: INFO: (9) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 9.937453ms)
  May 18 09:42:00.277: INFO: (9) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 10.129558ms)
  May 18 09:42:00.277: INFO: (9) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 10.874585ms)
  May 18 09:42:00.277: INFO: (9) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 10.661914ms)
  May 18 09:42:00.277: INFO: (9) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 10.973451ms)
  May 18 09:42:00.278: INFO: (9) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 11.793517ms)
  May 18 09:42:00.278: INFO: (9) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 11.748204ms)
  May 18 09:42:00.279: INFO: (9) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 11.998205ms)
  May 18 09:42:00.279: INFO: (9) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 12.510598ms)
  May 18 09:42:00.279: INFO: (9) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 12.664478ms)
  May 18 09:42:00.279: INFO: (9) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 12.465848ms)
  May 18 09:42:00.295: INFO: (10) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 16.082917ms)
  May 18 09:42:00.296: INFO: (10) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 15.52147ms)
  May 18 09:42:00.296: INFO: (10) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 15.937978ms)
  May 18 09:42:00.296: INFO: (10) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 16.689755ms)
  May 18 09:42:00.296: INFO: (10) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 16.019221ms)
  May 18 09:42:00.298: INFO: (10) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 17.486406ms)
  May 18 09:42:00.299: INFO: (10) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 18.522791ms)
  May 18 09:42:00.299: INFO: (10) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 18.843343ms)
  May 18 09:42:00.299: INFO: (10) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 19.08519ms)
  May 18 09:42:00.300: INFO: (10) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 19.773479ms)
  May 18 09:42:00.300: INFO: (10) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 20.032742ms)
  May 18 09:42:00.300: INFO: (10) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 20.441409ms)
  May 18 09:42:00.300: INFO: (10) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 20.785059ms)
  May 18 09:42:00.300: INFO: (10) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 21.079917ms)
  May 18 09:42:00.300: INFO: (10) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 20.900817ms)
  May 18 09:42:00.301: INFO: (10) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 20.473533ms)
  May 18 09:42:00.308: INFO: (11) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 6.33334ms)
  May 18 09:42:00.308: INFO: (11) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 6.937707ms)
  May 18 09:42:00.308: INFO: (11) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 6.895793ms)
  May 18 09:42:00.310: INFO: (11) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.334233ms)
  May 18 09:42:00.310: INFO: (11) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 8.911254ms)
  May 18 09:42:00.311: INFO: (11) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 9.103919ms)
  May 18 09:42:00.311: INFO: (11) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 9.163456ms)
  May 18 09:42:00.311: INFO: (11) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 10.012116ms)
  May 18 09:42:00.311: INFO: (11) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 9.687378ms)
  May 18 09:42:00.312: INFO: (11) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 10.694162ms)
  May 18 09:42:00.313: INFO: (11) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 11.49719ms)
  May 18 09:42:00.314: INFO: (11) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 11.614587ms)
  May 18 09:42:00.314: INFO: (11) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 12.730879ms)
  May 18 09:42:00.314: INFO: (11) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 12.600615ms)
  May 18 09:42:00.314: INFO: (11) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 12.999756ms)
  May 18 09:42:00.314: INFO: (11) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 13.354964ms)
  May 18 09:42:00.322: INFO: (12) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 7.238051ms)
  May 18 09:42:00.322: INFO: (12) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 7.519785ms)
  May 18 09:42:00.323: INFO: (12) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 7.782528ms)
  May 18 09:42:00.323: INFO: (12) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 7.556383ms)
  May 18 09:42:00.323: INFO: (12) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 7.48401ms)
  May 18 09:42:00.323: INFO: (12) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 6.778696ms)
  May 18 09:42:00.324: INFO: (12) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 7.594938ms)
  May 18 09:42:00.324: INFO: (12) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.028913ms)
  May 18 09:42:00.326: INFO: (12) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 9.03462ms)
  May 18 09:42:00.326: INFO: (12) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 9.465751ms)
  May 18 09:42:00.326: INFO: (12) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 9.632589ms)
  May 18 09:42:00.326: INFO: (12) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 10.728379ms)
  May 18 09:42:00.326: INFO: (12) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 11.288261ms)
  May 18 09:42:00.327: INFO: (12) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 10.763253ms)
  May 18 09:42:00.327: INFO: (12) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 11.020143ms)
  May 18 09:42:00.327: INFO: (12) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 10.755103ms)
  May 18 09:42:00.335: INFO: (13) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 7.512723ms)
  May 18 09:42:00.335: INFO: (13) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 7.33934ms)
  May 18 09:42:00.335: INFO: (13) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 7.849664ms)
  May 18 09:42:00.335: INFO: (13) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.321685ms)
  May 18 09:42:00.336: INFO: (13) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.495389ms)
  May 18 09:42:00.336: INFO: (13) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.575697ms)
  May 18 09:42:00.336: INFO: (13) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 8.244852ms)
  May 18 09:42:00.336: INFO: (13) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 8.540635ms)
  May 18 09:42:00.336: INFO: (13) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 9.066178ms)
  May 18 09:42:00.337: INFO: (13) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 8.813131ms)
  May 18 09:42:00.337: INFO: (13) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 9.28366ms)
  May 18 09:42:00.337: INFO: (13) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 9.431609ms)
  May 18 09:42:00.337: INFO: (13) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 9.649634ms)
  May 18 09:42:00.338: INFO: (13) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 9.848946ms)
  May 18 09:42:00.338: INFO: (13) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 9.890737ms)
  May 18 09:42:00.338: INFO: (13) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 10.546416ms)
  May 18 09:42:00.346: INFO: (14) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 6.782493ms)
  May 18 09:42:00.346: INFO: (14) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 7.877352ms)
  May 18 09:42:00.347: INFO: (14) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.545081ms)
  May 18 09:42:00.347: INFO: (14) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 9.111756ms)
  May 18 09:42:00.347: INFO: (14) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 8.345957ms)
  May 18 09:42:00.347: INFO: (14) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 8.713245ms)
  May 18 09:42:00.347: INFO: (14) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.862248ms)
  May 18 09:42:00.347: INFO: (14) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 9.169181ms)
  May 18 09:42:00.347: INFO: (14) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 9.131638ms)
  May 18 09:42:00.347: INFO: (14) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 8.609047ms)
  May 18 09:42:00.348: INFO: (14) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 9.351705ms)
  May 18 09:42:00.348: INFO: (14) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 8.876181ms)
  May 18 09:42:00.348: INFO: (14) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 9.092116ms)
  May 18 09:42:00.348: INFO: (14) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 9.325251ms)
  May 18 09:42:00.348: INFO: (14) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 9.380336ms)
  May 18 09:42:00.348: INFO: (14) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 9.090251ms)
  May 18 09:42:00.358: INFO: (15) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 3.994103ms)
  May 18 09:42:00.358: INFO: (15) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 4.551219ms)
  May 18 09:42:00.359: INFO: (15) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 5.288774ms)
  May 18 09:42:00.362: INFO: (15) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 8.100291ms)
  May 18 09:42:00.362: INFO: (15) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 8.81469ms)
  May 18 09:42:00.363: INFO: (15) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 8.642135ms)
  May 18 09:42:00.363: INFO: (15) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.704629ms)
  May 18 09:42:00.363: INFO: (15) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.513225ms)
  May 18 09:42:00.363: INFO: (15) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 8.825737ms)
  May 18 09:42:00.363: INFO: (15) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 9.551949ms)
  May 18 09:42:00.364: INFO: (15) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 10.115463ms)
  May 18 09:42:00.364: INFO: (15) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 10.29511ms)
  May 18 09:42:00.364: INFO: (15) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 10.875381ms)
  May 18 09:42:00.365: INFO: (15) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 10.880015ms)
  May 18 09:42:00.365: INFO: (15) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 11.10955ms)
  May 18 09:42:00.365: INFO: (15) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 11.290355ms)
  May 18 09:42:00.369: INFO: (16) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 3.740432ms)
  May 18 09:42:00.374: INFO: (16) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 6.699051ms)
  May 18 09:42:00.375: INFO: (16) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 7.411515ms)
  May 18 09:42:00.375: INFO: (16) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 8.879683ms)
  May 18 09:42:00.375: INFO: (16) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 8.543005ms)
  May 18 09:42:00.375: INFO: (16) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 9.151297ms)
  May 18 09:42:00.375: INFO: (16) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 8.813319ms)
  May 18 09:42:00.376: INFO: (16) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 9.518032ms)
  May 18 09:42:00.376: INFO: (16) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 9.828718ms)
  May 18 09:42:00.376: INFO: (16) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 10.271493ms)
  May 18 09:42:00.378: INFO: (16) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 11.037902ms)
  May 18 09:42:00.378: INFO: (16) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 10.769406ms)
  May 18 09:42:00.378: INFO: (16) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 11.620991ms)
  May 18 09:42:00.379: INFO: (16) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 11.294048ms)
  May 18 09:42:00.379: INFO: (16) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 11.767741ms)
  May 18 09:42:00.379: INFO: (16) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 13.205852ms)
  May 18 09:42:00.387: INFO: (17) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 7.503129ms)
  May 18 09:42:00.387: INFO: (17) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 7.501838ms)
  May 18 09:42:00.387: INFO: (17) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 7.309986ms)
  May 18 09:42:00.387: INFO: (17) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 7.192418ms)
  May 18 09:42:00.387: INFO: (17) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 8.119697ms)
  May 18 09:42:00.388: INFO: (17) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 8.235685ms)
  May 18 09:42:00.388: INFO: (17) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 8.05761ms)
  May 18 09:42:00.388: INFO: (17) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 9.089031ms)
  May 18 09:42:00.389: INFO: (17) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 9.517777ms)
  May 18 09:42:00.390: INFO: (17) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 10.851561ms)
  May 18 09:42:00.390: INFO: (17) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 9.721591ms)
  May 18 09:42:00.390: INFO: (17) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 9.88523ms)
  May 18 09:42:00.390: INFO: (17) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 10.008168ms)
  May 18 09:42:00.390: INFO: (17) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 10.528296ms)
  May 18 09:42:00.390: INFO: (17) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 10.709795ms)
  May 18 09:42:00.390: INFO: (17) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 10.664555ms)
  May 18 09:42:00.398: INFO: (18) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 6.577803ms)
  May 18 09:42:00.399: INFO: (18) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.428544ms)
  May 18 09:42:00.400: INFO: (18) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 8.988982ms)
  May 18 09:42:00.400: INFO: (18) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 9.154916ms)
  May 18 09:42:00.400: INFO: (18) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 8.946079ms)
  May 18 09:42:00.401: INFO: (18) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 9.494136ms)
  May 18 09:42:00.401: INFO: (18) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 9.671472ms)
  May 18 09:42:00.401: INFO: (18) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 9.463879ms)
  May 18 09:42:00.401: INFO: (18) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 9.82712ms)
  May 18 09:42:00.401: INFO: (18) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 10.134808ms)
  May 18 09:42:00.402: INFO: (18) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 10.420432ms)
  May 18 09:42:00.402: INFO: (18) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 10.635573ms)
  May 18 09:42:00.402: INFO: (18) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 10.562445ms)
  May 18 09:42:00.402: INFO: (18) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 10.813129ms)
  May 18 09:42:00.402: INFO: (18) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 10.85114ms)
  May 18 09:42:00.402: INFO: (18) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 10.968821ms)
  May 18 09:42:00.410: INFO: (19) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:462/proxy/: tls qux (200; 5.319654ms)
  May 18 09:42:00.412: INFO: (19) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw/proxy/rewriteme">test</a> (200; 6.383421ms)
  May 18 09:42:00.412: INFO: (19) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 6.338109ms)
  May 18 09:42:00.412: INFO: (19) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 6.685963ms)
  May 18 09:42:00.412: INFO: (19) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">t... (200; 6.293458ms)
  May 18 09:42:00.413: INFO: (19) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:160/proxy/: foo (200; 7.15119ms)
  May 18 09:42:00.413: INFO: (19) /api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/proxy-service-vwjj6-mnsvw:1080/proxy/rewriteme">test</... (200; 7.302772ms)
  May 18 09:42:00.413: INFO: (19) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname2/proxy/: tls qux (200; 7.771727ms)
  May 18 09:42:00.413: INFO: (19) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:460/proxy/: tls baz (200; 7.920708ms)
  May 18 09:42:00.413: INFO: (19) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname2/proxy/: bar (200; 7.598488ms)
  May 18 09:42:00.413: INFO: (19) /api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/: <a href="/api/v1/namespaces/proxy-411/pods/https:proxy-service-vwjj6-mnsvw:443/proxy/tlsrewriteme... (200; 7.517633ms)
  May 18 09:42:00.413: INFO: (19) /api/v1/namespaces/proxy-411/pods/http:proxy-service-vwjj6-mnsvw:162/proxy/: bar (200; 8.073534ms)
  May 18 09:42:00.415: INFO: (19) /api/v1/namespaces/proxy-411/services/http:proxy-service-vwjj6:portname1/proxy/: foo (200; 9.620005ms)
  May 18 09:42:00.415: INFO: (19) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname2/proxy/: bar (200; 9.576982ms)
  May 18 09:42:00.415: INFO: (19) /api/v1/namespaces/proxy-411/services/proxy-service-vwjj6:portname1/proxy/: foo (200; 9.604799ms)
  May 18 09:42:00.415: INFO: (19) /api/v1/namespaces/proxy-411/services/https:proxy-service-vwjj6:tlsportname1/proxy/: tls baz (200; 9.848849ms)
  May 18 09:42:00.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-vwjj6 in namespace proxy-411, will wait for the garbage collector to delete the pods @ 05/18/23 09:42:00.419
  May 18 09:42:00.480: INFO: Deleting ReplicationController proxy-service-vwjj6 took: 6.906182ms
  May 18 09:42:00.582: INFO: Terminating ReplicationController proxy-service-vwjj6 pods took: 101.592297ms
  E0518 09:42:01.128524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:02.131673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "proxy-411" for this suite. @ 05/18/23 09:42:02.883
• [5.876 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/18/23 09:42:02.896
  May 18 09:42:02.896: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:42:02.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:42:02.914
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:42:02.919
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/18/23 09:42:02.922
  E0518 09:42:03.132717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:04.132821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:05.133323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:06.133571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:42:06.95
  May 18 09:42:06.955: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-420ab80f-4e00-4809-b251-08ca9940cafc container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:42:06.966
  May 18 09:42:06.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5254" for this suite. @ 05/18/23 09:42:06.993
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/18/23 09:42:07.009
  May 18 09:42:07.009: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:42:07.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:42:07.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:42:07.036
  STEP: Creating secret with name secret-test-e45ba42a-5a66-4cb2-971e-804480934e55 @ 05/18/23 09:42:07.041
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:42:07.05
  E0518 09:42:07.134725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:08.134719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:09.135178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:10.136203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:42:11.074
  May 18 09:42:11.079: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-secrets-d9ce4e9f-55c5-46b6-ac0c-4be8d92e7b49 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:42:11.091
  May 18 09:42:11.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8867" for this suite. @ 05/18/23 09:42:11.122
• [4.123 seconds]
------------------------------
SSSSSSSSSSSS  E0518 09:42:11.136678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/18/23 09:42:11.144
  May 18 09:42:11.145: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename subpath @ 05/18/23 09:42:11.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:42:11.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:42:11.18
  STEP: Setting up data @ 05/18/23 09:42:11.187
  STEP: Creating pod pod-subpath-test-configmap-pf6x @ 05/18/23 09:42:11.201
  STEP: Creating a pod to test atomic-volume-subpath @ 05/18/23 09:42:11.201
  E0518 09:42:12.137958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:13.138330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:14.139091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:15.140105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:16.140046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:17.141150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:18.140801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:19.141803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:20.141564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:21.141956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:22.143168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:23.143545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:24.144267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:25.144457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:26.145317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:27.145935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:28.146125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:29.146378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:30.146767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:31.147242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:32.147988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:33.148537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:34.149500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:35.149731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:42:35.298
  May 18 09:42:35.302: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-subpath-test-configmap-pf6x container test-container-subpath-configmap-pf6x: <nil>
  STEP: delete the pod @ 05/18/23 09:42:35.312
  STEP: Deleting pod pod-subpath-test-configmap-pf6x @ 05/18/23 09:42:35.335
  May 18 09:42:35.335: INFO: Deleting pod "pod-subpath-test-configmap-pf6x" in namespace "subpath-6001"
  May 18 09:42:35.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6001" for this suite. @ 05/18/23 09:42:35.346
• [24.211 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/18/23 09:42:35.361
  May 18 09:42:35.362: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:42:35.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:42:35.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:42:35.385
  STEP: Setting up server cert @ 05/18/23 09:42:35.409
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:42:35.933
  STEP: Deploying the webhook pod @ 05/18/23 09:42:35.944
  STEP: Wait for the deployment to be ready @ 05/18/23 09:42:35.957
  May 18 09:42:35.965: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:42:36.150976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:37.151670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:42:38
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:42:38.019
  E0518 09:42:38.151719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:42:39.020: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 18 09:42:39.025: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:42:39.152621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3565-crds.webhook.example.com via the AdmissionRegistration API @ 05/18/23 09:42:39.543
  STEP: Creating a custom resource while v1 is storage version @ 05/18/23 09:42:39.565
  E0518 09:42:40.153402      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:41.153677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/18/23 09:42:41.655
  STEP: Patching the custom resource while v2 is storage version @ 05/18/23 09:42:41.665
  May 18 09:42:41.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0518 09:42:42.157046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-4056" for this suite. @ 05/18/23 09:42:42.321
  STEP: Destroying namespace "webhook-markers-312" for this suite. @ 05/18/23 09:42:42.344
• [7.003 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/18/23 09:42:42.366
  May 18 09:42:42.367: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/18/23 09:42:42.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:42:42.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:42:42.414
  STEP: mirroring a new custom Endpoint @ 05/18/23 09:42:42.448
  May 18 09:42:42.464: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0518 09:42:43.157212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:44.157726      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 05/18/23 09:42:44.47
  May 18 09:42:44.481: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0518 09:42:45.158065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:46.158925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 05/18/23 09:42:46.487
  May 18 09:42:46.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-4402" for this suite. @ 05/18/23 09:42:46.509
• [4.165 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/18/23 09:42:46.538
  May 18 09:42:46.539: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:42:46.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:42:46.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:42:46.574
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:42:46.58
  E0518 09:42:47.159463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:48.160429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:49.160426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:50.160420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:42:50.613
  May 18 09:42:50.618: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-a553a145-23e0-469f-833c-d846750eeecd container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:42:50.627
  May 18 09:42:50.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4609" for this suite. @ 05/18/23 09:42:50.649
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/18/23 09:42:50.688
  May 18 09:42:50.689: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-runtime @ 05/18/23 09:42:50.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:42:50.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:42:50.713
  STEP: create the container @ 05/18/23 09:42:50.719
  W0518 09:42:50.734614      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/18/23 09:42:50.734
  E0518 09:42:51.160499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:52.160801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:53.161162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/18/23 09:42:53.758
  STEP: the container should be terminated @ 05/18/23 09:42:53.761
  STEP: the termination message should be set @ 05/18/23 09:42:53.761
  May 18 09:42:53.762: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/18/23 09:42:53.762
  May 18 09:42:53.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5142" for this suite. @ 05/18/23 09:42:53.789
• [3.109 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/18/23 09:42:53.799
  May 18 09:42:53.799: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename deployment @ 05/18/23 09:42:53.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:42:53.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:42:53.82
  May 18 09:42:53.833: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0518 09:42:54.162123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:55.162682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:56.163230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:57.164304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:42:58.164572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:42:58.839: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/18/23 09:42:58.839
  May 18 09:42:58.839: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0518 09:42:59.164692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:00.165159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:00.844: INFO: Creating deployment "test-rollover-deployment"
  May 18 09:43:00.854: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0518 09:43:01.166158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:02.166674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:02.862: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May 18 09:43:02.871: INFO: Ensure that both replica sets have 1 created replica
  May 18 09:43:02.879: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May 18 09:43:02.891: INFO: Updating deployment test-rollover-deployment
  May 18 09:43:02.891: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0518 09:43:03.167653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:04.167771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:04.899: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May 18 09:43:04.904: INFO: Make sure deployment "test-rollover-deployment" is complete
  May 18 09:43:04.909: INFO: all replica sets need to contain the pod-template-hash label
  May 18 09:43:04.910: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:43:05.168349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:06.168801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:06.919: INFO: all replica sets need to contain the pod-template-hash label
  May 18 09:43:06.919: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:43:07.169015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:08.169425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:08.920: INFO: all replica sets need to contain the pod-template-hash label
  May 18 09:43:08.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:43:09.169476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:10.169727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:10.920: INFO: all replica sets need to contain the pod-template-hash label
  May 18 09:43:10.921: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:43:11.170639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:12.170899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:12.922: INFO: all replica sets need to contain the pod-template-hash label
  May 18 09:43:12.922: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 9, 43, 4, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 0, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 09:43:13.171945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:14.172274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:14.919: INFO: 
  May 18 09:43:14.919: INFO: Ensure that both old replica sets have no replicas
  May 18 09:43:14.931: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-6240  5bb12f7f-2728-494e-8249-fa902648bb13 1950626 2 2023-05-18 09:43:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-18 09:43:02 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0042d18e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-18 09:43:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-18 09:43:14 +0000 UTC,LastTransitionTime:2023-05-18 09:43:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 18 09:43:14.935: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-6240  b7c043dd-ae3c-4192-a7ee-bb8a45b98cc0 1950616 2 2023-05-18 09:43:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 5bb12f7f-2728-494e-8249-fa902648bb13 0xc003c594b7 0xc003c594b8}] [] [{kube-controller-manager Update apps/v1 2023-05-18 09:43:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5bb12f7f-2728-494e-8249-fa902648bb13\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:14 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c59568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:43:14.935: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May 18 09:43:14.935: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-6240  7458986c-b00a-4371-9a0c-8b42224d4823 1950625 2 2023-05-18 09:42:53 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 5bb12f7f-2728-494e-8249-fa902648bb13 0xc003c59387 0xc003c59388}] [] [{e2e.test Update apps/v1 2023-05-18 09:42:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:14 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5bb12f7f-2728-494e-8249-fa902648bb13\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:14 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003c59448 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:43:14.935: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-6240  89e4e450-c484-4f6e-b91c-176f141acb36 1950540 2 2023-05-18 09:43:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 5bb12f7f-2728-494e-8249-fa902648bb13 0xc003c595d7 0xc003c595d8}] [] [{kube-controller-manager Update apps/v1 2023-05-18 09:43:02 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5bb12f7f-2728-494e-8249-fa902648bb13\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:02 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003c59688 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:43:14.940: INFO: Pod "test-rollover-deployment-57777854c9-sbt98" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-sbt98 test-rollover-deployment-57777854c9- deployment-6240  0b9dcad2-b2d5-419d-8304-d0f9dde11497 1950566 0 2023-05-18 09:43:02 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:043ac7b822590704f8870ac0baac7ae646eb3e841142b7c94746f724f948f8b0 cni.projectcalico.org/podIP:172.16.161.125/32 cni.projectcalico.org/podIPs:172.16.161.125/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 b7c043dd-ae3c-4192-a7ee-bb8a45b98cc0 0xc003c59c17 0xc003c59c18}] [] [{kube-controller-manager Update v1 2023-05-18 09:43:02 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b7c043dd-ae3c-4192-a7ee-bb8a45b98cc0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:43:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:43:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.125\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2s2h7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2s2h7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:04 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:02 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.125,StartTime:2023-05-18 09:43:02 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:43:03 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://48fae6a301cae11b3bc2f50691364dfa3d741ae8b6383b9c0a2e27758a4bd883,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.125,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:43:14.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6240" for this suite. @ 05/18/23 09:43:14.95
• [21.160 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/18/23 09:43:14.963
  May 18 09:43:14.963: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:43:14.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:14.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:14.989
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:43:14.994
  E0518 09:43:15.172673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:16.172935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:17.173813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:18.173830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:43:19.023
  May 18 09:43:19.027: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-5b80d2b7-af16-4787-9a11-865ca56377c7 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:43:19.038
  May 18 09:43:19.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2518" for this suite. @ 05/18/23 09:43:19.062
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/18/23 09:43:19.1
  May 18 09:43:19.101: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:43:19.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:19.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:19.131
  STEP: creating a replication controller @ 05/18/23 09:43:19.136
  May 18 09:43:19.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 create -f -'
  E0518 09:43:19.174545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:19.974: INFO: stderr: ""
  May 18 09:43:19.974: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/18/23 09:43:19.974
  May 18 09:43:19.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 18 09:43:20.170: INFO: stderr: ""
  May 18 09:43:20.170: INFO: stdout: "update-demo-nautilus-g9g8v update-demo-nautilus-zcxk9 "
  May 18 09:43:20.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get pods update-demo-nautilus-g9g8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0518 09:43:20.175474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:20.309: INFO: stderr: ""
  May 18 09:43:20.309: INFO: stdout: ""
  May 18 09:43:20.309: INFO: update-demo-nautilus-g9g8v is created but not running
  E0518 09:43:21.176051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:22.175948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:23.175985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:24.176154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:25.176310      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:25.310: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 18 09:43:25.458: INFO: stderr: ""
  May 18 09:43:25.458: INFO: stdout: "update-demo-nautilus-g9g8v update-demo-nautilus-zcxk9 "
  May 18 09:43:25.458: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get pods update-demo-nautilus-g9g8v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 18 09:43:25.554: INFO: stderr: ""
  May 18 09:43:25.554: INFO: stdout: "true"
  May 18 09:43:25.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get pods update-demo-nautilus-g9g8v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 18 09:43:25.674: INFO: stderr: ""
  May 18 09:43:25.674: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 18 09:43:25.674: INFO: validating pod update-demo-nautilus-g9g8v
  May 18 09:43:25.725: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 18 09:43:25.725: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 18 09:43:25.725: INFO: update-demo-nautilus-g9g8v is verified up and running
  May 18 09:43:25.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get pods update-demo-nautilus-zcxk9 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 18 09:43:25.868: INFO: stderr: ""
  May 18 09:43:25.868: INFO: stdout: "true"
  May 18 09:43:25.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get pods update-demo-nautilus-zcxk9 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 18 09:43:25.983: INFO: stderr: ""
  May 18 09:43:25.983: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 18 09:43:25.983: INFO: validating pod update-demo-nautilus-zcxk9
  May 18 09:43:25.990: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 18 09:43:25.990: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 18 09:43:25.990: INFO: update-demo-nautilus-zcxk9 is verified up and running
  STEP: using delete to clean up resources @ 05/18/23 09:43:25.991
  May 18 09:43:25.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 delete --grace-period=0 --force -f -'
  May 18 09:43:26.097: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 09:43:26.097: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 18 09:43:26.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get rc,svc -l name=update-demo --no-headers'
  E0518 09:43:26.177494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:26.326: INFO: stderr: "No resources found in kubectl-9163 namespace.\n"
  May 18 09:43:26.326: INFO: stdout: ""
  May 18 09:43:26.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-9163 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 18 09:43:26.486: INFO: stderr: ""
  May 18 09:43:26.486: INFO: stdout: ""
  May 18 09:43:26.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9163" for this suite. @ 05/18/23 09:43:26.491
• [7.397 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/18/23 09:43:26.498
  May 18 09:43:26.498: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:43:26.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:26.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:26.519
  STEP: create deployment with httpd image @ 05/18/23 09:43:26.522
  May 18 09:43:26.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-7893 create -f -'
  May 18 09:43:27.042: INFO: stderr: ""
  May 18 09:43:27.042: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/18/23 09:43:27.042
  May 18 09:43:27.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-7893 diff -f -'
  E0518 09:43:27.178053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:27.563: INFO: rc: 1
  May 18 09:43:27.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-7893 delete -f -'
  May 18 09:43:27.673: INFO: stderr: ""
  May 18 09:43:27.673: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May 18 09:43:27.673: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7893" for this suite. @ 05/18/23 09:43:27.68
• [1.196 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/18/23 09:43:27.728
  May 18 09:43:27.728: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:43:27.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:27.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:27.754
  STEP: Creating Pod @ 05/18/23 09:43:27.76
  E0518 09:43:28.178369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:29.178966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 05/18/23 09:43:29.787
  May 18 09:43:29.787: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-8082 PodName:pod-sharedvolume-233ced70-c781-4fc7-938f-b937457999ab ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:43:29.787: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:43:29.789: INFO: ExecWithOptions: Clientset creation
  May 18 09:43:29.790: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-8082/pods/pod-sharedvolume-233ced70-c781-4fc7-938f-b937457999ab/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May 18 09:43:29.925: INFO: Exec stderr: ""
  May 18 09:43:29.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8082" for this suite. @ 05/18/23 09:43:29.931
• [2.209 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/18/23 09:43:29.941
  May 18 09:43:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename runtimeclass @ 05/18/23 09:43:29.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:29.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:29.965
  May 18 09:43:29.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4234" for this suite. @ 05/18/23 09:43:29.982
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/18/23 09:43:29.993
  May 18 09:43:29.993: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:43:29.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:30.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:30.01
  STEP: creating service endpoint-test2 in namespace services-1536 @ 05/18/23 09:43:30.015
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[] @ 05/18/23 09:43:30.046
  May 18 09:43:30.052: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0518 09:43:30.179774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:31.064: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1536 @ 05/18/23 09:43:31.065
  E0518 09:43:31.180354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:32.180762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[pod1:[80]] @ 05/18/23 09:43:33.089
  May 18 09:43:33.102: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/18/23 09:43:33.102
  May 18 09:43:33.102: INFO: Creating new exec pod
  E0518 09:43:33.181037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:34.181132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:35.182939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:36.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-1536 exec execpodxbmfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0518 09:43:36.182294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:36.424: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 18 09:43:36.424: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:43:36.424: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-1536 exec execpodxbmfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.29.48 80'
  May 18 09:43:36.683: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.29.48 80\nConnection to 10.111.29.48 80 port [tcp/http] succeeded!\n"
  May 18 09:43:36.683: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-1536 @ 05/18/23 09:43:36.683
  E0518 09:43:37.183004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:38.183229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/18/23 09:43:38.7
  May 18 09:43:38.714: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/18/23 09:43:38.715
  E0518 09:43:39.183511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:39.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-1536 exec execpodxbmfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 18 09:43:39.933: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 18 09:43:39.933: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:43:39.933: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-1536 exec execpodxbmfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.29.48 80'
  May 18 09:43:40.144: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.29.48 80\nConnection to 10.111.29.48 80 port [tcp/http] succeeded!\n"
  May 18 09:43:40.144: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1536 @ 05/18/23 09:43:40.144
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[pod2:[80]] @ 05/18/23 09:43:40.173
  E0518 09:43:40.184411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:40.187: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/18/23 09:43:40.187
  E0518 09:43:41.185755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:41.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-1536 exec execpodxbmfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 18 09:43:41.499: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 18 09:43:41.499: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:43:41.499: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-1536 exec execpodxbmfs -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.29.48 80'
  May 18 09:43:41.703: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.29.48 80\nConnection to 10.111.29.48 80 port [tcp/http] succeeded!\n"
  May 18 09:43:41.703: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-1536 @ 05/18/23 09:43:41.703
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1536 to expose endpoints map[] @ 05/18/23 09:43:41.721
  May 18 09:43:41.734: INFO: successfully validated that service endpoint-test2 in namespace services-1536 exposes endpoints map[]
  May 18 09:43:41.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1536" for this suite. @ 05/18/23 09:43:41.77
• [11.791 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/18/23 09:43:41.787
  May 18 09:43:41.788: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:43:41.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:41.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:41.813
  STEP: Creating a pod to test downward api env vars @ 05/18/23 09:43:41.818
  E0518 09:43:42.186862      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:43.187536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:44.187526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:45.187560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:43:45.849
  May 18 09:43:45.853: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downward-api-c419864f-7dfa-48be-ae23-a821ed23ac41 container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 09:43:45.875
  May 18 09:43:45.889: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4135" for this suite. @ 05/18/23 09:43:45.898
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/18/23 09:43:45.912
  May 18 09:43:45.912: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename deployment @ 05/18/23 09:43:45.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:45.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:45.938
  May 18 09:43:45.942: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May 18 09:43:45.953: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0518 09:43:46.189152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:47.189356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:48.189501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:49.189692      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:50.189787      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:50.959: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/18/23 09:43:50.959
  May 18 09:43:50.960: INFO: Creating deployment "test-rolling-update-deployment"
  May 18 09:43:50.969: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May 18 09:43:50.976: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0518 09:43:51.189979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:52.190447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:43:52.985: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May 18 09:43:52.990: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May 18 09:43:53.003: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2280  6b86867c-aef3-4476-8372-f7af903d4e33 1951189 1 2023-05-18 09:43:50 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-18 09:43:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004eacdf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-18 09:43:51 +0000 UTC,LastTransitionTime:2023-05-18 09:43:51 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-18 09:43:52 +0000 UTC,LastTransitionTime:2023-05-18 09:43:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 18 09:43:53.008: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-2280  082b91b6-27f8-4050-af85-20c5919787a4 1951179 1 2023-05-18 09:43:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 6b86867c-aef3-4476-8372-f7af903d4e33 0xc004af2317 0xc004af2318}] [] [{kube-controller-manager Update apps/v1 2023-05-18 09:43:50 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b86867c-aef3-4476-8372-f7af903d4e33\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:52 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004af23c8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:43:53.008: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May 18 09:43:53.009: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2280  795465ca-b00a-41e1-9a16-c4462b72bb67 1951188 2 2023-05-18 09:43:45 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 6b86867c-aef3-4476-8372-f7af903d4e33 0xc004af21e7 0xc004af21e8}] [] [{e2e.test Update apps/v1 2023-05-18 09:43:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6b86867c-aef3-4476-8372-f7af903d4e33\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-18 09:43:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004af22a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 18 09:43:53.014: INFO: Pod "test-rolling-update-deployment-656d657cd8-gtw67" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-gtw67 test-rolling-update-deployment-656d657cd8- deployment-2280  04e3f236-e6d0-4457-bf13-f1bae5809f0a 1951178 0 2023-05-18 09:43:50 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:9937b2faabed6e046afe4bfcbc301dca2e1b7ba6b9c08abbefa80c77cd927453 cni.projectcalico.org/podIP:172.16.161.96/32 cni.projectcalico.org/podIPs:172.16.161.96/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 082b91b6-27f8-4050-af85-20c5919787a4 0xc004ead1b7 0xc004ead1b8}] [] [{kube-controller-manager Update v1 2023-05-18 09:43:50 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"082b91b6-27f8-4050-af85-20c5919787a4\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 09:43:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 09:43:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.96\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kdqkx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kdqkx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 09:43:50 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.96,StartTime:2023-05-18 09:43:50 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 09:43:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://174fda82ece1cc47769fe08ac6cec6f541b909f3c9de27e2bef068114d7069ea,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.96,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 09:43:53.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2280" for this suite. @ 05/18/23 09:43:53.022
• [7.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/18/23 09:43:53.041
  May 18 09:43:53.041: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:43:53.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:53.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:53.07
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/18/23 09:43:53.075
  E0518 09:43:53.190519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:54.191112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:55.191205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:56.191450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:43:57.112
  May 18 09:43:57.119: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-423d3598-24a5-4d1d-953d-434aff3cd116 container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:43:57.132
  May 18 09:43:57.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6806" for this suite. @ 05/18/23 09:43:57.162
• [4.138 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/18/23 09:43:57.177
  May 18 09:43:57.177: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename init-container @ 05/18/23 09:43:57.179
  E0518 09:43:57.192257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:43:57.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:43:57.204
  STEP: creating the pod @ 05/18/23 09:43:57.208
  May 18 09:43:57.208: INFO: PodSpec: initContainers in spec.initContainers
  E0518 09:43:58.192442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:43:59.193674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:00.194010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:01.194970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:02.194831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:03.195027      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:04.194985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:05.195486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:06.196672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:07.197315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:08.197568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:09.198542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:10.198847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:11.199458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:12.200379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:13.200685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:14.201001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:15.201481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:16.201737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:17.202709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:18.202905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:19.203683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:20.204601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:21.204988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:22.205178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:23.205446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:24.205579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:25.205821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:26.206580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:27.206883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:28.206980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:29.207423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:30.207814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:31.208218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:32.209219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:33.209579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:34.209615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:35.210222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:36.211760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:37.211125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:38.212123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:39.212312      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:40.213053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:41.214794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:42.215360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:42.691: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-7e6e74d9-2d78-4121-be2c-5295ebebb98f", GenerateName:"", Namespace:"init-container-6634", SelfLink:"", UID:"4b968112-ca7f-4282-a1be-bf460d4cce07", ResourceVersion:"1951493", Generation:0, CreationTimestamp:time.Date(2023, time.May, 18, 9, 43, 57, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"208176326"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"0283dc584af8f4c3358aa242d7b74684a56d7c889b06bd56b140c247e1b3db86", "cni.projectcalico.org/podIP":"172.16.161.119/32", "cni.projectcalico.org/podIPs":"172.16.161.119/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 18, 9, 43, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ca120), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 18, 9, 43, 57, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ca150), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 18, 9, 44, 42, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0039ca180), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-kpb2h", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0035ae500), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kpb2h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kpb2h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-kpb2h", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0046e82a0), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ck-test-kube-1-27-worker", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000318000), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0046e8320)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0046e8340)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0046e8348), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0046e834c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00029fb80), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 57, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 57, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 18, 9, 43, 57, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.190.143", PodIP:"172.16.161.119", PodIPs:[]v1.PodIP{v1.PodIP{IP:"172.16.161.119"}}, StartTime:time.Date(2023, time.May, 18, 9, 43, 57, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0003180e0)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000318150)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://eced194716fc5f2a042d256f08f82f7943e432df8358424956f34655d6cd4938", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0035ae580), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0035ae560), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0046e8644), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May 18 09:44:42.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6634" for this suite. @ 05/18/23 09:44:42.705
• [45.537 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/18/23 09:44:42.719
  May 18 09:44:42.720: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename daemonsets @ 05/18/23 09:44:42.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:44:42.753
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:44:42.759
  May 18 09:44:42.785: INFO: Create a RollingUpdate DaemonSet
  May 18 09:44:42.792: INFO: Check that daemon pods launch on every node of the cluster
  May 18 09:44:42.797: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:44:42.800: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:44:42.801: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:44:43.215690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:43.808: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:44:43.813: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:44:43.813: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:44:44.215917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:44.808: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:44:44.812: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 09:44:44.813: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  May 18 09:44:44.813: INFO: Update the DaemonSet to trigger a rollout
  May 18 09:44:44.824: INFO: Updating DaemonSet daemon-set
  E0518 09:44:45.215922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:46.216048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:47.216528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:47.845: INFO: Roll back the DaemonSet before rollout is complete
  May 18 09:44:47.869: INFO: Updating DaemonSet daemon-set
  May 18 09:44:47.869: INFO: Make sure DaemonSet rollback is complete
  May 18 09:44:47.873: INFO: Wrong image for pod: daemon-set-gk2vc. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May 18 09:44:47.873: INFO: Pod daemon-set-gk2vc is not available
  May 18 09:44:47.878: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:44:48.217070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:48.886: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:44:49.219378      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:49.885: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:44:50.218989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:50.892: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:44:51.219921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:51.883: INFO: Pod daemon-set-q75mb is not available
  May 18 09:44:51.888: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 05/18/23 09:44:51.894
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4131, will wait for the garbage collector to delete the pods @ 05/18/23 09:44:51.894
  May 18 09:44:51.954: INFO: Deleting DaemonSet.extensions daemon-set took: 6.957572ms
  May 18 09:44:52.055: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.758004ms
  E0518 09:44:52.220470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:53.221009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:44:54.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:44:54.160: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 18 09:44:54.164: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1951686"},"items":null}

  May 18 09:44:54.168: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1951686"},"items":null}

  May 18 09:44:54.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4131" for this suite. @ 05/18/23 09:44:54.19
• [11.479 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/18/23 09:44:54.199
  May 18 09:44:54.199: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:44:54.201
  E0518 09:44:54.221720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:44:54.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:44:54.226
  STEP: Creating secret with name secret-test-3100ee18-8578-4994-acae-a1a6d1404277 @ 05/18/23 09:44:54.23
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:44:54.236
  E0518 09:44:55.222041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:56.222117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:57.222367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:44:58.222441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:44:58.264
  May 18 09:44:58.268: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-secrets-6d108a16-fdbf-4515-9126-1f533ac7a26d container secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:44:58.278
  May 18 09:44:58.298: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8738" for this suite. @ 05/18/23 09:44:58.304
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/18/23 09:44:58.313
  May 18 09:44:58.313: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replicaset @ 05/18/23 09:44:58.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:44:58.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:44:58.336
  May 18 09:44:58.355: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0518 09:44:59.223795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:00.225031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:01.225685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:02.226217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:03.226292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:45:03.359: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/18/23 09:45:03.359
  STEP: Scaling up "test-rs" replicaset  @ 05/18/23 09:45:03.359
  May 18 09:45:03.368: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/18/23 09:45:03.368
  W0518 09:45:03.381309      19 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 18 09:45:03.383: INFO: observed ReplicaSet test-rs in namespace replicaset-6888 with ReadyReplicas 1, AvailableReplicas 1
  May 18 09:45:03.388: INFO: observed ReplicaSet test-rs in namespace replicaset-6888 with ReadyReplicas 1, AvailableReplicas 1
  May 18 09:45:03.402: INFO: observed ReplicaSet test-rs in namespace replicaset-6888 with ReadyReplicas 1, AvailableReplicas 1
  May 18 09:45:03.407: INFO: observed ReplicaSet test-rs in namespace replicaset-6888 with ReadyReplicas 1, AvailableReplicas 1
  E0518 09:45:04.227025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:45:04.793: INFO: observed ReplicaSet test-rs in namespace replicaset-6888 with ReadyReplicas 2, AvailableReplicas 2
  May 18 09:45:05.163: INFO: observed Replicaset test-rs in namespace replicaset-6888 with ReadyReplicas 3 found true
  May 18 09:45:05.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6888" for this suite. @ 05/18/23 09:45:05.169
• [6.865 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/18/23 09:45:05.185
  May 18 09:45:05.186: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename daemonsets @ 05/18/23 09:45:05.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:45:05.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:45:05.21
  E0518 09:45:05.227364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating simple DaemonSet "daemon-set" @ 05/18/23 09:45:05.232
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/18/23 09:45:05.238
  May 18 09:45:05.242: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:45:05.244: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:45:05.244: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:45:06.227327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:45:06.251: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:45:06.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:45:06.256: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:45:07.228021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:45:07.251: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:45:07.256: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 09:45:07.256: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 05/18/23 09:45:07.261
  May 18 09:45:07.266: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/18/23 09:45:07.266
  May 18 09:45:07.275: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/18/23 09:45:07.275
  May 18 09:45:07.278: INFO: Observed &DaemonSet event: ADDED
  May 18 09:45:07.278: INFO: Observed &DaemonSet event: MODIFIED
  May 18 09:45:07.279: INFO: Observed &DaemonSet event: MODIFIED
  May 18 09:45:07.279: INFO: Observed &DaemonSet event: MODIFIED
  May 18 09:45:07.279: INFO: Found daemon set daemon-set in namespace daemonsets-144 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 18 09:45:07.280: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/18/23 09:45:07.28
  STEP: watching for the daemon set status to be patched @ 05/18/23 09:45:07.292
  May 18 09:45:07.295: INFO: Observed &DaemonSet event: ADDED
  May 18 09:45:07.296: INFO: Observed &DaemonSet event: MODIFIED
  May 18 09:45:07.296: INFO: Observed &DaemonSet event: MODIFIED
  May 18 09:45:07.296: INFO: Observed &DaemonSet event: MODIFIED
  May 18 09:45:07.297: INFO: Observed daemon set daemon-set in namespace daemonsets-144 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 18 09:45:07.297: INFO: Observed &DaemonSet event: MODIFIED
  May 18 09:45:07.297: INFO: Found daemon set daemon-set in namespace daemonsets-144 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May 18 09:45:07.297: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/18/23 09:45:07.3
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-144, will wait for the garbage collector to delete the pods @ 05/18/23 09:45:07.301
  May 18 09:45:07.364: INFO: Deleting DaemonSet.extensions daemon-set took: 8.825461ms
  May 18 09:45:07.467: INFO: Terminating DaemonSet.extensions daemon-set pods took: 102.740742ms
  E0518 09:45:08.228209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:09.229108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:10.229219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:45:10.373: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:45:10.374: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 18 09:45:10.381: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1951941"},"items":null}

  May 18 09:45:10.389: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1951942"},"items":null}

  May 18 09:45:10.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-144" for this suite. @ 05/18/23 09:45:10.43
• [5.262 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/18/23 09:45:10.451
  May 18 09:45:10.451: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:45:10.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:45:10.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:45:10.482
  STEP: Setting up server cert @ 05/18/23 09:45:10.577
  E0518 09:45:11.230012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:45:11.328
  STEP: Deploying the webhook pod @ 05/18/23 09:45:11.336
  STEP: Wait for the deployment to be ready @ 05/18/23 09:45:11.35
  May 18 09:45:11.360: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:45:12.231156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:13.232296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:45:13.381
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:45:13.418
  E0518 09:45:14.231874      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:45:14.419: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/18/23 09:45:14.425
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/18/23 09:45:14.459
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/18/23 09:45:14.478
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/18/23 09:45:14.493
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/18/23 09:45:14.506
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/18/23 09:45:14.515
  May 18 09:45:14.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7536" for this suite. @ 05/18/23 09:45:14.596
  STEP: Destroying namespace "webhook-markers-1471" for this suite. @ 05/18/23 09:45:14.603
• [4.166 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/18/23 09:45:14.621
  May 18 09:45:14.621: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename svcaccounts @ 05/18/23 09:45:14.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:45:14.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:45:14.669
  STEP: Creating a pod to test service account token:  @ 05/18/23 09:45:14.674
  E0518 09:45:15.231884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:16.233052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:17.233428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:18.233838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:45:18.696
  May 18 09:45:18.700: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod test-pod-b1038b31-c676-400f-acf6-9f1237c2d902 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 09:45:18.711
  May 18 09:45:18.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3599" for this suite. @ 05/18/23 09:45:18.739
• [4.129 seconds]
------------------------------
S
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/18/23 09:45:18.752
  May 18 09:45:18.752: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:45:18.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:45:18.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:45:18.789
  STEP: creating an Endpoint @ 05/18/23 09:45:18.8
  STEP: waiting for available Endpoint @ 05/18/23 09:45:18.806
  STEP: listing all Endpoints @ 05/18/23 09:45:18.809
  STEP: updating the Endpoint @ 05/18/23 09:45:18.815
  STEP: fetching the Endpoint @ 05/18/23 09:45:18.823
  STEP: patching the Endpoint @ 05/18/23 09:45:18.826
  STEP: fetching the Endpoint @ 05/18/23 09:45:18.834
  STEP: deleting the Endpoint by Collection @ 05/18/23 09:45:18.837
  STEP: waiting for Endpoint deletion @ 05/18/23 09:45:18.845
  STEP: fetching the Endpoint @ 05/18/23 09:45:18.848
  May 18 09:45:18.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3553" for this suite. @ 05/18/23 09:45:18.858
• [0.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/18/23 09:45:18.874
  May 18 09:45:18.875: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:45:18.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:45:18.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:45:18.9
  STEP: Setting up server cert @ 05/18/23 09:45:18.928
  E0518 09:45:19.234693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:45:19.537
  STEP: Deploying the webhook pod @ 05/18/23 09:45:19.545
  STEP: Wait for the deployment to be ready @ 05/18/23 09:45:19.561
  May 18 09:45:19.574: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:45:20.236502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:21.236823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:45:21.586
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:45:21.601
  E0518 09:45:22.236898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:45:22.604: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 18 09:45:22.611: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/18/23 09:45:23.123
  STEP: Creating a custom resource that should be denied by the webhook @ 05/18/23 09:45:23.14
  E0518 09:45:23.237698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:24.237964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/18/23 09:45:25.17
  STEP: Updating the custom resource with disallowed data should be denied @ 05/18/23 09:45:25.193
  STEP: Deleting the custom resource should be denied @ 05/18/23 09:45:25.205
  STEP: Remove the offending key and value from the custom resource data @ 05/18/23 09:45:25.215
  STEP: Deleting the updated custom resource should be successful @ 05/18/23 09:45:25.228
  E0518 09:45:25.238111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:45:25.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6774" for this suite. @ 05/18/23 09:45:25.838
  STEP: Destroying namespace "webhook-markers-2267" for this suite. @ 05/18/23 09:45:25.846
• [6.979 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/18/23 09:45:25.854
  May 18 09:45:25.854: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename gc @ 05/18/23 09:45:25.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:45:25.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:45:25.882
  STEP: create the rc @ 05/18/23 09:45:25.893
  W0518 09:45:25.899628      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0518 09:45:26.240489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:27.241386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:28.242109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:29.242325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:30.242560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:31.244415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/18/23 09:45:31.905
  STEP: wait for the rc to be deleted @ 05/18/23 09:45:31.911
  E0518 09:45:32.245003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:33.245216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:34.245510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:35.253734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:36.250674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/18/23 09:45:36.937
  E0518 09:45:37.262557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:38.253173      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:39.253323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:40.253731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:41.256458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:42.257953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:43.259289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:44.260154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:45.265912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:46.264664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:47.265612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:48.265738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:49.266000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:50.266038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:51.266151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:52.266813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:53.267493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:54.268212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:55.268711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:56.269180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:57.269956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:58.270974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:45:59.271878      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:00.272554      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:01.273627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:02.274662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:03.275475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:04.275401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:05.275351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:06.275555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/18/23 09:46:06.96
  May 18 09:46:07.183: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 18 09:46:07.183: INFO: Deleting pod "simpletest.rc-2482b" in namespace "gc-7060"
  May 18 09:46:07.199: INFO: Deleting pod "simpletest.rc-2d4wz" in namespace "gc-7060"
  May 18 09:46:07.218: INFO: Deleting pod "simpletest.rc-2lsvv" in namespace "gc-7060"
  May 18 09:46:07.233: INFO: Deleting pod "simpletest.rc-46stc" in namespace "gc-7060"
  May 18 09:46:07.248: INFO: Deleting pod "simpletest.rc-4gs2b" in namespace "gc-7060"
  May 18 09:46:07.266: INFO: Deleting pod "simpletest.rc-4vh62" in namespace "gc-7060"
  E0518 09:46:07.307059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:46:07.318: INFO: Deleting pod "simpletest.rc-52n5c" in namespace "gc-7060"
  May 18 09:46:07.332: INFO: Deleting pod "simpletest.rc-55s6h" in namespace "gc-7060"
  May 18 09:46:07.340: INFO: Deleting pod "simpletest.rc-5cnp4" in namespace "gc-7060"
  May 18 09:46:07.349: INFO: Deleting pod "simpletest.rc-6fbrk" in namespace "gc-7060"
  May 18 09:46:07.363: INFO: Deleting pod "simpletest.rc-6jbnz" in namespace "gc-7060"
  May 18 09:46:07.373: INFO: Deleting pod "simpletest.rc-6vj2s" in namespace "gc-7060"
  May 18 09:46:07.418: INFO: Deleting pod "simpletest.rc-748hl" in namespace "gc-7060"
  May 18 09:46:07.445: INFO: Deleting pod "simpletest.rc-77nk6" in namespace "gc-7060"
  May 18 09:46:07.461: INFO: Deleting pod "simpletest.rc-78jrl" in namespace "gc-7060"
  May 18 09:46:07.477: INFO: Deleting pod "simpletest.rc-7q6sh" in namespace "gc-7060"
  May 18 09:46:07.487: INFO: Deleting pod "simpletest.rc-7rbkh" in namespace "gc-7060"
  May 18 09:46:07.498: INFO: Deleting pod "simpletest.rc-88gcj" in namespace "gc-7060"
  May 18 09:46:07.507: INFO: Deleting pod "simpletest.rc-8l66q" in namespace "gc-7060"
  May 18 09:46:07.525: INFO: Deleting pod "simpletest.rc-8lkc5" in namespace "gc-7060"
  May 18 09:46:07.541: INFO: Deleting pod "simpletest.rc-8nld9" in namespace "gc-7060"
  May 18 09:46:07.564: INFO: Deleting pod "simpletest.rc-8pmk7" in namespace "gc-7060"
  May 18 09:46:07.645: INFO: Deleting pod "simpletest.rc-8rkdg" in namespace "gc-7060"
  May 18 09:46:07.682: INFO: Deleting pod "simpletest.rc-cbhrt" in namespace "gc-7060"
  May 18 09:46:07.729: INFO: Deleting pod "simpletest.rc-cllwt" in namespace "gc-7060"
  May 18 09:46:07.742: INFO: Deleting pod "simpletest.rc-cm4vc" in namespace "gc-7060"
  May 18 09:46:07.763: INFO: Deleting pod "simpletest.rc-crltx" in namespace "gc-7060"
  May 18 09:46:07.783: INFO: Deleting pod "simpletest.rc-cvtj2" in namespace "gc-7060"
  May 18 09:46:07.816: INFO: Deleting pod "simpletest.rc-cxb8s" in namespace "gc-7060"
  May 18 09:46:07.834: INFO: Deleting pod "simpletest.rc-cz7p2" in namespace "gc-7060"
  May 18 09:46:07.855: INFO: Deleting pod "simpletest.rc-czv5x" in namespace "gc-7060"
  May 18 09:46:07.871: INFO: Deleting pod "simpletest.rc-d5pvd" in namespace "gc-7060"
  May 18 09:46:07.896: INFO: Deleting pod "simpletest.rc-d9tfs" in namespace "gc-7060"
  May 18 09:46:07.907: INFO: Deleting pod "simpletest.rc-dbt7c" in namespace "gc-7060"
  May 18 09:46:07.930: INFO: Deleting pod "simpletest.rc-dddnf" in namespace "gc-7060"
  May 18 09:46:07.947: INFO: Deleting pod "simpletest.rc-dmp5k" in namespace "gc-7060"
  May 18 09:46:07.967: INFO: Deleting pod "simpletest.rc-dzmhr" in namespace "gc-7060"
  May 18 09:46:07.982: INFO: Deleting pod "simpletest.rc-f7ddl" in namespace "gc-7060"
  May 18 09:46:08.017: INFO: Deleting pod "simpletest.rc-f9fd2" in namespace "gc-7060"
  May 18 09:46:08.036: INFO: Deleting pod "simpletest.rc-fp5pw" in namespace "gc-7060"
  May 18 09:46:08.071: INFO: Deleting pod "simpletest.rc-fsxj6" in namespace "gc-7060"
  May 18 09:46:08.088: INFO: Deleting pod "simpletest.rc-ftkv7" in namespace "gc-7060"
  May 18 09:46:08.096: INFO: Deleting pod "simpletest.rc-fvzg2" in namespace "gc-7060"
  May 18 09:46:08.106: INFO: Deleting pod "simpletest.rc-g64zs" in namespace "gc-7060"
  May 18 09:46:08.116: INFO: Deleting pod "simpletest.rc-gfhz9" in namespace "gc-7060"
  May 18 09:46:08.132: INFO: Deleting pod "simpletest.rc-h8lb7" in namespace "gc-7060"
  May 18 09:46:08.149: INFO: Deleting pod "simpletest.rc-hfvqr" in namespace "gc-7060"
  May 18 09:46:08.160: INFO: Deleting pod "simpletest.rc-hm46c" in namespace "gc-7060"
  May 18 09:46:08.174: INFO: Deleting pod "simpletest.rc-hp748" in namespace "gc-7060"
  May 18 09:46:08.187: INFO: Deleting pod "simpletest.rc-hwn9r" in namespace "gc-7060"
  May 18 09:46:08.197: INFO: Deleting pod "simpletest.rc-j4m8j" in namespace "gc-7060"
  May 18 09:46:08.209: INFO: Deleting pod "simpletest.rc-jbfcb" in namespace "gc-7060"
  May 18 09:46:08.222: INFO: Deleting pod "simpletest.rc-jtqcg" in namespace "gc-7060"
  May 18 09:46:08.230: INFO: Deleting pod "simpletest.rc-kcldt" in namespace "gc-7060"
  May 18 09:46:08.243: INFO: Deleting pod "simpletest.rc-kln4g" in namespace "gc-7060"
  May 18 09:46:08.266: INFO: Deleting pod "simpletest.rc-kszbv" in namespace "gc-7060"
  May 18 09:46:08.281: INFO: Deleting pod "simpletest.rc-l2vwg" in namespace "gc-7060"
  May 18 09:46:08.294: INFO: Deleting pod "simpletest.rc-lctwg" in namespace "gc-7060"
  E0518 09:46:08.297536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:46:08.304: INFO: Deleting pod "simpletest.rc-ltpxf" in namespace "gc-7060"
  May 18 09:46:08.314: INFO: Deleting pod "simpletest.rc-m8bpd" in namespace "gc-7060"
  May 18 09:46:08.329: INFO: Deleting pod "simpletest.rc-mffg5" in namespace "gc-7060"
  May 18 09:46:08.364: INFO: Deleting pod "simpletest.rc-mjqqc" in namespace "gc-7060"
  May 18 09:46:08.374: INFO: Deleting pod "simpletest.rc-mlzb8" in namespace "gc-7060"
  May 18 09:46:08.391: INFO: Deleting pod "simpletest.rc-mnrqq" in namespace "gc-7060"
  May 18 09:46:08.403: INFO: Deleting pod "simpletest.rc-mnzfm" in namespace "gc-7060"
  May 18 09:46:08.411: INFO: Deleting pod "simpletest.rc-n7f25" in namespace "gc-7060"
  May 18 09:46:08.427: INFO: Deleting pod "simpletest.rc-n8nbv" in namespace "gc-7060"
  May 18 09:46:08.451: INFO: Deleting pod "simpletest.rc-ng8bv" in namespace "gc-7060"
  May 18 09:46:08.510: INFO: Deleting pod "simpletest.rc-nskdx" in namespace "gc-7060"
  May 18 09:46:08.527: INFO: Deleting pod "simpletest.rc-nw7qk" in namespace "gc-7060"
  May 18 09:46:08.556: INFO: Deleting pod "simpletest.rc-p26kx" in namespace "gc-7060"
  May 18 09:46:08.574: INFO: Deleting pod "simpletest.rc-p5gnb" in namespace "gc-7060"
  May 18 09:46:08.587: INFO: Deleting pod "simpletest.rc-prnld" in namespace "gc-7060"
  May 18 09:46:08.597: INFO: Deleting pod "simpletest.rc-pswsm" in namespace "gc-7060"
  May 18 09:46:08.615: INFO: Deleting pod "simpletest.rc-pzr9p" in namespace "gc-7060"
  May 18 09:46:08.624: INFO: Deleting pod "simpletest.rc-q2x2r" in namespace "gc-7060"
  May 18 09:46:08.635: INFO: Deleting pod "simpletest.rc-q9z9r" in namespace "gc-7060"
  May 18 09:46:08.658: INFO: Deleting pod "simpletest.rc-qhg8q" in namespace "gc-7060"
  May 18 09:46:08.685: INFO: Deleting pod "simpletest.rc-qn95h" in namespace "gc-7060"
  May 18 09:46:08.715: INFO: Deleting pod "simpletest.rc-qrhx9" in namespace "gc-7060"
  May 18 09:46:08.725: INFO: Deleting pod "simpletest.rc-qtns7" in namespace "gc-7060"
  May 18 09:46:08.745: INFO: Deleting pod "simpletest.rc-r4gtm" in namespace "gc-7060"
  May 18 09:46:08.804: INFO: Deleting pod "simpletest.rc-rl8b6" in namespace "gc-7060"
  May 18 09:46:08.844: INFO: Deleting pod "simpletest.rc-rmcb5" in namespace "gc-7060"
  May 18 09:46:08.894: INFO: Deleting pod "simpletest.rc-rmcvq" in namespace "gc-7060"
  May 18 09:46:08.947: INFO: Deleting pod "simpletest.rc-rwzch" in namespace "gc-7060"
  May 18 09:46:09.004: INFO: Deleting pod "simpletest.rc-sbffn" in namespace "gc-7060"
  May 18 09:46:09.048: INFO: Deleting pod "simpletest.rc-sdp2l" in namespace "gc-7060"
  May 18 09:46:09.101: INFO: Deleting pod "simpletest.rc-v6hjl" in namespace "gc-7060"
  May 18 09:46:09.146: INFO: Deleting pod "simpletest.rc-vqt2b" in namespace "gc-7060"
  May 18 09:46:09.214: INFO: Deleting pod "simpletest.rc-vtb9z" in namespace "gc-7060"
  May 18 09:46:09.256: INFO: Deleting pod "simpletest.rc-vvbll" in namespace "gc-7060"
  May 18 09:46:09.299: INFO: Deleting pod "simpletest.rc-vvbsc" in namespace "gc-7060"
  E0518 09:46:09.301992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:46:09.351: INFO: Deleting pod "simpletest.rc-wjtcl" in namespace "gc-7060"
  May 18 09:46:09.393: INFO: Deleting pod "simpletest.rc-z8v2v" in namespace "gc-7060"
  May 18 09:46:09.446: INFO: Deleting pod "simpletest.rc-zjtpt" in namespace "gc-7060"
  May 18 09:46:09.497: INFO: Deleting pod "simpletest.rc-znhft" in namespace "gc-7060"
  May 18 09:46:09.545: INFO: Deleting pod "simpletest.rc-zrrsz" in namespace "gc-7060"
  May 18 09:46:09.595: INFO: Deleting pod "simpletest.rc-zvpkk" in namespace "gc-7060"
  May 18 09:46:09.650: INFO: Deleting pod "simpletest.rc-zwvt8" in namespace "gc-7060"
  May 18 09:46:09.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7060" for this suite. @ 05/18/23 09:46:09.742
• [43.940 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/18/23 09:46:09.794
  May 18 09:46:09.794: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pod-network-test @ 05/18/23 09:46:09.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:46:09.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:46:09.817
  STEP: Performing setup for networking test in namespace pod-network-test-8093 @ 05/18/23 09:46:09.821
  STEP: creating a selector @ 05/18/23 09:46:09.821
  STEP: Creating the service pods in kubernetes @ 05/18/23 09:46:09.821
  May 18 09:46:09.821: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0518 09:46:10.301700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:11.302434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:12.303534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:13.303879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:14.304366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:15.304897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:16.305034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:17.305550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:18.308716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:19.309082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:20.309977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:21.310657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:22.311065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:23.311022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:24.311278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:25.311919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:26.312712      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:27.313573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:28.314448      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:29.314910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:30.315349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:31.316388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:32.316584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:33.317095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:34.317972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:35.318315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:36.319253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:37.319972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:38.319957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:39.320186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:40.320259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:41.320634      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/18/23 09:46:41.972
  E0518 09:46:42.321221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:43.322248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:46:44.004: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 18 09:46:44.004: INFO: Breadth first check of 172.16.161.119 on host 192.168.190.143...
  May 18 09:46:44.008: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.161.94:9080/dial?request=hostname&protocol=udp&host=172.16.161.119&port=8081&tries=1'] Namespace:pod-network-test-8093 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:46:44.008: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:46:44.010: INFO: ExecWithOptions: Clientset creation
  May 18 09:46:44.011: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8093/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.161.94%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.161.119%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 18 09:46:44.164: INFO: Waiting for responses: map[]
  May 18 09:46:44.164: INFO: reached 172.16.161.119 after 0/1 tries
  May 18 09:46:44.164: INFO: Breadth first check of 172.16.78.143 on host 192.168.190.135...
  May 18 09:46:44.168: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.161.94:9080/dial?request=hostname&protocol=udp&host=172.16.78.143&port=8081&tries=1'] Namespace:pod-network-test-8093 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 09:46:44.168: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 09:46:44.169: INFO: ExecWithOptions: Clientset creation
  May 18 09:46:44.169: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8093/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.161.94%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D172.16.78.143%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 18 09:46:44.273: INFO: Waiting for responses: map[]
  May 18 09:46:44.274: INFO: reached 172.16.78.143 after 0/1 tries
  May 18 09:46:44.274: INFO: Going to retry 0 out of 2 pods....
  May 18 09:46:44.275: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8093" for this suite. @ 05/18/23 09:46:44.281
• [34.493 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/18/23 09:46:44.289
  May 18 09:46:44.289: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:46:44.291
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:46:44.309
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:46:44.313
  E0518 09:46:44.322246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 05/18/23 09:46:44.336
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:46:44.754
  STEP: Deploying the webhook pod @ 05/18/23 09:46:44.761
  STEP: Wait for the deployment to be ready @ 05/18/23 09:46:44.776
  May 18 09:46:44.788: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 09:46:45.322562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:46.323301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:46:46.8
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:46:46.816
  E0518 09:46:47.323588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:46:47.817: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/18/23 09:46:47.824
  STEP: create a namespace for the webhook @ 05/18/23 09:46:47.871
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/18/23 09:46:47.901
  May 18 09:46:47.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4993" for this suite. @ 05/18/23 09:46:48.11
  STEP: Destroying namespace "webhook-markers-5431" for this suite. @ 05/18/23 09:46:48.119
  STEP: Destroying namespace "fail-closed-namespace-6601" for this suite. @ 05/18/23 09:46:48.126
• [3.843 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/18/23 09:46:48.135
  May 18 09:46:48.135: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 09:46:48.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:46:48.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:46:48.16
  STEP: validating cluster-info @ 05/18/23 09:46:48.167
  May 18 09:46:48.168: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-1563 cluster-info'
  May 18 09:46:48.286: INFO: stderr: ""
  May 18 09:46:48.287: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May 18 09:46:48.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1563" for this suite. @ 05/18/23 09:46:48.301
• [0.171 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/18/23 09:46:48.309
  May 18 09:46:48.309: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:46:48.31
  E0518 09:46:48.323936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:46:48.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:46:48.337
  STEP: Creating secret with name secret-test-c8371143-7c15-45c3-9d74-36394174d074 @ 05/18/23 09:46:48.355
  STEP: Creating a pod to test consume secrets @ 05/18/23 09:46:48.36
  E0518 09:46:49.325180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:50.325746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:51.325483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:52.326088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:46:52.387
  May 18 09:46:52.390: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-secrets-7dba005a-57fa-4d94-94a8-149a72b1afa8 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 09:46:52.412
  May 18 09:46:52.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7373" for this suite. @ 05/18/23 09:46:52.443
  STEP: Destroying namespace "secret-namespace-5784" for this suite. @ 05/18/23 09:46:52.459
• [4.160 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/18/23 09:46:52.47
  May 18 09:46:52.471: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename field-validation @ 05/18/23 09:46:52.473
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:46:52.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:46:52.49
  STEP: apply creating a deployment @ 05/18/23 09:46:52.493
  May 18 09:46:52.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5498" for this suite. @ 05/18/23 09:46:52.508
• [0.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/18/23 09:46:52.518
  May 18 09:46:52.518: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:46:52.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:46:52.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:46:52.538
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/18/23 09:46:52.541
  E0518 09:46:53.326296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:54.326537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:55.326584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:56.327041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:46:56.562
  May 18 09:46:56.565: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-f52a72c0-fed8-430b-a073-b9fc0654c3cf container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:46:56.577
  May 18 09:46:56.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-861" for this suite. @ 05/18/23 09:46:56.605
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/18/23 09:46:56.614
  May 18 09:46:56.614: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:46:56.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:46:56.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:46:56.635
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:46:56.639
  E0518 09:46:57.329961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:58.328188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:46:59.328213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:00.328641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:47:00.663
  May 18 09:47:00.667: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-5e51375f-8917-4603-9bf6-e7098b8b3365 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:47:00.673
  May 18 09:47:00.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5996" for this suite. @ 05/18/23 09:47:00.691
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/18/23 09:47:00.701
  May 18 09:47:00.701: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replicaset @ 05/18/23 09:47:00.702
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:47:00.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:47:00.719
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/18/23 09:47:00.724
  E0518 09:47:01.333087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:02.334198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 05/18/23 09:47:02.745
  STEP: Then the orphan pod is adopted @ 05/18/23 09:47:02.753
  E0518 09:47:03.335324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 05/18/23 09:47:03.762
  May 18 09:47:03.766: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/18/23 09:47:03.787
  E0518 09:47:04.335604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:47:04.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7943" for this suite. @ 05/18/23 09:47:04.815
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/18/23 09:47:04.826
  May 18 09:47:04.826: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:47:04.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:47:04.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:47:04.842
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/18/23 09:47:04.845
  E0518 09:47:05.336365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:06.337046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:07.337597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:08.337724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:47:08.867
  May 18 09:47:08.870: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-3ed81f76-8340-4c20-a1e0-42ccfbecc2c1 container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:47:08.877
  May 18 09:47:08.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5782" for this suite. @ 05/18/23 09:47:08.898
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/18/23 09:47:08.925
  May 18 09:47:08.925: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-preemption @ 05/18/23 09:47:08.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:47:08.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:47:08.953
  May 18 09:47:08.970: INFO: Waiting up to 1m0s for all nodes to be ready
  E0518 09:47:09.338404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:10.338605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:11.338893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:12.339072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:13.339278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:14.340386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:15.341335      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:16.341482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:17.342152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:18.342385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:19.342667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:20.343413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:21.344199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:22.344216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:23.344935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:24.345357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:25.345982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:26.346632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:27.346864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:28.347098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:29.347152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:30.347854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:31.349049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:32.349278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:33.350030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:34.350382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:35.351135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:36.351749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:37.352167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:38.352577      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:39.352976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:40.353550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:41.354470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:42.355519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:43.356365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:44.356686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:45.358062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:46.358062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:47.359288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:48.359241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:49.359411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:50.360167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:51.361057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:52.362029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:53.362849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:54.363564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:55.364562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:56.365155      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:57.365247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:58.365453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:47:59.366646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:00.367061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:01.368161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:02.368280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:03.369293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:04.369443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:05.369879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:06.370091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:07.370681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:08.371018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:09.037: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/18/23 09:48:09.046
  May 18 09:48:09.053: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/18/23 09:48:09.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:09.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:09.093
  STEP: Finding an available node @ 05/18/23 09:48:09.099
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/18/23 09:48:09.099
  E0518 09:48:09.371755      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:10.371888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/18/23 09:48:11.119
  May 18 09:48:11.132: INFO: found a healthy node: ck-test-kube-1-27-worker
  E0518 09:48:11.372358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:12.373602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:13.373964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:14.374140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:15.374608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:16.375796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:17.214: INFO: pods created so far: [1 1 1]
  May 18 09:48:17.214: INFO: length of pods created so far: 3
  E0518 09:48:17.376085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:18.376659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:19.227: INFO: pods created so far: [2 2 1]
  E0518 09:48:19.377795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:20.378482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:21.379370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:22.379324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:23.380134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:24.380653      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:25.381562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:26.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:48:26.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-9263" for this suite. @ 05/18/23 09:48:26.36
  STEP: Destroying namespace "sched-preemption-7104" for this suite. @ 05/18/23 09:48:26.368
• [77.453 seconds]
------------------------------
SSSSSS  E0518 09:48:26.381591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
SS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/18/23 09:48:26.383
  May 18 09:48:26.383: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename gc @ 05/18/23 09:48:26.388
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:26.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:26.41
  May 18 09:48:26.460: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"02df246e-e04a-4afc-a7b7-9d83cc572cf3", Controller:(*bool)(0xc0091118fa), BlockOwnerDeletion:(*bool)(0xc0091118fb)}}
  May 18 09:48:26.466: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4fe964c1-7da2-4c71-9116-6aea82dddf15", Controller:(*bool)(0xc009111b7a), BlockOwnerDeletion:(*bool)(0xc009111b7b)}}
  May 18 09:48:26.472: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"da3f2003-ed02-460c-907f-5e61edf7d4b5", Controller:(*bool)(0xc009111d92), BlockOwnerDeletion:(*bool)(0xc009111d93)}}
  E0518 09:48:27.381955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:28.382325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:29.382318      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:30.382449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:31.383595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:31.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1941" for this suite. @ 05/18/23 09:48:31.489
• [5.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/18/23 09:48:31.507
  May 18 09:48:31.507: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:48:31.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:31.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:31.531
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/18/23 09:48:31.535
  E0518 09:48:32.395994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:33.394168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:34.393968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:35.394231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:48:35.564
  May 18 09:48:35.569: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-64bd95a7-6d2b-4855-8faf-9d17eee52cd1 container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:48:35.581
  May 18 09:48:35.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6935" for this suite. @ 05/18/23 09:48:35.615
• [4.118 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/18/23 09:48:35.626
  May 18 09:48:35.626: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 09:48:35.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:35.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:35.654
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/18/23 09:48:35.658
  E0518 09:48:36.394665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:37.395478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:38.396172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:39.396479      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:48:39.695
  May 18 09:48:39.698: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-22d6c7a0-eeea-4c0c-ab93-624da2df7a6f container test-container: <nil>
  STEP: delete the pod @ 05/18/23 09:48:39.706
  May 18 09:48:39.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9415" for this suite. @ 05/18/23 09:48:39.722
• [4.102 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/18/23 09:48:39.729
  May 18 09:48:39.729: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:48:39.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:39.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:39.75
  STEP: Creating configMap with name cm-test-opt-del-57a28239-9473-4070-b568-4aa6f3d89f2e @ 05/18/23 09:48:39.759
  STEP: Creating configMap with name cm-test-opt-upd-82580d40-4e58-4370-8f3f-0fb240cf77dd @ 05/18/23 09:48:39.764
  STEP: Creating the pod @ 05/18/23 09:48:39.768
  E0518 09:48:40.399305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:41.400934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-57a28239-9473-4070-b568-4aa6f3d89f2e @ 05/18/23 09:48:41.826
  STEP: Updating configmap cm-test-opt-upd-82580d40-4e58-4370-8f3f-0fb240cf77dd @ 05/18/23 09:48:41.833
  STEP: Creating configMap with name cm-test-opt-create-9690f558-657c-47a9-9c49-7bf9f103e858 @ 05/18/23 09:48:41.84
  STEP: waiting to observe update in volume @ 05/18/23 09:48:41.846
  E0518 09:48:42.399884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:43.400459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:43.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2602" for this suite. @ 05/18/23 09:48:43.895
• [4.174 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/18/23 09:48:43.906
  May 18 09:48:43.906: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename daemonsets @ 05/18/23 09:48:43.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:43.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:43.93
  May 18 09:48:43.955: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/18/23 09:48:43.963
  May 18 09:48:43.968: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:43.968: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/18/23 09:48:43.968
  May 18 09:48:43.997: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:43.997: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:48:44.400610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:45.004: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:45.004: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:48:45.401148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:46.003: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 18 09:48:46.004: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/18/23 09:48:46.009
  May 18 09:48:46.034: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 18 09:48:46.034: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0518 09:48:46.401505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:47.041: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:47.042: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/18/23 09:48:47.042
  May 18 09:48:47.060: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:47.061: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:48:47.401638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:48.070: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:48.070: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:48:48.402839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:49.065: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:49.066: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:48:49.403395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:50.066: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:50.067: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:48:50.403432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:51.064: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 18 09:48:51.064: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/18/23 09:48:51.07
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5426, will wait for the garbage collector to delete the pods @ 05/18/23 09:48:51.07
  May 18 09:48:51.131: INFO: Deleting DaemonSet.extensions daemon-set took: 6.136474ms
  May 18 09:48:51.231: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.773306ms
  E0518 09:48:51.403450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:52.404117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:53.405053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:53.837: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:48:53.837: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 18 09:48:53.841: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1956323"},"items":null}

  May 18 09:48:53.844: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1956323"},"items":null}

  May 18 09:48:53.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5426" for this suite. @ 05/18/23 09:48:53.882
• [9.985 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/18/23 09:48:53.898
  May 18 09:48:53.898: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 09:48:53.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:53.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:53.921
  STEP: creating pod @ 05/18/23 09:48:53.925
  E0518 09:48:54.405710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:55.405882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:48:55.947: INFO: Pod pod-hostip-116ac3fc-c446-473d-84bb-58d205bfaed1 has hostIP: 192.168.190.143
  May 18 09:48:55.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7870" for this suite. @ 05/18/23 09:48:55.955
• [2.065 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/18/23 09:48:55.968
  May 18 09:48:55.969: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename namespaces @ 05/18/23 09:48:55.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:55.987
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:55.993
  STEP: Read namespace status @ 05/18/23 09:48:55.997
  May 18 09:48:56.003: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/18/23 09:48:56.004
  May 18 09:48:56.009: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/18/23 09:48:56.009
  May 18 09:48:56.021: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May 18 09:48:56.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4889" for this suite. @ 05/18/23 09:48:56.028
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/18/23 09:48:56.045
  May 18 09:48:56.046: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename var-expansion @ 05/18/23 09:48:56.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:48:56.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:48:56.075
  STEP: creating the pod with failed condition @ 05/18/23 09:48:56.079
  E0518 09:48:56.406599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:57.407636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:58.408729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:48:59.409045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:00.409593      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:01.409782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:02.410740      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:03.411133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:04.412362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:05.412645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:06.413603      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:07.413879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:08.414119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:09.414614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:10.415596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:11.415694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:12.416109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:13.416517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:14.417395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:15.417748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:16.418690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:17.419802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:18.419928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:19.420434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:20.421281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:21.421421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:22.422353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:23.422404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:24.422832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:25.423148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:26.423715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:27.424594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:28.425664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:29.426457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:30.427424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:31.427871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:32.428475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:33.428762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:34.429914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:35.430372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:36.431381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:37.432338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:38.433088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:39.433463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:40.433523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:41.433842      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:42.434267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:43.435225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:44.435576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:45.436495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:46.437323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:47.437885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:48.439420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:49.439561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:50.440061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:51.440435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:52.441397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:53.441573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:54.442364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:55.442558      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:56.443665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:57.444800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:58.445372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:49:59.445568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:00.446187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:01.446202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:02.446521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:03.446976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:04.448065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:05.448935      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:06.448917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:07.449899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:08.449933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:09.451191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:10.451222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:11.451495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:12.452099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:13.452451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:14.453098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:15.453906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:16.453809      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:17.454950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:18.455160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:19.455601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:20.455438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:21.457014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:22.457709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:23.457926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:24.458591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:25.458551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:26.459633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:27.459923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:28.460337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:29.460632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:30.461590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:31.461594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:32.462072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:33.462388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:34.462340      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:35.462618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:36.462976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:37.463912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:38.464972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:39.465973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:40.467204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:41.467981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:42.468489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:43.468526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:44.469515      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:45.470319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:46.470325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:47.470504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:48.471265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:49.472075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:50.472020      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:51.472819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:52.473455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:53.473782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:54.473796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:55.474085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 05/18/23 09:50:56.088
  E0518 09:50:56.473997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:50:56.636: INFO: Successfully updated pod "var-expansion-74b4e5cf-11ff-444b-8dd6-036da303604b"
  STEP: waiting for pod running @ 05/18/23 09:50:56.636
  E0518 09:50:57.474399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:50:58.474559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/18/23 09:50:58.648
  May 18 09:50:58.648: INFO: Deleting pod "var-expansion-74b4e5cf-11ff-444b-8dd6-036da303604b" in namespace "var-expansion-1724"
  May 18 09:50:58.665: INFO: Wait up to 5m0s for pod "var-expansion-74b4e5cf-11ff-444b-8dd6-036da303604b" to be fully deleted
  E0518 09:50:59.475525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:00.475743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:01.475871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:02.476053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:03.476979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:04.477085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:05.477540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:06.477588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:07.478583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:08.478664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:09.478771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:10.479093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:11.479141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:12.479355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:13.479557      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:14.480704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:15.479990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:16.480256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:17.481287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:18.481450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:19.482174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:20.482516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:21.482678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:22.483023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:23.485463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:24.484721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:25.484743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:26.485719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:27.489673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:28.486464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:29.487877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:30.488350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:30.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1724" for this suite. @ 05/18/23 09:51:30.785
• [154.748 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/18/23 09:51:30.795
  May 18 09:51:30.795: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 09:51:30.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:51:30.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:51:30.825
  STEP: Creating configMap configmap-2725/configmap-test-25b33666-07b6-43f2-976c-af2234aeff00 @ 05/18/23 09:51:30.83
  STEP: Creating a pod to test consume configMaps @ 05/18/23 09:51:30.836
  E0518 09:51:31.488595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:32.489022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:33.489092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:34.489660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:51:34.867
  May 18 09:51:34.871: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-f816fe94-eb6f-4e85-9046-e4f643f12a8b container env-test: <nil>
  STEP: delete the pod @ 05/18/23 09:51:34.906
  May 18 09:51:34.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2725" for this suite. @ 05/18/23 09:51:34.932
• [4.146 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/18/23 09:51:34.944
  May 18 09:51:34.944: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 09:51:34.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:51:34.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:51:34.964
  STEP: Creating a ResourceQuota @ 05/18/23 09:51:34.968
  STEP: Getting a ResourceQuota @ 05/18/23 09:51:34.973
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/18/23 09:51:34.976
  STEP: Patching the ResourceQuota @ 05/18/23 09:51:34.98
  STEP: Deleting a Collection of ResourceQuotas @ 05/18/23 09:51:34.985
  STEP: Verifying the deleted ResourceQuota @ 05/18/23 09:51:34.993
  May 18 09:51:34.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2596" for this suite. @ 05/18/23 09:51:35
• [0.063 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/18/23 09:51:35.008
  May 18 09:51:35.008: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-runtime @ 05/18/23 09:51:35.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:51:35.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:51:35.031
  STEP: create the container @ 05/18/23 09:51:35.034
  W0518 09:51:35.040980      19 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/18/23 09:51:35.041
  E0518 09:51:35.490198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:36.491152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:37.492911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/18/23 09:51:38.08
  STEP: the container should be terminated @ 05/18/23 09:51:38.085
  STEP: the termination message should be set @ 05/18/23 09:51:38.085
  May 18 09:51:38.085: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/18/23 09:51:38.085
  May 18 09:51:38.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8835" for this suite. @ 05/18/23 09:51:38.101
• [3.097 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/18/23 09:51:38.11
  May 18 09:51:38.110: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 09:51:38.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:51:38.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:51:38.127
  STEP: creating service in namespace services-4280 @ 05/18/23 09:51:38.131
  STEP: creating service affinity-clusterip-transition in namespace services-4280 @ 05/18/23 09:51:38.131
  STEP: creating replication controller affinity-clusterip-transition in namespace services-4280 @ 05/18/23 09:51:38.146
  I0518 09:51:38.154382      19 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-4280, replica count: 3
  E0518 09:51:38.493199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:39.493902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:40.493975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 09:51:41.206282      19 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 09:51:41.215: INFO: Creating new exec pod
  E0518 09:51:41.494939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:42.495280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:43.495267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:44.233: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4280 exec execpod-affinity45hbl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May 18 09:51:44.476: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May 18 09:51:44.476: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:51:44.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4280 exec execpod-affinity45hbl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.70.253 80'
  E0518 09:51:44.495431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:44.681: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.70.253 80\nConnection to 10.104.70.253 80 port [tcp/http] succeeded!\n"
  May 18 09:51:44.681: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 09:51:44.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4280 exec execpod-affinity45hbl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.70.253:80/ ; done'
  May 18 09:51:45.004: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n"
  May 18 09:51:45.005: INFO: stdout: "\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-nd7lc\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-nd7lc\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-nd7lc\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-k428x\naffinity-clusterip-transition-nd7lc\naffinity-clusterip-transition-k428x\naffinity-clusterip-transition-nd7lc\naffinity-clusterip-transition-nd7lc\naffinity-clusterip-transition-k428x\naffinity-clusterip-transition-k428x\naffinity-clusterip-transition-nd7lc"
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-nd7lc
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-nd7lc
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-nd7lc
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-k428x
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-nd7lc
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-k428x
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-nd7lc
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-nd7lc
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-k428x
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-k428x
  May 18 09:51:45.005: INFO: Received response from host: affinity-clusterip-transition-nd7lc
  May 18 09:51:45.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4280 exec execpod-affinity45hbl -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.104.70.253:80/ ; done'
  May 18 09:51:45.260: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.104.70.253:80/\n"
  May 18 09:51:45.260: INFO: stdout: "\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx\naffinity-clusterip-transition-tpkwx"
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Received response from host: affinity-clusterip-transition-tpkwx
  May 18 09:51:45.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:51:45.265: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4280, will wait for the garbage collector to delete the pods @ 05/18/23 09:51:45.28
  May 18 09:51:45.344: INFO: Deleting ReplicationController affinity-clusterip-transition took: 6.772514ms
  May 18 09:51:45.444: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.327521ms
  E0518 09:51:45.495801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:46.496786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:47.497436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4280" for this suite. @ 05/18/23 09:51:47.669
• [9.567 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/18/23 09:51:47.678
  May 18 09:51:47.678: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename daemonsets @ 05/18/23 09:51:47.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:51:47.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:51:47.705
  May 18 09:51:47.724: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/18/23 09:51:47.73
  May 18 09:51:47.736: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:51:47.739: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:51:47.740: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:51:48.497888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:48.746: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:51:48.750: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:51:48.750: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:51:49.497767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:49.746: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:51:49.751: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 09:51:49.751: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/18/23 09:51:49.768
  STEP: Check that daemon pods images are updated. @ 05/18/23 09:51:49.789
  May 18 09:51:49.795: INFO: Wrong image for pod: daemon-set-6fkn5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 18 09:51:49.807: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:51:50.498331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:50.814: INFO: Wrong image for pod: daemon-set-6fkn5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 18 09:51:50.820: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:51:51.498294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:51.814: INFO: Wrong image for pod: daemon-set-6fkn5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 18 09:51:51.820: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:51:52.499039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:52.812: INFO: Wrong image for pod: daemon-set-6fkn5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 18 09:51:52.812: INFO: Pod daemon-set-djnkp is not available
  May 18 09:51:52.817: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:51:53.499636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:53.814: INFO: Wrong image for pod: daemon-set-6fkn5. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 18 09:51:53.814: INFO: Pod daemon-set-djnkp is not available
  May 18 09:51:53.819: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:51:54.499612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:54.818: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0518 09:51:55.499919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:55.817: INFO: Pod daemon-set-xk5l5 is not available
  May 18 09:51:55.822: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/18/23 09:51:55.822
  May 18 09:51:55.827: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:51:55.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 18 09:51:55.831: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:51:56.500683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:56.838: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:51:56.844: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 09:51:56.845: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/18/23 09:51:56.866
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3037, will wait for the garbage collector to delete the pods @ 05/18/23 09:51:56.866
  May 18 09:51:56.927: INFO: Deleting DaemonSet.extensions daemon-set took: 6.094939ms
  May 18 09:51:57.028: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.156903ms
  E0518 09:51:57.502076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:58.502951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:51:59.503269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:51:59.733: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:51:59.733: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 18 09:51:59.737: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1957560"},"items":null}

  May 18 09:51:59.739: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1957560"},"items":null}

  May 18 09:51:59.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3037" for this suite. @ 05/18/23 09:51:59.752
• [12.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/18/23 09:51:59.766
  May 18 09:51:59.766: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:51:59.768
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:51:59.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:51:59.791
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:51:59.796
  E0518 09:52:00.520084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:01.520951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:02.521452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:03.521705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:52:03.832
  May 18 09:52:03.836: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-490e1fa4-b19d-4681-8c2d-0ed0b4ec95fb container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:52:03.85
  May 18 09:52:03.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4353" for this suite. @ 05/18/23 09:52:03.874
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/18/23 09:52:03.886
  May 18 09:52:03.886: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 09:52:03.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:52:03.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:52:03.912
  May 18 09:52:03.919: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:52:04.522210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:05.522985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:06.523061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/18/23 09:52:06.978
  May 18 09:52:06.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-3981 --namespace=crd-publish-openapi-3981 create -f -'
  E0518 09:52:07.524137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:08.524535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:08.821: INFO: stderr: ""
  May 18 09:52:08.821: INFO: stdout: "e2e-test-crd-publish-openapi-867-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 18 09:52:08.821: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-3981 --namespace=crd-publish-openapi-3981 delete e2e-test-crd-publish-openapi-867-crds test-cr'
  May 18 09:52:08.947: INFO: stderr: ""
  May 18 09:52:08.947: INFO: stdout: "e2e-test-crd-publish-openapi-867-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May 18 09:52:08.947: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-3981 --namespace=crd-publish-openapi-3981 apply -f -'
  May 18 09:52:09.457: INFO: stderr: ""
  May 18 09:52:09.457: INFO: stdout: "e2e-test-crd-publish-openapi-867-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 18 09:52:09.457: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-3981 --namespace=crd-publish-openapi-3981 delete e2e-test-crd-publish-openapi-867-crds test-cr'
  E0518 09:52:09.524924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:09.569: INFO: stderr: ""
  May 18 09:52:09.569: INFO: stdout: "e2e-test-crd-publish-openapi-867-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/18/23 09:52:09.569
  May 18 09:52:09.569: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-3981 explain e2e-test-crd-publish-openapi-867-crds'
  May 18 09:52:10.003: INFO: stderr: ""
  May 18 09:52:10.003: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-867-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0518 09:52:10.524968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:11.525992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:11.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3981" for this suite. @ 05/18/23 09:52:11.877
• [7.998 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/18/23 09:52:11.9
  May 18 09:52:11.901: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename daemonsets @ 05/18/23 09:52:11.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:52:11.921
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:52:11.924
  STEP: Creating simple DaemonSet "daemon-set" @ 05/18/23 09:52:11.941
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/18/23 09:52:11.946
  May 18 09:52:11.951: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:52:11.955: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:52:11.957: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:52:12.526675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:12.966: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:52:12.970: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 09:52:12.970: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 09:52:13.526829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:13.962: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 09:52:13.966: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 09:52:13.967: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/18/23 09:52:13.97
  STEP: DeleteCollection of the DaemonSets @ 05/18/23 09:52:13.975
  STEP: Verify that ReplicaSets have been deleted @ 05/18/23 09:52:13.982
  May 18 09:52:14.007: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1957737"},"items":null}

  May 18 09:52:14.019: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1957737"},"items":[{"metadata":{"name":"daemon-set-744l6","generateName":"daemon-set-","namespace":"daemonsets-1209","uid":"8ec4c801-b162-4ad4-b380-09c811b6baaf","resourceVersion":"1957736","creationTimestamp":"2023-05-18T09:52:11Z","deletionTimestamp":"2023-05-18T09:52:43Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"593762c35cd4c55fd954dd1e8c3fa897503219004cb9e3cffb3b42b600e2130e","cni.projectcalico.org/podIP":"172.16.161.71/32","cni.projectcalico.org/podIPs":"172.16.161.71/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"15bf4e63-f918-41aa-8297-6791cc9b2c9a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-18T09:52:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15bf4e63-f918-41aa-8297-6791cc9b2c9a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-18T09:52:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-18T09:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-g6dq4","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-g6dq4","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ck-test-kube-1-27-worker","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ck-test-kube-1-27-worker"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-18T09:52:11Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-18T09:52:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-18T09:52:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-18T09:52:11Z"}],"hostIP":"192.168.190.143","podIP":"172.16.161.71","podIPs":[{"ip":"172.16.161.71"}],"startTime":"2023-05-18T09:52:11Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-18T09:52:12Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://bd669a1874f89874eac48ff87cfa517afaf4183ad4fdc46f427b16602bf2d41d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-fn85h","generateName":"daemon-set-","namespace":"daemonsets-1209","uid":"9a33cf77-8bc8-428b-a8d9-e81a37ff72dc","resourceVersion":"1957737","creationTimestamp":"2023-05-18T09:52:11Z","deletionTimestamp":"2023-05-18T09:52:43Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"e3cd65a91caf674aeaefee2017b651c98e8a01654a9aeea928c9677313c805ea","cni.projectcalico.org/podIP":"172.16.78.147/32","cni.projectcalico.org/podIPs":"172.16.78.147/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"15bf4e63-f918-41aa-8297-6791cc9b2c9a","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-18T09:52:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"15bf4e63-f918-41aa-8297-6791cc9b2c9a\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-18T09:52:12Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-18T09:52:13Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.78.147\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-9t5bc","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-9t5bc","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ck-test-kube-1-27-worker-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ck-test-kube-1-27-worker-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-18T09:52:11Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-18T09:52:13Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-18T09:52:13Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-18T09:52:11Z"}],"hostIP":"192.168.190.135","podIP":"172.16.78.147","podIPs":[{"ip":"172.16.78.147"}],"startTime":"2023-05-18T09:52:11Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-18T09:52:13Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0e5922ac7659ca7b1f007588382c2d958d037a0cd7161b84da1982be51d1cfb5","started":true}],"qosClass":"BestEffort"}}]}

  May 18 09:52:14.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1209" for this suite. @ 05/18/23 09:52:14.044
• [2.151 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/18/23 09:52:14.054
  May 18 09:52:14.054: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename certificates @ 05/18/23 09:52:14.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:52:14.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:52:14.074
  E0518 09:52:14.527701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 05/18/23 09:52:14.902
  STEP: getting /apis/certificates.k8s.io @ 05/18/23 09:52:14.913
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/18/23 09:52:14.917
  STEP: creating @ 05/18/23 09:52:14.919
  STEP: getting @ 05/18/23 09:52:14.944
  STEP: listing @ 05/18/23 09:52:14.948
  STEP: watching @ 05/18/23 09:52:14.951
  May 18 09:52:14.951: INFO: starting watch
  STEP: patching @ 05/18/23 09:52:14.953
  STEP: updating @ 05/18/23 09:52:14.962
  May 18 09:52:14.968: INFO: waiting for watch events with expected annotations
  May 18 09:52:14.968: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/18/23 09:52:14.968
  STEP: patching /approval @ 05/18/23 09:52:14.971
  STEP: updating /approval @ 05/18/23 09:52:14.977
  STEP: getting /status @ 05/18/23 09:52:14.983
  STEP: patching /status @ 05/18/23 09:52:14.986
  STEP: updating /status @ 05/18/23 09:52:14.994
  STEP: deleting @ 05/18/23 09:52:15.002
  STEP: deleting a collection @ 05/18/23 09:52:15.013
  May 18 09:52:15.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-2157" for this suite. @ 05/18/23 09:52:15.029
• [0.983 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/18/23 09:52:15.038
  May 18 09:52:15.039: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename var-expansion @ 05/18/23 09:52:15.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:52:15.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:52:15.063
  E0518 09:52:15.528931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:16.529088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:17.082: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 09:52:17.090: INFO: Deleting pod "var-expansion-3384513c-22ef-4319-9e4f-2919cc161367" in namespace "var-expansion-5059"
  May 18 09:52:17.104: INFO: Wait up to 5m0s for pod "var-expansion-3384513c-22ef-4319-9e4f-2919cc161367" to be fully deleted
  E0518 09:52:17.529536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:18.529875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:19.530030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:20.530129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-5059" for this suite. @ 05/18/23 09:52:21.119
• [6.087 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/18/23 09:52:21.128
  May 18 09:52:21.128: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:52:21.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:52:21.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:52:21.153
  STEP: Creating configMap with name projected-configmap-test-volume-c15977ca-d63b-4b49-953f-c3be16b99afd @ 05/18/23 09:52:21.158
  STEP: Creating a pod to test consume configMaps @ 05/18/23 09:52:21.165
  E0518 09:52:21.530969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:22.532257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:23.532900      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:24.533162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:52:25.195
  May 18 09:52:25.199: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-configmaps-fbc39388-fd7b-4092-ae17-e75a11a4dc10 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 09:52:25.219
  May 18 09:52:25.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1727" for this suite. @ 05/18/23 09:52:25.246
• [4.127 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/18/23 09:52:25.259
  May 18 09:52:25.260: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename disruption @ 05/18/23 09:52:25.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:52:25.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:52:25.291
  STEP: Waiting for the pdb to be processed @ 05/18/23 09:52:25.314
  E0518 09:52:25.533241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:26.534345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 05/18/23 09:52:27.354
  May 18 09:52:27.371: INFO: running pods: 0 < 3
  E0518 09:52:27.534511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:28.534873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:29.378: INFO: running pods: 1 < 3
  E0518 09:52:29.535753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:30.536451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:31.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8025" for this suite. @ 05/18/23 09:52:31.387
• [6.136 seconds]
------------------------------
SS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/18/23 09:52:31.395
  May 18 09:52:31.395: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename events @ 05/18/23 09:52:31.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:52:31.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:52:31.422
  STEP: creating a test event @ 05/18/23 09:52:31.427
  STEP: listing events in all namespaces @ 05/18/23 09:52:31.436
  STEP: listing events in test namespace @ 05/18/23 09:52:31.442
  STEP: listing events with field selection filtering on source @ 05/18/23 09:52:31.446
  STEP: listing events with field selection filtering on reportingController @ 05/18/23 09:52:31.449
  STEP: getting the test event @ 05/18/23 09:52:31.452
  STEP: patching the test event @ 05/18/23 09:52:31.456
  STEP: getting the test event @ 05/18/23 09:52:31.465
  STEP: updating the test event @ 05/18/23 09:52:31.47
  STEP: getting the test event @ 05/18/23 09:52:31.476
  STEP: deleting the test event @ 05/18/23 09:52:31.48
  STEP: listing events in all namespaces @ 05/18/23 09:52:31.487
  STEP: listing events in test namespace @ 05/18/23 09:52:31.493
  May 18 09:52:31.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9166" for this suite. @ 05/18/23 09:52:31.502
• [0.114 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/18/23 09:52:31.513
  May 18 09:52:31.513: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 09:52:31.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:52:31.534
  E0518 09:52:31.536937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:52:31.538
  STEP: Creating pod busybox-69efe718-17ee-4a6b-aa26-6f7edbff5632 in namespace container-probe-7329 @ 05/18/23 09:52:31.543
  E0518 09:52:32.536720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:33.537150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:52:33.560: INFO: Started pod busybox-69efe718-17ee-4a6b-aa26-6f7edbff5632 in namespace container-probe-7329
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/18/23 09:52:33.561
  May 18 09:52:33.567: INFO: Initial restart count of pod busybox-69efe718-17ee-4a6b-aa26-6f7edbff5632 is 0
  E0518 09:52:34.538651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:35.538771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:36.539387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:37.540628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:38.541201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:39.541255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:40.542219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:41.542994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:42.544420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:43.543010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:44.546532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:45.545875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:46.545986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:47.547093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:48.547831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:49.548076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:50.548109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:51.548454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:52.548645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:53.548769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:54.549025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:55.549278      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:56.549456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:57.550487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:58.550671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:52:59.550848      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:00.551112      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:01.551305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:02.551503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:03.551660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:04.551962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:05.552143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:06.552255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:07.552602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:08.552688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:09.553097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:10.553348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:11.553723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:12.554133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:13.554390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:14.555398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:15.555496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:16.555717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:17.556687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:18.556993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:19.557122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:20.557338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:21.557509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:22.557827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:23.558035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:53:23.745: INFO: Restart count of pod container-probe-7329/busybox-69efe718-17ee-4a6b-aa26-6f7edbff5632 is now 1 (50.178456636s elapsed)
  May 18 09:53:23.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 09:53:23.753
  STEP: Destroying namespace "container-probe-7329" for this suite. @ 05/18/23 09:53:23.769
• [52.268 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/18/23 09:53:23.783
  May 18 09:53:23.783: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 09:53:23.785
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:53:23.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:53:23.816
  STEP: Setting up server cert @ 05/18/23 09:53:23.842
  E0518 09:53:24.558847      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 09:53:24.577
  STEP: Deploying the webhook pod @ 05/18/23 09:53:24.586
  STEP: Wait for the deployment to be ready @ 05/18/23 09:53:24.598
  May 18 09:53:24.603: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0518 09:53:25.560152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:26.561948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 09:53:26.62
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 09:53:26.641
  E0518 09:53:27.562079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:53:27.642: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 18 09:53:27.646: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9613-crds.webhook.example.com via the AdmissionRegistration API @ 05/18/23 09:53:28.16
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/18/23 09:53:28.196
  E0518 09:53:28.562450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:29.563604      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:53:30.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0518 09:53:30.564592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-1280" for this suite. @ 05/18/23 09:53:30.845
  STEP: Destroying namespace "webhook-markers-4327" for this suite. @ 05/18/23 09:53:30.85
• [7.073 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/18/23 09:53:30.857
  May 18 09:53:30.857: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename svcaccounts @ 05/18/23 09:53:30.858
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:53:30.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:53:30.885
  May 18 09:53:30.891: INFO: Got root ca configmap in namespace "svcaccounts-7020"
  May 18 09:53:30.895: INFO: Deleted root ca configmap in namespace "svcaccounts-7020"
  STEP: waiting for a new root ca configmap created @ 05/18/23 09:53:31.396
  May 18 09:53:31.402: INFO: Recreated root ca configmap in namespace "svcaccounts-7020"
  May 18 09:53:31.407: INFO: Updated root ca configmap in namespace "svcaccounts-7020"
  E0518 09:53:31.565084      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for the root ca configmap reconciled @ 05/18/23 09:53:31.909
  May 18 09:53:31.913: INFO: Reconciled root ca configmap in namespace "svcaccounts-7020"
  May 18 09:53:31.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7020" for this suite. @ 05/18/23 09:53:31.918
• [1.078 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/18/23 09:53:31.935
  May 18 09:53:31.935: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:53:31.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:53:31.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:53:31.967
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:53:31.972
  E0518 09:53:32.565069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:33.566494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:34.565735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:35.566561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:53:36.001
  May 18 09:53:36.005: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-6b065016-9fce-476e-a69c-347fe6ea9467 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:53:36.015
  May 18 09:53:36.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3729" for this suite. @ 05/18/23 09:53:36.045
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/18/23 09:53:36.103
  May 18 09:53:36.103: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 09:53:36.105
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:53:36.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:53:36.134
  STEP: Creating secret with name s-test-opt-del-66665cf6-f5aa-4777-aeb0-a7a07d4ded18 @ 05/18/23 09:53:36.142
  STEP: Creating secret with name s-test-opt-upd-e0bd6404-aaae-4bbf-96da-5d8ef6fa2451 @ 05/18/23 09:53:36.148
  STEP: Creating the pod @ 05/18/23 09:53:36.154
  E0518 09:53:36.566927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:37.567456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:38.567877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:39.569012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-66665cf6-f5aa-4777-aeb0-a7a07d4ded18 @ 05/18/23 09:53:40.203
  STEP: Updating secret s-test-opt-upd-e0bd6404-aaae-4bbf-96da-5d8ef6fa2451 @ 05/18/23 09:53:40.21
  STEP: Creating secret with name s-test-opt-create-f380609a-d97b-4300-9043-0dc214a51b80 @ 05/18/23 09:53:40.216
  STEP: waiting to observe update in volume @ 05/18/23 09:53:40.22
  E0518 09:53:40.569354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:41.569792      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:42.570422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:43.570766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:44.570843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:45.571315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:46.572267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:47.573050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:48.573217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:49.573748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:50.573834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:51.574621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:52.578684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:53.575646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:54.576308      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:55.576646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:56.577429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:57.577527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:58.577643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:53:59.581205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:00.581514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:01.581714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:02.582583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:03.582493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:04.582797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:05.583242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:06.583883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:07.584293      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:08.585011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:09.586096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:10.586169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:11.587000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:12.587118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:13.588295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:14.588136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:15.589142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:16.589985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:17.590365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:18.592302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:19.592503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:20.593449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:21.593508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:22.593831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:23.594180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:24.594745      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:25.595691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:26.595791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:27.596317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:28.596763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:29.597238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:30.597951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:31.598429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:32.599579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:33.600075      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:34.601237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:35.602011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:36.601925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:37.602177      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:38.602637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:39.602889      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:40.602944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:41.603146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:42.603358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:43.603513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:44.603752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:45.603987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:46.603989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:47.604385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:48.605103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:49.606117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:50.605783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:51.606270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:52.606919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:53.606861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:54.607834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:55.607232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:56.607345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:57.607916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:58.608478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:54:59.608913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:00.609103      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:01.609323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:02.609901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:03.610463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:04.611454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:55:04.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7345" for this suite. @ 05/18/23 09:55:04.773
• [88.681 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/18/23 09:55:04.785
  May 18 09:55:04.785: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename ingress @ 05/18/23 09:55:04.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:55:04.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:55:04.821
  STEP: getting /apis @ 05/18/23 09:55:04.827
  STEP: getting /apis/networking.k8s.io @ 05/18/23 09:55:04.839
  STEP: getting /apis/networking.k8s.iov1 @ 05/18/23 09:55:04.841
  STEP: creating @ 05/18/23 09:55:04.843
  STEP: getting @ 05/18/23 09:55:04.857
  STEP: listing @ 05/18/23 09:55:04.86
  STEP: watching @ 05/18/23 09:55:04.863
  May 18 09:55:04.863: INFO: starting watch
  STEP: cluster-wide listing @ 05/18/23 09:55:04.866
  STEP: cluster-wide watching @ 05/18/23 09:55:04.869
  May 18 09:55:04.869: INFO: starting watch
  STEP: patching @ 05/18/23 09:55:04.871
  STEP: updating @ 05/18/23 09:55:04.877
  May 18 09:55:04.890: INFO: waiting for watch events with expected annotations
  May 18 09:55:04.891: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/18/23 09:55:04.892
  STEP: updating /status @ 05/18/23 09:55:04.898
  STEP: get /status @ 05/18/23 09:55:04.906
  STEP: deleting @ 05/18/23 09:55:04.909
  STEP: deleting a collection @ 05/18/23 09:55:04.919
  May 18 09:55:04.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-1915" for this suite. @ 05/18/23 09:55:04.935
• [0.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/18/23 09:55:04.96
  May 18 09:55:04.960: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 09:55:04.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:55:04.979
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:55:04.985
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 09:55:04.99
  E0518 09:55:05.611708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:06.612174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:07.612211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:08.612991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:55:09.022
  May 18 09:55:09.027: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-d5f574ed-838a-4b6a-9bb6-a43ca3291a8a container client-container: <nil>
  STEP: delete the pod @ 05/18/23 09:55:09.039
  May 18 09:55:09.055: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2916" for this suite. @ 05/18/23 09:55:09.068
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/18/23 09:55:09.089
  May 18 09:55:09.089: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 09:55:09.094
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:55:09.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:55:09.121
  STEP: Creating secret with name s-test-opt-del-47f01697-b0ee-4b3d-b663-d25c46bb6bbe @ 05/18/23 09:55:09.133
  STEP: Creating secret with name s-test-opt-upd-ec21a510-e9b7-4d77-854b-7de3509ffed3 @ 05/18/23 09:55:09.141
  STEP: Creating the pod @ 05/18/23 09:55:09.149
  E0518 09:55:09.613148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:10.613336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:11.613685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:12.613816      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-47f01697-b0ee-4b3d-b663-d25c46bb6bbe @ 05/18/23 09:55:13.199
  STEP: Updating secret s-test-opt-upd-ec21a510-e9b7-4d77-854b-7de3509ffed3 @ 05/18/23 09:55:13.203
  STEP: Creating secret with name s-test-opt-create-343c197d-0515-4f3f-9e17-057b6ec29b95 @ 05/18/23 09:55:13.21
  STEP: waiting to observe update in volume @ 05/18/23 09:55:13.215
  E0518 09:55:13.614544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:14.615458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:15.615785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:16.616428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:17.616698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:18.617258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:19.618444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:20.618582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:21.619416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:22.619562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:23.620759      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:24.621298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:25.621646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:26.622056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:27.623220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:28.623619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:29.624721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:30.625229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:31.625737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:32.625970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:33.626995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:34.627734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:35.628026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:36.629660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:37.630175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:38.632408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:39.630717      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:40.630945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:41.631131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:42.631384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:43.632321      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:44.632649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:45.633481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:46.633633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:47.634730      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:48.634681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:49.634651      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:50.634785      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:51.635578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:52.636316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:53.637234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:54.637663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:55.638532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:56.638955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:57.639475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:58.639469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:55:59.640478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:00.640718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:01.641115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:02.641123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:03.641770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:04.642054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:05.642620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:06.642901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:07.643749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:08.645765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:09.644434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:10.645387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:11.645688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:12.646132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:13.647021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:14.646952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:15.647757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:16.648760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:17.648585      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:18.648992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:19.649298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:20.650297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:21.650892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:22.651138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:23.651257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:24.651700      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:25.651754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:26.651897      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:27.652034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:28.652253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:29.652464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:30.652679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:31.653521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:32.653696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:33.654228      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:34.654397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:35.654865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:36.654946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:37.655432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:38.655719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:39.656000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:40.656140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:41.656302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:42.656329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:43.656715      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:56:43.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8082" for this suite. @ 05/18/23 09:56:43.89
• [94.808 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/18/23 09:56:43.907
  May 18 09:56:43.907: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-runtime @ 05/18/23 09:56:43.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:56:43.936
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:56:43.941
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/18/23 09:56:43.957
  E0518 09:56:44.657158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:45.658136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:46.658172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:47.658596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:48.658589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:49.660994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:50.661134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:51.661476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:52.661450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:53.662337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:54.662744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:55.663012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:56.663006      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:57.663482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:58.663616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:56:59.664783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/18/23 09:57:00.049
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/18/23 09:57:00.051
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/18/23 09:57:00.056
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/18/23 09:57:00.056
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/18/23 09:57:00.079
  E0518 09:57:00.665035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:01.665791      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:02.665992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/18/23 09:57:03.098
  E0518 09:57:03.667047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:04.667197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/18/23 09:57:05.12
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/18/23 09:57:05.128
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/18/23 09:57:05.128
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/18/23 09:57:05.157
  E0518 09:57:05.667325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/18/23 09:57:06.165
  E0518 09:57:06.668189      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:07.669425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:08.670122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/18/23 09:57:09.184
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/18/23 09:57:09.194
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/18/23 09:57:09.194
  May 18 09:57:09.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8008" for this suite. @ 05/18/23 09:57:09.229
• [25.329 seconds]
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/18/23 09:57:09.237
  May 18 09:57:09.237: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename security-context-test @ 05/18/23 09:57:09.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:57:09.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:57:09.261
  E0518 09:57:09.671032      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:10.671800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:11.672767      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:12.673265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:57:13.310: INFO: Got logs for pod "busybox-privileged-false-99908ad0-7220-4ff8-bf7e-a799ce6434a1": "ip: RTNETLINK answers: Operation not permitted\n"
  May 18 09:57:13.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4213" for this suite. @ 05/18/23 09:57:13.318
• [4.089 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/18/23 09:57:13.33
  May 18 09:57:13.330: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename svcaccounts @ 05/18/23 09:57:13.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:57:13.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:57:13.355
  May 18 09:57:13.372: INFO: created pod
  E0518 09:57:13.673466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:14.674434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:15.674540      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:16.675169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 09:57:17.394
  E0518 09:57:17.675753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:18.675928      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:19.676940      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:20.677126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:21.677831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:22.678624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:23.679317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:24.680122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:25.680620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:26.681309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:27.682043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:28.682455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:29.682830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:30.683307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:31.684046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:32.684718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:33.685065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:34.685645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:35.685916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:36.687348      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:37.686532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:38.686794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:39.687282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:40.688105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:41.688472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:42.688632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:43.688933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:44.689051      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:45.689298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:46.689942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:57:47.395: INFO: polling logs
  May 18 09:57:47.409: INFO: Pod logs: 
  I0518 09:57:14.322148       1 log.go:198] OK: Got token
  I0518 09:57:14.322343       1 log.go:198] validating with in-cluster discovery
  I0518 09:57:14.323505       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0518 09:57:14.323620       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4947:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684404433, NotBefore:1684403833, IssuedAt:1684403833, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4947", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ff1556f9-a811-44a2-93f4-ac2316334a5f"}}}
  I0518 09:57:14.339647       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0518 09:57:14.347632       1 log.go:198] OK: Validated signature on JWT
  I0518 09:57:14.347809       1 log.go:198] OK: Got valid claims from token!
  I0518 09:57:14.347867       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4947:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1684404433, NotBefore:1684403833, IssuedAt:1684403833, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4947", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ff1556f9-a811-44a2-93f4-ac2316334a5f"}}}

  May 18 09:57:47.410: INFO: completed pod
  May 18 09:57:47.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4947" for this suite. @ 05/18/23 09:57:47.425
• [34.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/18/23 09:57:47.438
  May 18 09:57:47.438: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/18/23 09:57:47.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:57:47.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:57:47.47
  May 18 09:57:47.476: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 09:57:47.690261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:57:48.514: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7680" for this suite. @ 05/18/23 09:57:48.521
• [1.091 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/18/23 09:57:48.534
  May 18 09:57:48.534: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename taint-single-pod @ 05/18/23 09:57:48.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 09:57:48.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 09:57:48.563
  May 18 09:57:48.569: INFO: Waiting up to 1m0s for all nodes to be ready
  E0518 09:57:48.690731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:49.691727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:50.692074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:51.692271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:52.692649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:53.693247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:54.693860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:55.695145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:56.695724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:57.696244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:58.697136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:57:59.698202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:00.698279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:01.698462      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:02.699444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:03.699725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:04.700350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:05.701209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:06.702146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:07.702560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:08.702733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:09.703260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:10.704003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:11.704549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:12.707261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:13.706331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:14.706720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:15.707499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:16.707591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:17.708701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:18.710732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:19.711599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:20.712524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:21.712822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:22.713960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:23.714281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:24.715164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:25.715292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:26.715699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:27.716082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:28.716372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:29.717269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:30.718038      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:31.719131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:32.719299      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:33.719580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:34.720660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:35.721519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:36.722337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:37.722761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:38.723643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:39.724600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:40.725875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:41.726999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:42.726976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:43.727062      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:44.727742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:45.727846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:46.729080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:47.730373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 09:58:48.642: INFO: Waiting for terminating namespaces to be deleted...
  May 18 09:58:48.649: INFO: Starting informer...
  STEP: Starting pod... @ 05/18/23 09:58:48.65
  May 18 09:58:48.670: INFO: Pod is running on ck-test-kube-1-27-worker. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/18/23 09:58:48.672
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/18/23 09:58:48.695
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/18/23 09:58:48.7
  May 18 09:58:48.700: INFO: Pod wasn't evicted. Proceeding
  May 18 09:58:48.700: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/18/23 09:58:48.714
  STEP: Waiting some time to make sure that toleration time passed. @ 05/18/23 09:58:48.721
  E0518 09:58:48.730201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:49.731894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:50.732970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:51.733964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:52.734646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:53.734786      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:54.735030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:55.735289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:56.736243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:57.736460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:58.736630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:58:59.736720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:00.737063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:01.737637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:02.737903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:03.738054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:04.738298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:05.738567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:06.739391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:07.739720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:08.739674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:09.739811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:10.739996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:11.740952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:12.741168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:13.741414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:14.741591      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:15.741817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:16.742054      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:17.742338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:18.746182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:19.745210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:20.745706      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:21.745849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:22.746468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:23.746333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:24.747055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:25.747106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:26.747093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:27.747442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:28.747513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:29.747690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:30.747810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:31.748954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:32.749388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:33.749550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:34.749756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:35.750025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:36.750191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:37.750350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:38.750418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:39.750764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:40.750961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:41.751556      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:42.751564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:43.751796      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:44.752035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:45.752311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:46.753471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:47.753775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:48.753974      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:49.755079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:50.755395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:51.756153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:52.756130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:53.756209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:54.756379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:55.756495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:56.757307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:57.757681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:58.757879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 09:59:59.757981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:00.758195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:01.758798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:02.759331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:00:03.722: INFO: Pod wasn't evicted. Test successful
  May 18 10:00:03.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-8048" for this suite. @ 05/18/23 10:00:03.731
• [135.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/18/23 10:00:03.758
  May 18 10:00:03.758: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 10:00:03.761
  E0518 10:00:03.762611      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:00:03.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:00:03.782
  STEP: Creating secret with name secret-test-fdfe8fa3-6da7-4621-9ec3-ce21d9421f8d @ 05/18/23 10:00:03.785
  STEP: Creating a pod to test consume secrets @ 05/18/23 10:00:03.79
  E0518 10:00:04.763705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:05.763676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:06.764123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:07.764559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:00:07.814
  May 18 10:00:07.818: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-secrets-1ae95a42-a2d3-4e57-bd7f-95c1df2523ee container secret-env-test: <nil>
  STEP: delete the pod @ 05/18/23 10:00:07.862
  May 18 10:00:07.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7698" for this suite. @ 05/18/23 10:00:07.896
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/18/23 10:00:07.91
  May 18 10:00:07.910: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 10:00:07.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:00:07.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:00:07.97
  E0518 10:00:08.764637      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:09.774815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:10.766567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:11.766864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:12.768037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:13.768106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:14.768222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:15.768621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:16.769367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:17.769551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:18.770686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:19.771559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:20.771894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:21.772117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:22.772129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:23.772735      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:24.772521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:25.772705      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:26.772996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:27.773352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:28.773574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:29.773648      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:00:30.092: INFO: Container started at 2023-05-18 10:00:08 +0000 UTC, pod became ready at 2023-05-18 10:00:28 +0000 UTC
  May 18 10:00:30.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1896" for this suite. @ 05/18/23 10:00:30.098
• [22.198 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/18/23 10:00:30.115
  May 18 10:00:30.116: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replicaset @ 05/18/23 10:00:30.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:00:30.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:00:30.141
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/18/23 10:00:30.147
  May 18 10:00:30.161: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0518 10:00:30.774242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:31.774398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:32.775296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:33.775918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:34.776292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:00:35.168: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/18/23 10:00:35.168
  STEP: getting scale subresource @ 05/18/23 10:00:35.169
  STEP: updating a scale subresource @ 05/18/23 10:00:35.174
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/18/23 10:00:35.181
  STEP: Patch a scale subresource @ 05/18/23 10:00:35.185
  May 18 10:00:35.198: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-433" for this suite. @ 05/18/23 10:00:35.207
• [5.102 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/18/23 10:00:35.226
  May 18 10:00:35.226: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename namespaces @ 05/18/23 10:00:35.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:00:35.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:00:35.255
  STEP: creating a Namespace @ 05/18/23 10:00:35.26
  STEP: patching the Namespace @ 05/18/23 10:00:35.276
  STEP: get the Namespace and ensuring it has the label @ 05/18/23 10:00:35.282
  May 18 10:00:35.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-290" for this suite. @ 05/18/23 10:00:35.307
  STEP: Destroying namespace "nspatchtest-7d7b0fa5-3c37-49fb-acba-d3da1d222137-3629" for this suite. @ 05/18/23 10:00:35.317
• [0.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/18/23 10:00:35.346
  May 18 10:00:35.346: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pod-network-test @ 05/18/23 10:00:35.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:00:35.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:00:35.373
  STEP: Performing setup for networking test in namespace pod-network-test-3505 @ 05/18/23 10:00:35.377
  STEP: creating a selector @ 05/18/23 10:00:35.377
  STEP: Creating the service pods in kubernetes @ 05/18/23 10:00:35.377
  May 18 10:00:35.377: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0518 10:00:35.777369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:36.779207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:37.779826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:38.779804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:39.780273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:40.780823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:41.781369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:42.782088      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:43.782925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:44.783086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:45.783193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:46.784325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:47.785421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:48.786283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:49.786684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:50.786997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:51.787937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:52.788120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:53.788757      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:54.789035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:55.789922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:56.790381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/18/23 10:00:57.489
  E0518 10:00:57.790854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:00:58.791105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:00:59.530: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 18 10:00:59.530: INFO: Going to poll 172.16.161.88 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May 18 10:00:59.541: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.161.88:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:00:59.541: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:00:59.544: INFO: ExecWithOptions: Clientset creation
  May 18 10:00:59.549: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.161.88%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 18 10:00:59.673: INFO: Found all 1 expected endpoints: [netserver-0]
  May 18 10:00:59.673: INFO: Going to poll 172.16.78.148 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May 18 10:00:59.677: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://172.16.78.148:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-3505 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:00:59.677: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:00:59.678: INFO: ExecWithOptions: Clientset creation
  May 18 10:00:59.678: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-3505/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F172.16.78.148%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 18 10:00:59.785: INFO: Found all 1 expected endpoints: [netserver-1]
  May 18 10:00:59.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0518 10:00:59.790927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "pod-network-test-3505" for this suite. @ 05/18/23 10:00:59.793
• [24.456 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/18/23 10:00:59.806
  May 18 10:00:59.806: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename endpointslice @ 05/18/23 10:00:59.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:00:59.833
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:00:59.837
  May 18 10:00:59.847: INFO: Endpoints addresses: [192.168.190.140] , ports: [6443]
  May 18 10:00:59.847: INFO: EndpointSlices addresses: [192.168.190.140] , ports: [6443]
  May 18 10:00:59.847: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3685" for this suite. @ 05/18/23 10:00:59.851
• [0.052 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/18/23 10:00:59.859
  May 18 10:00:59.859: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 10:00:59.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:00:59.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:00:59.88
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/18/23 10:00:59.883
  May 18 10:00:59.885: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 10:01:00.791615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:01.791858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:02.686: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 10:01:02.792469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:03.793015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:04.793921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:05.793782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:06.794625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:07.795397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:08.796374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:09.797243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:10.797839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:11.798268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:11.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2705" for this suite. @ 05/18/23 10:01:11.978
• [12.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/18/23 10:01:12.021
  May 18 10:01:12.022: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 10:01:12.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:01:12.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:01:12.059
  May 18 10:01:12.065: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: creating the pod @ 05/18/23 10:01:12.066
  STEP: submitting the pod to kubernetes @ 05/18/23 10:01:12.066
  E0518 10:01:12.799204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:13.799567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:14.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5953" for this suite. @ 05/18/23 10:01:14.109
• [2.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/18/23 10:01:14.134
  May 18 10:01:14.134: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename field-validation @ 05/18/23 10:01:14.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:01:14.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:01:14.16
  May 18 10:01:14.164: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  W0518 10:01:14.165352      19 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc008a0d560 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0518 10:01:14.800238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:15.800703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0518 10:01:16.714130      19 warnings.go:70] unknown field "alpha"
  W0518 10:01:16.714160      19 warnings.go:70] unknown field "beta"
  W0518 10:01:16.714181      19 warnings.go:70] unknown field "delta"
  W0518 10:01:16.714190      19 warnings.go:70] unknown field "epsilon"
  W0518 10:01:16.714196      19 warnings.go:70] unknown field "gamma"
  May 18 10:01:16.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-558" for this suite. @ 05/18/23 10:01:16.742
• [2.614 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/18/23 10:01:16.756
  May 18 10:01:16.756: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 10:01:16.757
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:01:16.768
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:01:16.771
  STEP: Creating the pod @ 05/18/23 10:01:16.774
  E0518 10:01:16.801720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:17.801998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:18.801978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:19.330: INFO: Successfully updated pod "annotationupdate1cf12807-0856-4e69-91ce-7c42f6575836"
  E0518 10:01:19.802770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:20.803117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:21.803600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:22.804156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:23.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9548" for this suite. @ 05/18/23 10:01:23.368
• [6.619 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/18/23 10:01:23.375
  May 18 10:01:23.375: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 10:01:23.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:01:23.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:01:23.399
  STEP: Setting up server cert @ 05/18/23 10:01:23.424
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 10:01:23.747
  STEP: Deploying the webhook pod @ 05/18/23 10:01:23.754
  STEP: Wait for the deployment to be ready @ 05/18/23 10:01:23.768
  May 18 10:01:23.778: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0518 10:01:23.806727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:24.807338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 10:01:25.79
  E0518 10:01:25.807943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 10:01:25.809
  E0518 10:01:26.808420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:26.811: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/18/23 10:01:26.815
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/18/23 10:01:26.854
  May 18 10:01:26.854: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:01:26.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8994" for this suite. @ 05/18/23 10:01:26.927
  STEP: Destroying namespace "webhook-markers-6355" for this suite. @ 05/18/23 10:01:26.943
• [3.586 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/18/23 10:01:26.985
  May 18 10:01:26.985: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 10:01:26.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:01:27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:01:27.004
  STEP: Creating secret with name projected-secret-test-24008c77-7376-40a4-a6cf-4d0e91f97832 @ 05/18/23 10:01:27.007
  STEP: Creating a pod to test consume secrets @ 05/18/23 10:01:27.011
  E0518 10:01:27.809072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:28.816868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:29.810147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:30.810382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:01:31.044
  May 18 10:01:31.049: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-secrets-b6cf6975-1762-410e-8067-4c3020ba5537 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 10:01:31.065
  May 18 10:01:31.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5243" for this suite. @ 05/18/23 10:01:31.092
• [4.113 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/18/23 10:01:31.1
  May 18 10:01:31.101: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 10:01:31.103
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:01:31.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:01:31.122
  STEP: Creating a ResourceQuota with terminating scope @ 05/18/23 10:01:31.127
  STEP: Ensuring ResourceQuota status is calculated @ 05/18/23 10:01:31.133
  E0518 10:01:31.810595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:32.811419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 05/18/23 10:01:33.139
  STEP: Ensuring ResourceQuota status is calculated @ 05/18/23 10:01:33.145
  E0518 10:01:33.811494      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:34.812167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 05/18/23 10:01:35.152
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/18/23 10:01:35.171
  E0518 10:01:35.812948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:36.813121      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/18/23 10:01:37.179
  E0518 10:01:37.812875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:38.813424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/18/23 10:01:39.186
  STEP: Ensuring resource quota status released the pod usage @ 05/18/23 10:01:39.199
  E0518 10:01:39.813421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:40.813910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 05/18/23 10:01:41.205
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/18/23 10:01:41.216
  E0518 10:01:41.815210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:42.815147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/18/23 10:01:43.222
  E0518 10:01:43.816142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:44.816374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/18/23 10:01:45.228
  STEP: Ensuring resource quota status released the pod usage @ 05/18/23 10:01:45.243
  E0518 10:01:45.816426      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:46.817444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:47.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1874" for this suite. @ 05/18/23 10:01:47.259
• [16.167 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/18/23 10:01:47.271
  May 18 10:01:47.271: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 10:01:47.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:01:47.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:01:47.306
  STEP: Creating configMap with name projected-configmap-test-volume-map-c3be1b96-5dc5-471f-95e3-0e6692a4a500 @ 05/18/23 10:01:47.311
  STEP: Creating a pod to test consume configMaps @ 05/18/23 10:01:47.317
  E0518 10:01:47.817722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:48.819030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:49.818497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:50.818539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:01:51.348
  May 18 10:01:51.353: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-configmaps-10263aff-0f45-4875-87b0-1368c9bb0f22 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 10:01:51.363
  May 18 10:01:51.383: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8173" for this suite. @ 05/18/23 10:01:51.388
• [4.126 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/18/23 10:01:51.398
  May 18 10:01:51.398: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 10:01:51.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:01:51.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:01:51.422
  STEP: creating service nodeport-test with type=NodePort in namespace services-4601 @ 05/18/23 10:01:51.426
  STEP: creating replication controller nodeport-test in namespace services-4601 @ 05/18/23 10:01:51.445
  I0518 10:01:51.454100      19 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-4601, replica count: 2
  E0518 10:01:51.819536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:52.820523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:53.820626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 10:01:54.504944      19 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 10:01:54.505: INFO: Creating new exec pod
  E0518 10:01:54.821023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:55.821304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:01:56.822050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:57.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4601 exec execpodvpwp4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 18 10:01:57.806: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 18 10:01:57.806: INFO: stdout: "nodeport-test-t2n6g"
  May 18 10:01:57.806: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4601 exec execpodvpwp4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.30.124 80'
  E0518 10:01:57.822256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:58.128: INFO: stderr: "+ nc -v -t -w 2 10.102.30.124 80\nConnection to 10.102.30.124 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  May 18 10:01:58.128: INFO: stdout: ""
  E0518 10:01:58.823176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:01:59.129: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4601 exec execpodvpwp4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.30.124 80'
  May 18 10:01:59.400: INFO: stderr: "+ nc -v -t -w 2 10.102.30.124 80\n+ echo hostName\nConnection to 10.102.30.124 80 port [tcp/http] succeeded!\n"
  May 18 10:01:59.400: INFO: stdout: "nodeport-test-t2n6g"
  May 18 10:01:59.400: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4601 exec execpodvpwp4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.143 31461'
  May 18 10:01:59.605: INFO: stderr: "+ nc -v -t -w 2 192.168.190.143 31461\n+ echo hostName\nConnection to 192.168.190.143 31461 port [tcp/*] succeeded!\n"
  May 18 10:01:59.605: INFO: stdout: "nodeport-test-t2n6g"
  May 18 10:01:59.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4601 exec execpodvpwp4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.135 31461'
  May 18 10:01:59.796: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.190.135 31461\nConnection to 192.168.190.135 31461 port [tcp/*] succeeded!\n"
  May 18 10:01:59.796: INFO: stdout: ""
  E0518 10:01:59.823257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:00.796: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-4601 exec execpodvpwp4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.135 31461'
  E0518 10:02:00.823481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:01.019: INFO: stderr: "+ nc -v -t -w 2 192.168.190.135 31461\n+ echo hostName\nConnection to 192.168.190.135 31461 port [tcp/*] succeeded!\n"
  May 18 10:02:01.020: INFO: stdout: "nodeport-test-5khsr"
  May 18 10:02:01.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4601" for this suite. @ 05/18/23 10:02:01.027
• [9.638 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/18/23 10:02:01.036
  May 18 10:02:01.036: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/18/23 10:02:01.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:01.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:01.07
  May 18 10:02:01.076: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 10:02:01.824390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:02.824744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/18/23 10:02:03.091
  May 18 10:02:03.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-6402 --namespace=crd-publish-openapi-6402 create -f -'
  E0518 10:02:03.825117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:04.797: INFO: stderr: ""
  May 18 10:02:04.797: INFO: stdout: "e2e-test-crd-publish-openapi-3785-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 18 10:02:04.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-6402 --namespace=crd-publish-openapi-6402 delete e2e-test-crd-publish-openapi-3785-crds test-cr'
  E0518 10:02:04.825263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:04.912: INFO: stderr: ""
  May 18 10:02:04.912: INFO: stdout: "e2e-test-crd-publish-openapi-3785-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May 18 10:02:04.912: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-6402 --namespace=crd-publish-openapi-6402 apply -f -'
  May 18 10:02:05.398: INFO: stderr: ""
  May 18 10:02:05.398: INFO: stdout: "e2e-test-crd-publish-openapi-3785-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 18 10:02:05.398: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-6402 --namespace=crd-publish-openapi-6402 delete e2e-test-crd-publish-openapi-3785-crds test-cr'
  May 18 10:02:05.510: INFO: stderr: ""
  May 18 10:02:05.510: INFO: stdout: "e2e-test-crd-publish-openapi-3785-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/18/23 10:02:05.51
  May 18 10:02:05.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=crd-publish-openapi-6402 explain e2e-test-crd-publish-openapi-3785-crds'
  E0518 10:02:05.825519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:05.978: INFO: stderr: ""
  May 18 10:02:05.978: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-3785-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0518 10:02:06.826677      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:07.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0518 10:02:07.827107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-publish-openapi-6402" for this suite. @ 05/18/23 10:02:07.834
• [6.805 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/18/23 10:02:07.842
  May 18 10:02:07.842: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/18/23 10:02:07.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:07.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:07.872
  May 18 10:02:07.876: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 10:02:08.827324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:09.827966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:10.828725      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:11.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1115" for this suite. @ 05/18/23 10:02:11.016
• [3.183 seconds]
------------------------------
S
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/18/23 10:02:11.026
  May 18 10:02:11.026: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename disruption @ 05/18/23 10:02:11.028
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:11.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:11.074
  STEP: Creating a kubernetes client @ 05/18/23 10:02:11.081
  May 18 10:02:11.081: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename disruption-2 @ 05/18/23 10:02:11.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:11.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:11.103
  STEP: Waiting for the pdb to be processed @ 05/18/23 10:02:11.112
  E0518 10:02:11.829762      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:12.829810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/18/23 10:02:13.124
  E0518 10:02:13.830024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:14.830511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 05/18/23 10:02:15.137
  E0518 10:02:15.831586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:16.831870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 05/18/23 10:02:17.145
  STEP: listing a collection of PDBs in namespace disruption-6846 @ 05/18/23 10:02:17.151
  STEP: deleting a collection of PDBs @ 05/18/23 10:02:17.155
  STEP: Waiting for the PDB collection to be deleted @ 05/18/23 10:02:17.167
  May 18 10:02:17.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 10:02:17.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5990" for this suite. @ 05/18/23 10:02:17.182
  STEP: Destroying namespace "disruption-6846" for this suite. @ 05/18/23 10:02:17.19
• [6.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/18/23 10:02:17.21
  May 18 10:02:17.211: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl-logs @ 05/18/23 10:02:17.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:17.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:17.237
  STEP: creating an pod @ 05/18/23 10:02:17.242
  May 18 10:02:17.243: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-logs-8425 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May 18 10:02:17.401: INFO: stderr: ""
  May 18 10:02:17.401: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/18/23 10:02:17.407
  May 18 10:02:17.407: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0518 10:02:17.833052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:18.833470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:19.425: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/18/23 10:02:19.426
  May 18 10:02:19.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-logs-8425 logs logs-generator logs-generator'
  May 18 10:02:19.582: INFO: stderr: ""
  May 18 10:02:19.582: INFO: stdout: "I0518 10:02:18.469328       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/qvx 404\nI0518 10:02:18.669649       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/dds 532\nI0518 10:02:18.870227       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/b49v 575\nI0518 10:02:19.069515       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/ddr 433\nI0518 10:02:19.270037       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/w59 492\nI0518 10:02:19.470453       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/rxcr 412\n"
  STEP: limiting log lines @ 05/18/23 10:02:19.583
  May 18 10:02:19.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-logs-8425 logs logs-generator logs-generator --tail=1'
  May 18 10:02:19.686: INFO: stderr: ""
  May 18 10:02:19.686: INFO: stdout: "I0518 10:02:19.669874       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/z7w 224\n"
  May 18 10:02:19.686: INFO: got output "I0518 10:02:19.669874       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/z7w 224\n"
  STEP: limiting log bytes @ 05/18/23 10:02:19.686
  May 18 10:02:19.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-logs-8425 logs logs-generator logs-generator --limit-bytes=1'
  May 18 10:02:19.788: INFO: stderr: ""
  May 18 10:02:19.788: INFO: stdout: "I"
  May 18 10:02:19.788: INFO: got output "I"
  STEP: exposing timestamps @ 05/18/23 10:02:19.788
  May 18 10:02:19.789: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-logs-8425 logs logs-generator logs-generator --tail=1 --timestamps'
  E0518 10:02:19.833549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:19.898: INFO: stderr: ""
  May 18 10:02:19.898: INFO: stdout: "2023-05-18T10:02:19.873186409Z I0518 10:02:19.872958       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/l25 271\n"
  May 18 10:02:19.898: INFO: got output "2023-05-18T10:02:19.873186409Z I0518 10:02:19.872958       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/l25 271\n"
  STEP: restricting to a time range @ 05/18/23 10:02:19.898
  E0518 10:02:20.833744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:21.834486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:22.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-logs-8425 logs logs-generator logs-generator --since=1s'
  May 18 10:02:22.533: INFO: stderr: ""
  May 18 10:02:22.533: INFO: stdout: "I0518 10:02:21.669818       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/r6z 418\nI0518 10:02:21.870429       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/8gp 448\nI0518 10:02:22.069963       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/9cq8 281\nI0518 10:02:22.270462       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/87c9 588\nI0518 10:02:22.469969       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/2z9 259\n"
  May 18 10:02:22.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-logs-8425 logs logs-generator logs-generator --since=24h'
  May 18 10:02:22.665: INFO: stderr: ""
  May 18 10:02:22.665: INFO: stdout: "I0518 10:02:18.469328       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/qvx 404\nI0518 10:02:18.669649       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/dds 532\nI0518 10:02:18.870227       1 logs_generator.go:76] 2 POST /api/v1/namespaces/kube-system/pods/b49v 575\nI0518 10:02:19.069515       1 logs_generator.go:76] 3 GET /api/v1/namespaces/ns/pods/ddr 433\nI0518 10:02:19.270037       1 logs_generator.go:76] 4 POST /api/v1/namespaces/default/pods/w59 492\nI0518 10:02:19.470453       1 logs_generator.go:76] 5 PUT /api/v1/namespaces/ns/pods/rxcr 412\nI0518 10:02:19.669874       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/ns/pods/z7w 224\nI0518 10:02:19.872958       1 logs_generator.go:76] 7 POST /api/v1/namespaces/ns/pods/l25 271\nI0518 10:02:20.069504       1 logs_generator.go:76] 8 GET /api/v1/namespaces/kube-system/pods/jvl 223\nI0518 10:02:20.270052       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/vv8 207\nI0518 10:02:20.469433       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/lxw 483\nI0518 10:02:20.670222       1 logs_generator.go:76] 11 PUT /api/v1/namespaces/kube-system/pods/mjpk 263\nI0518 10:02:20.869472       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/default/pods/7s7v 460\nI0518 10:02:21.070334       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/62b6 231\nI0518 10:02:21.269752       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/plnt 596\nI0518 10:02:21.470318       1 logs_generator.go:76] 15 GET /api/v1/namespaces/kube-system/pods/knhq 586\nI0518 10:02:21.669818       1 logs_generator.go:76] 16 GET /api/v1/namespaces/ns/pods/r6z 418\nI0518 10:02:21.870429       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/ns/pods/8gp 448\nI0518 10:02:22.069963       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/9cq8 281\nI0518 10:02:22.270462       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/87c9 588\nI0518 10:02:22.469969       1 logs_generator.go:76] 20 GET /api/v1/namespaces/kube-system/pods/2z9 259\n"
  May 18 10:02:22.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-logs-8425 delete pod logs-generator'
  E0518 10:02:22.835170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:23.432: INFO: stderr: ""
  May 18 10:02:23.432: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May 18 10:02:23.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-8425" for this suite. @ 05/18/23 10:02:23.437
• [6.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/18/23 10:02:23.445
  May 18 10:02:23.445: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 10:02:23.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:23.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:23.468
  STEP: Setting up server cert @ 05/18/23 10:02:23.494
  E0518 10:02:23.835294      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:24.835505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 10:02:25.063
  STEP: Deploying the webhook pod @ 05/18/23 10:02:25.074
  STEP: Wait for the deployment to be ready @ 05/18/23 10:02:25.086
  May 18 10:02:25.103: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0518 10:02:25.835742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:26.836489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 10:02:27.119
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 10:02:27.136
  E0518 10:02:27.841268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:28.137: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/18/23 10:02:28.142
  STEP: create a pod that should be updated by the webhook @ 05/18/23 10:02:28.164
  May 18 10:02:28.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8415" for this suite. @ 05/18/23 10:02:28.249
  STEP: Destroying namespace "webhook-markers-8513" for this suite. @ 05/18/23 10:02:28.259
• [4.820 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/18/23 10:02:28.267
  May 18 10:02:28.267: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename svcaccounts @ 05/18/23 10:02:28.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:28.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:28.3
  STEP: creating a ServiceAccount @ 05/18/23 10:02:28.304
  STEP: watching for the ServiceAccount to be added @ 05/18/23 10:02:28.312
  STEP: patching the ServiceAccount @ 05/18/23 10:02:28.315
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/18/23 10:02:28.322
  STEP: deleting the ServiceAccount @ 05/18/23 10:02:28.328
  May 18 10:02:28.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9858" for this suite. @ 05/18/23 10:02:28.348
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/18/23 10:02:28.366
  May 18 10:02:28.366: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 10:02:28.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:28.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:28.388
  STEP: Creating pod liveness-c790ef1f-fc5b-426b-bc47-2e56f309ef11 in namespace container-probe-655 @ 05/18/23 10:02:28.392
  E0518 10:02:28.841920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:29.843111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:30.412: INFO: Started pod liveness-c790ef1f-fc5b-426b-bc47-2e56f309ef11 in namespace container-probe-655
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/18/23 10:02:30.413
  May 18 10:02:30.418: INFO: Initial restart count of pod liveness-c790ef1f-fc5b-426b-bc47-2e56f309ef11 is 0
  E0518 10:02:30.842977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:31.844090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:32.844926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:33.845523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:34.846089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:35.846354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:36.847656      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:37.847584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:38.847865      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:39.848263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:40.848280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:41.849213      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:42.849506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:43.849861      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:44.850583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:45.851035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:46.851810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:47.852392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:48.852682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:49.853008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:02:50.482: INFO: Restart count of pod container-probe-655/liveness-c790ef1f-fc5b-426b-bc47-2e56f309ef11 is now 1 (20.064631996s elapsed)
  May 18 10:02:50.483: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 10:02:50.488
  STEP: Destroying namespace "container-probe-655" for this suite. @ 05/18/23 10:02:50.503
• [22.144 seconds]
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/18/23 10:02:50.512
  May 18 10:02:50.513: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pod-network-test @ 05/18/23 10:02:50.533
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:02:50.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:02:50.559
  STEP: Performing setup for networking test in namespace pod-network-test-6739 @ 05/18/23 10:02:50.563
  STEP: creating a selector @ 05/18/23 10:02:50.563
  STEP: Creating the service pods in kubernetes @ 05/18/23 10:02:50.564
  May 18 10:02:50.564: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0518 10:02:50.853144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:51.853984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:52.854843      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:53.855009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:54.855923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:55.856484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:56.857384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:57.857683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:58.858727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:02:59.858870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:00.859833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:01.860536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:02.861093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:03.861659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:04.862535      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:05.863191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:06.863163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:07.864789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:08.865400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:09.866412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:10.867431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:11.868124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/18/23 10:03:12.682
  E0518 10:03:12.868254      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:13.869441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:14.700: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 18 10:03:14.700: INFO: Breadth first check of 172.16.161.119 on host 192.168.190.143...
  May 18 10:03:14.704: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.161.94:9080/dial?request=hostname&protocol=http&host=172.16.161.119&port=8083&tries=1'] Namespace:pod-network-test-6739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:03:14.704: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:03:14.706: INFO: ExecWithOptions: Clientset creation
  May 18 10:03:14.707: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.161.94%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.161.119%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  E0518 10:03:14.869588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:14.875: INFO: Waiting for responses: map[]
  May 18 10:03:14.875: INFO: reached 172.16.161.119 after 0/1 tries
  May 18 10:03:14.875: INFO: Breadth first check of 172.16.78.112 on host 192.168.190.135...
  May 18 10:03:14.879: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://172.16.161.94:9080/dial?request=hostname&protocol=http&host=172.16.78.112&port=8083&tries=1'] Namespace:pod-network-test-6739 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:03:14.879: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:03:14.881: INFO: ExecWithOptions: Clientset creation
  May 18 10:03:14.881: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6739/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F172.16.161.94%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D172.16.78.112%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 18 10:03:14.970: INFO: Waiting for responses: map[]
  May 18 10:03:14.970: INFO: reached 172.16.78.112 after 0/1 tries
  May 18 10:03:14.970: INFO: Going to retry 0 out of 2 pods....
  May 18 10:03:14.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6739" for this suite. @ 05/18/23 10:03:14.976
• [24.473 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/18/23 10:03:14.999
  May 18 10:03:14.999: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 10:03:15.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:03:15.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:03:15.025
  STEP: Creating secret with name secret-test-map-b4e520b5-02fb-4f77-9f9d-f2ca8552f819 @ 05/18/23 10:03:15.03
  STEP: Creating a pod to test consume secrets @ 05/18/23 10:03:15.039
  E0518 10:03:15.870412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:16.870492      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:17.871130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:18.872612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:03:19.063
  May 18 10:03:19.067: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-secrets-8d7a515a-b48d-47ec-85e1-b69ad248ad63 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 10:03:19.077
  May 18 10:03:19.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3214" for this suite. @ 05/18/23 10:03:19.102
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/18/23 10:03:19.11
  May 18 10:03:19.110: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 10:03:19.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:03:19.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:03:19.134
  STEP: creating service in namespace services-5965 @ 05/18/23 10:03:19.138
  STEP: creating service affinity-nodeport-transition in namespace services-5965 @ 05/18/23 10:03:19.138
  STEP: creating replication controller affinity-nodeport-transition in namespace services-5965 @ 05/18/23 10:03:19.158
  I0518 10:03:19.169593      19 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-5965, replica count: 3
  E0518 10:03:19.884517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:20.887288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:21.887884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 10:03:22.220536      19 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 18 10:03:22.230: INFO: Creating new exec pod
  E0518 10:03:22.889050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:23.889395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:24.890134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:25.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-5965 exec execpod-affinitykb5w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  E0518 10:03:25.891097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:26.190: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May 18 10:03:26.190: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 10:03:26.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-5965 exec execpod-affinitykb5w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.109.18.134 80'
  E0518 10:03:26.891532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:27.163: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.109.18.134 80\nConnection to 10.109.18.134 80 port [tcp/http] succeeded!\n"
  May 18 10:03:27.163: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 10:03:27.163: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-5965 exec execpod-affinitykb5w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.143 30849'
  May 18 10:03:27.413: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.190.143 30849\nConnection to 192.168.190.143 30849 port [tcp/*] succeeded!\n"
  May 18 10:03:27.413: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 10:03:27.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-5965 exec execpod-affinitykb5w6 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.190.135 30849'
  May 18 10:03:27.642: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.190.135 30849\nConnection to 192.168.190.135 30849 port [tcp/*] succeeded!\n"
  May 18 10:03:27.642: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 18 10:03:27.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-5965 exec execpod-affinitykb5w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.190.143:30849/ ; done'
  E0518 10:03:27.892017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:28.125: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n"
  May 18 10:03:28.126: INFO: stdout: "\naffinity-nodeport-transition-6jknq\naffinity-nodeport-transition-6jknq\naffinity-nodeport-transition-rlhlb\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-6jknq\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-rlhlb\naffinity-nodeport-transition-6jknq\naffinity-nodeport-transition-rlhlb\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-rlhlb\naffinity-nodeport-transition-rlhlb\naffinity-nodeport-transition-rlhlb"
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-6jknq
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-6jknq
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-rlhlb
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-6jknq
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-rlhlb
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-6jknq
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-rlhlb
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-rlhlb
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-rlhlb
  May 18 10:03:28.126: INFO: Received response from host: affinity-nodeport-transition-rlhlb
  May 18 10:03:28.137: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-5965 exec execpod-affinitykb5w6 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.190.143:30849/ ; done'
  May 18 10:03:28.425: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.190.143:30849/\n"
  May 18 10:03:28.425: INFO: stdout: "\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl\naffinity-nodeport-transition-wg7fl"
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Received response from host: affinity-nodeport-transition-wg7fl
  May 18 10:03:28.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 10:03:28.429: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-5965, will wait for the garbage collector to delete the pods @ 05/18/23 10:03:28.441
  May 18 10:03:28.509: INFO: Deleting ReplicationController affinity-nodeport-transition took: 9.015976ms
  May 18 10:03:28.610: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.129357ms
  E0518 10:03:28.892954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:29.894044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5965" for this suite. @ 05/18/23 10:03:30.841
• [11.738 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/18/23 10:03:30.852
  May 18 10:03:30.852: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename hostport @ 05/18/23 10:03:30.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:03:30.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:03:30.873
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/18/23 10:03:30.881
  E0518 10:03:30.894436      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:31.894896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:32.894893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.190.135 on the node which pod1 resides and expect scheduled @ 05/18/23 10:03:32.897
  E0518 10:03:33.896096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:34.896592      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.190.135 but use UDP protocol on the node which pod2 resides @ 05/18/23 10:03:34.915
  E0518 10:03:35.897295      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:36.898214      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:37.898704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:38.899349      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:39.899719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:40.899655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/18/23 10:03:40.968
  May 18 10:03:40.968: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.190.135 http://127.0.0.1:54323/hostname] Namespace:hostport-3550 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:03:40.968: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:03:40.972: INFO: ExecWithOptions: Clientset creation
  May 18 10:03:40.972: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3550/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.190.135+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.190.135, port: 54323 @ 05/18/23 10:03:41.169
  May 18 10:03:41.169: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.190.135:54323/hostname] Namespace:hostport-3550 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:03:41.170: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:03:41.172: INFO: ExecWithOptions: Clientset creation
  May 18 10:03:41.172: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3550/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.190.135%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.190.135, port: 54323 UDP @ 05/18/23 10:03:41.561
  May 18 10:03:41.561: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.190.135 54323] Namespace:hostport-3550 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:03:41.561: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:03:41.562: INFO: ExecWithOptions: Clientset creation
  May 18 10:03:41.562: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-3550/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.190.135+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0518 10:03:41.900424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:42.900754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:43.901896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:44.901985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:45.903456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:46.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-3550" for this suite. @ 05/18/23 10:03:46.692
• [15.849 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/18/23 10:03:46.703
  May 18 10:03:46.703: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 10:03:46.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:03:46.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:03:46.73
  STEP: creating all guestbook components @ 05/18/23 10:03:46.735
  May 18 10:03:46.735: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May 18 10:03:46.737: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 create -f -'
  E0518 10:03:46.903022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:47.903984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:48.904188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:48.956: INFO: stderr: ""
  May 18 10:03:48.957: INFO: stdout: "service/agnhost-replica created\n"
  May 18 10:03:48.957: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May 18 10:03:48.957: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 create -f -'
  May 18 10:03:49.551: INFO: stderr: ""
  May 18 10:03:49.551: INFO: stdout: "service/agnhost-primary created\n"
  May 18 10:03:49.551: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May 18 10:03:49.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 create -f -'
  E0518 10:03:49.904224      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:50.133: INFO: stderr: ""
  May 18 10:03:50.133: INFO: stdout: "service/frontend created\n"
  May 18 10:03:50.133: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May 18 10:03:50.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 create -f -'
  May 18 10:03:50.731: INFO: stderr: ""
  May 18 10:03:50.731: INFO: stdout: "deployment.apps/frontend created\n"
  May 18 10:03:50.731: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 18 10:03:50.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 create -f -'
  E0518 10:03:50.904484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:51.450: INFO: stderr: ""
  May 18 10:03:51.450: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May 18 10:03:51.450: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 18 10:03:51.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 create -f -'
  E0518 10:03:51.909856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:52.354: INFO: stderr: ""
  May 18 10:03:52.354: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/18/23 10:03:52.354
  May 18 10:03:52.354: INFO: Waiting for all frontend pods to be Running.
  E0518 10:03:52.910661      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:53.910830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:54.911041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:55.911167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:03:56.911956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:57.407: INFO: Waiting for frontend to serve content.
  May 18 10:03:57.415: INFO: Trying to add a new entry to the guestbook.
  May 18 10:03:57.429: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/18/23 10:03:57.436
  May 18 10:03:57.436: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 delete --grace-period=0 --force -f -'
  May 18 10:03:57.533: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 10:03:57.533: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/18/23 10:03:57.533
  May 18 10:03:57.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 delete --grace-period=0 --force -f -'
  May 18 10:03:57.640: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 10:03:57.640: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/18/23 10:03:57.64
  May 18 10:03:57.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 delete --grace-period=0 --force -f -'
  May 18 10:03:57.764: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 10:03:57.764: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/18/23 10:03:57.765
  May 18 10:03:57.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 delete --grace-period=0 --force -f -'
  May 18 10:03:57.861: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 10:03:57.861: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/18/23 10:03:57.861
  May 18 10:03:57.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 delete --grace-period=0 --force -f -'
  E0518 10:03:57.912508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:03:58.063: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 10:03:58.063: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/18/23 10:03:58.063
  May 18 10:03:58.063: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-8107 delete --grace-period=0 --force -f -'
  May 18 10:03:58.326: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 10:03:58.326: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May 18 10:03:58.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8107" for this suite. @ 05/18/23 10:03:58.345
• [11.648 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/18/23 10:03:58.353
  May 18 10:03:58.353: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename gc @ 05/18/23 10:03:58.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:03:58.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:03:58.371
  STEP: create the deployment @ 05/18/23 10:03:58.374
  W0518 10:03:58.379337      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/18/23 10:03:58.379
  STEP: delete the deployment @ 05/18/23 10:03:58.888
  STEP: wait for all rs to be garbage collected @ 05/18/23 10:03:58.897
  STEP: expected 0 pods, got 2 pods @ 05/18/23 10:03:58.901
  E0518 10:03:58.913016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/18/23 10:03:59.418
  May 18 10:03:59.580: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 18 10:03:59.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6088" for this suite. @ 05/18/23 10:03:59.586
• [1.238 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/18/23 10:03:59.591
  May 18 10:03:59.591: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/18/23 10:03:59.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:03:59.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:03:59.614
  STEP: create the container to handle the HTTPGet hook request. @ 05/18/23 10:03:59.622
  E0518 10:03:59.913844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:00.914081      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:01.914236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:02.914539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/18/23 10:04:03.655
  E0518 10:04:03.915053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:04.915201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:05.916012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:06.916778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/18/23 10:04:07.686
  STEP: delete the pod with lifecycle hook @ 05/18/23 10:04:07.717
  E0518 10:04:07.929521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:08.917421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:04:09.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-7264" for this suite. @ 05/18/23 10:04:09.746
• [10.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/18/23 10:04:09.761
  May 18 10:04:09.761: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename field-validation @ 05/18/23 10:04:09.765
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:04:09.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:04:09.795
  STEP: apply creating a deployment @ 05/18/23 10:04:09.801
  May 18 10:04:09.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9035" for this suite. @ 05/18/23 10:04:09.827
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/18/23 10:04:09.843
  May 18 10:04:09.843: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename subpath @ 05/18/23 10:04:09.846
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:04:09.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:04:09.87
  STEP: Setting up data @ 05/18/23 10:04:09.876
  STEP: Creating pod pod-subpath-test-secret-lkgf @ 05/18/23 10:04:09.889
  STEP: Creating a pod to test atomic-volume-subpath @ 05/18/23 10:04:09.89
  E0518 10:04:09.918339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:10.918497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:11.919252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:12.919737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:13.920412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:14.923731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:15.923643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:16.924639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:17.925325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:18.925455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:19.925506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:20.926431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:21.926454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:22.926671      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:23.926804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:24.927125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:25.927204      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:26.927997      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:27.931353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:28.930153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:29.930313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:30.930814      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:31.931033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:32.931246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:33.931391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:04:33.98
  May 18 10:04:33.985: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-subpath-test-secret-lkgf container test-container-subpath-secret-lkgf: <nil>
  STEP: delete the pod @ 05/18/23 10:04:33.994
  STEP: Deleting pod pod-subpath-test-secret-lkgf @ 05/18/23 10:04:34.011
  May 18 10:04:34.012: INFO: Deleting pod "pod-subpath-test-secret-lkgf" in namespace "subpath-7631"
  May 18 10:04:34.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-7631" for this suite. @ 05/18/23 10:04:34.02
• [24.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/18/23 10:04:34.029
  May 18 10:04:34.029: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 10:04:34.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:04:34.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:04:34.055
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-218 @ 05/18/23 10:04:34.059
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/18/23 10:04:34.073
  STEP: creating service externalsvc in namespace services-218 @ 05/18/23 10:04:34.073
  STEP: creating replication controller externalsvc in namespace services-218 @ 05/18/23 10:04:34.089
  I0518 10:04:34.095737      19 runners.go:194] Created replication controller with name: externalsvc, namespace: services-218, replica count: 2
  E0518 10:04:34.933190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:35.934345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:36.933812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0518 10:04:37.147092      19 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/18/23 10:04:37.153
  May 18 10:04:37.184: INFO: Creating new exec pod
  E0518 10:04:37.934364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:38.934971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:04:39.217: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=services-218 exec execpodg4zrv -- /bin/sh -x -c nslookup nodeport-service.services-218.svc.cluster.local'
  May 18 10:04:39.477: INFO: stderr: "+ nslookup nodeport-service.services-218.svc.cluster.local\n"
  May 18 10:04:39.477: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-218.svc.cluster.local\tcanonical name = externalsvc.services-218.svc.cluster.local.\nName:\texternalsvc.services-218.svc.cluster.local\nAddress: 10.108.65.55\n\n"
  May 18 10:04:39.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-218, will wait for the garbage collector to delete the pods @ 05/18/23 10:04:39.482
  May 18 10:04:39.543: INFO: Deleting ReplicationController externalsvc took: 7.342528ms
  May 18 10:04:39.645: INFO: Terminating ReplicationController externalsvc pods took: 101.337876ms
  E0518 10:04:39.934911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:40.935746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:04:41.474: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-218" for this suite. @ 05/18/23 10:04:41.501
• [7.479 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/18/23 10:04:41.51
  May 18 10:04:41.510: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename proxy @ 05/18/23 10:04:41.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:04:41.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:04:41.539
  May 18 10:04:41.542: INFO: Creating pod...
  E0518 10:04:41.935962      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:42.936336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:04:43.562: INFO: Creating service...
  May 18 10:04:43.592: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/pods/agnhost/proxy/some/path/with/DELETE
  May 18 10:04:43.602: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 18 10:04:43.602: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/pods/agnhost/proxy/some/path/with/GET
  May 18 10:04:43.608: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 18 10:04:43.608: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/pods/agnhost/proxy/some/path/with/HEAD
  May 18 10:04:43.614: INFO: http.Client request:HEAD | StatusCode:200
  May 18 10:04:43.614: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/pods/agnhost/proxy/some/path/with/OPTIONS
  May 18 10:04:43.619: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 18 10:04:43.619: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/pods/agnhost/proxy/some/path/with/PATCH
  May 18 10:04:43.624: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 18 10:04:43.626: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/pods/agnhost/proxy/some/path/with/POST
  May 18 10:04:43.632: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 18 10:04:43.633: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/pods/agnhost/proxy/some/path/with/PUT
  May 18 10:04:43.642: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 18 10:04:43.642: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/services/test-service/proxy/some/path/with/DELETE
  May 18 10:04:43.649: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 18 10:04:43.650: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/services/test-service/proxy/some/path/with/GET
  May 18 10:04:43.656: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 18 10:04:43.656: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/services/test-service/proxy/some/path/with/HEAD
  May 18 10:04:43.661: INFO: http.Client request:HEAD | StatusCode:200
  May 18 10:04:43.662: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/services/test-service/proxy/some/path/with/OPTIONS
  May 18 10:04:43.671: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 18 10:04:43.671: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/services/test-service/proxy/some/path/with/PATCH
  May 18 10:04:43.679: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 18 10:04:43.679: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/services/test-service/proxy/some/path/with/POST
  May 18 10:04:43.684: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 18 10:04:43.685: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9694/services/test-service/proxy/some/path/with/PUT
  May 18 10:04:43.691: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 18 10:04:43.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-9694" for this suite. @ 05/18/23 10:04:43.701
• [2.208 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/18/23 10:04:43.72
  May 18 10:04:43.720: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename security-context-test @ 05/18/23 10:04:43.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:04:43.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:04:43.743
  E0518 10:04:43.937334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:44.937216      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:45.938146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:46.938336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:04:47.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9791" for this suite. @ 05/18/23 10:04:47.79
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/18/23 10:04:47.802
  May 18 10:04:47.802: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 10:04:47.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:04:47.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:04:47.833
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-dc1876f8-814a-408b-9a73-816b6511ca45 @ 05/18/23 10:04:47.86
  STEP: Creating the pod @ 05/18/23 10:04:47.868
  E0518 10:04:47.939251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:48.940639      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-dc1876f8-814a-408b-9a73-816b6511ca45 @ 05/18/23 10:04:49.912
  STEP: waiting to observe update in volume @ 05/18/23 10:04:49.92
  E0518 10:04:49.942290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:50.942613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:04:51.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4519" for this suite. @ 05/18/23 10:04:51.942
  E0518 10:04:51.942772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.146 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/18/23 10:04:51.949
  May 18 10:04:51.949: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubelet-test @ 05/18/23 10:04:51.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:04:51.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:04:51.971
  STEP: Waiting for pod completion @ 05/18/23 10:04:51.99
  E0518 10:04:52.942833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:53.943111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:54.943605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:55.943916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:04:56.024: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2572" for this suite. @ 05/18/23 10:04:56.031
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/18/23 10:04:56.047
  May 18 10:04:56.047: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename prestop @ 05/18/23 10:04:56.049
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:04:56.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:04:56.077
  STEP: Creating server pod server in namespace prestop-7701 @ 05/18/23 10:04:56.083
  STEP: Waiting for pods to come up. @ 05/18/23 10:04:56.093
  E0518 10:04:56.944737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:57.953401      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-7701 @ 05/18/23 10:04:58.107
  E0518 10:04:58.953815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:04:59.954223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 05/18/23 10:05:00.127
  E0518 10:05:00.954618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:01.955724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:02.955380      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:03.955602      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:04.955776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:05:05.154: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May 18 10:05:05.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/18/23 10:05:05.162
  STEP: Destroying namespace "prestop-7701" for this suite. @ 05/18/23 10:05:05.177
• [9.144 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/18/23 10:05:05.195
  May 18 10:05:05.195: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 10:05:05.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:05:05.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:05:05.226
  STEP: Creating projection with secret that has name projected-secret-test-map-96a8a57e-06a3-468c-aae7-2e4a8a0a22ca @ 05/18/23 10:05:05.232
  STEP: Creating a pod to test consume secrets @ 05/18/23 10:05:05.238
  E0518 10:05:05.955898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:06.956363      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:07.957105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:08.957466      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:05:09.259
  May 18 10:05:09.262: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-secrets-8cb6784e-d647-4782-9959-a642a5dcd554 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 10:05:09.272
  May 18 10:05:09.290: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5116" for this suite. @ 05/18/23 10:05:09.296
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/18/23 10:05:09.305
  May 18 10:05:09.305: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename podtemplate @ 05/18/23 10:05:09.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:05:09.322
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:05:09.327
  STEP: Create a pod template @ 05/18/23 10:05:09.331
  STEP: Replace a pod template @ 05/18/23 10:05:09.338
  May 18 10:05:09.346: INFO: Found updated podtemplate annotation: "true"

  May 18 10:05:09.347: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9196" for this suite. @ 05/18/23 10:05:09.351
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/18/23 10:05:09.361
  May 18 10:05:09.361: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replicaset @ 05/18/23 10:05:09.362
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:05:09.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:05:09.381
  May 18 10:05:09.385: INFO: Creating ReplicaSet my-hostname-basic-6c0be55d-04b7-4925-80ac-8a62b089de74
  May 18 10:05:09.395: INFO: Pod name my-hostname-basic-6c0be55d-04b7-4925-80ac-8a62b089de74: Found 0 pods out of 1
  E0518 10:05:09.958568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:10.959166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:11.960146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:12.960263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:13.960666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:05:14.400: INFO: Pod name my-hostname-basic-6c0be55d-04b7-4925-80ac-8a62b089de74: Found 1 pods out of 1
  May 18 10:05:14.400: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-6c0be55d-04b7-4925-80ac-8a62b089de74" is running
  May 18 10:05:14.404: INFO: Pod "my-hostname-basic-6c0be55d-04b7-4925-80ac-8a62b089de74-lfbtq" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-18 10:05:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-18 10:05:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-18 10:05:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-18 10:05:09 +0000 UTC Reason: Message:}])
  May 18 10:05:14.404: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/18/23 10:05:14.404
  May 18 10:05:14.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-680" for this suite. @ 05/18/23 10:05:14.422
• [5.069 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/18/23 10:05:14.434
  May 18 10:05:14.435: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 10:05:14.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:05:14.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:05:14.461
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/18/23 10:05:14.465
  E0518 10:05:14.961675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:15.964297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:16.964475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:17.965161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:05:18.488
  May 18 10:05:18.493: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-de72011e-8d8e-4dec-b371-624828237eee container test-container: <nil>
  STEP: delete the pod @ 05/18/23 10:05:18.502
  May 18 10:05:18.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2151" for this suite. @ 05/18/23 10:05:18.527
• [4.103 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/18/23 10:05:18.541
  May 18 10:05:18.541: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 10:05:18.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:05:18.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:05:18.569
  E0518 10:05:18.966221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:19.966421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:20.967231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:21.968098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:22.969143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:23.969511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:05:24.637
  May 18 10:05:24.641: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod client-envvars-f397ca81-ab1c-4bf0-9483-68a86fe518c4 container env3cont: <nil>
  STEP: delete the pod @ 05/18/23 10:05:24.651
  May 18 10:05:24.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1150" for this suite. @ 05/18/23 10:05:24.675
• [6.144 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/18/23 10:05:24.687
  May 18 10:05:24.688: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/18/23 10:05:24.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:05:24.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:05:24.733
  STEP: create the container to handle the HTTPGet hook request. @ 05/18/23 10:05:24.745
  E0518 10:05:24.970545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:25.971442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/18/23 10:05:26.771
  E0518 10:05:26.972095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:27.974061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/18/23 10:05:28.792
  STEP: delete the pod with lifecycle hook @ 05/18/23 10:05:28.803
  E0518 10:05:28.974455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:29.974211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:30.975175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:31.975678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:05:32.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5371" for this suite. @ 05/18/23 10:05:32.835
• [8.152 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/18/23 10:05:32.842
  May 18 10:05:32.842: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename sched-preemption @ 05/18/23 10:05:32.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:05:32.857
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:05:32.86
  May 18 10:05:32.877: INFO: Waiting up to 1m0s for all nodes to be ready
  E0518 10:05:32.975771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:33.975968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:34.976996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:35.977902      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:36.978772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:37.979916      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:38.980464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:39.980508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:40.981223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:41.982300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:42.983339      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:43.983612      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:44.984361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:45.984609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:46.984977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:47.989844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:48.989939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:49.990170      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:50.990473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:51.991019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:52.991708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:53.991624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:54.992076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:55.992220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:56.993053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:57.993178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:58.993238      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:05:59.994126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:00.994573      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:01.995476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:02.996000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:03.996141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:04.997126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:05.997541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:06.998070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:08.000722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:08.999830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:10.000218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:11.001025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:12.002584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:13.003696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:14.004022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:15.004185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:16.004594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:17.004806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:18.005451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:19.005693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:20.006520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:21.006813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:22.007138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:23.007598      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:24.008083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:25.008542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:26.008686      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:27.009290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:28.014050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:29.014635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:30.015262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:31.016175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:32.017172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:06:32.932: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/18/23 10:06:32.936
  May 18 10:06:32.972: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 18 10:06:32.991: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  E0518 10:06:33.021973      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:06:33.026: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 18 10:06:33.040: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/18/23 10:06:33.04
  E0518 10:06:34.022184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:35.022650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/18/23 10:06:35.061
  E0518 10:06:36.023508      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:37.024157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:38.025806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:39.026144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:40.026783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:41.026703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:06:41.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9855" for this suite. @ 05/18/23 10:06:41.189
• [68.358 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/18/23 10:06:41.206
  May 18 10:06:41.206: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename controllerrevisions @ 05/18/23 10:06:41.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:06:41.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:06:41.235
  STEP: Creating DaemonSet "e2e-q2qfl-daemon-set" @ 05/18/23 10:06:41.268
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/18/23 10:06:41.278
  May 18 10:06:41.289: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:06:41.296: INFO: Number of nodes with available pods controlled by daemonset e2e-q2qfl-daemon-set: 0
  May 18 10:06:41.296: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:06:42.027281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:06:42.303: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:06:42.307: INFO: Number of nodes with available pods controlled by daemonset e2e-q2qfl-daemon-set: 0
  May 18 10:06:42.307: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:06:43.027638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:06:43.304: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:06:43.309: INFO: Number of nodes with available pods controlled by daemonset e2e-q2qfl-daemon-set: 2
  May 18 10:06:43.309: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-q2qfl-daemon-set
  STEP: Confirm DaemonSet "e2e-q2qfl-daemon-set" successfully created with "daemonset-name=e2e-q2qfl-daemon-set" label @ 05/18/23 10:06:43.313
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-q2qfl-daemon-set" @ 05/18/23 10:06:43.323
  May 18 10:06:43.328: INFO: Located ControllerRevision: "e2e-q2qfl-daemon-set-6b8954c69c"
  STEP: Patching ControllerRevision "e2e-q2qfl-daemon-set-6b8954c69c" @ 05/18/23 10:06:43.332
  May 18 10:06:43.340: INFO: e2e-q2qfl-daemon-set-6b8954c69c has been patched
  STEP: Create a new ControllerRevision @ 05/18/23 10:06:43.341
  May 18 10:06:43.347: INFO: Created ControllerRevision: e2e-q2qfl-daemon-set-c788554fc
  STEP: Confirm that there are two ControllerRevisions @ 05/18/23 10:06:43.348
  May 18 10:06:43.348: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 18 10:06:43.352: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-q2qfl-daemon-set-6b8954c69c" @ 05/18/23 10:06:43.353
  STEP: Confirm that there is only one ControllerRevision @ 05/18/23 10:06:43.358
  May 18 10:06:43.358: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 18 10:06:43.362: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-q2qfl-daemon-set-c788554fc" @ 05/18/23 10:06:43.366
  May 18 10:06:43.376: INFO: e2e-q2qfl-daemon-set-c788554fc has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/18/23 10:06:43.377
  W0518 10:06:43.387272      19 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/18/23 10:06:43.387
  May 18 10:06:43.387: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0518 10:06:44.027806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:06:44.393: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 18 10:06:44.399: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-q2qfl-daemon-set-c788554fc=updated" @ 05/18/23 10:06:44.399
  STEP: Confirm that there is only one ControllerRevision @ 05/18/23 10:06:44.406
  May 18 10:06:44.406: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 18 10:06:44.410: INFO: Found 1 ControllerRevisions
  May 18 10:06:44.413: INFO: ControllerRevision "e2e-q2qfl-daemon-set-7db5c55fb4" has revision 3
  STEP: Deleting DaemonSet "e2e-q2qfl-daemon-set" @ 05/18/23 10:06:44.418
  STEP: deleting DaemonSet.extensions e2e-q2qfl-daemon-set in namespace controllerrevisions-4539, will wait for the garbage collector to delete the pods @ 05/18/23 10:06:44.419
  May 18 10:06:44.480: INFO: Deleting DaemonSet.extensions e2e-q2qfl-daemon-set took: 7.722848ms
  May 18 10:06:44.581: INFO: Terminating DaemonSet.extensions e2e-q2qfl-daemon-set pods took: 100.940759ms
  E0518 10:06:45.028441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:46.029696      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:06:46.288: INFO: Number of nodes with available pods controlled by daemonset e2e-q2qfl-daemon-set: 0
  May 18 10:06:46.288: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-q2qfl-daemon-set
  May 18 10:06:46.291: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1964530"},"items":null}

  May 18 10:06:46.296: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1964530"},"items":null}

  May 18 10:06:46.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-4539" for this suite. @ 05/18/23 10:06:46.329
• [5.130 seconds]
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/18/23 10:06:46.335
  May 18 10:06:46.336: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename svcaccounts @ 05/18/23 10:06:46.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:06:46.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:06:46.358
  E0518 10:06:47.029760      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:48.031452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/18/23 10:06:48.392
  May 18 10:06:48.393: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3143 pod-service-account-4e92c23d-8c09-44f1-baea-58e64cbf4e8d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/18/23 10:06:48.644
  May 18 10:06:48.644: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3143 pod-service-account-4e92c23d-8c09-44f1-baea-58e64cbf4e8d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/18/23 10:06:48.818
  May 18 10:06:48.818: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3143 pod-service-account-4e92c23d-8c09-44f1-baea-58e64cbf4e8d -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May 18 10:06:49.013: INFO: Got root ca configmap in namespace "svcaccounts-3143"
  May 18 10:06:49.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3143" for this suite. @ 05/18/23 10:06:49.021
• [2.693 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS  E0518 10:06:49.031218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/18/23 10:06:49.031
  May 18 10:06:49.031: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename dns @ 05/18/23 10:06:49.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:06:49.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:06:49.058
  STEP: Creating a test headless service @ 05/18/23 10:06:49.062
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7244.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7244.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/18/23 10:06:49.07
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7244.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7244.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/18/23 10:06:49.07
  STEP: creating a pod to probe DNS @ 05/18/23 10:06:49.071
  STEP: submitting the pod to kubernetes @ 05/18/23 10:06:49.071
  E0518 10:06:50.031635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:51.031906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 10:06:51.088
  STEP: looking for the results for each expected name from probers @ 05/18/23 10:06:51.09
  May 18 10:06:51.102: INFO: DNS probes using dns-7244/dns-test-f8811a34-52e0-4788-a39a-ec3d551728a9 succeeded

  May 18 10:06:51.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 10:06:51.106
  STEP: deleting the test headless service @ 05/18/23 10:06:51.121
  STEP: Destroying namespace "dns-7244" for this suite. @ 05/18/23 10:06:51.133
• [2.110 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/18/23 10:06:51.143
  May 18 10:06:51.144: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 10:06:51.145
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:06:51.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:06:51.16
  STEP: Creating resourceQuota "e2e-rq-status-sh2wg" @ 05/18/23 10:06:51.165
  May 18 10:06:51.171: INFO: Resource quota "e2e-rq-status-sh2wg" reports spec: hard cpu limit of 500m
  May 18 10:06:51.172: INFO: Resource quota "e2e-rq-status-sh2wg" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-sh2wg" /status @ 05/18/23 10:06:51.172
  STEP: Confirm /status for "e2e-rq-status-sh2wg" resourceQuota via watch @ 05/18/23 10:06:51.18
  May 18 10:06:51.182: INFO: observed resourceQuota "e2e-rq-status-sh2wg" in namespace "resourcequota-699" with hard status: v1.ResourceList(nil)
  May 18 10:06:51.182: INFO: Found resourceQuota "e2e-rq-status-sh2wg" in namespace "resourcequota-699" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 18 10:06:51.182: INFO: ResourceQuota "e2e-rq-status-sh2wg" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/18/23 10:06:51.184
  May 18 10:06:51.189: INFO: Resource quota "e2e-rq-status-sh2wg" reports spec: hard cpu limit of 1
  May 18 10:06:51.189: INFO: Resource quota "e2e-rq-status-sh2wg" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-sh2wg" /status @ 05/18/23 10:06:51.19
  STEP: Confirm /status for "e2e-rq-status-sh2wg" resourceQuota via watch @ 05/18/23 10:06:51.195
  May 18 10:06:51.197: INFO: observed resourceQuota "e2e-rq-status-sh2wg" in namespace "resourcequota-699" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 18 10:06:51.197: INFO: Found resourceQuota "e2e-rq-status-sh2wg" in namespace "resourcequota-699" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May 18 10:06:51.198: INFO: ResourceQuota "e2e-rq-status-sh2wg" /status was patched
  STEP: Get "e2e-rq-status-sh2wg" /status @ 05/18/23 10:06:51.199
  May 18 10:06:51.203: INFO: Resourcequota "e2e-rq-status-sh2wg" reports status: hard cpu of 1
  May 18 10:06:51.203: INFO: Resourcequota "e2e-rq-status-sh2wg" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-sh2wg" /status before checking Spec is unchanged @ 05/18/23 10:06:51.205
  May 18 10:06:51.210: INFO: Resourcequota "e2e-rq-status-sh2wg" reports status: hard cpu of 2
  May 18 10:06:51.211: INFO: Resourcequota "e2e-rq-status-sh2wg" reports status: hard memory of 2Gi
  May 18 10:06:51.213: INFO: Found resourceQuota "e2e-rq-status-sh2wg" in namespace "resourcequota-699" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0518 10:06:52.033010      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:53.033071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:54.033207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:55.033688      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:56.033875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:57.035454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:58.035446      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:06:59.037685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:00.038221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:01.038590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:02.039019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:03.041590      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:04.039975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:05.040438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:06.040813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:07.041837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:08.041995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:09.042384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:10.042856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:11.043071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:12.043795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:13.044694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:14.044670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:15.044956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:16.045140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:17.045269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:18.049946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:19.050716      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:20.050880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:21.051437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:22.052279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:23.052734      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:24.052875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:25.053129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:26.054047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:27.054911      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:28.055925      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:29.056124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:30.056333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:31.056921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:32.057192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:33.058080      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:34.059060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:35.059693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:36.060253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:37.061056      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:38.063120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:39.062944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:40.062984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:41.063774      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:42.064236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:43.064737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:44.065584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:45.066143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:46.066802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:47.067683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:48.068418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:49.069192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:50.070265      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:51.070445      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:52.071288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:53.071283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:54.071524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:55.071772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:56.072574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:57.073582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:58.074368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:07:59.074904      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:00.075704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:01.075797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:02.076636      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:03.076625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:04.076890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:05.077118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:06.077151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:07.077257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:08.077500      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:09.077520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:10.077800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:11.078419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:12.079605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:13.079708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:14.080106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:15.080505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:16.080739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:17.081229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:18.081132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:19.082596      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:20.083140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:21.084129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:22.083808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:23.084140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:24.084074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:25.084303      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:26.084443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:27.085501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:28.085287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:29.085960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:30.086178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:31.086337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:32.087151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:33.089784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:34.089884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:35.090256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:36.090422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:37.091407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:38.091572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:39.091927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:40.092172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:41.093127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:42.093646      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:43.093965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:44.094937      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:45.095182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:46.095229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:47.095520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:48.096439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:49.097018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:50.097261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:51.097452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:52.097662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:53.098194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:54.098468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:55.098676      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:56.099203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:57.099615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:58.099777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:08:59.100644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:00.101723      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:01.102351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:02.103106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:03.103909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:04.104789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:05.104992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:06.105215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:07.105944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:08.106237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:09.106315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:10.106328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:11.106794      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:12.106770      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:13.106846      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:14.107128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:15.107784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:16.107811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:17.109138      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:18.108943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:19.109631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:20.110472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:21.111215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:22.111923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:23.112520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:24.113230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:25.113451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:26.113645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:27.114450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:28.114921      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:29.115118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:30.116026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:31.116298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:09:31.221: INFO: ResourceQuota "e2e-rq-status-sh2wg" Spec was unchanged and /status reset
  May 18 10:09:31.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-699" for this suite. @ 05/18/23 10:09:31.228
• [160.092 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/18/23 10:09:31.239
  May 18 10:09:31.239: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename dns @ 05/18/23 10:09:31.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:09:31.267
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:09:31.272
  STEP: Creating a test headless service @ 05/18/23 10:09:31.278
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4867 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4867;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4867 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4867;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4867.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-4867.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4867.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-4867.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4867.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-4867.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4867.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-4867.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4867.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-4867.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4867.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-4867.svc;check="$$(dig +notcp +noall +answer +search 71.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.71_udp@PTR;check="$$(dig +tcp +noall +answer +search 71.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.71_tcp@PTR;sleep 1; done
   @ 05/18/23 10:09:31.304
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4867 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4867;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4867 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4867;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-4867.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-4867.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-4867.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-4867.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-4867.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-4867.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-4867.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-4867.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-4867.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-4867.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-4867.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-4867.svc;check="$$(dig +notcp +noall +answer +search 71.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.71_udp@PTR;check="$$(dig +tcp +noall +answer +search 71.111.103.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.103.111.71_tcp@PTR;sleep 1; done
   @ 05/18/23 10:09:31.305
  STEP: creating a pod to probe DNS @ 05/18/23 10:09:31.307
  STEP: submitting the pod to kubernetes @ 05/18/23 10:09:31.309
  E0518 10:09:32.117400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:33.121568      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:34.121896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:35.122341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 10:09:35.352
  STEP: looking for the results for each expected name from probers @ 05/18/23 10:09:35.355
  May 18 10:09:35.364: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.369: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.374: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.379: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.383: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.388: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.394: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.424: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.429: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.435: INFO: Unable to read jessie_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.441: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.448: INFO: Unable to read jessie_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.452: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.456: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.460: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:35.476: INFO: Lookups using dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4867 wheezy_tcp@dns-test-service.dns-4867 wheezy_udp@dns-test-service.dns-4867.svc wheezy_tcp@dns-test-service.dns-4867.svc wheezy_udp@_http._tcp.dns-test-service.dns-4867.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4867 jessie_tcp@dns-test-service.dns-4867 jessie_udp@dns-test-service.dns-4867.svc jessie_tcp@dns-test-service.dns-4867.svc jessie_udp@_http._tcp.dns-test-service.dns-4867.svc jessie_tcp@_http._tcp.dns-test-service.dns-4867.svc]

  E0518 10:09:36.123175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:37.123964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:38.131179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:39.131438      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:40.131652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:09:40.493: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.501: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.506: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.512: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.518: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.523: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.565: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.571: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.577: INFO: Unable to read jessie_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.581: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.584: INFO: Unable to read jessie_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.588: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:40.613: INFO: Lookups using dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4867 wheezy_tcp@dns-test-service.dns-4867 wheezy_udp@dns-test-service.dns-4867.svc wheezy_tcp@dns-test-service.dns-4867.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4867 jessie_tcp@dns-test-service.dns-4867 jessie_udp@dns-test-service.dns-4867.svc jessie_tcp@dns-test-service.dns-4867.svc]

  E0518 10:09:41.132031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:42.133154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:43.133526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:44.134039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:45.134163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:09:45.482: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.485: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.489: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.493: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.500: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.526: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.529: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.533: INFO: Unable to read jessie_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.537: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.541: INFO: Unable to read jessie_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.545: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:45.564: INFO: Lookups using dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4867 wheezy_tcp@dns-test-service.dns-4867 wheezy_udp@dns-test-service.dns-4867.svc wheezy_tcp@dns-test-service.dns-4867.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4867 jessie_tcp@dns-test-service.dns-4867 jessie_udp@dns-test-service.dns-4867.svc jessie_tcp@dns-test-service.dns-4867.svc]

  E0518 10:09:46.134830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:47.135795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:48.137452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:49.138109      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:50.138275      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:09:50.487: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.495: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.501: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.507: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.514: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.521: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.558: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.564: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.570: INFO: Unable to read jessie_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.575: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.581: INFO: Unable to read jessie_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.585: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:50.613: INFO: Lookups using dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4867 wheezy_tcp@dns-test-service.dns-4867 wheezy_udp@dns-test-service.dns-4867.svc wheezy_tcp@dns-test-service.dns-4867.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4867 jessie_tcp@dns-test-service.dns-4867 jessie_udp@dns-test-service.dns-4867.svc jessie_tcp@dns-test-service.dns-4867.svc]

  E0518 10:09:51.138433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:52.139415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:53.139727      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:54.139825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:55.139986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:09:55.483: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.487: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.491: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.495: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.499: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.504: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.535: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.539: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.545: INFO: Unable to read jessie_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.549: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.555: INFO: Unable to read jessie_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.559: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:09:55.585: INFO: Lookups using dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4867 wheezy_tcp@dns-test-service.dns-4867 wheezy_udp@dns-test-service.dns-4867.svc wheezy_tcp@dns-test-service.dns-4867.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4867 jessie_tcp@dns-test-service.dns-4867 jessie_udp@dns-test-service.dns-4867.svc jessie_tcp@dns-test-service.dns-4867.svc]

  E0518 10:09:56.141205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:57.141397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:58.142043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:09:59.142683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:00.142859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:10:00.484: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.488: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.513: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.522: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.527: INFO: Unable to read wheezy_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.532: INFO: Unable to read wheezy_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.557: INFO: Unable to read jessie_udp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.561: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.565: INFO: Unable to read jessie_udp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.569: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867 from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.573: INFO: Unable to read jessie_udp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.577: INFO: Unable to read jessie_tcp@dns-test-service.dns-4867.svc from pod dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289: the server could not find the requested resource (get pods dns-test-4a1cc4c0-caca-457a-985e-457e0c818289)
  May 18 10:10:00.595: INFO: Lookups using dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-4867 wheezy_tcp@dns-test-service.dns-4867 wheezy_udp@dns-test-service.dns-4867.svc wheezy_tcp@dns-test-service.dns-4867.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-4867 jessie_tcp@dns-test-service.dns-4867 jessie_udp@dns-test-service.dns-4867.svc jessie_tcp@dns-test-service.dns-4867.svc]

  E0518 10:10:01.144004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:02.144919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:03.145451      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:04.148307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:05.148621      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:10:05.630: INFO: DNS probes using dns-4867/dns-test-4a1cc4c0-caca-457a-985e-457e0c818289 succeeded

  May 18 10:10:05.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 10:10:05.637
  STEP: deleting the test service @ 05/18/23 10:10:05.663
  STEP: deleting the test headless service @ 05/18/23 10:10:05.709
  STEP: Destroying namespace "dns-4867" for this suite. @ 05/18/23 10:10:05.74
• [34.510 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/18/23 10:10:05.753
  May 18 10:10:05.753: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 10:10:05.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:10:05.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:10:05.775
  STEP: Creating secret with name secret-test-2878e24d-1824-4c9b-8363-b5fc9a35fc47 @ 05/18/23 10:10:05.779
  STEP: Creating a pod to test consume secrets @ 05/18/23 10:10:05.785
  E0518 10:10:06.150749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:07.150892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:08.151478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:09.152439      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:10:09.807
  May 18 10:10:09.811: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-secrets-1725546a-94bf-4b42-8d30-89d6cb1ea71a container secret-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 10:10:09.842
  May 18 10:10:09.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9659" for this suite. @ 05/18/23 10:10:09.864
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/18/23 10:10:09.883
  May 18 10:10:09.883: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename replication-controller @ 05/18/23 10:10:09.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:10:09.904
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:10:09.908
  STEP: Creating replication controller my-hostname-basic-0f53848b-f601-4065-b430-8732b53f5fc7 @ 05/18/23 10:10:09.913
  May 18 10:10:09.922: INFO: Pod name my-hostname-basic-0f53848b-f601-4065-b430-8732b53f5fc7: Found 0 pods out of 1
  E0518 10:10:10.153484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:11.154217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:12.154485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:13.155030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:14.155135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:10:14.929: INFO: Pod name my-hostname-basic-0f53848b-f601-4065-b430-8732b53f5fc7: Found 1 pods out of 1
  May 18 10:10:14.930: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-0f53848b-f601-4065-b430-8732b53f5fc7" are running
  May 18 10:10:14.934: INFO: Pod "my-hostname-basic-0f53848b-f601-4065-b430-8732b53f5fc7-hsx6n" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-18 10:10:09 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-18 10:10:11 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-18 10:10:11 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-18 10:10:09 +0000 UTC Reason: Message:}])
  May 18 10:10:14.935: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/18/23 10:10:14.935
  May 18 10:10:14.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-28" for this suite. @ 05/18/23 10:10:14.956
• [5.079 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/18/23 10:10:14.967
  May 18 10:10:14.967: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename containers @ 05/18/23 10:10:14.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:10:14.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:10:14.994
  STEP: Creating a pod to test override arguments @ 05/18/23 10:10:14.999
  E0518 10:10:15.155729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:16.156831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:17.156956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:18.157382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:10:19.023
  May 18 10:10:19.028: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod client-containers-a582d6fe-f1ca-4e95-9edd-8037f5f4d9c1 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 10:10:19.041
  May 18 10:10:19.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8134" for this suite. @ 05/18/23 10:10:19.074
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/18/23 10:10:19.101
  May 18 10:10:19.101: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 10:10:19.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:10:19.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:10:19.143
  STEP: Creating configMap with name projected-configmap-test-volume-map-cd4216b6-97dc-4e37-b04b-0c5767a685df @ 05/18/23 10:10:19.15
  STEP: Creating a pod to test consume configMaps @ 05/18/23 10:10:19.157
  E0518 10:10:19.158258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:20.170044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:21.172756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:22.173412      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:23.173549      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:10:23.193
  May 18 10:10:23.195: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-configmaps-c57c1add-540a-473c-aeba-8a171368302c container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 10:10:23.201
  May 18 10:10:23.211: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4096" for this suite. @ 05/18/23 10:10:23.216
• [4.121 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/18/23 10:10:23.225
  May 18 10:10:23.225: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename cronjob @ 05/18/23 10:10:23.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:10:23.244
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:10:23.248
  STEP: Creating a cronjob @ 05/18/23 10:10:23.251
  STEP: creating @ 05/18/23 10:10:23.251
  STEP: getting @ 05/18/23 10:10:23.256
  STEP: listing @ 05/18/23 10:10:23.258
  STEP: watching @ 05/18/23 10:10:23.261
  May 18 10:10:23.261: INFO: starting watch
  STEP: cluster-wide listing @ 05/18/23 10:10:23.263
  STEP: cluster-wide watching @ 05/18/23 10:10:23.265
  May 18 10:10:23.266: INFO: starting watch
  STEP: patching @ 05/18/23 10:10:23.267
  STEP: updating @ 05/18/23 10:10:23.273
  May 18 10:10:23.281: INFO: waiting for watch events with expected annotations
  May 18 10:10:23.281: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/18/23 10:10:23.282
  STEP: updating /status @ 05/18/23 10:10:23.288
  STEP: get /status @ 05/18/23 10:10:23.295
  STEP: deleting @ 05/18/23 10:10:23.298
  STEP: deleting a collection @ 05/18/23 10:10:23.308
  May 18 10:10:23.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8647" for this suite. @ 05/18/23 10:10:23.319
• [0.099 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/18/23 10:10:23.325
  May 18 10:10:23.325: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename gc @ 05/18/23 10:10:23.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:10:23.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:10:23.34
  STEP: create the rc @ 05/18/23 10:10:23.344
  W0518 10:10:23.347983      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0518 10:10:24.174873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:25.175522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:26.175946      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:27.176813      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:28.176988      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/18/23 10:10:28.353
  STEP: wait for all pods to be garbage collected @ 05/18/23 10:10:28.359
  E0518 10:10:29.177186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:30.177413      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:31.178116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:32.179127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:33.179237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/18/23 10:10:33.371
  May 18 10:10:33.600: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 18 10:10:33.600: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-54" for this suite. @ 05/18/23 10:10:33.608
• [10.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/18/23 10:10:33.623
  May 18 10:10:33.623: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 10:10:33.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:10:33.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:10:33.647
  STEP: creating Agnhost RC @ 05/18/23 10:10:33.65
  May 18 10:10:33.650: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-3714 create -f -'
  E0518 10:10:34.180457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:10:34.260: INFO: stderr: ""
  May 18 10:10:34.260: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/18/23 10:10:34.26
  E0518 10:10:35.181226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:10:35.266: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 10:10:35.266: INFO: Found 0 / 1
  E0518 10:10:36.181957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:10:36.266: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 10:10:36.267: INFO: Found 1 / 1
  May 18 10:10:36.268: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/18/23 10:10:36.269
  May 18 10:10:36.273: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 10:10:36.273: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 18 10:10:36.273: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-3714 patch pod agnhost-primary-pr82c -p {"metadata":{"annotations":{"x":"y"}}}'
  May 18 10:10:36.415: INFO: stderr: ""
  May 18 10:10:36.415: INFO: stdout: "pod/agnhost-primary-pr82c patched\n"
  STEP: checking annotations @ 05/18/23 10:10:36.415
  May 18 10:10:36.420: INFO: Selector matched 1 pods for map[app:agnhost]
  May 18 10:10:36.420: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 18 10:10:36.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3714" for this suite. @ 05/18/23 10:10:36.424
• [2.809 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/18/23 10:10:36.433
  May 18 10:10:36.433: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 10:10:36.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:10:36.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:10:36.455
  STEP: Creating pod test-grpc-a96dac29-e233-4b14-bbce-278d31efbf6f in namespace container-probe-1728 @ 05/18/23 10:10:36.459
  E0518 10:10:37.183797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:38.183968      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:10:38.478: INFO: Started pod test-grpc-a96dac29-e233-4b14-bbce-278d31efbf6f in namespace container-probe-1728
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/18/23 10:10:38.479
  May 18 10:10:38.483: INFO: Initial restart count of pod test-grpc-a96dac29-e233-4b14-bbce-278d31efbf6f is 0
  E0518 10:10:39.184289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:40.185049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:41.185226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:42.186163      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:43.186484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:44.186672      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:45.186885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:46.187744      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:47.188562      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:48.188750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:49.188907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:50.189137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:51.189256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:52.189913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:53.191221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:54.191092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:55.191493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:56.192093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:57.192449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:58.192737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:10:59.193188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:00.193252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:01.193523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:02.193894      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:03.194161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:04.194681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:05.195364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:06.196499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:07.198784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:08.197474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:09.198165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:10.198572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:11.199226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:12.199285      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:13.200184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:14.201079      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:15.202142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:16.202388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:17.203594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:18.203763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:19.204242      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:20.204519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:21.204606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:22.205236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:23.206147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:24.206323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:25.206927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:26.206840      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:27.207152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:28.207991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:29.208961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:30.209459      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:31.209516      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:32.209662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:33.210296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:34.210468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:35.210764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:36.210905      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:37.212262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:38.212283      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:39.212357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:40.212522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:41.212947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:42.213104      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:43.213393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:44.214030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:45.214218      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:46.214633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:47.214907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:48.215015      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:49.215772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:50.216408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:51.216522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:52.217551      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:53.218337      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:54.218891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:55.219221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:56.219534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:57.220029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:58.220253      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:11:59.220484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:00.221116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:01.222090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:02.222701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:03.223534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:04.223838      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:05.224055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:06.224237      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:07.224352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:08.225058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:09.225270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:10.226070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:11.226732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:12.227030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:13.227274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:14.227532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:15.227595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:16.228222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:17.228397      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:18.228600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:19.228743      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:20.229043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:21.230008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:22.230422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:23.230361      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:24.230601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:25.231367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:26.231542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:27.231736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:28.231957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:29.233210      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:30.233105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:31.233221      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:32.234398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:33.234418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:34.235156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:35.235560      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:36.236197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:37.236145      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:38.236354      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:39.237143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:40.237279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:41.238226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:42.239179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:43.239976      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:44.240552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:45.241326      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:46.241550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:47.241729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:48.242209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:49.243319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:50.243564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:51.243808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:52.243953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:53.244207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:54.244358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:55.244689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:56.244799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:57.245739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:58.245837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:12:59.246995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:00.247220      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:01.248194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:02.248768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:03.249423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:04.249624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:05.250252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:06.250420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:07.250694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:08.251334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:09.252247      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:10.252607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:11.253328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:12.253607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:13.253574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:14.253996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:15.254600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:16.254980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:17.255977      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:18.256533      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:19.257048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:20.258552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:21.257609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:22.258638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:23.259939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:24.259673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:25.260768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:26.261300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:27.262076      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:28.262465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:29.262245      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:30.263018      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:31.263419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:32.264373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:33.264934      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:34.264803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:35.265485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:36.266404      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:37.266955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:38.267095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:39.268379      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:40.268387      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:41.268695      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:42.268670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:43.269691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:44.269959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:45.270150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:46.270333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:47.271093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:48.271147      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:49.271517      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:50.271485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:51.272135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:52.272306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:53.273455      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:54.273771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:55.274008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:56.274127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:57.275460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:58.276194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:13:59.277034      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:00.277662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:01.278230      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:02.278542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:03.278880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:04.278689      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:05.279566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:06.281096      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:07.282808      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:08.282171      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:09.282731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:10.283162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:11.283443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:12.283640      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:13.284729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:14.285890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:15.286544      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:16.286844      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:17.286965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:18.287148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:19.287659      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:20.287823      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:21.288679      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:22.288994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:23.289731      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:24.290472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:25.290729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:26.291461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:27.292261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:28.292728      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:29.293154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:30.294012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:31.294989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:32.295920      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:33.296434      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:34.297282      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:35.297863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:36.298420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:37.299276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:38.300089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:14:39.180: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 10:14:39.187
  STEP: Destroying namespace "container-probe-1728" for this suite. @ 05/18/23 10:14:39.199
• [242.790 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/18/23 10:14:39.285
  May 18 10:14:39.285: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename secrets @ 05/18/23 10:14:39.287
  E0518 10:14:39.301128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:14:39.306
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:14:39.311
  STEP: Creating projection with secret that has name secret-emptykey-test-3433de17-7503-469d-b41c-25aa3e05b5c4 @ 05/18/23 10:14:39.321
  May 18 10:14:39.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2910" for this suite. @ 05/18/23 10:14:39.329
• [0.050 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/18/23 10:14:39.334
  May 18 10:14:39.334: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename watch @ 05/18/23 10:14:39.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:14:39.35
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:14:39.353
  STEP: creating a watch on configmaps with label A @ 05/18/23 10:14:39.356
  STEP: creating a watch on configmaps with label B @ 05/18/23 10:14:39.358
  STEP: creating a watch on configmaps with label A or B @ 05/18/23 10:14:39.359
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/18/23 10:14:39.361
  May 18 10:14:39.365: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4335  1f901fcf-21b0-41cd-aafc-3647535be35d 1967046 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:14:39.366: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4335  1f901fcf-21b0-41cd-aafc-3647535be35d 1967046 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/18/23 10:14:39.366
  May 18 10:14:39.382: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4335  1f901fcf-21b0-41cd-aafc-3647535be35d 1967047 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:14:39.382: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4335  1f901fcf-21b0-41cd-aafc-3647535be35d 1967047 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/18/23 10:14:39.382
  May 18 10:14:39.390: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4335  1f901fcf-21b0-41cd-aafc-3647535be35d 1967048 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:14:39.390: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4335  1f901fcf-21b0-41cd-aafc-3647535be35d 1967048 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/18/23 10:14:39.39
  May 18 10:14:39.395: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4335  1f901fcf-21b0-41cd-aafc-3647535be35d 1967049 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:14:39.395: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4335  1f901fcf-21b0-41cd-aafc-3647535be35d 1967049 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/18/23 10:14:39.395
  May 18 10:14:39.399: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4335  b0ae536b-4d75-49fe-9354-c6a1f92b2757 1967050 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:14:39.400: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4335  b0ae536b-4d75-49fe-9354-c6a1f92b2757 1967050 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0518 10:14:40.301880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:41.302317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:42.303553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:43.304315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:44.306481      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:45.305772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:46.306022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:47.313097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:48.313357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:49.313960      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/18/23 10:14:49.401
  May 18 10:14:49.409: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4335  b0ae536b-4d75-49fe-9354-c6a1f92b2757 1967110 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:14:49.410: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4335  b0ae536b-4d75-49fe-9354-c6a1f92b2757 1967110 0 2023-05-18 10:14:39 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-18 10:14:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0518 10:14:50.314172      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:51.314371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:52.314619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:53.314887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:54.314944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:55.315257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:56.315833      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:57.317039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:58.317641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:14:59.317783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:14:59.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4335" for this suite. @ 05/18/23 10:14:59.416
• [20.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/18/23 10:14:59.424
  May 18 10:14:59.424: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename var-expansion @ 05/18/23 10:14:59.426
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:14:59.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:14:59.444
  STEP: creating the pod @ 05/18/23 10:14:59.448
  STEP: waiting for pod running @ 05/18/23 10:14:59.455
  E0518 10:15:00.317965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:01.318255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 05/18/23 10:15:01.465
  May 18 10:15:01.468: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2239 PodName:var-expansion-d1a38fc2-d8bc-4eec-b085-268f73ef79bf ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:15:01.468: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:15:01.469: INFO: ExecWithOptions: Clientset creation
  May 18 10:15:01.469: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2239/pods/var-expansion-d1a38fc2-d8bc-4eec-b085-268f73ef79bf/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/18/23 10:15:01.575
  May 18 10:15:01.578: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2239 PodName:var-expansion-d1a38fc2-d8bc-4eec-b085-268f73ef79bf ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 18 10:15:01.579: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  May 18 10:15:01.580: INFO: ExecWithOptions: Clientset creation
  May 18 10:15:01.580: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2239/pods/var-expansion-d1a38fc2-d8bc-4eec-b085-268f73ef79bf/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/18/23 10:15:01.655
  May 18 10:15:02.179: INFO: Successfully updated pod "var-expansion-d1a38fc2-d8bc-4eec-b085-268f73ef79bf"
  STEP: waiting for annotated pod running @ 05/18/23 10:15:02.182
  STEP: deleting the pod gracefully @ 05/18/23 10:15:02.187
  May 18 10:15:02.188: INFO: Deleting pod "var-expansion-d1a38fc2-d8bc-4eec-b085-268f73ef79bf" in namespace "var-expansion-2239"
  May 18 10:15:02.207: INFO: Wait up to 5m0s for pod "var-expansion-d1a38fc2-d8bc-4eec-b085-268f73ef79bf" to be fully deleted
  E0518 10:15:02.319025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:03.319260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:04.320484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:05.321391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:06.321736      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:07.321850      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:08.322710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:09.322650      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:10.323074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:11.323430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:12.323662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:13.324684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:14.325199      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:15.326089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:16.332383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:17.327099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:18.329366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:19.329714      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:20.329778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:21.329924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:22.330675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:23.330799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:24.331570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:25.332174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:26.332643      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:27.333315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:28.334193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:29.334536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:30.335703      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:31.336279      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:32.337367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:33.337607      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:34.337834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:35.338200      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:15:36.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2239" for this suite. @ 05/18/23 10:15:36.33
  E0518 10:15:36.339804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
• [36.916 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/18/23 10:15:36.348
  May 18 10:15:36.348: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 10:15:36.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:36.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:36.376
  STEP: Creating configMap with name configmap-test-volume-f50cf0c4-f687-434c-9a88-4059cb541025 @ 05/18/23 10:15:36.38
  STEP: Creating a pod to test consume configMaps @ 05/18/23 10:15:36.386
  E0518 10:15:37.340414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:38.340369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:39.340542      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:40.341514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:15:40.413
  May 18 10:15:40.417: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-d584dd73-ebf6-4261-9d78-422ab7bc71c0 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 10:15:40.448
  May 18 10:15:40.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2685" for this suite. @ 05/18/23 10:15:40.466
• [4.124 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/18/23 10:15:40.474
  May 18 10:15:40.474: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 10:15:40.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:40.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:40.492
  STEP: Setting up server cert @ 05/18/23 10:15:40.51
  E0518 10:15:41.341513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 10:15:41.449
  STEP: Deploying the webhook pod @ 05/18/23 10:15:41.458
  STEP: Wait for the deployment to be ready @ 05/18/23 10:15:41.47
  May 18 10:15:41.482: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 10:15:42.342345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:43.343225      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 10:15:43.498
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 10:15:43.517
  E0518 10:15:44.343342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:15:44.519: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/18/23 10:15:44.594
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/18/23 10:15:44.661
  STEP: Deleting the collection of validation webhooks @ 05/18/23 10:15:44.702
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/18/23 10:15:44.741
  May 18 10:15:44.749: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7427" for this suite. @ 05/18/23 10:15:44.79
  STEP: Destroying namespace "webhook-markers-4794" for this suite. @ 05/18/23 10:15:44.798
• [4.340 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/18/23 10:15:44.818
  May 18 10:15:44.818: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename job @ 05/18/23 10:15:44.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:44.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:44.848
  STEP: Creating Indexed job @ 05/18/23 10:15:44.852
  STEP: Ensuring job reaches completions @ 05/18/23 10:15:44.857
  E0518 10:15:45.344699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:46.344141      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:47.344599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:48.345154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:49.345475      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:50.346619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:51.346978      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:52.347966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:53.348936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:54.349606      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 05/18/23 10:15:54.866
  May 18 10:15:54.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3599" for this suite. @ 05/18/23 10:15:54.88
• [10.071 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/18/23 10:15:54.892
  May 18 10:15:54.892: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 10:15:54.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:54.919
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:54.924
  May 18 10:15:54.929: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: creating the pod @ 05/18/23 10:15:54.931
  STEP: submitting the pod to kubernetes @ 05/18/23 10:15:54.931
  E0518 10:15:55.349890      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:56.350017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:15:57.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4327" for this suite. @ 05/18/23 10:15:57.089
• [2.205 seconds]
------------------------------
S
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/18/23 10:15:57.099
  May 18 10:15:57.099: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 10:15:57.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:57.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:57.128
  STEP: creating the pod @ 05/18/23 10:15:57.132
  STEP: submitting the pod to kubernetes @ 05/18/23 10:15:57.132
  STEP: verifying QOS class is set on the pod @ 05/18/23 10:15:57.143
  May 18 10:15:57.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2800" for this suite. @ 05/18/23 10:15:57.149
• [0.062 seconds]
------------------------------
S
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/18/23 10:15:57.162
  May 18 10:15:57.162: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/18/23 10:15:57.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:57.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:57.194
  STEP: creating @ 05/18/23 10:15:57.198
  STEP: getting @ 05/18/23 10:15:57.212
  STEP: listing @ 05/18/23 10:15:57.217
  STEP: deleting @ 05/18/23 10:15:57.22
  May 18 10:15:57.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3263" for this suite. @ 05/18/23 10:15:57.236
• [0.078 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/18/23 10:15:57.242
  May 18 10:15:57.242: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename podtemplate @ 05/18/23 10:15:57.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:57.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:57.271
  May 18 10:15:57.304: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3590" for this suite. @ 05/18/23 10:15:57.308
• [0.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/18/23 10:15:57.315
  May 18 10:15:57.315: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename podtemplate @ 05/18/23 10:15:57.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:57.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:57.333
  STEP: Create set of pod templates @ 05/18/23 10:15:57.336
  May 18 10:15:57.341: INFO: created test-podtemplate-1
  May 18 10:15:57.346: INFO: created test-podtemplate-2
  May 18 10:15:57.350: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/18/23 10:15:57.35
  E0518 10:15:57.350652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete collection of pod templates @ 05/18/23 10:15:57.353
  May 18 10:15:57.353: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/18/23 10:15:57.363
  May 18 10:15:57.363: INFO: requesting list of pod templates to confirm quantity
  May 18 10:15:57.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2099" for this suite. @ 05/18/23 10:15:57.368
• [0.058 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/18/23 10:15:57.374
  May 18 10:15:57.374: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename namespaces @ 05/18/23 10:15:57.376
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:57.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:57.39
  STEP: Creating namespace "e2e-ns-4d7nv" @ 05/18/23 10:15:57.394
  May 18 10:15:57.403: INFO: Namespace "e2e-ns-4d7nv-6410" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-4d7nv-6410" @ 05/18/23 10:15:57.404
  May 18 10:15:57.413: INFO: Namespace "e2e-ns-4d7nv-6410" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-4d7nv-6410" @ 05/18/23 10:15:57.413
  May 18 10:15:57.425: INFO: Namespace "e2e-ns-4d7nv-6410" has []v1.FinalizerName{"kubernetes"}
  May 18 10:15:57.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7100" for this suite. @ 05/18/23 10:15:57.438
  STEP: Destroying namespace "e2e-ns-4d7nv-6410" for this suite. @ 05/18/23 10:15:57.443
• [0.075 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/18/23 10:15:57.451
  May 18 10:15:57.451: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename watch @ 05/18/23 10:15:57.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:57.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:57.47
  STEP: creating a watch on configmaps @ 05/18/23 10:15:57.473
  STEP: creating a new configmap @ 05/18/23 10:15:57.475
  STEP: modifying the configmap once @ 05/18/23 10:15:57.479
  STEP: closing the watch once it receives two notifications @ 05/18/23 10:15:57.485
  May 18 10:15:57.485: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1299  8de8fbff-6b39-49be-b24d-1d3d884bd76f 1967705 0 2023-05-18 10:15:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-18 10:15:57 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:15:57.486: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1299  8de8fbff-6b39-49be-b24d-1d3d884bd76f 1967706 0 2023-05-18 10:15:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-18 10:15:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/18/23 10:15:57.486
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/18/23 10:15:57.493
  STEP: deleting the configmap @ 05/18/23 10:15:57.495
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/18/23 10:15:57.499
  May 18 10:15:57.499: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1299  8de8fbff-6b39-49be-b24d-1d3d884bd76f 1967707 0 2023-05-18 10:15:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-18 10:15:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:15:57.499: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1299  8de8fbff-6b39-49be-b24d-1d3d884bd76f 1967708 0 2023-05-18 10:15:57 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-18 10:15:57 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:15:57.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1299" for this suite. @ 05/18/23 10:15:57.503
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/18/23 10:15:57.527
  May 18 10:15:57.527: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename crd-webhook @ 05/18/23 10:15:57.529
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:15:57.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:15:57.545
  STEP: Setting up server cert @ 05/18/23 10:15:57.548
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/18/23 10:15:57.793
  STEP: Deploying the custom resource conversion webhook pod @ 05/18/23 10:15:57.799
  STEP: Wait for the deployment to be ready @ 05/18/23 10:15:57.811
  May 18 10:15:57.817: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0518 10:15:58.351175      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:15:59.351741      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 10:15:59.832
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 10:15:59.89
  E0518 10:16:00.352465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:00.891: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 18 10:16:00.897: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 10:16:01.353358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:02.353963      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:03.354374      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/18/23 10:16:03.51
  STEP: v2 custom resource should be converted @ 05/18/23 10:16:03.516
  May 18 10:16:03.525: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-5232" for this suite. @ 05/18/23 10:16:04.127
• [6.610 seconds]
------------------------------
SSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/18/23 10:16:04.138
  May 18 10:16:04.138: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/18/23 10:16:04.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:04.152
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:04.164
  STEP: creating @ 05/18/23 10:16:04.174
  STEP: getting @ 05/18/23 10:16:04.193
  STEP: listing in namespace @ 05/18/23 10:16:04.198
  STEP: patching @ 05/18/23 10:16:04.201
  STEP: deleting @ 05/18/23 10:16:04.21
  May 18 10:16:04.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3592" for this suite. @ 05/18/23 10:16:04.229
• [0.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/18/23 10:16:04.251
  May 18 10:16:04.251: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 10:16:04.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:04.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:04.27
  STEP: Creating the pod @ 05/18/23 10:16:04.274
  E0518 10:16:04.355069      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:05.355999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:06.355936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:06.827: INFO: Successfully updated pod "annotationupdatef181009b-6d2a-4b1b-ac08-901ddcd4332a"
  E0518 10:16:07.356070      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:08.356483      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:08.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5263" for this suite. @ 05/18/23 10:16:08.865
• [4.625 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/18/23 10:16:08.884
  May 18 10:16:08.885: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 10:16:08.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:08.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:08.918
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/18/23 10:16:08.923
  E0518 10:16:09.356882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:10.357663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:11.374710      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:12.366107      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:16:12.949
  May 18 10:16:12.955: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-0a74791b-f198-4ea6-a988-63ae1f4d1da8 container test-container: <nil>
  STEP: delete the pod @ 05/18/23 10:16:12.965
  May 18 10:16:12.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1723" for this suite. @ 05/18/23 10:16:12.99
• [4.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/18/23 10:16:13.011
  May 18 10:16:13.012: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename daemonsets @ 05/18/23 10:16:13.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:13.03
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:13.035
  STEP: Creating simple DaemonSet "daemon-set" @ 05/18/23 10:16:13.059
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/18/23 10:16:13.067
  May 18 10:16:13.072: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:13.078: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 10:16:13.080: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:16:13.367280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:14.086: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:14.091: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 10:16:14.091: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:16:14.368009      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:15.087: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:15.091: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 10:16:15.092: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:16:15.368195      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:16.089: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:16.095: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 10:16:16.095: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/18/23 10:16:16.102
  May 18 10:16:16.140: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:16.147: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 18 10:16:16.147: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:16:16.368417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:17.155: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:17.160: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 18 10:16:17.161: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:16:17.368914      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:18.155: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:18.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 18 10:16:18.163: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:16:18.370394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:19.157: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:19.165: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 18 10:16:19.165: INFO: Node ck-test-kube-1-27-worker is running 0 daemon pod, expected 1
  E0518 10:16:19.371391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:20.153: INFO: DaemonSet pods can't tolerate node ck-test-kube-1-27-master with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 18 10:16:20.157: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 18 10:16:20.157: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/18/23 10:16:20.161
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5841, will wait for the garbage collector to delete the pods @ 05/18/23 10:16:20.161
  May 18 10:16:20.220: INFO: Deleting DaemonSet.extensions daemon-set took: 5.512572ms
  May 18 10:16:20.322: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.283002ms
  E0518 10:16:20.371269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:21.371407      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:22.327: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 18 10:16:22.327: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 18 10:16:22.330: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"1968133"},"items":null}

  May 18 10:16:22.333: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"1968133"},"items":null}

  May 18 10:16:22.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5841" for this suite. @ 05/18/23 10:16:22.351
• [9.358 seconds]
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  E0518 10:16:22.373485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a kubernetes client @ 05/18/23 10:16:22.374
  May 18 10:16:22.375: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename statefulset @ 05/18/23 10:16:22.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:22.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:22.416
  STEP: Creating service test in namespace statefulset-4241 @ 05/18/23 10:16:22.422
  STEP: Creating statefulset ss in namespace statefulset-4241 @ 05/18/23 10:16:22.442
  May 18 10:16:22.457: INFO: Found 0 stateful pods, waiting for 1
  E0518 10:16:23.373478      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:24.373678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:25.373984      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:26.374691      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:27.375819      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:28.385433      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:29.378474      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:30.378610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:31.378777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:32.378998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:32.466: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/18/23 10:16:32.474
  STEP: Getting /status @ 05/18/23 10:16:32.482
  May 18 10:16:32.487: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/18/23 10:16:32.488
  May 18 10:16:32.498: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/18/23 10:16:32.499
  May 18 10:16:32.501: INFO: Observed &StatefulSet event: ADDED
  May 18 10:16:32.501: INFO: Found Statefulset ss in namespace statefulset-4241 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 18 10:16:32.501: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/18/23 10:16:32.501
  May 18 10:16:32.501: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 18 10:16:32.510: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/18/23 10:16:32.51
  May 18 10:16:32.513: INFO: Observed &StatefulSet event: ADDED
  May 18 10:16:32.513: INFO: Observed Statefulset ss in namespace statefulset-4241 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 18 10:16:32.513: INFO: Observed &StatefulSet event: MODIFIED
  May 18 10:16:32.513: INFO: Deleting all statefulset in ns statefulset-4241
  May 18 10:16:32.517: INFO: Scaling statefulset ss to 0
  E0518 10:16:33.380087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:34.380454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:35.380382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:36.381231      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:37.380708      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:38.381176      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:39.381724      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:40.382150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:41.383235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:42.383699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:42.538: INFO: Waiting for statefulset status.replicas updated to 0
  May 18 10:16:42.540: INFO: Deleting statefulset ss
  May 18 10:16:42.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4241" for this suite. @ 05/18/23 10:16:42.556
• [20.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/18/23 10:16:42.568
  May 18 10:16:42.568: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename security-context-test @ 05/18/23 10:16:42.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:42.586
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:42.59
  E0518 10:16:43.385473      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:44.386398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:45.386910      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:46.387248      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:46.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9015" for this suite. @ 05/18/23 10:16:46.614
• [4.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/18/23 10:16:46.631
  May 18 10:16:46.631: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename security-context @ 05/18/23 10:16:46.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:46.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:46.654
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/18/23 10:16:46.658
  E0518 10:16:47.391742      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:48.388125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:49.388384      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:50.389039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:16:50.679
  May 18 10:16:50.682: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod security-context-646ed363-1ae7-4644-b474-6662566ee661 container test-container: <nil>
  STEP: delete the pod @ 05/18/23 10:16:50.688
  May 18 10:16:50.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-8029" for this suite. @ 05/18/23 10:16:50.709
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/18/23 10:16:50.721
  May 18 10:16:50.721: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename watch @ 05/18/23 10:16:50.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:50.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:50.743
  STEP: getting a starting resourceVersion @ 05/18/23 10:16:50.747
  STEP: starting a background goroutine to produce watch events @ 05/18/23 10:16:50.75
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/18/23 10:16:50.751
  E0518 10:16:51.388995      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:52.390101      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:53.390879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:53.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4671" for this suite. @ 05/18/23 10:16:53.585
• [2.910 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/18/23 10:16:53.632
  May 18 10:16:53.632: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 10:16:53.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:53.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:53.663
  STEP: Creating configMap with name configmap-projected-all-test-volume-3bd8b760-f7ce-44f8-8876-3c62aa3798fd @ 05/18/23 10:16:53.681
  STEP: Creating secret with name secret-projected-all-test-volume-4c25ac06-52a1-42e5-835d-8ca14a2e3627 @ 05/18/23 10:16:53.689
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/18/23 10:16:53.696
  E0518 10:16:54.393790      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:55.393800      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:56.393766      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:57.394428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:16:57.74
  May 18 10:16:57.746: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod projected-volume-51c099a6-17e7-4115-abe3-75f1797ced7d container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/18/23 10:16:57.759
  May 18 10:16:57.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5044" for this suite. @ 05/18/23 10:16:57.786
• [4.161 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/18/23 10:16:57.803
  May 18 10:16:57.803: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 10:16:57.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:16:57.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:16:57.842
  STEP: Creating pod liveness-7619d76b-3efb-4cbd-a788-569f975b388a in namespace container-probe-4045 @ 05/18/23 10:16:57.85
  E0518 10:16:58.394355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:16:59.395428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:16:59.883: INFO: Started pod liveness-7619d76b-3efb-4cbd-a788-569f975b388a in namespace container-probe-4045
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/18/23 10:16:59.883
  May 18 10:16:59.889: INFO: Initial restart count of pod liveness-7619d76b-3efb-4cbd-a788-569f975b388a is 0
  E0518 10:17:00.395505      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:01.396431      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:02.396956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:03.397422      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:04.398284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:05.398901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:06.399371      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:07.400277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:08.401168      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:09.401539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:10.401595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:11.401856      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:12.402301      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:13.403300      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:14.403627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:15.404367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:16.404408      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:17.404559      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:18.407061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:19.406657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:17:19.949: INFO: Restart count of pod container-probe-4045/liveness-7619d76b-3efb-4cbd-a788-569f975b388a is now 1 (20.059549661s elapsed)
  E0518 10:17:20.405854      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:21.406013      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:22.406178      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:23.406539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:24.407530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:25.407951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:26.409219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:27.409469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:28.409969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:29.410986      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:30.411468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:31.412222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:32.412060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:33.412153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:34.412545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:35.413064      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:36.414241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:37.414638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:38.414798      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:39.415398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:17:40.017: INFO: Restart count of pod container-probe-4045/liveness-7619d76b-3efb-4cbd-a788-569f975b388a is now 2 (40.127676554s elapsed)
  E0518 10:17:40.415892      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:41.416529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:42.417385      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:43.418530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:44.418893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:45.419467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:46.420309      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:47.421633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:48.421658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:49.421872      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:50.422091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:51.422383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:52.422351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:53.423754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:54.423418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:55.424179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:56.425040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:57.426344      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:58.426662      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:17:59.427134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:18:00.077: INFO: Restart count of pod container-probe-4045/liveness-7619d76b-3efb-4cbd-a788-569f975b388a is now 3 (1m0.187650985s elapsed)
  E0518 10:18:00.428317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:01.429296      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:02.430357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:03.431192      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:04.431922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:05.432157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:06.432135      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:07.433089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:08.434046      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:09.434971      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:10.435041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:11.434827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:12.435151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:13.435193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:14.435633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:15.435899      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:16.436658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:17.437769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:18.437948      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:19.438684      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:18:20.141: INFO: Restart count of pod container-probe-4045/liveness-7619d76b-3efb-4cbd-a788-569f975b388a is now 4 (1m20.251566102s elapsed)
  E0518 10:18:20.438943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:21.438975      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:22.440152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:23.442870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:24.444152      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:25.444325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:26.444392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:27.444955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:28.445570      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:29.446234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:30.447186      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:31.447333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:32.447780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:33.448322      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:34.448812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:35.449185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:36.449777      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:37.449950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:38.451810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:39.451430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:40.452130      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:41.452831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:42.454036      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:43.454391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:44.455269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:45.456100      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:46.457110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:47.457801      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:48.458632      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:49.459424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:50.460174      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:51.461049      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:52.461205      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:53.461626      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:54.461655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:55.461919      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:56.462356      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:57.462629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:58.463343      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:18:59.463641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:00.463778      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:01.464674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:02.465526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:03.466154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:04.466817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:05.467066      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:06.467119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:07.468251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:08.469048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:09.469165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:10.470004      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:11.470768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:12.471297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:13.471687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:14.471756      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:15.472526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:16.472669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:17.472949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:18.473642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:19.474453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:20.475000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:21.476014      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:22.476207      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:23.476472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:24.477468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:25.477595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:26.478182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:27.479116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:28.480564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:29.480600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:19:30.339: INFO: Restart count of pod container-probe-4045/liveness-7619d76b-3efb-4cbd-a788-569f975b388a is now 5 (2m30.449439351s elapsed)
  May 18 10:19:30.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 10:19:30.345
  STEP: Destroying namespace "container-probe-4045" for this suite. @ 05/18/23 10:19:30.368
• [152.576 seconds]
------------------------------
SSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/18/23 10:19:30.404
  May 18 10:19:30.407: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename events @ 05/18/23 10:19:30.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:30.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:30.454
  STEP: Create set of events @ 05/18/23 10:19:30.46
  May 18 10:19:30.472: INFO: created test-event-1
  May 18 10:19:30.476: INFO: created test-event-2
  May 18 10:19:30.480: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/18/23 10:19:30.48
  E0518 10:19:30.480958      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete collection of events @ 05/18/23 10:19:30.483
  May 18 10:19:30.483: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/18/23 10:19:30.497
  May 18 10:19:30.497: INFO: requesting list of events to confirm quantity
  May 18 10:19:30.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6802" for this suite. @ 05/18/23 10:19:30.504
• [0.122 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/18/23 10:19:30.51
  May 18 10:19:30.511: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename deployment @ 05/18/23 10:19:30.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:30.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:30.544
  May 18 10:19:30.547: INFO: Creating simple deployment test-new-deployment
  May 18 10:19:30.558: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
  E0518 10:19:31.481461      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:32.493748      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 05/18/23 10:19:32.578
  STEP: updating a scale subresource @ 05/18/23 10:19:32.582
  STEP: verifying the deployment Spec.Replicas was modified @ 05/18/23 10:19:32.591
  STEP: Patch a scale subresource @ 05/18/23 10:19:32.595
  May 18 10:19:32.625: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-177  12238e14-6523-4502-85c5-206ec921aa02 1969347 3 2023-05-18 10:19:30 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-18 10:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 10:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00928c6e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-18 10:19:32 +0000 UTC,LastTransitionTime:2023-05-18 10:19:32 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-18 10:19:32 +0000 UTC,LastTransitionTime:2023-05-18 10:19:30 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 18 10:19:32.631: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-177  27aefb49-cc49-45ab-a82f-353dc6e88381 1969350 2 2023-05-18 10:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 12238e14-6523-4502-85c5-206ec921aa02 0xc00928cb17 0xc00928cb18}] [] [{kube-controller-manager Update apps/v1 2023-05-18 10:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12238e14-6523-4502-85c5-206ec921aa02\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-18 10:19:32 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00928cba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 18 10:19:32.637: INFO: Pod "test-new-deployment-67bd4bf6dc-6ztp2" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-6ztp2 test-new-deployment-67bd4bf6dc- deployment-177  70dbe27f-0ee8-4186-90d7-8a56b3c4bb66 1969335 0 2023-05-18 10:19:30 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f2762826f11326968ba8ac6164b89bf1ae9aec5816c317b190a75787e7cf3be7 cni.projectcalico.org/podIP:172.16.161.113/32 cni.projectcalico.org/podIPs:172.16.161.113/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 27aefb49-cc49-45ab-a82f-353dc6e88381 0xc00928cfa7 0xc00928cfa8}] [] [{kube-controller-manager Update v1 2023-05-18 10:19:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27aefb49-cc49-45ab-a82f-353dc6e88381\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-18 10:19:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-18 10:19:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"172.16.161.113\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mpnq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mpnq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 10:19:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 10:19:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 10:19:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 10:19:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.190.143,PodIP:172.16.161.113,StartTime:2023-05-18 10:19:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-18 10:19:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e7fdc85cdb4afd7dfa4130e38bdc964a3239cef6c6c7aa8483569a9d42afac24,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:172.16.161.113,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 10:19:32.639: INFO: Pod "test-new-deployment-67bd4bf6dc-jbmkp" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-jbmkp test-new-deployment-67bd4bf6dc- deployment-177  a9284b87-2a07-4da1-8946-94d80db50877 1969349 0 2023-05-18 10:19:32 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 27aefb49-cc49-45ab-a82f-353dc6e88381 0xc00928d1a7 0xc00928d1a8}] [] [{kube-controller-manager Update v1 2023-05-18 10:19:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"27aefb49-cc49-45ab-a82f-353dc6e88381\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wfzrx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wfzrx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ck-test-kube-1-27-worker-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-18 10:19:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 18 10:19:32.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-177" for this suite. @ 05/18/23 10:19:32.646
• [2.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/18/23 10:19:32.679
  May 18 10:19:32.679: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 10:19:32.682
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:32.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:32.703
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/18/23 10:19:32.706
  E0518 10:19:33.485157      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:34.486477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:35.486333      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:36.486752      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:19:36.728
  May 18 10:19:36.731: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-04f1dbf3-e2f0-4348-800f-a37201e99441 container test-container: <nil>
  STEP: delete the pod @ 05/18/23 10:19:36.748
  May 18 10:19:36.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-606" for this suite. @ 05/18/23 10:19:36.768
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/18/23 10:19:36.775
  May 18 10:19:36.775: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename job @ 05/18/23 10:19:36.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:36.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:36.791
  STEP: Creating a job @ 05/18/23 10:19:36.794
  STEP: Ensuring active pods == parallelism @ 05/18/23 10:19:36.799
  E0518 10:19:37.487480      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:38.487023      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:39.488131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:40.488616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 05/18/23 10:19:40.806
  May 18 10:19:41.326: INFO: Successfully updated pod "adopt-release-47mrn"
  STEP: Checking that the Job readopts the Pod @ 05/18/23 10:19:41.326
  E0518 10:19:41.488822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:42.489873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 05/18/23 10:19:43.336
  E0518 10:19:43.491035      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:19:43.856: INFO: Successfully updated pod "adopt-release-47mrn"
  STEP: Checking that the Job releases the Pod @ 05/18/23 10:19:43.856
  E0518 10:19:44.491891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:45.494456      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:19:45.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9040" for this suite. @ 05/18/23 10:19:45.872
• [9.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/18/23 10:19:45.885
  May 18 10:19:45.885: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 10:19:45.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:45.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:45.906
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/18/23 10:19:45.908
  May 18 10:19:45.908: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-6904 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 18 10:19:46.009: INFO: stderr: ""
  May 18 10:19:46.009: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/18/23 10:19:46.009
  May 18 10:19:46.009: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-6904 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May 18 10:19:46.151: INFO: stderr: ""
  May 18 10:19:46.151: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/18/23 10:19:46.151
  May 18 10:19:46.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-6904 delete pods e2e-test-httpd-pod'
  E0518 10:19:46.495291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:47.495832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:19:48.201: INFO: stderr: ""
  May 18 10:19:48.201: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 18 10:19:48.201: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6904" for this suite. @ 05/18/23 10:19:48.205
• [2.325 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/18/23 10:19:48.211
  May 18 10:19:48.211: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename services @ 05/18/23 10:19:48.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:48.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:48.226
  STEP: fetching services @ 05/18/23 10:19:48.229
  May 18 10:19:48.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1219" for this suite. @ 05/18/23 10:19:48.237
• [0.031 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/18/23 10:19:48.25
  May 18 10:19:48.250: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/18/23 10:19:48.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:48.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:48.269
  STEP: create the container to handle the HTTPGet hook request. @ 05/18/23 10:19:48.277
  E0518 10:19:48.496826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:49.497209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/18/23 10:19:50.299
  E0518 10:19:50.497454      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:51.499041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 05/18/23 10:19:52.319
  E0518 10:19:52.499346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:53.499589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:54.500323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:55.500924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 05/18/23 10:19:56.342
  May 18 10:19:56.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3926" for this suite. @ 05/18/23 10:19:56.358
• [8.114 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/18/23 10:19:56.367
  May 18 10:19:56.367: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 10:19:56.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:56.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:56.392
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/18/23 10:19:56.396
  May 18 10:19:56.397: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-7739 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  E0518 10:19:56.501955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:19:56.571: INFO: stderr: ""
  May 18 10:19:56.571: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/18/23 10:19:56.572
  May 18 10:19:56.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-7739 delete pods e2e-test-httpd-pod'
  E0518 10:19:57.501954      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:19:58.503106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:19:59.296: INFO: stderr: ""
  May 18 10:19:59.297: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 18 10:19:59.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7739" for this suite. @ 05/18/23 10:19:59.304
• [2.946 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/18/23 10:19:59.318
  May 18 10:19:59.318: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename var-expansion @ 05/18/23 10:19:59.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:19:59.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:19:59.348
  E0518 10:19:59.504086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:00.504202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:20:01.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 18 10:20:01.379: INFO: Deleting pod "var-expansion-2dd54af7-7cb2-416e-9c3a-dd0462724b4f" in namespace "var-expansion-3324"
  May 18 10:20:01.388: INFO: Wait up to 5m0s for pod "var-expansion-2dd54af7-7cb2-416e-9c3a-dd0462724b4f" to be fully deleted
  E0518 10:20:01.505085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:02.505859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-3324" for this suite. @ 05/18/23 10:20:03.398
• [4.089 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/18/23 10:20:03.414
  May 18 10:20:03.414: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename subpath @ 05/18/23 10:20:03.417
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:20:03.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:20:03.443
  STEP: Setting up data @ 05/18/23 10:20:03.45
  STEP: Creating pod pod-subpath-test-projected-p77d @ 05/18/23 10:20:03.459
  STEP: Creating a pod to test atomic-volume-subpath @ 05/18/23 10:20:03.459
  E0518 10:20:03.506000      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:04.506883      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:05.507091      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:06.507334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:07.507906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:08.508133      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:09.508620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:10.509364      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:11.510159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:12.510529      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:13.510457      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:14.510831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:15.510885      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:16.512484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:17.511582      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:18.512298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:19.512804      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:20.512749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:21.513877      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:22.515357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:23.515191      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:24.515314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:25.516150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:26.516373      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:27.516574      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:20:27.553
  May 18 10:20:27.555: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-subpath-test-projected-p77d container test-container-subpath-projected-p77d: <nil>
  STEP: delete the pod @ 05/18/23 10:20:27.562
  STEP: Deleting pod pod-subpath-test-projected-p77d @ 05/18/23 10:20:27.576
  May 18 10:20:27.576: INFO: Deleting pod "pod-subpath-test-projected-p77d" in namespace "subpath-6834"
  May 18 10:20:27.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6834" for this suite. @ 05/18/23 10:20:27.586
• [24.178 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/18/23 10:20:27.594
  May 18 10:20:27.594: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename field-validation @ 05/18/23 10:20:27.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:20:27.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:20:27.62
  May 18 10:20:27.625: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  E0518 10:20:28.517212      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:29.517406      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0518 10:20:30.204459      19 warnings.go:70] unknown field "alpha"
  W0518 10:20:30.204532      19 warnings.go:70] unknown field "beta"
  W0518 10:20:30.204554      19 warnings.go:70] unknown field "delta"
  W0518 10:20:30.204729      19 warnings.go:70] unknown field "epsilon"
  W0518 10:20:30.204743      19 warnings.go:70] unknown field "gamma"
  May 18 10:20:30.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2513" for this suite. @ 05/18/23 10:20:30.253
• [2.667 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/18/23 10:20:30.276
  May 18 10:20:30.276: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename lease-test @ 05/18/23 10:20:30.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:20:30.293
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:20:30.296
  May 18 10:20:30.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-9060" for this suite. @ 05/18/23 10:20:30.361
• [0.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/18/23 10:20:30.378
  May 18 10:20:30.378: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename job @ 05/18/23 10:20:30.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:20:30.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:20:30.403
  STEP: Creating a job @ 05/18/23 10:20:30.406
  STEP: Ensuring job reaches completions @ 05/18/23 10:20:30.413
  E0518 10:20:30.517799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:31.518368      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:32.520116      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:33.520630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:34.532936      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:35.524476      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:36.525520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:37.526355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:38.526831      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:39.527441      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:40.527749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:41.527845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:20:42.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2976" for this suite. @ 05/18/23 10:20:42.429
• [12.060 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/18/23 10:20:42.444
  May 18 10:20:42.445: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir @ 05/18/23 10:20:42.447
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:20:42.47
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:20:42.477
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/18/23 10:20:42.485
  E0518 10:20:42.528642      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:43.529503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:44.530395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:45.531718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:20:46.513
  May 18 10:20:46.516: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-ba94822f-d0ee-428c-bf28-c7baabb1b88e container test-container: <nil>
  STEP: delete the pod @ 05/18/23 10:20:46.525
  E0518 10:20:46.531678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:20:46.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7312" for this suite. @ 05/18/23 10:20:46.548
• [4.109 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/18/23 10:20:46.555
  May 18 10:20:46.555: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename cronjob @ 05/18/23 10:20:46.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:20:46.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:20:46.575
  STEP: Creating a suspended cronjob @ 05/18/23 10:20:46.58
  STEP: Ensuring no jobs are scheduled @ 05/18/23 10:20:46.585
  E0518 10:20:47.532951      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:48.532996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:49.533124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:50.533307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:51.533501      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:52.533828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:53.533870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:54.534539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:55.534891      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:56.534855      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:57.535286      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:58.536415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:20:59.535931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:00.537074      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:01.537268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:02.537284      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:03.537486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:04.537771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:05.537952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:06.538048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:07.538183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:08.538420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:09.538527      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:10.539267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:11.538979      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:12.539019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:13.539655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:14.539882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:15.540142      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:16.540211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:17.540458      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:18.540991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:19.541360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:20.541896      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:21.541807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:22.542665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:23.542805      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:24.543090      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:25.543086      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:26.543983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:27.545067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:28.545693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:29.545972      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:30.546464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:31.546719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:32.547114      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:33.549414      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:34.547784      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:35.548164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:36.548394      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:37.548467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:38.549052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:39.549772      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:40.550427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:41.551055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:42.551594      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:43.551858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:44.552017      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:45.553198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:46.553834      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:47.554058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:48.554297      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:49.554421      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:50.554638      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:51.554825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:52.555150      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:53.555115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:54.555429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:55.556108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:56.556499      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:57.557263      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:58.560166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:21:59.558418      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:00.558982      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:01.559802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:02.561011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:03.561375      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:04.563941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:05.562519      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:06.563052      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:07.565110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:08.565334      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:09.565509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:10.565719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:11.565849      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:12.566555      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:13.566719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:14.567223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:15.568290      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:16.568575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:17.568806      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:18.569040      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:19.569395      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:20.569381      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:21.569625      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:22.573208      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:23.573504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:24.575880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:25.576113      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:26.576099      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:27.576347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:28.576435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:29.576678      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:30.577181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:31.577539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:32.578226      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:33.578820      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:34.579240      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:35.579718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:36.580067      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:37.580579      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:38.581509      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:39.581338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:40.581600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:41.582249      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:42.582453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:43.582789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:44.582780      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:45.583489      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:46.584003      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:47.583887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:48.584280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:49.584468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:50.584470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:51.584763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:52.585694      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:53.586260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:54.586372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:55.587184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:56.587430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:57.587652      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:58.588031      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:22:59.588193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:00.589139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:01.589901      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:02.590484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:03.590959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:04.590898      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:05.592016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:06.592425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:07.592799      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:08.593043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:09.593605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:10.594537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:11.595261      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:12.596139      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:13.596342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:14.597411      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:15.598071      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:16.598523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:17.599641      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:18.599720      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:19.599985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:20.600269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:21.600815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:22.602514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:23.602860      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:24.604277      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:25.604022      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:26.604042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:27.605830      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:28.606350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:29.606274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:30.606670      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:31.607601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:32.607858      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:33.609058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:34.609880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:35.610424      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:36.610953      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:37.611203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:38.611827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:39.611938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:40.612209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:41.612357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:42.612795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:43.613504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:44.614002      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:45.614158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:46.614697      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:47.614822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:48.615682      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:49.615880      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:50.616795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:51.617115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:52.617599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:53.617493      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:54.617903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:55.618144      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:56.618635      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:57.618907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:58.619749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:23:59.620219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:00.620827      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:01.621234      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:02.622280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:03.622693      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:04.625996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:05.626511      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:06.626667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:07.626584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:08.627822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:09.628028      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:10.628444      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:11.628739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:12.628737      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:13.629292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:14.629992      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:15.630305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:16.630942      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:17.631128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:18.631419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:19.631783      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:20.632797      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:21.633137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:22.633541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:23.633987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:24.634811      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:25.635966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:26.635257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:27.635497      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:28.635961      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:29.636488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:30.637045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:31.637633      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:32.638572      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:33.639357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:34.640143      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:35.640776      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:36.641826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:37.641939      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:38.642600      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:39.643001      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:40.644092      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:41.644618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:42.645613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:43.646311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:44.646351      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:45.646578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:46.647053      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:47.647789      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:48.647768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:49.647763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:50.647824      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:51.647884      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:52.648241      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:53.649165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:54.649391      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:55.651262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:56.651944      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:57.652319      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:58.652506      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:24:59.652956      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:00.653417      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:01.653561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:02.654132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:03.654802      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:04.655685      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:05.656203      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:06.656761      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:07.656949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:08.657367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:09.658187      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:10.658415      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:11.658576      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:12.659521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:13.659993      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:14.660039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:15.660485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:16.660673      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:17.661398      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:18.662037      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:19.662324      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:20.663072      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:21.663376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:22.664487      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:23.665628      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:24.666345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:25.666629      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:26.667432      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:27.667950      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:28.668998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:29.669219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:30.669274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:31.669552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:32.670233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:33.670467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:34.671008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:35.671197      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:36.671271      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:37.671392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:38.671764      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:39.672233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:40.673595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:41.673063      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:42.673663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:43.674025      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:44.674763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:45.675588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/18/23 10:25:46.591
  STEP: Removing cronjob @ 05/18/23 10:25:46.593
  May 18 10:25:46.598: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5396" for this suite. @ 05/18/23 10:25:46.606
• [300.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/18/23 10:25:46.618
  May 18 10:25:46.618: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 10:25:46.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:25:46.645
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:25:46.658
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 10:25:46.665
  E0518 10:25:46.676655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:47.677613      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:48.678315      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:49.680701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:50.681158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:25:50.714
  May 18 10:25:50.717: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-38b4c0f4-cfa0-44e1-8362-55bf1cbabfc7 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 10:25:50.739
  May 18 10:25:50.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7248" for this suite. @ 05/18/23 10:25:50.758
• [4.145 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/18/23 10:25:50.764
  May 18 10:25:50.764: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 10:25:50.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:25:50.778
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:25:50.781
  STEP: Creating configMap configmap-340/configmap-test-2912530f-32e9-4181-8254-738c2c583eec @ 05/18/23 10:25:50.784
  STEP: Creating a pod to test consume configMaps @ 05/18/23 10:25:50.789
  E0518 10:25:51.681367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:52.681520      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:53.682372      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:54.683016      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:25:54.812
  May 18 10:25:54.817: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-4e2ff1ed-055d-4a34-90b2-78419316e64b container env-test: <nil>
  STEP: delete the pod @ 05/18/23 10:25:54.827
  May 18 10:25:54.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-340" for this suite. @ 05/18/23 10:25:54.852
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/18/23 10:25:54.873
  May 18 10:25:54.873: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-probe @ 05/18/23 10:25:54.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:25:54.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:25:54.901
  E0518 10:25:55.683443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:56.684342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:57.684955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:58.685008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:25:59.685065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:00.685117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:01.685251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:02.685912      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:03.686864      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:04.687108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:05.688209      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:06.689085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:07.690331      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:08.690449      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:09.690681      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:10.690839      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:11.691117      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:12.691523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:13.692314      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:14.692803      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:15.693957      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:16.694729      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:17.694763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:18.694746      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:19.695106      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:20.695232      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:21.696057      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:22.697021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:23.697137      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:24.698111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:25.699136      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:26.699256      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:27.699358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:28.699906      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:29.700429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:30.701389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:31.702362      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:32.702711      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:33.702907      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:34.703980      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:35.704085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:36.704564      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:37.705134      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:38.705983      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:39.706566      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:40.707609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:41.707699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:42.708608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:43.708779      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:44.709156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:45.709264      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:46.709769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:47.710903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:48.711870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:49.712029      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:50.713045      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:51.713273      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:52.713645      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:53.714154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:54.714156      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:26:54.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-2239" for this suite. @ 05/18/23 10:26:54.933
• [60.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/18/23 10:26:54.945
  May 18 10:26:54.945: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 10:26:54.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:26:54.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:26:54.98
  STEP: Creating configMap with name configmap-test-volume-6010100b-4138-48e1-865c-2c1108f762a9 @ 05/18/23 10:26:54.985
  STEP: Creating a pod to test consume configMaps @ 05/18/23 10:26:54.992
  E0518 10:26:55.715233      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:56.716235      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:57.716719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:26:58.716835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:26:59.04
  May 18 10:26:59.043: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-c15f47ee-a1fd-4cd2-89fb-0032c57d8880 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 10:26:59.058
  May 18 10:26:59.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4587" for this suite. @ 05/18/23 10:26:59.087
• [4.151 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/18/23 10:26:59.102
  May 18 10:26:59.102: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename init-container @ 05/18/23 10:26:59.104
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:26:59.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:26:59.127
  STEP: creating the pod @ 05/18/23 10:26:59.131
  May 18 10:26:59.131: INFO: PodSpec: initContainers in spec.initContainers
  E0518 10:26:59.717055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:00.717399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:01.719477      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:02.719618      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:27:03.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1756" for this suite. @ 05/18/23 10:27:03.419
• [4.326 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/18/23 10:27:03.429
  May 18 10:27:03.430: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 10:27:03.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:03.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:03.456
  STEP: creating the pod @ 05/18/23 10:27:03.459
  STEP: setting up watch @ 05/18/23 10:27:03.46
  STEP: submitting the pod to kubernetes @ 05/18/23 10:27:03.564
  STEP: verifying the pod is in kubernetes @ 05/18/23 10:27:03.576
  STEP: verifying pod creation was observed @ 05/18/23 10:27:03.58
  E0518 10:27:03.720443      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:04.720690      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/18/23 10:27:05.597
  STEP: verifying pod deletion was observed @ 05/18/23 10:27:05.604
  E0518 10:27:05.721291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:06.722390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:07.723153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:27:08.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2507" for this suite. @ 05/18/23 10:27:08.097
• [4.673 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/18/23 10:27:08.104
  May 18 10:27:08.104: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename endpointslice @ 05/18/23 10:27:08.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:08.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:08.127
  E0518 10:27:08.723553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:09.723765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:10.723949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:11.725124      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:12.725496      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 05/18/23 10:27:13.212
  E0518 10:27:13.725857      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:14.726251      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:15.726987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:16.727222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:17.727437      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 05/18/23 10:27:18.223
  E0518 10:27:18.727452      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:19.727952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:20.728472      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:21.729146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:22.728947      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/18/23 10:27:23.234
  E0518 10:27:23.730663      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:24.730532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:25.731115      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:26.732370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:27.732382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 05/18/23 10:27:28.247
  May 18 10:27:28.273: INFO: EndpointSlice for Service endpointslice-9991/example-named-port not found
  E0518 10:27:28.733021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:29.733929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:30.733845      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:31.734292      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:32.734870      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:33.735188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:34.735305      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:35.735658      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:36.736567      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:37.736886      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:27:38.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9991" for this suite. @ 05/18/23 10:27:38.29
• [30.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/18/23 10:27:38.33
  May 18 10:27:38.330: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename runtimeclass @ 05/18/23 10:27:38.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:38.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:38.36
  May 18 10:27:38.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9735" for this suite. @ 05/18/23 10:27:38.403
• [0.088 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/18/23 10:27:38.418
  May 18 10:27:38.418: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 10:27:38.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:38.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:38.443
  STEP: Create set of pods @ 05/18/23 10:27:38.448
  May 18 10:27:38.458: INFO: created test-pod-1
  May 18 10:27:38.466: INFO: created test-pod-2
  May 18 10:27:38.477: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/18/23 10:27:38.477
  E0518 10:27:38.737548      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:39.741365      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 05/18/23 10:27:40.517
  May 18 10:27:40.522: INFO: Pod quantity 3 is different from expected quantity 0
  E0518 10:27:40.741763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:27:41.530: INFO: Pod quantity 3 is different from expected quantity 0
  E0518 10:27:41.742718      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:27:42.529: INFO: Pod quantity 3 is different from expected quantity 0
  E0518 10:27:42.743707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:27:43.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6982" for this suite. @ 05/18/23 10:27:43.54
• [5.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/18/23 10:27:43.556
  May 18 10:27:43.557: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 10:27:43.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:43.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:43.581
  STEP: Creating a pod to test downward API volume plugin @ 05/18/23 10:27:43.586
  E0518 10:27:43.744118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:44.744699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:45.745376      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:46.745966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:27:47.632
  May 18 10:27:47.637: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downwardapi-volume-7a5e2166-c131-4f27-8c90-00587aed73d9 container client-container: <nil>
  STEP: delete the pod @ 05/18/23 10:27:47.644
  May 18 10:27:47.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8041" for this suite. @ 05/18/23 10:27:47.665
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/18/23 10:27:47.675
  May 18 10:27:47.675: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename dns @ 05/18/23 10:27:47.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:47.689
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:47.691
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/18/23 10:27:47.694
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/18/23 10:27:47.694
  STEP: creating a pod to probe DNS @ 05/18/23 10:27:47.695
  STEP: submitting the pod to kubernetes @ 05/18/23 10:27:47.695
  E0518 10:27:47.746999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:48.747704      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/18/23 10:27:49.713
  STEP: looking for the results for each expected name from probers @ 05/18/23 10:27:49.719
  May 18 10:27:49.735: INFO: DNS probes using dns-6683/dns-test-94be6cb2-69eb-44ec-b0b6-7aab1c467432 succeeded

  May 18 10:27:49.735: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/18/23 10:27:49.739
  E0518 10:27:49.750471      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "dns-6683" for this suite. @ 05/18/23 10:27:49.762
• [2.093 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/18/23 10:27:49.771
  May 18 10:27:49.771: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubectl @ 05/18/23 10:27:49.773
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:49.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:49.793
  STEP: creating the pod @ 05/18/23 10:27:49.797
  May 18 10:27:49.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-181 create -f -'
  E0518 10:27:50.749131      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:51.749859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:27:51.949: INFO: stderr: ""
  May 18 10:27:51.949: INFO: stdout: "pod/pause created\n"
  E0518 10:27:52.750887      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:53.751033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/18/23 10:27:53.962
  May 18 10:27:53.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-181 label pods pause testing-label=testing-label-value'
  May 18 10:27:54.143: INFO: stderr: ""
  May 18 10:27:54.143: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/18/23 10:27:54.143
  May 18 10:27:54.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-181 get pod pause -L testing-label'
  May 18 10:27:54.260: INFO: stderr: ""
  May 18 10:27:54.260: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/18/23 10:27:54.26
  May 18 10:27:54.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-181 label pods pause testing-label-'
  May 18 10:27:54.365: INFO: stderr: ""
  May 18 10:27:54.365: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/18/23 10:27:54.365
  May 18 10:27:54.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-181 get pod pause -L testing-label'
  May 18 10:27:54.468: INFO: stderr: ""
  May 18 10:27:54.468: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 05/18/23 10:27:54.468
  May 18 10:27:54.469: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-181 delete --grace-period=0 --force -f -'
  May 18 10:27:54.586: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 18 10:27:54.586: INFO: stdout: "pod \"pause\" force deleted\n"
  May 18 10:27:54.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-181 get rc,svc -l name=pause --no-headers'
  May 18 10:27:54.720: INFO: stderr: "No resources found in kubectl-181 namespace.\n"
  May 18 10:27:54.721: INFO: stdout: ""
  May 18 10:27:54.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=kubectl-181 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  E0518 10:27:54.751541      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:27:54.841: INFO: stderr: ""
  May 18 10:27:54.841: INFO: stdout: ""
  May 18 10:27:54.842: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-181" for this suite. @ 05/18/23 10:27:54.849
• [5.086 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/18/23 10:27:54.862
  May 18 10:27:54.862: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename pods @ 05/18/23 10:27:54.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:54.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:54.885
  STEP: Create a pod @ 05/18/23 10:27:54.892
  E0518 10:27:55.751851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:56.752655      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/18/23 10:27:56.912
  May 18 10:27:56.924: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May 18 10:27:56.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3230" for this suite. @ 05/18/23 10:27:56.932
• [2.086 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/18/23 10:27:56.957
  May 18 10:27:56.958: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename watch @ 05/18/23 10:27:56.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:27:56.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:27:56.98
  STEP: creating a watch on configmaps with a certain label @ 05/18/23 10:27:56.985
  STEP: creating a new configmap @ 05/18/23 10:27:56.993
  STEP: modifying the configmap once @ 05/18/23 10:27:56.996
  STEP: changing the label value of the configmap @ 05/18/23 10:27:57.002
  STEP: Expecting to observe a delete notification for the watched object @ 05/18/23 10:27:57.008
  May 18 10:27:57.008: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030  db4e1cb0-f890-4066-aeb1-a2f1fd4ba77f 1972604 0 2023-05-18 10:27:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-18 10:27:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:27:57.009: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030  db4e1cb0-f890-4066-aeb1-a2f1fd4ba77f 1972605 0 2023-05-18 10:27:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-18 10:27:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:27:57.009: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030  db4e1cb0-f890-4066-aeb1-a2f1fd4ba77f 1972606 0 2023-05-18 10:27:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-18 10:27:56 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/18/23 10:27:57.01
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/18/23 10:27:57.015
  E0518 10:27:57.752595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:58.752605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:27:59.753388      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:00.753624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:01.754355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:02.754425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:03.754990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:04.755268      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:05.756184      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:06.756989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 05/18/23 10:28:07.016
  STEP: modifying the configmap a third time @ 05/18/23 10:28:07.027
  STEP: deleting the configmap @ 05/18/23 10:28:07.037
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/18/23 10:28:07.044
  May 18 10:28:07.044: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030  db4e1cb0-f890-4066-aeb1-a2f1fd4ba77f 1972676 0 2023-05-18 10:27:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-18 10:28:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:28:07.045: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030  db4e1cb0-f890-4066-aeb1-a2f1fd4ba77f 1972677 0 2023-05-18 10:27:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-18 10:28:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:28:07.045: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-7030  db4e1cb0-f890-4066-aeb1-a2f1fd4ba77f 1972678 0 2023-05-18 10:27:56 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-18 10:28:07 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 18 10:28:07.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7030" for this suite. @ 05/18/23 10:28:07.053
• [10.104 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/18/23 10:28:07.063
  May 18 10:28:07.063: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename downward-api @ 05/18/23 10:28:07.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:07.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:07.094
  STEP: Creating a pod to test downward api env vars @ 05/18/23 10:28:07.099
  E0518 10:28:07.757329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:08.757923      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:09.758252      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:10.758219      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:28:11.122
  May 18 10:28:11.128: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod downward-api-e5329ee5-33bd-4944-9718-f25ec2d7d144 container dapi-container: <nil>
  STEP: delete the pod @ 05/18/23 10:28:11.146
  May 18 10:28:11.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-243" for this suite. @ 05/18/23 10:28:11.176
• [4.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/18/23 10:28:11.193
  May 18 10:28:11.193: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename events @ 05/18/23 10:28:11.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:11.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:11.227
  STEP: Create set of events @ 05/18/23 10:28:11.233
  STEP: get a list of Events with a label in the current namespace @ 05/18/23 10:28:11.25
  STEP: delete a list of events @ 05/18/23 10:28:11.254
  May 18 10:28:11.254: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/18/23 10:28:11.268
  May 18 10:28:11.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6168" for this suite. @ 05/18/23 10:28:11.278
• [0.092 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/18/23 10:28:11.286
  May 18 10:28:11.286: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename projected @ 05/18/23 10:28:11.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:11.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:11.304
  STEP: Creating configMap with name projected-configmap-test-volume-187751ed-79e1-4d36-bd31-55bd7f0f6c95 @ 05/18/23 10:28:11.308
  STEP: Creating a pod to test consume configMaps @ 05/18/23 10:28:11.313
  E0518 10:28:11.758793      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:12.759011      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:13.759893      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:14.760269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:28:15.339
  May 18 10:28:15.344: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-projected-configmaps-755baee8-2ec1-4794-8420-32529e5cc7d7 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 10:28:15.355
  May 18 10:28:15.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2214" for this suite. @ 05/18/23 10:28:15.383
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/18/23 10:28:15.392
  May 18 10:28:15.392: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 10:28:15.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:15.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:15.418
  STEP: creating a ConfigMap @ 05/18/23 10:28:15.423
  STEP: fetching the ConfigMap @ 05/18/23 10:28:15.428
  STEP: patching the ConfigMap @ 05/18/23 10:28:15.433
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/18/23 10:28:15.44
  STEP: deleting the ConfigMap by collection with a label selector @ 05/18/23 10:28:15.456
  STEP: listing all ConfigMaps in test namespace @ 05/18/23 10:28:15.467
  May 18 10:28:15.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1531" for this suite. @ 05/18/23 10:28:15.478
• [0.095 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/18/23 10:28:15.49
  May 18 10:28:15.490: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename subjectreview @ 05/18/23 10:28:15.492
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:15.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:15.511
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-8239" @ 05/18/23 10:28:15.515
  May 18 10:28:15.520: INFO: saUsername: "system:serviceaccount:subjectreview-8239:e2e"
  May 18 10:28:15.521: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-8239"}
  May 18 10:28:15.521: INFO: saUID: "ca242723-a519-4f48-8fa5-c81c2d362f56"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-8239:e2e" @ 05/18/23 10:28:15.522
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-8239:e2e" @ 05/18/23 10:28:15.523
  May 18 10:28:15.526: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-8239:e2e" api 'list' configmaps in "subjectreview-8239" namespace @ 05/18/23 10:28:15.526
  May 18 10:28:15.529: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-8239:e2e" @ 05/18/23 10:28:15.53
  May 18 10:28:15.532: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May 18 10:28:15.532: INFO: LocalSubjectAccessReview has been verified
  May 18 10:28:15.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-8239" for this suite. @ 05/18/23 10:28:15.539
• [0.056 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/18/23 10:28:15.552
  May 18 10:28:15.553: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename subpath @ 05/18/23 10:28:15.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:15.575
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:15.581
  STEP: Setting up data @ 05/18/23 10:28:15.587
  STEP: Creating pod pod-subpath-test-downwardapi-zjlq @ 05/18/23 10:28:15.598
  STEP: Creating a pod to test atomic-volume-subpath @ 05/18/23 10:28:15.598
  E0518 10:28:15.760306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:16.761055      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:17.762198      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:18.762550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:19.763087      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:20.763236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:21.763485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:22.763588      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:23.764482      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:24.764674      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:25.765162      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:26.765917      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:27.766048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:28.766148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:29.767012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:30.768190      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:31.768822      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:32.769222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:33.769649      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:34.770306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:35.771342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:36.771876      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:37.772347      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:38.772987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:28:39.685
  May 18 10:28:39.688: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-subpath-test-downwardapi-zjlq container test-container-subpath-downwardapi-zjlq: <nil>
  STEP: delete the pod @ 05/18/23 10:28:39.698
  STEP: Deleting pod pod-subpath-test-downwardapi-zjlq @ 05/18/23 10:28:39.715
  May 18 10:28:39.716: INFO: Deleting pod "pod-subpath-test-downwardapi-zjlq" in namespace "subpath-1430"
  May 18 10:28:39.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1430" for this suite. @ 05/18/23 10:28:39.725
• [24.181 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/18/23 10:28:39.739
  May 18 10:28:39.740: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 10:28:39.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:39.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:39.764
  STEP: Creating configMap with name configmap-test-volume-map-fa442724-d4d0-4883-be93-8b2ec74a39fb @ 05/18/23 10:28:39.768
  E0518 10:28:39.773140      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume configMaps @ 05/18/23 10:28:39.773
  E0518 10:28:40.774382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:41.775453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:42.775561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:43.776202      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:28:43.797
  May 18 10:28:43.800: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-configmaps-8b43917c-2ebd-4f1b-b05f-86285d273b78 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 10:28:43.808
  May 18 10:28:43.832: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7835" for this suite. @ 05/18/23 10:28:43.837
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/18/23 10:28:43.852
  May 18 10:28:43.852: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename job @ 05/18/23 10:28:43.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:43.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:43.872
  STEP: Creating a job @ 05/18/23 10:28:43.876
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/18/23 10:28:43.881
  E0518 10:28:44.777098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:45.777276      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/18/23 10:28:45.896
  STEP: updating /status @ 05/18/23 10:28:45.914
  STEP: get /status @ 05/18/23 10:28:45.982
  May 18 10:28:45.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5629" for this suite. @ 05/18/23 10:28:45.999
• [2.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/18/23 10:28:46.022
  May 18 10:28:46.023: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 10:28:46.024
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:46.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:46.052
  STEP: Setting up server cert @ 05/18/23 10:28:46.081
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 10:28:46.538
  STEP: Deploying the webhook pod @ 05/18/23 10:28:46.544
  STEP: Wait for the deployment to be ready @ 05/18/23 10:28:46.56
  May 18 10:28:46.569: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0518 10:28:46.781525      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:47.782127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:28:48.588: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 18, 10, 28, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 10, 28, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 18, 10, 28, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 18, 10, 28, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0518 10:28:48.784043      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:49.784078      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 10:28:50.595
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 10:28:50.611
  E0518 10:28:50.784524      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:28:51.612: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/18/23 10:28:51.617
  STEP: create a configmap that should be updated by the webhook @ 05/18/23 10:28:51.64
  May 18 10:28:51.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3510" for this suite. @ 05/18/23 10:28:51.754
  STEP: Destroying namespace "webhook-markers-9583" for this suite. @ 05/18/23 10:28:51.768
• [5.763 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/18/23 10:28:51.786
  May 18 10:28:51.787: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename kubelet-test @ 05/18/23 10:28:51.788
  E0518 10:28:51.803881      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:51.807
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:51.819
  E0518 10:28:52.786419      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:53.788111      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:28:53.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3758" for this suite. @ 05/18/23 10:28:53.884
• [2.109 seconds]
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/18/23 10:28:53.896
  May 18 10:28:53.896: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename runtimeclass @ 05/18/23 10:28:53.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:53.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:53.916
  STEP: Deleting RuntimeClass runtimeclass-7891-delete-me @ 05/18/23 10:28:53.924
  STEP: Waiting for the RuntimeClass to disappear @ 05/18/23 10:28:53.929
  May 18 10:28:53.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7891" for this suite. @ 05/18/23 10:28:53.941
• [0.052 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/18/23 10:28:53.95
  May 18 10:28:53.950: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename webhook @ 05/18/23 10:28:53.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:28:53.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:28:53.964
  STEP: Setting up server cert @ 05/18/23 10:28:53.984
  E0518 10:28:54.787532      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/18/23 10:28:54.889
  STEP: Deploying the webhook pod @ 05/18/23 10:28:54.897
  STEP: Wait for the deployment to be ready @ 05/18/23 10:28:54.91
  May 18 10:28:54.917: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0518 10:28:55.789453      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:56.790586      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/18/23 10:28:56.927
  STEP: Verifying the service has paired with the endpoint @ 05/18/23 10:28:56.943
  E0518 10:28:57.791215      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:28:57.948: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/18/23 10:28:57.978
  STEP: create a pod @ 05/18/23 10:28:58.015
  E0518 10:28:58.791616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:28:59.792042      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/18/23 10:29:00.03
  May 18 10:29:00.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-429586168 --namespace=webhook-9891 attach --namespace=webhook-9891 to-be-attached-pod -i -c=container1'
  May 18 10:29:00.145: INFO: rc: 1
  May 18 10:29:00.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9891" for this suite. @ 05/18/23 10:29:00.2
  STEP: Destroying namespace "webhook-markers-3574" for this suite. @ 05/18/23 10:29:00.213
• [6.271 seconds]
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/18/23 10:29:00.221
  May 18 10:29:00.222: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename containers @ 05/18/23 10:29:00.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:29:00.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:29:00.253
  E0518 10:29:00.792970      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:01.793924      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:29:02.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6616" for this suite. @ 05/18/23 10:29:02.297
• [2.081 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/18/23 10:29:02.308
  May 18 10:29:02.308: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename init-container @ 05/18/23 10:29:02.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:29:02.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:29:02.328
  STEP: creating the pod @ 05/18/23 10:29:02.332
  May 18 10:29:02.332: INFO: PodSpec: initContainers in spec.initContainers
  E0518 10:29:02.793938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:03.795619      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:04.796183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:05.796304      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:06.796620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:29:07.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1586" for this suite. @ 05/18/23 10:29:07.174
• [4.875 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/18/23 10:29:07.186
  May 18 10:29:07.186: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename gc @ 05/18/23 10:29:07.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:29:07.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:29:07.229
  STEP: create the deployment @ 05/18/23 10:29:07.234
  W0518 10:29:07.240390      19 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/18/23 10:29:07.241
  STEP: delete the deployment @ 05/18/23 10:29:07.753
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/18/23 10:29:07.764
  E0518 10:29:07.796699      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/18/23 10:29:08.286
  May 18 10:29:08.420: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 18 10:29:08.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4248" for this suite. @ 05/18/23 10:29:08.425
• [1.244 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/18/23 10:29:08.433
  May 18 10:29:08.433: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename subpath @ 05/18/23 10:29:08.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:29:08.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:29:08.449
  STEP: Setting up data @ 05/18/23 10:29:08.453
  STEP: Creating pod pod-subpath-test-configmap-58t4 @ 05/18/23 10:29:08.461
  STEP: Creating a pod to test atomic-volume-subpath @ 05/18/23 10:29:08.461
  E0518 10:29:08.797317      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:09.798201      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:10.798536      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:11.799313      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:12.800158      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:13.800514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:14.801624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:15.802879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:16.802873      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:17.802580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:18.803675      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:19.803698      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:20.804423      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:21.805330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:22.805280      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:23.806403      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:24.807154      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:25.807425      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:26.807664      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:27.808105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:28.809370      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:29.810259      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:30.812470      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:31.813346      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:29:32.548
  May 18 10:29:32.551: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod pod-subpath-test-configmap-58t4 container test-container-subpath-configmap-58t4: <nil>
  STEP: delete the pod @ 05/18/23 10:29:32.558
  STEP: Deleting pod pod-subpath-test-configmap-58t4 @ 05/18/23 10:29:32.572
  May 18 10:29:32.572: INFO: Deleting pod "pod-subpath-test-configmap-58t4" in namespace "subpath-5950"
  May 18 10:29:32.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5950" for this suite. @ 05/18/23 10:29:32.58
• [24.152 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/18/23 10:29:32.586
  May 18 10:29:32.587: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/18/23 10:29:32.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:29:32.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:29:32.609
  STEP: Creating 50 configmaps @ 05/18/23 10:29:32.614
  E0518 10:29:32.813771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 05/18/23 10:29:32.844
  May 18 10:29:32.959: INFO: Pod name wrapped-volume-race-0fd61420-cb8e-4dea-bbf4-7021096d07d9: Found 4 pods out of 5
  E0518 10:29:33.814781      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:34.815522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:35.816048      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:36.816369      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:37.816768      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:29:37.975: INFO: Pod name wrapped-volume-race-0fd61420-cb8e-4dea-bbf4-7021096d07d9: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/18/23 10:29:37.975
  STEP: Creating RC which spawns configmap-volume pods @ 05/18/23 10:29:38.038
  May 18 10:29:38.070: INFO: Pod name wrapped-volume-race-fa3f0261-eaea-465f-8519-d87084955e87: Found 0 pods out of 5
  E0518 10:29:38.816812      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:39.820513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:40.820687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:41.821120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:42.821546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:29:43.082: INFO: Pod name wrapped-volume-race-fa3f0261-eaea-465f-8519-d87084955e87: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/18/23 10:29:43.082
  STEP: Creating RC which spawns configmap-volume pods @ 05/18/23 10:29:43.12
  May 18 10:29:43.150: INFO: Pod name wrapped-volume-race-2645a683-0b51-4db7-8a50-861eb4a04f1f: Found 0 pods out of 5
  E0518 10:29:43.825702      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:44.825669      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:45.825807      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:46.826073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:47.826429      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:29:48.165: INFO: Pod name wrapped-volume-race-2645a683-0b51-4db7-8a50-861eb4a04f1f: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/18/23 10:29:48.165
  May 18 10:29:48.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-2645a683-0b51-4db7-8a50-861eb4a04f1f in namespace emptydir-wrapper-7282, will wait for the garbage collector to delete the pods @ 05/18/23 10:29:48.195
  May 18 10:29:48.261: INFO: Deleting ReplicationController wrapped-volume-race-2645a683-0b51-4db7-8a50-861eb4a04f1f took: 10.025ms
  May 18 10:29:48.364: INFO: Terminating ReplicationController wrapped-volume-race-2645a683-0b51-4db7-8a50-861eb4a04f1f pods took: 103.072452ms
  E0518 10:29:48.827211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:49.828085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-fa3f0261-eaea-465f-8519-d87084955e87 in namespace emptydir-wrapper-7282, will wait for the garbage collector to delete the pods @ 05/18/23 10:29:50.566
  May 18 10:29:50.628: INFO: Deleting ReplicationController wrapped-volume-race-fa3f0261-eaea-465f-8519-d87084955e87 took: 6.837253ms
  May 18 10:29:50.730: INFO: Terminating ReplicationController wrapped-volume-race-fa3f0261-eaea-465f-8519-d87084955e87 pods took: 101.1385ms
  E0518 10:29:50.829151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:51.833098      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-0fd61420-cb8e-4dea-bbf4-7021096d07d9 in namespace emptydir-wrapper-7282, will wait for the garbage collector to delete the pods @ 05/18/23 10:29:52.542
  May 18 10:29:52.610: INFO: Deleting ReplicationController wrapped-volume-race-0fd61420-cb8e-4dea-bbf4-7021096d07d9 took: 7.503225ms
  May 18 10:29:52.710: INFO: Terminating ReplicationController wrapped-volume-race-0fd61420-cb8e-4dea-bbf4-7021096d07d9 pods took: 100.286949ms
  E0518 10:29:52.833922      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:53.834341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:54.834913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 05/18/23 10:29:55.611
  E0518 10:29:55.835026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-7282" for this suite. @ 05/18/23 10:29:55.872
• [23.291 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/18/23 10:29:55.879
  May 18 10:29:55.879: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename configmap @ 05/18/23 10:29:55.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:29:55.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:29:55.898
  STEP: Creating configMap that has name configmap-test-emptyKey-d5a1db46-6512-4b90-9332-ac1477b2a8b5 @ 05/18/23 10:29:55.901
  May 18 10:29:55.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-889" for this suite. @ 05/18/23 10:29:55.907
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/18/23 10:29:55.917
  May 18 10:29:55.917: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename cronjob @ 05/18/23 10:29:55.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:29:55.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:29:55.932
  STEP: Creating a ForbidConcurrent cronjob @ 05/18/23 10:29:55.936
  STEP: Ensuring a job is scheduled @ 05/18/23 10:29:55.942
  E0518 10:29:56.835994      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:57.836188      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:58.836328      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:29:59.836464      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:00.836713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:01.836707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/18/23 10:30:01.947
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/18/23 10:30:01.951
  STEP: Ensuring no more jobs are scheduled @ 05/18/23 10:30:01.954
  E0518 10:30:02.837082      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:03.837287      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:04.837485      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:05.838615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:06.838469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:07.839288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:08.840654      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:09.841291      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:10.842246      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:11.842990      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:12.843083      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:13.843390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:14.843665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:15.844211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:16.845095      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:17.845468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:18.845955      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:19.846666      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:20.847060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:21.847620      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:22.847938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:23.848269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:24.848267      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:25.848765      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:26.849888      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:27.850782      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:28.851389      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:29.851680      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:30.851926      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:31.851949      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:32.852828      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:33.853530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:34.853815      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:35.854660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:36.855307      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:37.855428      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:38.856583      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:39.856484      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:40.856624      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:41.856929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:42.857110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:43.857430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:44.857879      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:45.858795      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:46.859750      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:47.859998      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:48.860552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:49.860732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:50.861033      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:51.861999      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:52.862108      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:53.863667      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:54.863707      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:55.864024      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:56.865288      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:57.865330      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:58.865353      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:30:59.865342      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:00.866019      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:01.866151      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:02.866504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:03.867341      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:04.867733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:05.869012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:06.869169      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:07.871526      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:08.871825      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:09.872311      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:10.872465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:11.873601      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:12.873754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:13.874327      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:14.874578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:15.874701      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:16.875222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:17.875435      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:18.875539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:19.875722      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:20.876416      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:21.876687      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:22.877148      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:23.877539      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:24.878060      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:25.878358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:26.878733      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:27.879119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:28.879236      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:29.880039      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:30.881059      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:31.882021      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:32.882561      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:33.883302      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:34.885222      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:35.885545      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:36.886125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:37.886929      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:38.886468      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:39.887269      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:40.887503      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:41.887627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:42.887882      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:43.888323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:44.888835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:45.889127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:46.890274      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:47.890610      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:48.890719      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:49.890851      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:50.891513      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:51.891996      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:52.896119      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:53.897352      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:54.896181      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:55.896938      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:56.897959      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:57.900008      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:58.900132      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:31:59.900721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:00.900965      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:01.901089      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:02.901323      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:03.901523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:04.902122      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:05.902495      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:06.903597      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:07.904041      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:08.904546      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:09.905589      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:10.906260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:11.907167      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:12.907386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:13.907614      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:14.907630      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:15.907915      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:16.908258      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:17.909183      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:18.910565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:19.911026      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:20.911442      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:21.912127      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:22.912660      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:23.913355      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:24.913420      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:25.913578      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:26.914085      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:27.916522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:28.916769      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:29.917599      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:30.918126      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:31.918584      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:32.920153      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:33.920260      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:34.920538      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:35.920909      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:36.921358      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:37.922338      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:38.922709      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:39.923329      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:40.924061      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:41.925105      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:42.925386      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:43.926166      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:44.926534      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:45.926771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:46.927211      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:47.927537      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:48.927615      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:49.927775      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:50.928390      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:51.928450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:52.928644      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:53.929336      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:54.929989      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:55.930488      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:56.931110      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:57.933853      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:58.933945      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:32:59.934165      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:00.935179      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:01.935933      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:02.936427      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:03.937047      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:04.936987      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:05.937868      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:06.938367      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:07.949609      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:08.950193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:09.952065      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:10.952058      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:11.953196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:12.953522      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:13.954357      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:14.954510      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:15.955243      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:16.955360      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:17.956514      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:18.957097      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:19.957565      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:20.958255      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:21.958817      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:22.958969      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:23.959491      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:24.960382      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:25.960580      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:26.961400      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:27.970430      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:28.970754      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:29.971467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:30.971753      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:31.971832      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:32.972050      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:33.972571      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:34.973044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:35.973399      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:36.973738      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:37.981125      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:38.982257      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:39.983504      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:40.984223      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:41.985160      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:42.985289      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:43.986229      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:44.986469      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:45.986575      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:46.987161      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:47.993120      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:48.993530      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:49.994185      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:50.995463      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:51.995281      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:52.995528      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:53.995771      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:54.995763      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:55.996964      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:56.998859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:58.000383      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:33:59.000608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:00.000665      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:01.000918      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:02.001931      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:03.002952      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:04.003392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:05.003595      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:06.004512      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:07.005182      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:08.007262      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:09.008012      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:10.009193      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:11.009393      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:12.010523      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:13.010683      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:14.010749      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:15.011044      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:16.011270      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:17.012146      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:18.012608      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:19.013128      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:20.013194      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:21.013903      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:22.014826      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:23.015366      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:24.016392      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:25.017030      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:26.017521      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:27.017871      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:28.024991      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:29.019966      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:30.021073      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:31.021460      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:32.022007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:33.022118      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:34.022810      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:35.023550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:36.023550      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:37.024605      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:38.032159      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:39.032450      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:40.033627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:41.033841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:42.034447      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:43.035180      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:44.035486      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:45.035732      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:46.036345      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:47.036627      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:48.038164      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:49.038217      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:50.039350      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:51.039835      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:52.040943      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:53.041739      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:54.042859      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:55.043467      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:56.043713      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:57.044616      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:58.044657      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:34:59.044981      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:00.045913      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:01.046196      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/18/23 10:35:01.965
  May 18 10:35:01.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2552" for this suite. @ 05/18/23 10:35:01.983
• [306.081 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/18/23 10:35:02.007
  May 18 10:35:02.008: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename containers @ 05/18/23 10:35:02.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:35:02.041
  E0518 10:35:02.046306      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:35:02.051
  STEP: Creating a pod to test override all @ 05/18/23 10:35:02.057
  E0518 10:35:03.046502      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:04.047093      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:05.047007      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:06.047316      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/18/23 10:35:06.086
  May 18 10:35:06.092: INFO: Trying to get logs from node ck-test-kube-1-27-worker pod client-containers-d32169b2-37eb-4dc2-887b-a0dd22097a04 container agnhost-container: <nil>
  STEP: delete the pod @ 05/18/23 10:35:06.133
  May 18 10:35:06.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-119" for this suite. @ 05/18/23 10:35:06.165
• [4.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/18/23 10:35:06.196
  May 18 10:35:06.196: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/18/23 10:35:06.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:35:06.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:35:06.219
  STEP: create the container to handle the HTTPGet hook request. @ 05/18/23 10:35:06.229
  E0518 10:35:07.048721      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:08.049863      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/18/23 10:35:08.256
  E0518 10:35:09.049941      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:10.050927      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 05/18/23 10:35:10.283
  E0518 10:35:11.051553      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:12.051325      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:13.051821      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:14.052298      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 05/18/23 10:35:14.309
  May 18 10:35:14.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1501" for this suite. @ 05/18/23 10:35:14.337
• [8.148 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/18/23 10:35:14.345
  May 18 10:35:14.345: INFO: >>> kubeConfig: /tmp/kubeconfig-429586168
  STEP: Building a namespace api object, basename resourcequota @ 05/18/23 10:35:14.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/18/23 10:35:14.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/18/23 10:35:14.364
  STEP: Counting existing ResourceQuota @ 05/18/23 10:35:14.369
  E0518 10:35:15.052465      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:16.052985      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:17.053829      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:18.057123      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:19.057552      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/18/23 10:35:19.375
  STEP: Ensuring resource quota status is calculated @ 05/18/23 10:35:19.38
  E0518 10:35:20.057875      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:21.058837      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 05/18/23 10:35:21.385
  STEP: Ensuring resource quota status captures replication controller creation @ 05/18/23 10:35:21.4
  E0518 10:35:22.059631      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:23.061244      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 05/18/23 10:35:23.407
  STEP: Ensuring resource quota status released usage @ 05/18/23 10:35:23.413
  E0518 10:35:24.061129      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0518 10:35:25.061841      19 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 18 10:35:25.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8787" for this suite. @ 05/18/23 10:35:25.424
• [11.086 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May 18 10:35:25.447: INFO: Running AfterSuite actions on node 1
  May 18 10:35:25.447: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.001 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.086 seconds]
------------------------------

Ran 378 of 7207 Specs in 6148.424 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h42m29.04669996s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

