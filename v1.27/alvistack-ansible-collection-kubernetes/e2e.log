  I0729 15:25:34.950927      14 e2e.go:117] Starting e2e run "4165bb08-2af4-4df8-8e08-20c77064136a" on Ginkgo node 1
  Jul 29 15:25:35.058: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1690644334 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jul 29 15:25:35.395: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:25:35.401: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jul 29 15:25:35.473: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jul 29 15:25:35.483: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
  Jul 29 15:25:35.483: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
  Jul 29 15:25:35.483: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Jul 29 15:25:35.483: INFO: e2e test version: v1.27.4
  Jul 29 15:25:35.485: INFO: kube-apiserver version: v1.27.4
  Jul 29 15:25:35.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:25:35.494: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.099 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 07/29/23 15:25:36.107
  Jul 29 15:25:36.107: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 15:25:36.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:25:36.138
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:25:36.141
  STEP: Creating pod busybox-1d61d2cd-761d-4daa-aa08-eebcb2efd0bf in namespace container-probe-8800 @ 07/29/23 15:25:36.144
  Jul 29 15:25:40.187: INFO: Started pod busybox-1d61d2cd-761d-4daa-aa08-eebcb2efd0bf in namespace container-probe-8800
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 15:25:40.187
  Jul 29 15:25:40.192: INFO: Initial restart count of pod busybox-1d61d2cd-761d-4daa-aa08-eebcb2efd0bf is 0
  Jul 29 15:29:41.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 15:29:41.398
  STEP: Destroying namespace "container-probe-8800" for this suite. @ 07/29/23 15:29:41.419
• [245.323 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 07/29/23 15:29:41.436
  Jul 29 15:29:41.436: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 15:29:41.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:29:41.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:29:41.504
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/29/23 15:29:41.51
  STEP: Saw pod success @ 07/29/23 15:30:13.738
  Jul 29 15:30:13.744: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-8751f4f3-e073-4020-80e8-6fb1db6f56f3 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 15:30:13.778
  Jul 29 15:30:13.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7196" for this suite. @ 07/29/23 15:30:13.82
• [32.396 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 07/29/23 15:30:13.834
  Jul 29 15:30:13.834: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 15:30:13.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:30:13.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:30:13.873
  Jul 29 15:30:13.925: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 07/29/23 15:30:13.938
  Jul 29 15:30:13.948: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:13.948: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 07/29/23 15:30:13.948
  Jul 29 15:30:14.014: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:14.014: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:15.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:15.023: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:16.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:16.023: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:17.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:17.023: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:18.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:18.027: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:19.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:19.020: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:20.028: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:20.028: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:21.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:21.023: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:22.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:22.023: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:23.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:23.027: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:24.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:24.024: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:25.023: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:25.023: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:26.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:26.024: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:27.022: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:27.022: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:28.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:28.025: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:29.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:29.024: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:30.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:30.024: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:31.026: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:31.026: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:32.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:32.024: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:33.025: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:33.025: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:34.024: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:34.024: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:35.027: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 15:30:35.027: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 07/29/23 15:30:35.033
  Jul 29 15:30:35.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:35.094: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 07/29/23 15:30:35.095
  Jul 29 15:30:35.115: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:35.118: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:36.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:36.127: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:37.130: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:37.130: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:38.126: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:38.126: INFO: Node vucheipi7kei-3 is running 0 daemon pod, expected 1
  Jul 29 15:30:39.127: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 15:30:39.127: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 15:30:39.136
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-441, will wait for the garbage collector to delete the pods @ 07/29/23 15:30:39.137
  Jul 29 15:30:39.207: INFO: Deleting DaemonSet.extensions daemon-set took: 13.651016ms
  Jul 29 15:30:39.308: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.305351ms
  Jul 29 15:30:40.318: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:30:40.318: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 15:30:40.331: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"3080"},"items":null}

  Jul 29 15:30:40.340: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"3080"},"items":null}

  Jul 29 15:30:40.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-441" for this suite. @ 07/29/23 15:30:40.408
• [26.586 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 07/29/23 15:30:40.424
  Jul 29 15:30:40.424: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 15:30:40.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:30:40.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:30:40.467
  STEP: Create a Replicaset @ 07/29/23 15:30:40.476
  STEP: Verify that the required pods have come up. @ 07/29/23 15:30:40.489
  Jul 29 15:30:40.494: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 15:30:45.515: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 15:30:45.516
  STEP: Getting /status @ 07/29/23 15:30:45.517
  Jul 29 15:30:45.529: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 07/29/23 15:30:45.53
  Jul 29 15:30:45.562: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 07/29/23 15:30:45.562
  Jul 29 15:30:45.572: INFO: Observed &ReplicaSet event: ADDED
  Jul 29 15:30:45.572: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 15:30:45.572: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 15:30:45.573: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 15:30:45.574: INFO: Found replicaset test-rs in namespace replicaset-7154 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 15:30:45.574: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 07/29/23 15:30:45.574
  Jul 29 15:30:45.574: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 29 15:30:45.586: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 07/29/23 15:30:45.586
  Jul 29 15:30:45.589: INFO: Observed &ReplicaSet event: ADDED
  Jul 29 15:30:45.589: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 15:30:45.590: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 15:30:45.590: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 15:30:45.591: INFO: Observed replicaset test-rs in namespace replicaset-7154 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 15:30:45.591: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 15:30:45.591: INFO: Found replicaset test-rs in namespace replicaset-7154 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jul 29 15:30:45.592: INFO: Replicaset test-rs has a patched status
  Jul 29 15:30:45.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7154" for this suite. @ 07/29/23 15:30:45.61
• [5.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 07/29/23 15:30:45.647
  Jul 29 15:30:45.647: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 15:30:45.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:30:45.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:30:45.682
  Jul 29 15:30:45.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4398" for this suite. @ 07/29/23 15:30:45.817
• [0.182 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 07/29/23 15:30:45.833
  Jul 29 15:30:45.833: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 15:30:45.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:30:45.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:30:45.873
  Jul 29 15:30:45.909: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 15:31:45.979: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/29/23 15:31:45.995
  Jul 29 15:31:46.041: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 29 15:31:46.075: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 29 15:31:46.173: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 29 15:31:46.225: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 29 15:31:46.263: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 29 15:31:46.284: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/29/23 15:31:46.285
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 07/29/23 15:31:50.349
  Jul 29 15:31:54.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-9607" for this suite. @ 07/29/23 15:31:54.561
• [68.736 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 07/29/23 15:31:54.572
  Jul 29 15:31:54.572: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 07/29/23 15:31:54.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:31:54.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:31:54.612
  STEP: creating a target pod @ 07/29/23 15:31:54.618
  STEP: adding an ephemeral container @ 07/29/23 15:31:56.658
  STEP: checking pod container endpoints @ 07/29/23 15:32:00.714
  Jul 29 15:32:00.715: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-3872 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 15:32:00.715: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:32:00.716: INFO: ExecWithOptions: Clientset creation
  Jul 29 15:32:00.716: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-3872/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jul 29 15:32:00.882: INFO: Exec stderr: ""
  Jul 29 15:32:00.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-3872" for this suite. @ 07/29/23 15:32:00.945
• [6.384 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 07/29/23 15:32:00.966
  Jul 29 15:32:00.966: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:32:00.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:01.007
  STEP: Creating projection with secret that has name projected-secret-test-c11affc7-bf3c-454f-bd7b-9832b81bb520 @ 07/29/23 15:32:01.013
  STEP: Creating a pod to test consume secrets @ 07/29/23 15:32:01.026
  STEP: Saw pod success @ 07/29/23 15:32:05.075
  Jul 29 15:32:05.082: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-secrets-54aeb06b-450b-45f2-95db-0c019632161d container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 15:32:05.094
  Jul 29 15:32:05.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-624" for this suite. @ 07/29/23 15:32:05.14
• [4.184 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 07/29/23 15:32:05.152
  Jul 29 15:32:05.152: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename disruption @ 07/29/23 15:32:05.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:05.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:05.187
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:32:05.2
  STEP: Waiting for all pods to be running @ 07/29/23 15:32:07.275
  Jul 29 15:32:07.297: INFO: running pods: 0 < 3
  Jul 29 15:32:09.307: INFO: running pods: 2 < 3
  Jul 29 15:32:11.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4738" for this suite. @ 07/29/23 15:32:11.318
• [6.179 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 07/29/23 15:32:11.332
  Jul 29 15:32:11.332: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 15:32:11.334
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:11.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:11.363
  STEP: Creating a pod to test substitution in container's command @ 07/29/23 15:32:11.367
  STEP: Saw pod success @ 07/29/23 15:32:15.406
  Jul 29 15:32:15.414: INFO: Trying to get logs from node vucheipi7kei-3 pod var-expansion-737d0117-3c10-481d-aa4d-826382156c08 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 15:32:15.427
  Jul 29 15:32:15.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8595" for this suite. @ 07/29/23 15:32:15.458
• [4.135 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 07/29/23 15:32:15.47
  Jul 29 15:32:15.470: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 15:32:15.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:15.5
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:15.507
  STEP: creating service multi-endpoint-test in namespace services-5797 @ 07/29/23 15:32:15.512
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5797 to expose endpoints map[] @ 07/29/23 15:32:15.54
  Jul 29 15:32:15.562: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Jul 29 15:32:16.581: INFO: successfully validated that service multi-endpoint-test in namespace services-5797 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-5797 @ 07/29/23 15:32:16.582
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5797 to expose endpoints map[pod1:[100]] @ 07/29/23 15:32:18.638
  Jul 29 15:32:18.666: INFO: successfully validated that service multi-endpoint-test in namespace services-5797 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-5797 @ 07/29/23 15:32:18.666
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5797 to expose endpoints map[pod1:[100] pod2:[101]] @ 07/29/23 15:32:28.736
  Jul 29 15:32:28.760: INFO: successfully validated that service multi-endpoint-test in namespace services-5797 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 07/29/23 15:32:28.76
  Jul 29 15:32:28.761: INFO: Creating new exec pod
  Jul 29 15:32:31.798: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-5797 exec execpodr775c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jul 29 15:32:32.260: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jul 29 15:32:32.260: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:32:32.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-5797 exec execpodr775c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.39.132 80'
  Jul 29 15:32:32.512: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.39.132 80\nConnection to 10.233.39.132 80 port [tcp/http] succeeded!\n"
  Jul 29 15:32:32.512: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:32:32.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-5797 exec execpodr775c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jul 29 15:32:32.754: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jul 29 15:32:32.754: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:32:32.754: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-5797 exec execpodr775c -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.39.132 81'
  Jul 29 15:32:33.020: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.39.132 81\nConnection to 10.233.39.132 81 port [tcp/*] succeeded!\n"
  Jul 29 15:32:33.021: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-5797 @ 07/29/23 15:32:33.021
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5797 to expose endpoints map[pod2:[101]] @ 07/29/23 15:32:33.049
  Jul 29 15:32:33.091: INFO: successfully validated that service multi-endpoint-test in namespace services-5797 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-5797 @ 07/29/23 15:32:33.092
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5797 to expose endpoints map[] @ 07/29/23 15:32:33.151
  Jul 29 15:32:33.190: INFO: successfully validated that service multi-endpoint-test in namespace services-5797 exposes endpoints map[]
  Jul 29 15:32:33.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5797" for this suite. @ 07/29/23 15:32:33.268
• [17.827 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 07/29/23 15:32:33.307
  Jul 29 15:32:33.307: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 15:32:33.309
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:33.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:33.343
  STEP: Given a ReplicationController is created @ 07/29/23 15:32:33.353
  STEP: When the matched label of one of its pods change @ 07/29/23 15:32:33.37
  Jul 29 15:32:33.377: INFO: Pod name pod-release: Found 0 pods out of 1
  Jul 29 15:32:38.389: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/29/23 15:32:38.41
  Jul 29 15:32:39.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2908" for this suite. @ 07/29/23 15:32:39.494
• [6.198 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 07/29/23 15:32:39.521
  Jul 29 15:32:39.522: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 15:32:39.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:39.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:39.563
  STEP: Creating configMap with name configmap-test-volume-map-9566d7dd-8ed8-42c6-aebe-5c6af3cce88a @ 07/29/23 15:32:39.57
  STEP: Creating a pod to test consume configMaps @ 07/29/23 15:32:39.582
  STEP: Saw pod success @ 07/29/23 15:32:43.631
  Jul 29 15:32:43.639: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-1a21af74-575d-4d1b-8e2f-0a8cce7c07be container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 15:32:43.652
  Jul 29 15:32:43.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7275" for this suite. @ 07/29/23 15:32:43.704
• [4.214 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 07/29/23 15:32:43.737
  Jul 29 15:32:43.738: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename job @ 07/29/23 15:32:43.74
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:43.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:43.776
  STEP: Creating a suspended job @ 07/29/23 15:32:43.785
  STEP: Patching the Job @ 07/29/23 15:32:43.799
  STEP: Watching for Job to be patched @ 07/29/23 15:32:43.829
  Jul 29 15:32:43.833: INFO: Event ADDED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 29 15:32:43.834: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 29 15:32:43.834: INFO: Event MODIFIED found for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 07/29/23 15:32:43.835
  STEP: Watching for Job to be updated @ 07/29/23 15:32:43.859
  Jul 29 15:32:43.863: INFO: Event MODIFIED found for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:43.863: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 07/29/23 15:32:43.863
  Jul 29 15:32:43.877: INFO: Job: e2e-pkzkl as labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched]
  STEP: Waiting for job to complete @ 07/29/23 15:32:43.877
  STEP: Delete a job collection with a labelselector @ 07/29/23 15:32:55.885
  STEP: Watching for Job to be deleted @ 07/29/23 15:32:55.901
  Jul 29 15:32:55.910: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.910: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.910: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.910: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.911: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.911: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.912: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.912: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.912: INFO: Event MODIFIED observed for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 15:32:55.913: INFO: Event DELETED found for Job e2e-pkzkl in namespace job-3117 with labels: map[e2e-job-label:e2e-pkzkl e2e-pkzkl:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 07/29/23 15:32:55.913
  Jul 29 15:32:55.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3117" for this suite. @ 07/29/23 15:32:55.953
• [12.230 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 07/29/23 15:32:55.968
  Jul 29 15:32:55.968: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 15:32:55.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:56.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:56.006
  STEP: Setting up server cert @ 07/29/23 15:32:56.059
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 15:32:57.419
  STEP: Deploying the webhook pod @ 07/29/23 15:32:57.439
  STEP: Wait for the deployment to be ready @ 07/29/23 15:32:57.459
  Jul 29 15:32:57.526: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 15:32:59.546
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 15:32:59.574
  Jul 29 15:33:00.575: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/29/23 15:33:00.696
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 15:33:00.764
  STEP: Deleting the collection of validation webhooks @ 07/29/23 15:33:00.842
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 15:33:00.946
  Jul 29 15:33:00.974: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9227" for this suite. @ 07/29/23 15:33:01.124
  STEP: Destroying namespace "webhook-markers-7140" for this suite. @ 07/29/23 15:33:01.153
• [5.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 07/29/23 15:33:01.183
  Jul 29 15:33:01.183: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename tables @ 07/29/23 15:33:01.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:33:01.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:33:01.229
  Jul 29 15:33:01.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-3331" for this suite. @ 07/29/23 15:33:01.251
• [0.086 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 07/29/23 15:33:01.27
  Jul 29 15:33:01.270: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 15:33:01.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:33:01.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:33:01.31
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/29/23 15:33:01.319
  STEP: Saw pod success @ 07/29/23 15:33:05.369
  Jul 29 15:33:05.375: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-85580b8c-8462-4519-9abd-e38013d7a90b container test-container: <nil>
  STEP: delete the pod @ 07/29/23 15:33:05.387
  Jul 29 15:33:05.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7088" for this suite. @ 07/29/23 15:33:05.422
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 07/29/23 15:33:05.435
  Jul 29 15:33:05.435: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:33:05.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:33:05.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:33:05.519
  STEP: Creating configMap with name projected-configmap-test-volume-4174c932-e8f6-47a2-b625-020f596dcb5e @ 07/29/23 15:33:05.524
  STEP: Creating a pod to test consume configMaps @ 07/29/23 15:33:05.532
  STEP: Saw pod success @ 07/29/23 15:33:09.602
  Jul 29 15:33:09.608: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-configmaps-d7db2b2c-f013-4e18-bfb4-3fd6bd93f2e3 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 15:33:09.621
  Jul 29 15:33:09.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4920" for this suite. @ 07/29/23 15:33:09.685
• [4.265 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 07/29/23 15:33:09.705
  Jul 29 15:33:09.705: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 15:33:09.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:33:09.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:33:09.748
  STEP: Creating a suspended cronjob @ 07/29/23 15:33:09.753
  STEP: Ensuring no jobs are scheduled @ 07/29/23 15:33:09.767
  STEP: Ensuring no job exists by listing jobs explicitly @ 07/29/23 15:38:09.787
  STEP: Removing cronjob @ 07/29/23 15:38:09.793
  Jul 29 15:38:09.805: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7306" for this suite. @ 07/29/23 15:38:09.822
• [300.129 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 07/29/23 15:38:09.834
  Jul 29 15:38:09.834: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 15:38:09.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:38:09.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:38:09.878
  Jul 29 15:38:09.882: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5326 version'
  Jul 29 15:38:10.011: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jul 29 15:38:10.011: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:20:54Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:14:49Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jul 29 15:38:10.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5326" for this suite. @ 07/29/23 15:38:10.025
• [0.207 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 07/29/23 15:38:10.042
  Jul 29 15:38:10.042: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 15:38:10.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:38:10.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:38:10.072
  Jul 29 15:38:10.076: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 15:38:10.099: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 15:38:10.105: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-1 before test
  Jul 29 15:38:10.121: INFO: cilium-lk6kf from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.122: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 15:38:10.122: INFO: cilium-node-init-gs57c from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.123: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 15:38:10.123: INFO: kube-addon-manager-vucheipi7kei-1 from kube-system started at 2023-07-29 15:23:00 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.123: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 15:38:10.124: INFO: kube-apiserver-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.124: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 15:38:10.125: INFO: kube-controller-manager-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.125: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 15:38:10.125: INFO: kube-proxy-qkz2r from kube-system started at 2023-07-29 15:14:13 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.126: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 15:38:10.126: INFO: kube-scheduler-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.126: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 15:38:10.127: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-4kwpr from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 15:38:10.127: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 15:38:10.127: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 15:38:10.128: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-2 before test
  Jul 29 15:38:10.146: INFO: cilium-cqdq2 from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: cilium-node-init-5p29d from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: coredns-5d78c9869d-67zm5 from kube-system started at 2023-07-29 15:24:06 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: coredns-5d78c9869d-kpth8 from kube-system started at 2023-07-29 15:24:06 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: kube-addon-manager-vucheipi7kei-2 from kube-system started at 2023-07-29 15:23:00 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: kube-apiserver-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: kube-controller-manager-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: kube-proxy-n67bc from kube-system started at 2023-07-29 15:14:48 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: kube-scheduler-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-hpqzb from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 15:38:10.146: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 15:38:10.146: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-3 before test
  Jul 29 15:38:10.163: INFO: cilium-9924s from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.163: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 15:38:10.163: INFO: cilium-node-init-ndt4w from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.163: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 15:38:10.163: INFO: cilium-operator-64cdf5fc9d-wj2rs from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.163: INFO: 	Container cilium-operator ready: true, restart count 0
  Jul 29 15:38:10.163: INFO: kube-proxy-szdbr from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.163: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 15:38:10.163: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (1 container statuses recorded)
  Jul 29 15:38:10.163: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 15:38:10.163: INFO: sonobuoy-e2e-job-e435975ae918422f from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 15:38:10.163: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 15:38:10.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 15:38:10.163: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-rwzlm from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 15:38:10.163: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 15:38:10.163: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node vucheipi7kei-1 @ 07/29/23 15:38:10.212
  STEP: verifying the node has the label node vucheipi7kei-2 @ 07/29/23 15:38:10.252
  STEP: verifying the node has the label node vucheipi7kei-3 @ 07/29/23 15:38:10.301
  Jul 29 15:38:10.342: INFO: Pod cilium-9924s requesting resource cpu=0m on Node vucheipi7kei-3
  Jul 29 15:38:10.343: INFO: Pod cilium-cqdq2 requesting resource cpu=0m on Node vucheipi7kei-2
  Jul 29 15:38:10.343: INFO: Pod cilium-lk6kf requesting resource cpu=0m on Node vucheipi7kei-1
  Jul 29 15:38:10.343: INFO: Pod cilium-node-init-5p29d requesting resource cpu=100m on Node vucheipi7kei-2
  Jul 29 15:38:10.344: INFO: Pod cilium-node-init-gs57c requesting resource cpu=100m on Node vucheipi7kei-1
  Jul 29 15:38:10.345: INFO: Pod cilium-node-init-ndt4w requesting resource cpu=100m on Node vucheipi7kei-3
  Jul 29 15:38:10.345: INFO: Pod cilium-operator-64cdf5fc9d-wj2rs requesting resource cpu=0m on Node vucheipi7kei-3
  Jul 29 15:38:10.345: INFO: Pod coredns-5d78c9869d-67zm5 requesting resource cpu=100m on Node vucheipi7kei-2
  Jul 29 15:38:10.346: INFO: Pod coredns-5d78c9869d-kpth8 requesting resource cpu=100m on Node vucheipi7kei-2
  Jul 29 15:38:10.346: INFO: Pod kube-addon-manager-vucheipi7kei-1 requesting resource cpu=5m on Node vucheipi7kei-1
  Jul 29 15:38:10.347: INFO: Pod kube-addon-manager-vucheipi7kei-2 requesting resource cpu=5m on Node vucheipi7kei-2
  Jul 29 15:38:10.347: INFO: Pod kube-apiserver-vucheipi7kei-1 requesting resource cpu=250m on Node vucheipi7kei-1
  Jul 29 15:38:10.347: INFO: Pod kube-apiserver-vucheipi7kei-2 requesting resource cpu=250m on Node vucheipi7kei-2
  Jul 29 15:38:10.347: INFO: Pod kube-controller-manager-vucheipi7kei-1 requesting resource cpu=200m on Node vucheipi7kei-1
  Jul 29 15:38:10.348: INFO: Pod kube-controller-manager-vucheipi7kei-2 requesting resource cpu=200m on Node vucheipi7kei-2
  Jul 29 15:38:10.348: INFO: Pod kube-proxy-n67bc requesting resource cpu=0m on Node vucheipi7kei-2
  Jul 29 15:38:10.348: INFO: Pod kube-proxy-qkz2r requesting resource cpu=0m on Node vucheipi7kei-1
  Jul 29 15:38:10.348: INFO: Pod kube-proxy-szdbr requesting resource cpu=0m on Node vucheipi7kei-3
  Jul 29 15:38:10.349: INFO: Pod kube-scheduler-vucheipi7kei-1 requesting resource cpu=100m on Node vucheipi7kei-1
  Jul 29 15:38:10.349: INFO: Pod kube-scheduler-vucheipi7kei-2 requesting resource cpu=100m on Node vucheipi7kei-2
  Jul 29 15:38:10.349: INFO: Pod sonobuoy requesting resource cpu=0m on Node vucheipi7kei-3
  Jul 29 15:38:10.350: INFO: Pod sonobuoy-e2e-job-e435975ae918422f requesting resource cpu=0m on Node vucheipi7kei-3
  Jul 29 15:38:10.350: INFO: Pod sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-4kwpr requesting resource cpu=0m on Node vucheipi7kei-1
  Jul 29 15:38:10.350: INFO: Pod sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-hpqzb requesting resource cpu=0m on Node vucheipi7kei-2
  Jul 29 15:38:10.351: INFO: Pod sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-rwzlm requesting resource cpu=0m on Node vucheipi7kei-3
  STEP: Starting Pods to consume most of the cluster CPU. @ 07/29/23 15:38:10.351
  Jul 29 15:38:10.351: INFO: Creating a pod which consumes cpu=661m on Node vucheipi7kei-1
  Jul 29 15:38:10.379: INFO: Creating a pod which consumes cpu=521m on Node vucheipi7kei-2
  Jul 29 15:38:10.430: INFO: Creating a pod which consumes cpu=1050m on Node vucheipi7kei-3
  STEP: Creating another pod that requires unavailable amount of CPU. @ 07/29/23 15:38:14.517
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-023bb88f-3ab9-43da-a1ee-62084b3da0cd.177660c09c181657], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6528/filler-pod-023bb88f-3ab9-43da-a1ee-62084b3da0cd to vucheipi7kei-2] @ 07/29/23 15:38:14.527
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-023bb88f-3ab9-43da-a1ee-62084b3da0cd.177660c0d7db982f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 15:38:14.527
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-023bb88f-3ab9-43da-a1ee-62084b3da0cd.177660c0e66785f8], Reason = [Created], Message = [Created container filler-pod-023bb88f-3ab9-43da-a1ee-62084b3da0cd] @ 07/29/23 15:38:14.527
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-023bb88f-3ab9-43da-a1ee-62084b3da0cd.177660c0ea603643], Reason = [Started], Message = [Started container filler-pod-023bb88f-3ab9-43da-a1ee-62084b3da0cd] @ 07/29/23 15:38:14.527
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7a709549-2150-4cda-959e-a277384af9b2.177660c096d55d2d], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6528/filler-pod-7a709549-2150-4cda-959e-a277384af9b2 to vucheipi7kei-1] @ 07/29/23 15:38:14.527
  STEP: Considering event: 
  Type = [Warning], Name = [filler-pod-7a709549-2150-4cda-959e-a277384af9b2.177660c0e18454b1], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-58mnj" : failed to sync configmap cache: timed out waiting for the condition] @ 07/29/23 15:38:14.527
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7a709549-2150-4cda-959e-a277384af9b2.177660c132a6be8b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 15:38:14.528
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7a709549-2150-4cda-959e-a277384af9b2.177660c13f62e2fa], Reason = [Created], Message = [Created container filler-pod-7a709549-2150-4cda-959e-a277384af9b2] @ 07/29/23 15:38:14.528
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7a709549-2150-4cda-959e-a277384af9b2.177660c140d5bfd3], Reason = [Started], Message = [Started container filler-pod-7a709549-2150-4cda-959e-a277384af9b2] @ 07/29/23 15:38:14.528
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f06ece8d-5509-4b05-af22-0323f1608021.177660c09c217730], Reason = [Scheduled], Message = [Successfully assigned sched-pred-6528/filler-pod-f06ece8d-5509-4b05-af22-0323f1608021 to vucheipi7kei-3] @ 07/29/23 15:38:14.528
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f06ece8d-5509-4b05-af22-0323f1608021.177660c0d5148432], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 15:38:14.528
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f06ece8d-5509-4b05-af22-0323f1608021.177660c0dfb84d51], Reason = [Created], Message = [Created container filler-pod-f06ece8d-5509-4b05-af22-0323f1608021] @ 07/29/23 15:38:14.528
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f06ece8d-5509-4b05-af22-0323f1608021.177660c0e1dfdf76], Reason = [Started], Message = [Started container filler-pod-f06ece8d-5509-4b05-af22-0323f1608021] @ 07/29/23 15:38:14.528
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.177660c18d5e90b4], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 07/29/23 15:38:14.553
  STEP: removing the label node off the node vucheipi7kei-1 @ 07/29/23 15:38:15.553
  STEP: verifying the node doesn't have the label node @ 07/29/23 15:38:15.576
  STEP: removing the label node off the node vucheipi7kei-2 @ 07/29/23 15:38:15.586
  STEP: verifying the node doesn't have the label node @ 07/29/23 15:38:15.632
  STEP: removing the label node off the node vucheipi7kei-3 @ 07/29/23 15:38:15.64
  STEP: verifying the node doesn't have the label node @ 07/29/23 15:38:15.666
  Jul 29 15:38:15.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-6528" for this suite. @ 07/29/23 15:38:15.691
• [5.687 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 07/29/23 15:38:15.81
  Jul 29 15:38:15.810: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:38:15.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:38:15.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:38:15.865
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-a118997c-cfc5-4adf-823b-88a786eb77f9 @ 07/29/23 15:38:15.892
  STEP: Creating the pod @ 07/29/23 15:38:15.907
  STEP: Updating configmap projected-configmap-test-upd-a118997c-cfc5-4adf-823b-88a786eb77f9 @ 07/29/23 15:38:18.029
  STEP: waiting to observe update in volume @ 07/29/23 15:38:18.036
  Jul 29 15:39:40.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2251" for this suite. @ 07/29/23 15:39:40.812
• [85.015 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 07/29/23 15:39:40.833
  Jul 29 15:39:40.833: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 15:39:40.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:39:40.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:39:40.875
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 07/29/23 15:39:40.881
  Jul 29 15:39:40.883: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:39:42.940: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:39:50.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4906" for this suite. @ 07/29/23 15:39:50.497
• [9.680 seconds]
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 07/29/23 15:39:50.514
  Jul 29 15:39:50.514: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 15:39:50.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:39:50.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:39:50.561
  STEP: getting /apis @ 07/29/23 15:39:50.568
  STEP: getting /apis/discovery.k8s.io @ 07/29/23 15:39:50.583
  STEP: getting /apis/discovery.k8s.iov1 @ 07/29/23 15:39:50.586
  STEP: creating @ 07/29/23 15:39:50.589
  STEP: getting @ 07/29/23 15:39:50.625
  STEP: listing @ 07/29/23 15:39:50.631
  STEP: watching @ 07/29/23 15:39:50.641
  Jul 29 15:39:50.641: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 15:39:50.644
  STEP: cluster-wide watching @ 07/29/23 15:39:50.653
  Jul 29 15:39:50.653: INFO: starting watch
  STEP: patching @ 07/29/23 15:39:50.656
  STEP: updating @ 07/29/23 15:39:50.666
  Jul 29 15:39:50.688: INFO: waiting for watch events with expected annotations
  Jul 29 15:39:50.688: INFO: saw patched and updated annotations
  STEP: deleting @ 07/29/23 15:39:50.688
  STEP: deleting a collection @ 07/29/23 15:39:50.722
  Jul 29 15:39:50.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5172" for this suite. @ 07/29/23 15:39:50.769
• [0.273 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 07/29/23 15:39:50.787
  Jul 29 15:39:50.787: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 15:39:50.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:39:50.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:39:50.824
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 15:39:50.838
  STEP: create the pod with lifecycle hook @ 07/29/23 15:40:06.946
  STEP: delete the pod with lifecycle hook @ 07/29/23 15:40:08.98
  STEP: check prestop hook @ 07/29/23 15:40:11.008
  Jul 29 15:40:11.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2719" for this suite. @ 07/29/23 15:40:11.05
• [20.274 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 07/29/23 15:40:11.069
  Jul 29 15:40:11.069: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename subpath @ 07/29/23 15:40:11.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:40:11.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:40:11.105
  STEP: Setting up data @ 07/29/23 15:40:11.108
  STEP: Creating pod pod-subpath-test-secret-vp9q @ 07/29/23 15:40:11.125
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 15:40:11.125
  STEP: Saw pod success @ 07/29/23 15:40:35.29
  Jul 29 15:40:35.296: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-subpath-test-secret-vp9q container test-container-subpath-secret-vp9q: <nil>
  STEP: delete the pod @ 07/29/23 15:40:35.331
  STEP: Deleting pod pod-subpath-test-secret-vp9q @ 07/29/23 15:40:35.361
  Jul 29 15:40:35.361: INFO: Deleting pod "pod-subpath-test-secret-vp9q" in namespace "subpath-3143"
  Jul 29 15:40:35.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3143" for this suite. @ 07/29/23 15:40:35.382
• [24.325 seconds]
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 07/29/23 15:40:35.396
  Jul 29 15:40:35.396: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename gc @ 07/29/23 15:40:35.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:40:35.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:40:35.443
  STEP: create the rc @ 07/29/23 15:40:35.452
  W0729 15:40:35.464236      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/29/23 15:40:41.608
  STEP: wait for the rc to be deleted @ 07/29/23 15:40:41.846
  Jul 29 15:40:43.137: INFO: 88 pods remaining
  Jul 29 15:40:43.141: INFO: 80 pods has nil DeletionTimestamp
  Jul 29 15:40:43.141: INFO: 
  Jul 29 15:40:44.029: INFO: 83 pods remaining
  Jul 29 15:40:44.029: INFO: 71 pods has nil DeletionTimestamp
  Jul 29 15:40:44.029: INFO: 
  Jul 29 15:40:45.470: INFO: 77 pods remaining
  Jul 29 15:40:45.470: INFO: 56 pods has nil DeletionTimestamp
  Jul 29 15:40:45.470: INFO: 
  Jul 29 15:40:46.745: INFO: 66 pods remaining
  Jul 29 15:40:46.760: INFO: 33 pods has nil DeletionTimestamp
  Jul 29 15:40:46.840: INFO: 
  Jul 29 15:40:47.083: INFO: 63 pods remaining
  Jul 29 15:40:47.083: INFO: 27 pods has nil DeletionTimestamp
  Jul 29 15:40:47.083: INFO: 
  Jul 29 15:40:48.001: INFO: 62 pods remaining
  Jul 29 15:40:48.002: INFO: 17 pods has nil DeletionTimestamp
  Jul 29 15:40:48.002: INFO: 
  Jul 29 15:40:48.906: INFO: 59 pods remaining
  Jul 29 15:40:48.907: INFO: 3 pods has nil DeletionTimestamp
  Jul 29 15:40:48.907: INFO: 
  Jul 29 15:40:49.953: INFO: 53 pods remaining
  Jul 29 15:40:49.953: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:49.953: INFO: 
  Jul 29 15:40:50.902: INFO: 50 pods remaining
  Jul 29 15:40:50.902: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:50.902: INFO: 
  Jul 29 15:40:51.886: INFO: 42 pods remaining
  Jul 29 15:40:51.886: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:51.886: INFO: 
  Jul 29 15:40:52.906: INFO: 40 pods remaining
  Jul 29 15:40:52.907: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:52.907: INFO: 
  Jul 29 15:40:54.000: INFO: 32 pods remaining
  Jul 29 15:40:54.000: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:54.001: INFO: 
  Jul 29 15:40:54.915: INFO: 23 pods remaining
  Jul 29 15:40:54.915: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:54.915: INFO: 
  Jul 29 15:40:56.020: INFO: 15 pods remaining
  Jul 29 15:40:56.021: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:56.021: INFO: 
  Jul 29 15:40:56.915: INFO: 9 pods remaining
  Jul 29 15:40:56.915: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:56.915: INFO: 
  Jul 29 15:40:58.011: INFO: 1 pods remaining
  Jul 29 15:40:58.012: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:58.012: INFO: 
  Jul 29 15:40:58.893: INFO: 1 pods remaining
  Jul 29 15:40:58.893: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 15:40:58.894: INFO: 
  STEP: Gathering metrics @ 07/29/23 15:40:59.897
  Jul 29 15:41:00.320: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 15:41:00.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4767" for this suite. @ 07/29/23 15:41:00.33
• [25.005 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 07/29/23 15:41:00.404
  Jul 29 15:41:00.404: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename deployment @ 07/29/23 15:41:00.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:41:00.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:41:00.958
  Jul 29 15:41:01.057: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Jul 29 15:41:06.085: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 15:41:06.085
  Jul 29 15:41:06.086: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 07/29/23 15:41:06.148
  Jul 29 15:41:06.215: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2267  e235a436-48f6-4175-b655-4ddfba4f5b65 6829 1 2023-07-29 15:41:06 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-29 15:41:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0064c49b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 29 15:41:06.251: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-2267  ecc2cba9-71bf-4312-bc17-764169bad3cd 6833 1 2023-07-29 15:41:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment e235a436-48f6-4175-b655-4ddfba4f5b65 0xc0063691c7 0xc0063691c8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 15:41:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e235a436-48f6-4175-b655-4ddfba4f5b65\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006369258 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 15:41:06.251: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jul 29 15:41:06.252: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-2267  2401c6e1-dea4-433c-a0d0-cd156f3a9e0c 6832 1 2023-07-29 15:41:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment e235a436-48f6-4175-b655-4ddfba4f5b65 0xc00636909f 0xc0063690b0}] [] [{e2e.test Update apps/v1 2023-07-29 15:41:00 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:41:02 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 15:41:06 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"e235a436-48f6-4175-b655-4ddfba4f5b65\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc006369168 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 15:41:06.271: INFO: Pod "test-cleanup-controller-4l2jh" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-4l2jh test-cleanup-controller- deployment-2267  0eb00005-7e8d-4b7d-846e-f8bfe1cad4e4 6797 0 2023-07-29 15:41:01 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 2401c6e1-dea4-433c-a0d0-cd156f3a9e0c 0xc00636969f 0xc0063696b0}] [] [{kube-controller-manager Update v1 2023-07-29 15:41:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2401c6e1-dea4-433c-a0d0-cd156f3a9e0c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 15:41:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.212\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-ql8g4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-ql8g4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:41:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:41:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:41:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:10.233.65.212,StartTime:2023-07-29 15:41:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 15:41:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://8aa9f4ffd000da7367af7dfd5c004510dd2ddc6ff880615a0f30e5b0de1fb658,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.212,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 15:41:06.271: INFO: Pod "test-cleanup-deployment-68b75d69f8-rtpcx" is not available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-rtpcx test-cleanup-deployment-68b75d69f8- deployment-2267  7728482f-384b-4f7a-9c1c-4eb97042b82b 6837 0 2023-07-29 15:41:06 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 ecc2cba9-71bf-4312-bc17-764169bad3cd 0xc00636986f 0xc0063698a0}] [] [{kube-controller-manager Update v1 2023-07-29 15:41:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ecc2cba9-71bf-4312-bc17-764169bad3cd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d228b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d228b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:41:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 15:41:06.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2267" for this suite. @ 07/29/23 15:41:06.289
• [5.911 seconds]
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 07/29/23 15:41:06.317
  Jul 29 15:41:06.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename dns @ 07/29/23 15:41:06.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:41:06.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:41:06.426
  STEP: Creating a test externalName service @ 07/29/23 15:41:06.431
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1329.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1329.svc.cluster.local; sleep 1; done
   @ 07/29/23 15:41:06.461
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1329.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1329.svc.cluster.local; sleep 1; done
   @ 07/29/23 15:41:06.461
  STEP: creating a pod to probe DNS @ 07/29/23 15:41:06.461
  STEP: submitting the pod to kubernetes @ 07/29/23 15:41:06.461
  STEP: retrieving the pod @ 07/29/23 15:41:44.752
  STEP: looking for the results for each expected name from probers @ 07/29/23 15:41:44.758
  Jul 29 15:41:44.777: INFO: DNS probes using dns-test-cd6c345e-fd50-4bb9-adab-32054c327ca3 succeeded

  STEP: changing the externalName to bar.example.com @ 07/29/23 15:41:44.777
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1329.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1329.svc.cluster.local; sleep 1; done
   @ 07/29/23 15:41:44.802
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1329.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1329.svc.cluster.local; sleep 1; done
   @ 07/29/23 15:41:44.803
  STEP: creating a second pod to probe DNS @ 07/29/23 15:41:44.803
  STEP: submitting the pod to kubernetes @ 07/29/23 15:41:44.803
  STEP: retrieving the pod @ 07/29/23 15:42:23.072
  STEP: looking for the results for each expected name from probers @ 07/29/23 15:42:23.078
  Jul 29 15:42:23.101: INFO: DNS probes using dns-test-6838f44b-7802-4152-94ac-c400dcc1dcb8 succeeded

  STEP: changing the service to type=ClusterIP @ 07/29/23 15:42:23.101
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1329.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1329.svc.cluster.local; sleep 1; done
   @ 07/29/23 15:42:23.14
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1329.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1329.svc.cluster.local; sleep 1; done
   @ 07/29/23 15:42:23.14
  STEP: creating a third pod to probe DNS @ 07/29/23 15:42:23.141
  STEP: submitting the pod to kubernetes @ 07/29/23 15:42:23.149
  STEP: retrieving the pod @ 07/29/23 15:42:25.182
  STEP: looking for the results for each expected name from probers @ 07/29/23 15:42:25.189
  Jul 29 15:42:25.207: INFO: DNS probes using dns-test-eea5d99a-a3bd-4473-a18a-d9b5ef2ed075 succeeded

  Jul 29 15:42:25.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 15:42:25.217
  STEP: deleting the pod @ 07/29/23 15:42:25.243
  STEP: deleting the pod @ 07/29/23 15:42:25.272
  STEP: deleting the test externalName service @ 07/29/23 15:42:25.296
  STEP: Destroying namespace "dns-1329" for this suite. @ 07/29/23 15:42:25.355
• [79.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 07/29/23 15:42:25.376
  Jul 29 15:42:25.376: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename deployment @ 07/29/23 15:42:25.379
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:42:25.408
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:42:25.415
  STEP: creating a Deployment @ 07/29/23 15:42:25.428
  Jul 29 15:42:25.428: INFO: Creating simple deployment test-deployment-dlncq
  Jul 29 15:42:25.460: INFO: new replicaset for deployment "test-deployment-dlncq" is yet to be created
  STEP: Getting /status @ 07/29/23 15:42:27.49
  Jul 29 15:42:27.500: INFO: Deployment test-deployment-dlncq has Conditions: [{Available True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dlncq-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 07/29/23 15:42:27.5
  Jul 29 15:42:27.515: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 42, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 42, 27, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 42, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 42, 25, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dlncq-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 07/29/23 15:42:27.515
  Jul 29 15:42:27.519: INFO: Observed &Deployment event: ADDED
  Jul 29 15:42:27.519: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dlncq-5994cf9475"}
  Jul 29 15:42:27.519: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.520: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dlncq-5994cf9475"}
  Jul 29 15:42:27.520: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 15:42:27.520: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.520: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 15:42:27.520: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dlncq-5994cf9475" is progressing.}
  Jul 29 15:42:27.521: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.521: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 15:42:27.521: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dlncq-5994cf9475" has successfully progressed.}
  Jul 29 15:42:27.521: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.521: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 15:42:27.521: INFO: Observed Deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dlncq-5994cf9475" has successfully progressed.}
  Jul 29 15:42:27.521: INFO: Found Deployment test-deployment-dlncq in namespace deployment-5934 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 15:42:27.521: INFO: Deployment test-deployment-dlncq has an updated status
  STEP: patching the Statefulset Status @ 07/29/23 15:42:27.521
  Jul 29 15:42:27.521: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 29 15:42:27.534: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 07/29/23 15:42:27.534
  Jul 29 15:42:27.538: INFO: Observed &Deployment event: ADDED
  Jul 29 15:42:27.538: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dlncq-5994cf9475"}
  Jul 29 15:42:27.538: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.538: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dlncq-5994cf9475"}
  Jul 29 15:42:27.538: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 15:42:27.538: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.538: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 15:42:27.538: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:25 +0000 UTC 2023-07-29 15:42:25 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dlncq-5994cf9475" is progressing.}
  Jul 29 15:42:27.539: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.539: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 15:42:27.539: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dlncq-5994cf9475" has successfully progressed.}
  Jul 29 15:42:27.539: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.539: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:27 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 15:42:27.539: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 15:42:27 +0000 UTC 2023-07-29 15:42:25 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dlncq-5994cf9475" has successfully progressed.}
  Jul 29 15:42:27.539: INFO: Observed deployment test-deployment-dlncq in namespace deployment-5934 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 15:42:27.539: INFO: Observed &Deployment event: MODIFIED
  Jul 29 15:42:27.539: INFO: Found deployment test-deployment-dlncq in namespace deployment-5934 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jul 29 15:42:27.540: INFO: Deployment test-deployment-dlncq has a patched status
  Jul 29 15:42:27.550: INFO: Deployment "test-deployment-dlncq":
  &Deployment{ObjectMeta:{test-deployment-dlncq  deployment-5934  ff3f79ad-4469-4ff5-993c-ad5b9f149b6a 7609 1 2023-07-29 15:42:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-29 15:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-07-29 15:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-07-29 15:42:27 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025de208 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-dlncq-5994cf9475",LastUpdateTime:2023-07-29 15:42:27 +0000 UTC,LastTransitionTime:2023-07-29 15:42:27 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 15:42:27.558: INFO: New ReplicaSet "test-deployment-dlncq-5994cf9475" of Deployment "test-deployment-dlncq":
  &ReplicaSet{ObjectMeta:{test-deployment-dlncq-5994cf9475  deployment-5934  ccad3b59-d196-4db0-baca-88579f37ffe8 7604 1 2023-07-29 15:42:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dlncq ff3f79ad-4469-4ff5-993c-ad5b9f149b6a 0xc0025deb47 0xc0025deb48}] [] [{kube-controller-manager Update apps/v1 2023-07-29 15:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ff3f79ad-4469-4ff5-993c-ad5b9f149b6a\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:42:27 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0025dee48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 15:42:27.570: INFO: Pod "test-deployment-dlncq-5994cf9475-zbdh4" is available:
  &Pod{ObjectMeta:{test-deployment-dlncq-5994cf9475-zbdh4 test-deployment-dlncq-5994cf9475- deployment-5934  366e0173-aa6f-45bd-8e59-1f7f58c4aa2c 7603 0 2023-07-29 15:42:25 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-dlncq-5994cf9475 ccad3b59-d196-4db0-baca-88579f37ffe8 0xc0026a1587 0xc0026a1588}] [] [{kube-controller-manager Update v1 2023-07-29 15:42:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ccad3b59-d196-4db0-baca-88579f37ffe8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 15:42:27 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.46\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwmt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwmt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:42:25 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:42:27 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:42:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:10.233.65.46,StartTime:2023-07-29 15:42:25 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 15:42:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://19df30d790cbf6f79fb2d6188cd4f62205346e220fd7b909b6dbb4171496a1de,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.46,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 15:42:27.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5934" for this suite. @ 07/29/23 15:42:27.578
• [2.211 seconds]
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 07/29/23 15:42:27.59
  Jul 29 15:42:27.590: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sysctl @ 07/29/23 15:42:27.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:42:27.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:42:27.639
  STEP: Creating a pod with one valid and two invalid sysctls @ 07/29/23 15:42:27.65
  Jul 29 15:42:27.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3651" for this suite. @ 07/29/23 15:42:27.669
• [0.091 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 07/29/23 15:42:27.682
  Jul 29 15:42:27.682: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 15:42:27.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:42:27.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:42:27.787
  STEP: Creating configMap with name configmap-test-volume-53d5f948-70ef-4e56-82b9-02ad289c415d @ 07/29/23 15:42:27.793
  STEP: Creating a pod to test consume configMaps @ 07/29/23 15:42:27.827
  STEP: Saw pod success @ 07/29/23 15:42:31.895
  Jul 29 15:42:31.902: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-a4869506-570b-4540-851c-a34ccd6c1063 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 15:42:31.94
  Jul 29 15:42:31.968: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7413" for this suite. @ 07/29/23 15:42:31.979
• [4.311 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 07/29/23 15:42:32.005
  Jul 29 15:42:32.005: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename subpath @ 07/29/23 15:42:32.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:42:32.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:42:32.056
  STEP: Setting up data @ 07/29/23 15:42:32.061
  STEP: Creating pod pod-subpath-test-configmap-755t @ 07/29/23 15:42:32.08
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 15:42:32.081
  STEP: Saw pod success @ 07/29/23 15:42:56.207
  Jul 29 15:42:56.213: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-subpath-test-configmap-755t container test-container-subpath-configmap-755t: <nil>
  STEP: delete the pod @ 07/29/23 15:42:56.23
  STEP: Deleting pod pod-subpath-test-configmap-755t @ 07/29/23 15:42:56.253
  Jul 29 15:42:56.253: INFO: Deleting pod "pod-subpath-test-configmap-755t" in namespace "subpath-4349"
  Jul 29 15:42:56.258: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4349" for this suite. @ 07/29/23 15:42:56.266
• [24.271 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 07/29/23 15:42:56.282
  Jul 29 15:42:56.282: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 15:42:56.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:42:56.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:42:56.318
  Jul 29 15:42:56.342: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 15:43:56.392: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/29/23 15:43:56.401
  Jul 29 15:43:56.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/29/23 15:43:56.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:43:56.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:43:56.443
  STEP: Finding an available node @ 07/29/23 15:43:56.447
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 15:43:56.448
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 15:43:58.492
  Jul 29 15:43:58.524: INFO: found a healthy node: vucheipi7kei-3
  Jul 29 15:44:04.692: INFO: pods created so far: [1 1 1]
  Jul 29 15:44:04.692: INFO: length of pods created so far: 3
  Jul 29 15:44:06.715: INFO: pods created so far: [2 2 1]
  Jul 29 15:44:13.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 15:44:13.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-1578" for this suite. @ 07/29/23 15:44:13.89
  STEP: Destroying namespace "sched-preemption-6274" for this suite. @ 07/29/23 15:44:13.903
• [77.630 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 07/29/23 15:44:13.915
  Jul 29 15:44:13.915: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 15:44:13.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:44:13.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:44:13.954
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 15:44:13.967
  STEP: create the pod with lifecycle hook @ 07/29/23 15:44:16.003
  STEP: delete the pod with lifecycle hook @ 07/29/23 15:44:18.031
  STEP: check prestop hook @ 07/29/23 15:44:20.053
  Jul 29 15:44:20.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9763" for this suite. @ 07/29/23 15:44:20.079
• [6.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 07/29/23 15:44:20.092
  Jul 29 15:44:20.092: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 15:44:20.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:44:20.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:44:20.136
  STEP: Counting existing ResourceQuota @ 07/29/23 15:44:37.147
  STEP: Creating a ResourceQuota @ 07/29/23 15:44:42.157
  STEP: Ensuring resource quota status is calculated @ 07/29/23 15:44:42.167
  STEP: Creating a ConfigMap @ 07/29/23 15:44:44.173
  STEP: Ensuring resource quota status captures configMap creation @ 07/29/23 15:44:44.188
  STEP: Deleting a ConfigMap @ 07/29/23 15:44:46.199
  STEP: Ensuring resource quota status released usage @ 07/29/23 15:44:46.21
  Jul 29 15:44:48.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7838" for this suite. @ 07/29/23 15:44:48.226
• [28.143 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 07/29/23 15:44:48.238
  Jul 29 15:44:48.238: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename gc @ 07/29/23 15:44:48.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:44:48.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:44:48.276
  STEP: create the deployment @ 07/29/23 15:44:48.28
  W0729 15:44:48.291014      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/29/23 15:44:48.291
  STEP: delete the deployment @ 07/29/23 15:44:48.807
  STEP: wait for all rs to be garbage collected @ 07/29/23 15:44:48.818
  STEP: expected 0 rs, got 1 rs @ 07/29/23 15:44:48.835
  STEP: expected 0 pods, got 2 pods @ 07/29/23 15:44:48.873
  STEP: Gathering metrics @ 07/29/23 15:44:49.393
  Jul 29 15:44:49.551: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 15:44:49.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9572" for this suite. @ 07/29/23 15:44:49.569
• [1.345 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 07/29/23 15:44:49.59
  Jul 29 15:44:49.590: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 15:44:49.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:44:49.711
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:44:49.717
  STEP: Discovering how many secrets are in namespace by default @ 07/29/23 15:44:49.722
  STEP: Counting existing ResourceQuota @ 07/29/23 15:44:54.728
  STEP: Creating a ResourceQuota @ 07/29/23 15:44:59.737
  STEP: Ensuring resource quota status is calculated @ 07/29/23 15:44:59.77
  STEP: Creating a Secret @ 07/29/23 15:45:01.777
  STEP: Ensuring resource quota status captures secret creation @ 07/29/23 15:45:01.797
  STEP: Deleting a secret @ 07/29/23 15:45:03.809
  STEP: Ensuring resource quota status released usage @ 07/29/23 15:45:03.827
  Jul 29 15:45:05.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6956" for this suite. @ 07/29/23 15:45:05.844
• [16.265 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 07/29/23 15:45:05.855
  Jul 29 15:45:05.855: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/29/23 15:45:05.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:45:05.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:45:05.891
  STEP: Creating 50 configmaps @ 07/29/23 15:45:05.903
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 15:45:06.23
  Jul 29 15:45:06.257: INFO: Pod name wrapped-volume-race-7bdff10a-a1e7-42e4-80bb-6946ee125964: Found 0 pods out of 5
  Jul 29 15:45:11.274: INFO: Pod name wrapped-volume-race-7bdff10a-a1e7-42e4-80bb-6946ee125964: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 15:45:11.274
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 15:45:11.324
  Jul 29 15:45:11.344: INFO: Pod name wrapped-volume-race-a98fea5c-322f-4606-8ad2-fb0322c502fa: Found 0 pods out of 5
  Jul 29 15:45:16.371: INFO: Pod name wrapped-volume-race-a98fea5c-322f-4606-8ad2-fb0322c502fa: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 15:45:16.371
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 15:45:18.452
  Jul 29 15:45:18.523: INFO: Pod name wrapped-volume-race-f35b9549-07bd-4020-a86e-f4309c76c275: Found 0 pods out of 5
  Jul 29 15:45:23.569: INFO: Pod name wrapped-volume-race-f35b9549-07bd-4020-a86e-f4309c76c275: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 15:45:23.569
  Jul 29 15:45:23.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-f35b9549-07bd-4020-a86e-f4309c76c275 in namespace emptydir-wrapper-9524, will wait for the garbage collector to delete the pods @ 07/29/23 15:45:23.613
  Jul 29 15:45:23.687: INFO: Deleting ReplicationController wrapped-volume-race-f35b9549-07bd-4020-a86e-f4309c76c275 took: 18.624077ms
  Jul 29 15:45:23.890: INFO: Terminating ReplicationController wrapped-volume-race-f35b9549-07bd-4020-a86e-f4309c76c275 pods took: 202.361852ms
  STEP: deleting ReplicationController wrapped-volume-race-a98fea5c-322f-4606-8ad2-fb0322c502fa in namespace emptydir-wrapper-9524, will wait for the garbage collector to delete the pods @ 07/29/23 15:45:25.993
  Jul 29 15:45:26.070: INFO: Deleting ReplicationController wrapped-volume-race-a98fea5c-322f-4606-8ad2-fb0322c502fa took: 15.396565ms
  Jul 29 15:45:26.271: INFO: Terminating ReplicationController wrapped-volume-race-a98fea5c-322f-4606-8ad2-fb0322c502fa pods took: 201.373021ms
  STEP: deleting ReplicationController wrapped-volume-race-7bdff10a-a1e7-42e4-80bb-6946ee125964 in namespace emptydir-wrapper-9524, will wait for the garbage collector to delete the pods @ 07/29/23 15:45:28.373
  Jul 29 15:45:28.451: INFO: Deleting ReplicationController wrapped-volume-race-7bdff10a-a1e7-42e4-80bb-6946ee125964 took: 15.480312ms
  Jul 29 15:45:28.652: INFO: Terminating ReplicationController wrapped-volume-race-7bdff10a-a1e7-42e4-80bb-6946ee125964 pods took: 200.413967ms
  STEP: Cleaning up the configMaps @ 07/29/23 15:45:30.455
  STEP: Destroying namespace "emptydir-wrapper-9524" for this suite. @ 07/29/23 15:45:30.94
• [25.094 seconds]
------------------------------
S
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 07/29/23 15:45:30.951
  Jul 29 15:45:30.951: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 15:45:30.955
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:45:30.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:45:30.982
  STEP: Performing setup for networking test in namespace pod-network-test-1149 @ 07/29/23 15:45:30.987
  STEP: creating a selector @ 07/29/23 15:45:30.987
  STEP: Creating the service pods in kubernetes @ 07/29/23 15:45:30.987
  Jul 29 15:45:30.987: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 15:45:53.246
  Jul 29 15:45:55.285: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 15:45:55.285: INFO: Breadth first check of 10.233.66.229 on host 192.168.121.88...
  Jul 29 15:45:55.292: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.60:9080/dial?request=hostname&protocol=udp&host=10.233.66.229&port=8081&tries=1'] Namespace:pod-network-test-1149 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 15:45:55.293: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:45:55.295: INFO: ExecWithOptions: Clientset creation
  Jul 29 15:45:55.296: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1149/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.60%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.229%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 15:45:55.454: INFO: Waiting for responses: map[]
  Jul 29 15:45:55.454: INFO: reached 10.233.66.229 after 0/1 tries
  Jul 29 15:45:55.454: INFO: Breadth first check of 10.233.64.148 on host 192.168.121.77...
  Jul 29 15:45:55.462: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.60:9080/dial?request=hostname&protocol=udp&host=10.233.64.148&port=8081&tries=1'] Namespace:pod-network-test-1149 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 15:45:55.462: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:45:55.464: INFO: ExecWithOptions: Clientset creation
  Jul 29 15:45:55.464: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1149/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.60%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.148%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 15:45:55.563: INFO: Waiting for responses: map[]
  Jul 29 15:45:55.563: INFO: reached 10.233.64.148 after 0/1 tries
  Jul 29 15:45:55.564: INFO: Breadth first check of 10.233.65.241 on host 192.168.121.129...
  Jul 29 15:45:55.574: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.60:9080/dial?request=hostname&protocol=udp&host=10.233.65.241&port=8081&tries=1'] Namespace:pod-network-test-1149 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 15:45:55.574: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:45:55.575: INFO: ExecWithOptions: Clientset creation
  Jul 29 15:45:55.575: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-1149/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.60%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.241%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 15:45:55.685: INFO: Waiting for responses: map[]
  Jul 29 15:45:55.685: INFO: reached 10.233.65.241 after 0/1 tries
  Jul 29 15:45:55.686: INFO: Going to retry 0 out of 3 pods....
  Jul 29 15:45:55.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1149" for this suite. @ 07/29/23 15:45:55.696
• [24.759 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 07/29/23 15:45:55.724
  Jul 29 15:45:55.724: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 15:45:55.726
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:45:55.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:45:55.782
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/29/23 15:45:55.786
  STEP: Saw pod success @ 07/29/23 15:45:59.833
  Jul 29 15:45:59.838: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-4e264692-1338-45cf-8986-0aa2f6159765 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 15:45:59.863
  Jul 29 15:45:59.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-571" for this suite. @ 07/29/23 15:45:59.902
• [4.187 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 07/29/23 15:45:59.912
  Jul 29 15:45:59.912: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 15:45:59.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:45:59.941
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:45:59.947
  STEP: Creating resourceQuota "e2e-rq-status-mkjct" @ 07/29/23 15:45:59.954
  Jul 29 15:45:59.966: INFO: Resource quota "e2e-rq-status-mkjct" reports spec: hard cpu limit of 500m
  Jul 29 15:45:59.967: INFO: Resource quota "e2e-rq-status-mkjct" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-mkjct" /status @ 07/29/23 15:45:59.968
  STEP: Confirm /status for "e2e-rq-status-mkjct" resourceQuota via watch @ 07/29/23 15:45:59.979
  Jul 29 15:45:59.981: INFO: observed resourceQuota "e2e-rq-status-mkjct" in namespace "resourcequota-2047" with hard status: v1.ResourceList(nil)
  Jul 29 15:45:59.981: INFO: Found resourceQuota "e2e-rq-status-mkjct" in namespace "resourcequota-2047" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 29 15:45:59.982: INFO: ResourceQuota "e2e-rq-status-mkjct" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 07/29/23 15:45:59.986
  Jul 29 15:45:59.996: INFO: Resource quota "e2e-rq-status-mkjct" reports spec: hard cpu limit of 1
  Jul 29 15:45:59.996: INFO: Resource quota "e2e-rq-status-mkjct" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-mkjct" /status @ 07/29/23 15:45:59.996
  STEP: Confirm /status for "e2e-rq-status-mkjct" resourceQuota via watch @ 07/29/23 15:46:00.009
  Jul 29 15:46:00.012: INFO: observed resourceQuota "e2e-rq-status-mkjct" in namespace "resourcequota-2047" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 29 15:46:00.013: INFO: Found resourceQuota "e2e-rq-status-mkjct" in namespace "resourcequota-2047" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul 29 15:46:00.013: INFO: ResourceQuota "e2e-rq-status-mkjct" /status was patched
  STEP: Get "e2e-rq-status-mkjct" /status @ 07/29/23 15:46:00.014
  Jul 29 15:46:00.023: INFO: Resourcequota "e2e-rq-status-mkjct" reports status: hard cpu of 1
  Jul 29 15:46:00.023: INFO: Resourcequota "e2e-rq-status-mkjct" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-mkjct" /status before checking Spec is unchanged @ 07/29/23 15:46:00.029
  Jul 29 15:46:00.039: INFO: Resourcequota "e2e-rq-status-mkjct" reports status: hard cpu of 2
  Jul 29 15:46:00.040: INFO: Resourcequota "e2e-rq-status-mkjct" reports status: hard memory of 2Gi
  Jul 29 15:46:00.043: INFO: Found resourceQuota "e2e-rq-status-mkjct" in namespace "resourcequota-2047" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Jul 29 15:49:15.059: INFO: ResourceQuota "e2e-rq-status-mkjct" Spec was unchanged and /status reset
  Jul 29 15:49:15.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2047" for this suite. @ 07/29/23 15:49:15.07
• [195.171 seconds]
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 07/29/23 15:49:15.085
  Jul 29 15:49:15.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 15:49:15.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:49:15.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:49:15.139
  STEP: getting /apis @ 07/29/23 15:49:15.144
  STEP: getting /apis/node.k8s.io @ 07/29/23 15:49:15.153
  STEP: getting /apis/node.k8s.io/v1 @ 07/29/23 15:49:15.155
  STEP: creating @ 07/29/23 15:49:15.157
  STEP: watching @ 07/29/23 15:49:15.195
  Jul 29 15:49:15.195: INFO: starting watch
  STEP: getting @ 07/29/23 15:49:15.207
  STEP: listing @ 07/29/23 15:49:15.212
  STEP: patching @ 07/29/23 15:49:15.222
  STEP: updating @ 07/29/23 15:49:15.235
  Jul 29 15:49:15.247: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 07/29/23 15:49:15.248
  STEP: deleting a collection @ 07/29/23 15:49:15.274
  Jul 29 15:49:15.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6174" for this suite. @ 07/29/23 15:49:15.32
• [0.246 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 07/29/23 15:49:15.339
  Jul 29 15:49:15.339: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename limitrange @ 07/29/23 15:49:15.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:49:15.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:49:15.376
  STEP: Creating a LimitRange @ 07/29/23 15:49:15.381
  STEP: Setting up watch @ 07/29/23 15:49:15.382
  STEP: Submitting a LimitRange @ 07/29/23 15:49:15.494
  STEP: Verifying LimitRange creation was observed @ 07/29/23 15:49:15.512
  STEP: Fetching the LimitRange to ensure it has proper values @ 07/29/23 15:49:15.512
  Jul 29 15:49:15.521: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 29 15:49:15.521: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 07/29/23 15:49:15.521
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 07/29/23 15:49:15.53
  Jul 29 15:49:15.538: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 29 15:49:15.538: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 07/29/23 15:49:15.538
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 07/29/23 15:49:15.547
  Jul 29 15:49:15.556: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jul 29 15:49:15.556: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 07/29/23 15:49:15.557
  STEP: Failing to create a Pod with more than max resources @ 07/29/23 15:49:15.56
  STEP: Updating a LimitRange @ 07/29/23 15:49:15.563
  STEP: Verifying LimitRange updating is effective @ 07/29/23 15:49:15.572
  STEP: Creating a Pod with less than former min resources @ 07/29/23 15:49:17.581
  STEP: Failing to create a Pod with more than max resources @ 07/29/23 15:49:17.59
  STEP: Deleting a LimitRange @ 07/29/23 15:49:17.594
  STEP: Verifying the LimitRange was deleted @ 07/29/23 15:49:17.634
  Jul 29 15:49:22.648: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 07/29/23 15:49:22.648
  Jul 29 15:49:22.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-8254" for this suite. @ 07/29/23 15:49:22.679
• [7.352 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 07/29/23 15:49:22.692
  Jul 29 15:49:22.692: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 15:49:22.696
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:49:22.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:49:22.731
  Jul 29 15:49:22.737: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 07/29/23 15:49:23.759
  STEP: Checking rc "condition-test" has the desired failure condition set @ 07/29/23 15:49:23.767
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 07/29/23 15:49:24.791
  Jul 29 15:49:24.811: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 07/29/23 15:49:24.811
  Jul 29 15:49:25.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5474" for this suite. @ 07/29/23 15:49:25.837
• [3.157 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 07/29/23 15:49:25.852
  Jul 29 15:49:25.852: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:49:25.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:49:25.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:49:25.944
  STEP: Creating secret with name projected-secret-test-730fb232-9c97-4944-a368-800f9df9a175 @ 07/29/23 15:49:25.948
  STEP: Creating a pod to test consume secrets @ 07/29/23 15:49:25.958
  STEP: Saw pod success @ 07/29/23 15:49:30.012
  Jul 29 15:49:30.019: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-secrets-d45ddea1-d5bb-4bb5-94dc-4e4c02aab8a2 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 15:49:30.051
  Jul 29 15:49:30.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3685" for this suite. @ 07/29/23 15:49:30.082
• [4.241 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 07/29/23 15:49:30.096
  Jul 29 15:49:30.096: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename subpath @ 07/29/23 15:49:30.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:49:30.133
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:49:30.136
  STEP: Setting up data @ 07/29/23 15:49:30.141
  STEP: Creating pod pod-subpath-test-projected-6qqg @ 07/29/23 15:49:30.155
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 15:49:30.156
  STEP: Saw pod success @ 07/29/23 15:49:54.299
  Jul 29 15:49:54.305: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-subpath-test-projected-6qqg container test-container-subpath-projected-6qqg: <nil>
  STEP: delete the pod @ 07/29/23 15:49:54.322
  STEP: Deleting pod pod-subpath-test-projected-6qqg @ 07/29/23 15:49:54.354
  Jul 29 15:49:54.354: INFO: Deleting pod "pod-subpath-test-projected-6qqg" in namespace "subpath-9393"
  Jul 29 15:49:54.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9393" for this suite. @ 07/29/23 15:49:54.371
• [24.289 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 07/29/23 15:49:54.403
  Jul 29 15:49:54.403: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 15:49:54.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:49:54.437
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:49:54.443
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 15:49:54.497
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 15:49:54.508
  Jul 29 15:49:54.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:49:54.528: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  Jul 29 15:49:55.549: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:49:55.550: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  Jul 29 15:49:56.548: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 15:49:56.548: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  Jul 29 15:49:57.572: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 15:49:57.572: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  Jul 29 15:49:58.544: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 15:49:58.544: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  Jul 29 15:49:59.540: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 15:49:59.541: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  Jul 29 15:50:00.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 15:50:00.543: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  Jul 29 15:50:01.544: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 15:50:01.544: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  Jul 29 15:50:02.543: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 15:50:02.544: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 07/29/23 15:50:02.551
  Jul 29 15:50:02.558: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 07/29/23 15:50:02.558
  Jul 29 15:50:02.575: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 07/29/23 15:50:02.575
  Jul 29 15:50:02.580: INFO: Observed &DaemonSet event: ADDED
  Jul 29 15:50:02.580: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.580: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.581: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.581: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.581: INFO: Found daemon set daemon-set in namespace daemonsets-743 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 15:50:02.581: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 07/29/23 15:50:02.581
  STEP: watching for the daemon set status to be patched @ 07/29/23 15:50:02.595
  Jul 29 15:50:02.599: INFO: Observed &DaemonSet event: ADDED
  Jul 29 15:50:02.600: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.600: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.601: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.601: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.601: INFO: Observed daemon set daemon-set in namespace daemonsets-743 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 15:50:02.602: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 15:50:02.602: INFO: Found daemon set daemon-set in namespace daemonsets-743 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jul 29 15:50:02.602: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 15:50:02.611
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-743, will wait for the garbage collector to delete the pods @ 07/29/23 15:50:02.611
  Jul 29 15:50:02.679: INFO: Deleting DaemonSet.extensions daemon-set took: 12.031926ms
  Jul 29 15:50:02.780: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.40062ms
  Jul 29 15:50:05.587: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 15:50:05.588: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 15:50:05.596: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10078"},"items":null}

  Jul 29 15:50:05.660: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10078"},"items":null}

  Jul 29 15:50:05.707: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-743" for this suite. @ 07/29/23 15:50:05.713
• [11.321 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 07/29/23 15:50:05.727
  Jul 29 15:50:05.727: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 15:50:05.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:50:05.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:50:05.769
  Jul 29 15:50:05.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8004" for this suite. @ 07/29/23 15:50:05.793
• [0.076 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 07/29/23 15:50:05.808
  Jul 29 15:50:05.809: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename watch @ 07/29/23 15:50:05.811
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:50:05.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:50:05.849
  STEP: creating a new configmap @ 07/29/23 15:50:05.854
  STEP: modifying the configmap once @ 07/29/23 15:50:05.864
  STEP: modifying the configmap a second time @ 07/29/23 15:50:05.879
  STEP: deleting the configmap @ 07/29/23 15:50:05.902
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 07/29/23 15:50:05.912
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 07/29/23 15:50:05.915
  Jul 29 15:50:05.915: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7460  b3f98b88-d7d1-4b50-88af-7db3be46f424 10090 0 2023-07-29 15:50:05 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 15:50:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 15:50:05.916: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-7460  b3f98b88-d7d1-4b50-88af-7db3be46f424 10091 0 2023-07-29 15:50:05 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 15:50:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 15:50:05.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-7460" for this suite. @ 07/29/23 15:50:05.924
• [0.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 07/29/23 15:50:05.94
  Jul 29 15:50:05.940: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 15:50:05.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:50:05.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:50:05.975
  STEP: Creating pod liveness-59d64fad-3847-45d7-ab67-ddd842e56381 in namespace container-probe-4441 @ 07/29/23 15:50:05.979
  Jul 29 15:50:08.010: INFO: Started pod liveness-59d64fad-3847-45d7-ab67-ddd842e56381 in namespace container-probe-4441
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 15:50:08.01
  Jul 29 15:50:08.015: INFO: Initial restart count of pod liveness-59d64fad-3847-45d7-ab67-ddd842e56381 is 0
  Jul 29 15:54:09.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 15:54:09.149
  STEP: Destroying namespace "container-probe-4441" for this suite. @ 07/29/23 15:54:09.18
• [243.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 07/29/23 15:54:09.203
  Jul 29 15:54:09.203: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 15:54:09.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:54:09.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:54:09.244
  STEP: Counting existing ResourceQuota @ 07/29/23 15:54:09.249
  STEP: Creating a ResourceQuota @ 07/29/23 15:54:14.255
  STEP: Ensuring resource quota status is calculated @ 07/29/23 15:54:14.265
  STEP: Creating a Pod that fits quota @ 07/29/23 15:54:16.272
  STEP: Ensuring ResourceQuota status captures the pod usage @ 07/29/23 15:54:16.298
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 07/29/23 15:54:18.314
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 07/29/23 15:54:18.319
  STEP: Ensuring a pod cannot update its resource requirements @ 07/29/23 15:54:18.324
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 07/29/23 15:54:18.334
  STEP: Deleting the pod @ 07/29/23 15:54:20.34
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 15:54:20.368
  Jul 29 15:54:22.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1743" for this suite. @ 07/29/23 15:54:22.391
• [13.206 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 07/29/23 15:54:22.41
  Jul 29 15:54:22.410: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 15:54:22.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:54:22.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:54:22.448
  STEP: Creating service test in namespace statefulset-4530 @ 07/29/23 15:54:22.453
  STEP: Creating statefulset ss in namespace statefulset-4530 @ 07/29/23 15:54:22.472
  Jul 29 15:54:22.491: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 15:54:32.502: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 07/29/23 15:54:32.512
  STEP: Getting /status @ 07/29/23 15:54:32.532
  Jul 29 15:54:32.541: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 07/29/23 15:54:32.541
  Jul 29 15:54:32.557: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 07/29/23 15:54:32.557
  Jul 29 15:54:32.560: INFO: Observed &StatefulSet event: ADDED
  Jul 29 15:54:32.560: INFO: Found Statefulset ss in namespace statefulset-4530 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 15:54:32.560: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 07/29/23 15:54:32.56
  Jul 29 15:54:32.560: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 29 15:54:32.571: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 07/29/23 15:54:32.571
  Jul 29 15:54:32.574: INFO: Observed &StatefulSet event: ADDED
  Jul 29 15:54:32.576: INFO: Deleting all statefulset in ns statefulset-4530
  Jul 29 15:54:32.583: INFO: Scaling statefulset ss to 0
  Jul 29 15:54:42.615: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 15:54:42.622: INFO: Deleting statefulset ss
  Jul 29 15:54:42.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4530" for this suite. @ 07/29/23 15:54:42.664
• [20.266 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 07/29/23 15:54:42.679
  Jul 29 15:54:42.679: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 15:54:42.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:54:42.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:54:42.714
  STEP: apply creating a deployment @ 07/29/23 15:54:42.718
  Jul 29 15:54:42.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5610" for this suite. @ 07/29/23 15:54:42.751
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 07/29/23 15:54:42.768
  Jul 29 15:54:42.768: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 15:54:42.77
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:54:42.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:54:42.809
  STEP: Setting up server cert @ 07/29/23 15:54:42.86
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 15:54:43.703
  STEP: Deploying the webhook pod @ 07/29/23 15:54:43.718
  STEP: Wait for the deployment to be ready @ 07/29/23 15:54:43.738
  Jul 29 15:54:43.793: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 54, 43, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 43, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-7497495989\""}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/29/23 15:54:45.805
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 15:54:45.826
  Jul 29 15:54:46.826: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 15:54:46.835: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 07/29/23 15:54:47.356
  STEP: Creating a custom resource that should be denied by the webhook @ 07/29/23 15:54:47.386
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 07/29/23 15:54:49.502
  STEP: Updating the custom resource with disallowed data should be denied @ 07/29/23 15:54:49.535
  STEP: Deleting the custom resource should be denied @ 07/29/23 15:54:49.559
  STEP: Remove the offending key and value from the custom resource data @ 07/29/23 15:54:49.571
  STEP: Deleting the updated custom resource should be successful @ 07/29/23 15:54:49.587
  Jul 29 15:54:49.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6715" for this suite. @ 07/29/23 15:54:50.313
  STEP: Destroying namespace "webhook-markers-8618" for this suite. @ 07/29/23 15:54:50.329
• [7.573 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 07/29/23 15:54:50.347
  Jul 29 15:54:50.347: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename deployment @ 07/29/23 15:54:50.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:54:50.382
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:54:50.387
  Jul 29 15:54:50.410: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Jul 29 15:54:55.417: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 15:54:55.418
  Jul 29 15:54:55.418: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Jul 29 15:54:57.430: INFO: Creating deployment "test-rollover-deployment"
  Jul 29 15:54:57.453: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Jul 29 15:54:59.470: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jul 29 15:54:59.481: INFO: Ensure that both replica sets have 1 created replica
  Jul 29 15:54:59.494: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jul 29 15:54:59.512: INFO: Updating deployment test-rollover-deployment
  Jul 29 15:54:59.513: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Jul 29 15:55:01.526: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jul 29 15:55:01.543: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jul 29 15:55:01.557: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 15:55:01.558: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 55, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:55:03.571: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 15:55:03.571: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 55, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:55:05.578: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 15:55:05.579: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 55, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:55:07.572: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 15:55:07.572: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 55, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:55:09.580: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 15:55:09.580: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 55, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 54, 57, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:55:11.572: INFO: 
  Jul 29 15:55:11.572: INFO: Ensure that both old replica sets have no replicas
  Jul 29 15:55:11.590: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1226  3d8139ca-ed6d-4766-9bcf-145be21d1a8b 11072 2 2023-07-29 15:54:57 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 15:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:55:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049c5ff8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 15:54:57 +0000 UTC,LastTransitionTime:2023-07-29 15:54:57 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-07-29 15:55:11 +0000 UTC,LastTransitionTime:2023-07-29 15:54:57 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 15:55:11.600: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-1226  acd35238-057b-47fa-b2ec-9316faab9854 11062 2 2023-07-29 15:54:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3d8139ca-ed6d-4766-9bcf-145be21d1a8b 0xc002776c77 0xc002776c78}] [] [{kube-controller-manager Update apps/v1 2023-07-29 15:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d8139ca-ed6d-4766-9bcf-145be21d1a8b\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:55:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002776e08 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 15:55:11.603: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jul 29 15:55:11.617: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1226  6f827af7-7ae4-4198-91b3-f3f9baa1113b 11070 2 2023-07-29 15:54:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3d8139ca-ed6d-4766-9bcf-145be21d1a8b 0xc0027768b7 0xc0027768b8}] [] [{e2e.test Update apps/v1 2023-07-29 15:54:50 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:55:11 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d8139ca-ed6d-4766-9bcf-145be21d1a8b\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:55:11 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc002776a98 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 15:55:11.621: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-1226  82241658-8340-4d4e-9ab9-3d52b462b41d 11018 2 2023-07-29 15:54:57 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3d8139ca-ed6d-4766-9bcf-145be21d1a8b 0xc002776e77 0xc002776e78}] [] [{kube-controller-manager Update apps/v1 2023-07-29 15:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3d8139ca-ed6d-4766-9bcf-145be21d1a8b\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 15:54:59 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc002776f28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 15:55:11.631: INFO: Pod "test-rollover-deployment-57777854c9-vgv6p" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-vgv6p test-rollover-deployment-57777854c9- deployment-1226  728349ad-c847-4ec9-aabe-503edef6a3ba 11037 0 2023-07-29 15:54:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 acd35238-057b-47fa-b2ec-9316faab9854 0xc002777f27 0xc002777f28}] [] [{kube-controller-manager Update v1 2023-07-29 15:54:59 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"acd35238-057b-47fa-b2ec-9316faab9854\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 15:55:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.27\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rw6ng,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rw6ng,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:54:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:55:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 15:54:59 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:10.233.65.27,StartTime:2023-07-29 15:54:59 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 15:55:00 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://a6df331dac082b6373313adb702ade4e938bcf2ee0fb09982bcd09ea9a65d62c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.27,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 15:55:11.631: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1226" for this suite. @ 07/29/23 15:55:11.64
• [21.307 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 07/29/23 15:55:11.654
  Jul 29 15:55:11.654: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 15:55:11.657
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:11.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:11.711
  Jul 29 15:55:11.716: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: creating the pod @ 07/29/23 15:55:11.718
  STEP: submitting the pod to kubernetes @ 07/29/23 15:55:11.718
  Jul 29 15:55:13.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2429" for this suite. @ 07/29/23 15:55:13.876
• [2.234 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 07/29/23 15:55:13.893
  Jul 29 15:55:13.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 15:55:13.896
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:13.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:13.932
  STEP: Creating configMap with name configmap-test-volume-fa9ddeaa-cb6a-4d88-9a9c-6558f376cce8 @ 07/29/23 15:55:13.937
  STEP: Creating a pod to test consume configMaps @ 07/29/23 15:55:13.946
  STEP: Saw pod success @ 07/29/23 15:55:17.996
  Jul 29 15:55:18.002: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-007d2a60-de25-48bc-b94a-4fc3010cc208 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 15:55:18.031
  Jul 29 15:55:18.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8470" for this suite. @ 07/29/23 15:55:18.066
• [4.185 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 07/29/23 15:55:18.083
  Jul 29 15:55:18.083: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:55:18.086
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:18.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:18.122
  STEP: Creating configMap with name projected-configmap-test-volume-map-be5eadb6-8ad5-4fc8-93dd-51085592fcbe @ 07/29/23 15:55:18.127
  STEP: Creating a pod to test consume configMaps @ 07/29/23 15:55:18.135
  STEP: Saw pod success @ 07/29/23 15:55:22.176
  Jul 29 15:55:22.183: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-configmaps-c67dd616-a0fa-4d70-a180-bfdb6a38eaa9 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 15:55:22.195
  Jul 29 15:55:22.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-823" for this suite. @ 07/29/23 15:55:22.229
• [4.157 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 07/29/23 15:55:22.242
  Jul 29 15:55:22.242: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:55:22.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:22.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:22.286
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 15:55:22.291
  STEP: Saw pod success @ 07/29/23 15:55:26.347
  Jul 29 15:55:26.353: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-491f0f07-5de7-403d-99fb-0f1d23f3ce5d container client-container: <nil>
  STEP: delete the pod @ 07/29/23 15:55:26.367
  Jul 29 15:55:26.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2443" for this suite. @ 07/29/23 15:55:26.406
• [4.174 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 07/29/23 15:55:26.418
  Jul 29 15:55:26.418: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 15:55:26.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:26.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:26.452
  STEP: Creating ServiceAccount "e2e-sa-h69zs"  @ 07/29/23 15:55:26.456
  Jul 29 15:55:26.462: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-h69zs"  @ 07/29/23 15:55:26.463
  Jul 29 15:55:26.474: INFO: AutomountServiceAccountToken: true
  Jul 29 15:55:26.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3641" for this suite. @ 07/29/23 15:55:26.483
• [0.076 seconds]
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 07/29/23 15:55:26.495
  Jul 29 15:55:26.495: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/29/23 15:55:26.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:26.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:26.538
  Jul 29 15:55:28.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 07/29/23 15:55:28.602
  STEP: Cleaning up the configmap @ 07/29/23 15:55:28.615
  STEP: Cleaning up the pod @ 07/29/23 15:55:28.627
  STEP: Destroying namespace "emptydir-wrapper-980" for this suite. @ 07/29/23 15:55:28.65
• [2.170 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 07/29/23 15:55:28.669
  Jul 29 15:55:28.669: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:55:28.671
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:28.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:28.701
  STEP: Creating configMap with name projected-configmap-test-volume-8752c39e-8672-4168-a4f8-31bc614427b3 @ 07/29/23 15:55:28.705
  STEP: Creating a pod to test consume configMaps @ 07/29/23 15:55:28.715
  STEP: Saw pod success @ 07/29/23 15:55:32.751
  Jul 29 15:55:32.757: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-configmaps-37ecfcfb-bee0-4de2-a069-db4f9495ac9c container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 15:55:32.769
  Jul 29 15:55:32.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6668" for this suite. @ 07/29/23 15:55:32.801
• [4.144 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 07/29/23 15:55:32.816
  Jul 29 15:55:32.816: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 15:55:32.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:32.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:32.857
  STEP: Creating secret with name secret-test-map-3695d668-8396-4310-ae59-3a684a86b6c3 @ 07/29/23 15:55:32.861
  STEP: Creating a pod to test consume secrets @ 07/29/23 15:55:32.868
  STEP: Saw pod success @ 07/29/23 15:55:36.912
  Jul 29 15:55:36.920: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-secrets-cf75960a-036d-4410-b9be-7689415e88a0 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 15:55:36.931
  Jul 29 15:55:36.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-310" for this suite. @ 07/29/23 15:55:36.96
• [4.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 07/29/23 15:55:36.977
  Jul 29 15:55:36.977: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 15:55:36.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:37.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:37.012
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 15:55:37.027
  STEP: create the pod with lifecycle hook @ 07/29/23 15:55:39.071
  STEP: check poststart hook @ 07/29/23 15:55:41.101
  STEP: delete the pod with lifecycle hook @ 07/29/23 15:55:41.135
  Jul 29 15:55:43.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-684" for this suite. @ 07/29/23 15:55:43.176
• [6.213 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 07/29/23 15:55:43.195
  Jul 29 15:55:43.195: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 15:55:43.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:43.228
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:43.233
  Jul 29 15:55:43.264: INFO: created pod pod-service-account-defaultsa
  Jul 29 15:55:43.264: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jul 29 15:55:43.276: INFO: created pod pod-service-account-mountsa
  Jul 29 15:55:43.278: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jul 29 15:55:43.298: INFO: created pod pod-service-account-nomountsa
  Jul 29 15:55:43.298: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jul 29 15:55:43.307: INFO: created pod pod-service-account-defaultsa-mountspec
  Jul 29 15:55:43.307: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jul 29 15:55:43.326: INFO: created pod pod-service-account-mountsa-mountspec
  Jul 29 15:55:43.326: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jul 29 15:55:43.342: INFO: created pod pod-service-account-nomountsa-mountspec
  Jul 29 15:55:43.342: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jul 29 15:55:43.355: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jul 29 15:55:43.355: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jul 29 15:55:43.367: INFO: created pod pod-service-account-mountsa-nomountspec
  Jul 29 15:55:43.367: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jul 29 15:55:43.375: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jul 29 15:55:43.375: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jul 29 15:55:43.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3686" for this suite. @ 07/29/23 15:55:43.412
• [0.240 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 07/29/23 15:55:43.436
  Jul 29 15:55:43.436: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 15:55:43.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:43.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:43.548
  STEP: creating the pod @ 07/29/23 15:55:43.552
  STEP: submitting the pod to kubernetes @ 07/29/23 15:55:43.552
  STEP: verifying the pod is in kubernetes @ 07/29/23 15:55:45.629
  STEP: updating the pod @ 07/29/23 15:55:45.634
  Jul 29 15:55:46.157: INFO: Successfully updated pod "pod-update-85163335-fe01-44c9-a234-68142d4a8662"
  STEP: verifying the updated pod is in kubernetes @ 07/29/23 15:55:46.163
  Jul 29 15:55:46.168: INFO: Pod update OK
  Jul 29 15:55:46.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8350" for this suite. @ 07/29/23 15:55:46.178
• [2.761 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 07/29/23 15:55:46.198
  Jul 29 15:55:46.199: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 15:55:46.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:46.232
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:46.235
  Jul 29 15:56:46.265: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1455" for this suite. @ 07/29/23 15:56:46.275
• [60.092 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 07/29/23 15:56:46.291
  Jul 29 15:56:46.292: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:56:46.294
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:56:46.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:56:46.334
  STEP: Creating projection with secret that has name projected-secret-test-map-26fbdd53-e559-475b-8e98-cf0bfc30b5e5 @ 07/29/23 15:56:46.339
  STEP: Creating a pod to test consume secrets @ 07/29/23 15:56:46.348
  STEP: Saw pod success @ 07/29/23 15:56:50.388
  Jul 29 15:56:50.395: INFO: Trying to get logs from node vucheipi7kei-1 pod pod-projected-secrets-f5811dbf-c004-41e3-81f2-36879180d504 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 15:56:50.415
  Jul 29 15:56:50.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3939" for this suite. @ 07/29/23 15:56:50.454
• [4.176 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 07/29/23 15:56:50.471
  Jul 29 15:56:50.471: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 15:56:50.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:56:50.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:56:50.511
  Jul 29 15:56:50.550: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 15:57:50.608: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/29/23 15:57:50.615
  Jul 29 15:57:50.653: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 29 15:57:50.668: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 29 15:57:50.711: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 29 15:57:50.725: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 29 15:57:50.796: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 29 15:57:50.815: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/29/23 15:57:50.815
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 07/29/23 15:57:52.876
  Jul 29 15:57:56.948: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-7576" for this suite. @ 07/29/23 15:57:57.03
• [66.569 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 07/29/23 15:57:57.043
  Jul 29 15:57:57.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 15:57:57.045
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:57:57.071
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:57:57.075
  STEP: Creating a pod to test substitution in volume subpath @ 07/29/23 15:57:57.079
  STEP: Saw pod success @ 07/29/23 15:58:01.117
  Jul 29 15:58:01.123: INFO: Trying to get logs from node vucheipi7kei-3 pod var-expansion-154ca05d-946c-4faa-8c28-93c6296605c4 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 15:58:01.152
  Jul 29 15:58:01.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9584" for this suite. @ 07/29/23 15:58:01.185
• [4.155 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 07/29/23 15:58:01.199
  Jul 29 15:58:01.200: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:58:01.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:01.233
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:01.239
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 15:58:01.244
  STEP: Saw pod success @ 07/29/23 15:58:05.304
  Jul 29 15:58:05.311: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-10e2dc04-effb-4caf-8ef9-4a5f8c5ea0d5 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 15:58:05.324
  Jul 29 15:58:05.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2700" for this suite. @ 07/29/23 15:58:05.358
• [4.168 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 07/29/23 15:58:05.372
  Jul 29 15:58:05.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename job @ 07/29/23 15:58:05.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:05.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:05.411
  STEP: Creating Indexed job @ 07/29/23 15:58:05.416
  STEP: Ensuring job reaches completions @ 07/29/23 15:58:05.426
  STEP: Ensuring pods with index for job exist @ 07/29/23 15:58:13.439
  Jul 29 15:58:13.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-233" for this suite. @ 07/29/23 15:58:13.471
• [8.113 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 07/29/23 15:58:13.486
  Jul 29 15:58:13.486: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename subjectreview @ 07/29/23 15:58:13.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:13.55
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:13.554
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-1242" @ 07/29/23 15:58:13.56
  Jul 29 15:58:13.568: INFO: saUsername: "system:serviceaccount:subjectreview-1242:e2e"
  Jul 29 15:58:13.568: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-1242"}
  Jul 29 15:58:13.569: INFO: saUID: "cbf88dc9-9929-4bd9-9cfe-964fd51aed6f"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-1242:e2e" @ 07/29/23 15:58:13.57
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-1242:e2e" @ 07/29/23 15:58:13.571
  Jul 29 15:58:13.575: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-1242:e2e" api 'list' configmaps in "subjectreview-1242" namespace @ 07/29/23 15:58:13.576
  Jul 29 15:58:13.578: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-1242:e2e" @ 07/29/23 15:58:13.579
  Jul 29 15:58:13.582: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jul 29 15:58:13.583: INFO: LocalSubjectAccessReview has been verified
  Jul 29 15:58:13.584: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-1242" for this suite. @ 07/29/23 15:58:13.597
• [0.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 07/29/23 15:58:13.644
  Jul 29 15:58:13.645: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 15:58:13.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:13.678
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:13.684
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 07/29/23 15:58:13.688
  STEP: When a replication controller with a matching selector is created @ 07/29/23 15:58:15.73
  STEP: Then the orphan pod is adopted @ 07/29/23 15:58:15.741
  Jul 29 15:58:16.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6356" for this suite. @ 07/29/23 15:58:16.765
• [3.134 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 07/29/23 15:58:16.778
  Jul 29 15:58:16.779: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 15:58:16.783
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:16.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:16.829
  STEP: Setting up server cert @ 07/29/23 15:58:16.863
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 15:58:18.069
  STEP: Deploying the webhook pod @ 07/29/23 15:58:18.086
  STEP: Wait for the deployment to be ready @ 07/29/23 15:58:18.107
  Jul 29 15:58:18.120: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 15:58:20.136
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 15:58:20.155
  Jul 29 15:58:21.156: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 07/29/23 15:58:21.164
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 07/29/23 15:58:21.206
  STEP: Creating a configMap that should not be mutated @ 07/29/23 15:58:21.218
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 07/29/23 15:58:21.238
  STEP: Creating a configMap that should be mutated @ 07/29/23 15:58:21.252
  Jul 29 15:58:21.293: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5762" for this suite. @ 07/29/23 15:58:21.38
  STEP: Destroying namespace "webhook-markers-3512" for this suite. @ 07/29/23 15:58:21.397
• [4.639 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 07/29/23 15:58:21.422
  Jul 29 15:58:21.422: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename controllerrevisions @ 07/29/23 15:58:21.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:21.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:21.473
  STEP: Creating DaemonSet "e2e-zhxlh-daemon-set" @ 07/29/23 15:58:21.559
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 15:58:21.569
  Jul 29 15:58:21.584: INFO: Number of nodes with available pods controlled by daemonset e2e-zhxlh-daemon-set: 0
  Jul 29 15:58:21.584: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  Jul 29 15:58:22.609: INFO: Number of nodes with available pods controlled by daemonset e2e-zhxlh-daemon-set: 0
  Jul 29 15:58:22.609: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  Jul 29 15:58:23.600: INFO: Number of nodes with available pods controlled by daemonset e2e-zhxlh-daemon-set: 2
  Jul 29 15:58:23.600: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  Jul 29 15:58:24.607: INFO: Number of nodes with available pods controlled by daemonset e2e-zhxlh-daemon-set: 2
  Jul 29 15:58:24.607: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  Jul 29 15:58:25.607: INFO: Number of nodes with available pods controlled by daemonset e2e-zhxlh-daemon-set: 3
  Jul 29 15:58:25.608: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-zhxlh-daemon-set
  STEP: Confirm DaemonSet "e2e-zhxlh-daemon-set" successfully created with "daemonset-name=e2e-zhxlh-daemon-set" label @ 07/29/23 15:58:25.618
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-zhxlh-daemon-set" @ 07/29/23 15:58:25.634
  Jul 29 15:58:25.639: INFO: Located ControllerRevision: "e2e-zhxlh-daemon-set-66d595cc99"
  STEP: Patching ControllerRevision "e2e-zhxlh-daemon-set-66d595cc99" @ 07/29/23 15:58:25.674
  Jul 29 15:58:25.687: INFO: e2e-zhxlh-daemon-set-66d595cc99 has been patched
  STEP: Create a new ControllerRevision @ 07/29/23 15:58:25.687
  Jul 29 15:58:25.696: INFO: Created ControllerRevision: e2e-zhxlh-daemon-set-5dcbd7d7b7
  STEP: Confirm that there are two ControllerRevisions @ 07/29/23 15:58:25.696
  Jul 29 15:58:25.696: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 15:58:25.702: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-zhxlh-daemon-set-66d595cc99" @ 07/29/23 15:58:25.702
  STEP: Confirm that there is only one ControllerRevision @ 07/29/23 15:58:25.711
  Jul 29 15:58:25.711: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 15:58:25.717: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-zhxlh-daemon-set-5dcbd7d7b7" @ 07/29/23 15:58:25.722
  Jul 29 15:58:25.735: INFO: e2e-zhxlh-daemon-set-5dcbd7d7b7 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 07/29/23 15:58:25.735
  W0729 15:58:25.753146      14 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 07/29/23 15:58:25.753
  Jul 29 15:58:25.754: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 15:58:26.763: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 15:58:26.769: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-zhxlh-daemon-set-5dcbd7d7b7=updated" @ 07/29/23 15:58:26.769
  STEP: Confirm that there is only one ControllerRevision @ 07/29/23 15:58:26.788
  Jul 29 15:58:26.788: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 15:58:26.793: INFO: Found 1 ControllerRevisions
  Jul 29 15:58:26.804: INFO: ControllerRevision "e2e-zhxlh-daemon-set-584cf64854" has revision 3
  STEP: Deleting DaemonSet "e2e-zhxlh-daemon-set" @ 07/29/23 15:58:26.81
  STEP: deleting DaemonSet.extensions e2e-zhxlh-daemon-set in namespace controllerrevisions-871, will wait for the garbage collector to delete the pods @ 07/29/23 15:58:26.81
  Jul 29 15:58:26.885: INFO: Deleting DaemonSet.extensions e2e-zhxlh-daemon-set took: 19.375379ms
  Jul 29 15:58:26.986: INFO: Terminating DaemonSet.extensions e2e-zhxlh-daemon-set pods took: 101.558905ms
  Jul 29 15:58:29.793: INFO: Number of nodes with available pods controlled by daemonset e2e-zhxlh-daemon-set: 0
  Jul 29 15:58:29.793: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-zhxlh-daemon-set
  Jul 29 15:58:29.800: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12526"},"items":null}

  Jul 29 15:58:29.805: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12526"},"items":null}

  Jul 29 15:58:29.835: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-871" for this suite. @ 07/29/23 15:58:29.847
• [8.440 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 07/29/23 15:58:29.872
  Jul 29 15:58:29.873: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 15:58:29.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:29.906
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:29.913
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/29/23 15:58:29.917
  STEP: Saw pod success @ 07/29/23 15:58:33.955
  Jul 29 15:58:33.962: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-c55792e5-407c-44b5-8aaf-caedcdc9adf9 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 15:58:33.977
  Jul 29 15:58:34.005: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-305" for this suite. @ 07/29/23 15:58:34.014
• [4.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 07/29/23 15:58:34.038
  Jul 29 15:58:34.038: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 15:58:34.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:34.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:34.079
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 07/29/23 15:58:34.084
  Jul 29 15:58:34.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 07/29/23 15:58:41.484
  Jul 29 15:58:41.486: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:58:43.745: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 15:58:51.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2054" for this suite. @ 07/29/23 15:58:51.736
• [17.706 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 07/29/23 15:58:51.745
  Jul 29 15:58:51.745: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename security-context @ 07/29/23 15:58:51.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:51.795
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:51.8
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/29/23 15:58:51.805
  STEP: Saw pod success @ 07/29/23 15:58:55.847
  Jul 29 15:58:55.856: INFO: Trying to get logs from node vucheipi7kei-3 pod security-context-928c6ede-5fb9-4684-a83a-fbd4184b9368 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 15:58:55.89
  Jul 29 15:58:55.916: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4878" for this suite. @ 07/29/23 15:58:55.925
• [4.191 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 07/29/23 15:58:55.949
  Jul 29 15:58:55.949: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:58:55.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:55.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:55.982
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 15:58:55.986
  STEP: Saw pod success @ 07/29/23 15:59:00.038
  Jul 29 15:59:00.044: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-73dca8a7-c777-426f-9ec3-5590c64fc79a container client-container: <nil>
  STEP: delete the pod @ 07/29/23 15:59:00.058
  Jul 29 15:59:00.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5529" for this suite. @ 07/29/23 15:59:00.098
• [4.161 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 07/29/23 15:59:00.116
  Jul 29 15:59:00.116: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename csistoragecapacity @ 07/29/23 15:59:00.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:59:00.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:59:00.146
  STEP: getting /apis @ 07/29/23 15:59:00.15
  STEP: getting /apis/storage.k8s.io @ 07/29/23 15:59:00.159
  STEP: getting /apis/storage.k8s.io/v1 @ 07/29/23 15:59:00.161
  STEP: creating @ 07/29/23 15:59:00.163
  STEP: watching @ 07/29/23 15:59:00.193
  Jul 29 15:59:00.193: INFO: starting watch
  STEP: getting @ 07/29/23 15:59:00.207
  STEP: listing in namespace @ 07/29/23 15:59:00.211
  STEP: listing across namespaces @ 07/29/23 15:59:00.218
  STEP: patching @ 07/29/23 15:59:00.223
  STEP: updating @ 07/29/23 15:59:00.231
  Jul 29 15:59:00.239: INFO: waiting for watch events with expected annotations in namespace
  Jul 29 15:59:00.240: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 07/29/23 15:59:00.24
  STEP: deleting a collection @ 07/29/23 15:59:00.26
  Jul 29 15:59:00.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-7727" for this suite. @ 07/29/23 15:59:00.29
• [0.184 seconds]
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 07/29/23 15:59:00.3
  Jul 29 15:59:00.301: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/29/23 15:59:00.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:59:00.328
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:59:00.333
  STEP: creating @ 07/29/23 15:59:00.337
  STEP: getting @ 07/29/23 15:59:00.369
  STEP: listing @ 07/29/23 15:59:00.378
  STEP: deleting @ 07/29/23 15:59:00.383
  Jul 29 15:59:00.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-7593" for this suite. @ 07/29/23 15:59:00.418
• [0.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 07/29/23 15:59:00.436
  Jul 29 15:59:00.436: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 15:59:00.438
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:59:00.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:59:00.473
  Jul 29 15:59:00.504: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 16:00:00.558: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/29/23 16:00:00.563
  Jul 29 16:00:00.563: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/29/23 16:00:00.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:00:00.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:00:00.598
  Jul 29 16:00:00.627: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jul 29 16:00:00.637: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jul 29 16:00:00.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:00:00.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-629" for this suite. @ 07/29/23 16:00:00.808
  STEP: Destroying namespace "sched-preemption-5626" for this suite. @ 07/29/23 16:00:00.82
• [60.397 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 07/29/23 16:00:00.847
  Jul 29 16:00:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename aggregator @ 07/29/23 16:00:00.849
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:00:00.882
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:00:00.887
  Jul 29 16:00:00.892: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Registering the sample API server. @ 07/29/23 16:00:00.895
  Jul 29 16:00:01.815: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jul 29 16:00:01.868: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  Jul 29 16:00:03.986: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:06.003: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:07.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:09.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:11.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:13.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:16.005: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:17.995: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:19.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:21.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:23.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:25.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:27.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:30.001: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:31.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:33.998: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:35.999: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:37.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:39.994: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:41.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:43.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:45.997: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:47.996: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 1, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:00:50.154: INFO: Waited 135.074725ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 07/29/23 16:00:50.224
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 07/29/23 16:00:50.233
  STEP: List APIServices @ 07/29/23 16:00:50.249
  Jul 29 16:00:50.262: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 07/29/23 16:00:50.262
  Jul 29 16:00:50.301: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 07/29/23 16:00:50.301
  Jul 29 16:00:50.332: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.July, 29, 16, 0, 50, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 07/29/23 16:00:50.332
  Jul 29 16:00:50.339: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-07-29 16:00:50 +0000 UTC Passed all checks passed}
  Jul 29 16:00:50.339: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 16:00:50.339: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 07/29/23 16:00:50.339
  Jul 29 16:00:50.354: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1790799681" @ 07/29/23 16:00:50.355
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 07/29/23 16:00:50.376
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 07/29/23 16:00:50.386
  STEP: Patch APIService Status @ 07/29/23 16:00:50.393
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 07/29/23 16:00:50.404
  Jul 29 16:00:50.417: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-07-29 16:00:50 +0000 UTC Passed all checks passed}
  Jul 29 16:00:50.417: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 16:00:50.417: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jul 29 16:00:50.418: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 07/29/23 16:00:50.418
  STEP: Confirm that the generated APIService has been deleted @ 07/29/23 16:00:50.43
  Jul 29 16:00:50.431: INFO: Requesting list of APIServices to confirm quantity
  Jul 29 16:00:50.442: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jul 29 16:00:50.443: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jul 29 16:00:50.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-7217" for this suite. @ 07/29/23 16:00:50.788
• [49.968 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 07/29/23 16:00:50.818
  Jul 29 16:00:50.818: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:00:50.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:00:50.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:00:50.863
  STEP: creating a Deployment @ 07/29/23 16:00:50.885
  STEP: waiting for Deployment to be created @ 07/29/23 16:00:50.9
  STEP: waiting for all Replicas to be Ready @ 07/29/23 16:00:50.912
  Jul 29 16:00:50.922: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:00:50.922: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:00:50.927: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:00:50.927: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:00:50.981: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:00:50.981: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:00:51.054: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:00:51.054: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:00:52.115: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul 29 16:00:52.115: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul 29 16:00:52.198: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 07/29/23 16:00:52.198
  W0729 16:00:52.213254      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 16:00:52.217: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 07/29/23 16:00:52.217
  Jul 29 16:00:52.219: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0
  Jul 29 16:00:52.220: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0
  Jul 29 16:00:52.220: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0
  Jul 29 16:00:52.220: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0
  Jul 29 16:00:52.220: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0
  Jul 29 16:00:52.220: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0
  Jul 29 16:00:52.221: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0
  Jul 29 16:00:52.221: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 0
  Jul 29 16:00:52.221: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:52.222: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:52.222: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:52.222: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:52.222: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:52.222: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:52.242: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:52.242: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:52.274: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:52.274: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:52.361: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:52.362: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:52.387: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:52.387: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:54.228: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:54.228: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:54.316: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  STEP: listing Deployments @ 07/29/23 16:00:54.316
  Jul 29 16:00:54.323: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 07/29/23 16:00:54.323
  Jul 29 16:00:54.349: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 07/29/23 16:00:54.349
  Jul 29 16:00:54.365: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:54.391: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:54.538: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:54.576: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:54.602: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:54.626: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:56.180: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:56.252: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:56.286: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:56.326: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:56.367: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:00:58.343: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 07/29/23 16:00:58.418
  STEP: fetching the DeploymentStatus @ 07/29/23 16:00:58.437
  Jul 29 16:00:58.446: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:58.446: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:58.447: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:58.447: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:58.447: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:58.447: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 1
  Jul 29 16:00:58.447: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:58.447: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:58.447: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:58.448: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:58.448: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 2
  Jul 29 16:00:58.448: INFO: observed Deployment test-deployment in namespace deployment-9494 with ReadyReplicas 3
  STEP: deleting the Deployment @ 07/29/23 16:00:58.448
  Jul 29 16:00:58.471: INFO: observed event type MODIFIED
  Jul 29 16:00:58.471: INFO: observed event type MODIFIED
  Jul 29 16:00:58.474: INFO: observed event type MODIFIED
  Jul 29 16:00:58.475: INFO: observed event type MODIFIED
  Jul 29 16:00:58.475: INFO: observed event type MODIFIED
  Jul 29 16:00:58.476: INFO: observed event type MODIFIED
  Jul 29 16:00:58.477: INFO: observed event type MODIFIED
  Jul 29 16:00:58.477: INFO: observed event type MODIFIED
  Jul 29 16:00:58.478: INFO: observed event type MODIFIED
  Jul 29 16:00:58.478: INFO: observed event type MODIFIED
  Jul 29 16:00:58.479: INFO: observed event type MODIFIED
  Jul 29 16:00:58.481: INFO: observed event type MODIFIED
  Jul 29 16:00:58.482: INFO: observed event type MODIFIED
  Jul 29 16:00:58.495: INFO: Log out all the ReplicaSets if there is no deployment created
  Jul 29 16:00:58.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9494" for this suite. @ 07/29/23 16:00:58.526
• [7.732 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 07/29/23 16:00:58.551
  Jul 29 16:00:58.552: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename dns @ 07/29/23 16:00:58.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:00:58.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:00:58.61
  STEP: Creating a test headless service @ 07/29/23 16:00:58.615
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7245 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7245;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7245 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7245;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7245.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7245.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7245.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7245.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7245.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7245.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7245.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7245.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7245.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7245.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7245.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7245.svc;check="$$(dig +notcp +noall +answer +search 159.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.159_udp@PTR;check="$$(dig +tcp +noall +answer +search 159.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.159_tcp@PTR;sleep 1; done
   @ 07/29/23 16:00:58.672
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7245 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7245;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7245 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7245;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7245.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7245.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7245.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7245.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7245.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7245.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7245.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7245.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7245.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7245.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7245.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7245.svc;check="$$(dig +notcp +noall +answer +search 159.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.159_udp@PTR;check="$$(dig +tcp +noall +answer +search 159.19.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.19.159_tcp@PTR;sleep 1; done
   @ 07/29/23 16:00:58.672
  STEP: creating a pod to probe DNS @ 07/29/23 16:00:58.672
  STEP: submitting the pod to kubernetes @ 07/29/23 16:00:58.672
  STEP: retrieving the pod @ 07/29/23 16:01:00.715
  STEP: looking for the results for each expected name from probers @ 07/29/23 16:01:00.724
  Jul 29 16:01:00.735: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.743: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.749: INFO: Unable to read wheezy_udp@dns-test-service.dns-7245 from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.755: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7245 from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.762: INFO: Unable to read wheezy_udp@dns-test-service.dns-7245.svc from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.828: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.836: INFO: Unable to read jessie_udp@dns-test-service.dns-7245 from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.841: INFO: Unable to read jessie_tcp@dns-test-service.dns-7245 from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.859: INFO: Unable to read jessie_udp@dns-test-service.dns-7245.svc from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:00.913: INFO: Lookups using dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7245 wheezy_tcp@dns-test-service.dns-7245 wheezy_udp@dns-test-service.dns-7245.svc jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7245 jessie_tcp@dns-test-service.dns-7245 jessie_udp@dns-test-service.dns-7245.svc]

  Jul 29 16:01:05.931: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:05.960: INFO: Unable to read wheezy_udp@dns-test-service.dns-7245.svc from pod dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7: the server could not find the requested resource (get pods dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7)
  Jul 29 16:01:06.113: INFO: Lookups using dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7 failed for: [wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7245.svc]

  Jul 29 16:01:11.149: INFO: DNS probes using dns-7245/dns-test-3d695f84-415f-4f35-bc55-c10368c26fe7 succeeded

  Jul 29 16:01:11.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:01:11.158
  STEP: deleting the test service @ 07/29/23 16:01:11.22
  STEP: deleting the test headless service @ 07/29/23 16:01:11.294
  STEP: Destroying namespace "dns-7245" for this suite. @ 07/29/23 16:01:11.341
• [12.814 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 07/29/23 16:01:11.367
  Jul 29 16:01:11.367: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:01:11.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:11.396
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:11.401
  STEP: creating the pod with failed condition @ 07/29/23 16:01:11.405
  STEP: updating the pod @ 07/29/23 16:03:11.432
  Jul 29 16:03:11.967: INFO: Successfully updated pod "var-expansion-c00d2eab-c8f1-49f5-bcb4-d11940cb39bb"
  STEP: waiting for pod running @ 07/29/23 16:03:11.967
  STEP: deleting the pod gracefully @ 07/29/23 16:03:13.987
  Jul 29 16:03:13.987: INFO: Deleting pod "var-expansion-c00d2eab-c8f1-49f5-bcb4-d11940cb39bb" in namespace "var-expansion-3180"
  Jul 29 16:03:14.001: INFO: Wait up to 5m0s for pod "var-expansion-c00d2eab-c8f1-49f5-bcb4-d11940cb39bb" to be fully deleted
  Jul 29 16:03:46.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3180" for this suite. @ 07/29/23 16:03:46.168
• [154.810 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 07/29/23 16:03:46.183
  Jul 29 16:03:46.183: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:03:46.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:03:46.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:03:46.245
  STEP: Creating service test in namespace statefulset-9156 @ 07/29/23 16:03:46.25
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 07/29/23 16:03:46.259
  STEP: Creating stateful set ss in namespace statefulset-9156 @ 07/29/23 16:03:46.267
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-9156 @ 07/29/23 16:03:46.28
  Jul 29 16:03:46.292: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 16:03:56.303: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 07/29/23 16:03:56.304
  Jul 29 16:03:56.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-9156 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:03:56.606: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:03:56.606: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:03:56.606: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:03:56.619: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jul 29 16:04:06.631: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:04:06.631: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:04:06.664: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.99999966s
  Jul 29 16:04:07.672: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993396294s
  Jul 29 16:04:08.682: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.984724018s
  Jul 29 16:04:09.691: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.97476302s
  Jul 29 16:04:10.699: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.965931233s
  Jul 29 16:04:11.711: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.957958931s
  Jul 29 16:04:12.719: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.946043888s
  Jul 29 16:04:13.727: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.937941428s
  Jul 29 16:04:14.736: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.930152398s
  Jul 29 16:04:15.766: INFO: Verifying statefulset ss doesn't scale past 1 for another 921.141921ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-9156 @ 07/29/23 16:04:16.767
  Jul 29 16:04:16.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-9156 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:04:17.041: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:04:17.041: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:04:17.041: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:04:17.049: INFO: Found 1 stateful pods, waiting for 3
  Jul 29 16:04:27.059: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:04:27.059: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:04:27.060: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 07/29/23 16:04:27.06
  STEP: Scale down will halt with unhealthy stateful pod @ 07/29/23 16:04:27.06
  Jul 29 16:04:27.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-9156 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:04:27.355: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:04:27.355: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:04:27.355: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:04:27.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-9156 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:04:27.654: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:04:27.654: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:04:27.654: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:04:27.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-9156 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:04:27.962: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:04:27.962: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:04:27.962: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:04:27.962: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:04:27.967: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Jul 29 16:04:37.985: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:04:37.985: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:04:37.985: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:04:38.017: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999535s
  Jul 29 16:04:39.027: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989919472s
  Jul 29 16:04:40.034: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.980235058s
  Jul 29 16:04:41.044: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.972596913s
  Jul 29 16:04:42.052: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962811134s
  Jul 29 16:04:43.061: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.955169489s
  Jul 29 16:04:44.068: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.945426926s
  Jul 29 16:04:45.079: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.938313349s
  Jul 29 16:04:46.088: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927370149s
  Jul 29 16:04:47.099: INFO: Verifying statefulset ss doesn't scale past 3 for another 917.952986ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-9156 @ 07/29/23 16:04:48.1
  Jul 29 16:04:48.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-9156 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:04:48.349: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:04:48.349: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:04:48.349: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:04:48.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-9156 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:04:48.646: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:04:48.646: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:04:48.646: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:04:48.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-9156 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:04:48.905: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:04:48.905: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:04:48.905: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:04:48.905: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 07/29/23 16:04:58.945
  Jul 29 16:04:58.945: INFO: Deleting all statefulset in ns statefulset-9156
  Jul 29 16:04:58.951: INFO: Scaling statefulset ss to 0
  Jul 29 16:04:58.975: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:04:58.981: INFO: Deleting statefulset ss
  Jul 29 16:04:59.009: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9156" for this suite. @ 07/29/23 16:04:59.019
• [72.855 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 07/29/23 16:04:59.042
  Jul 29 16:04:59.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename watch @ 07/29/23 16:04:59.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:59.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:59.085
  STEP: creating a watch on configmaps with a certain label @ 07/29/23 16:04:59.09
  STEP: creating a new configmap @ 07/29/23 16:04:59.092
  STEP: modifying the configmap once @ 07/29/23 16:04:59.102
  STEP: changing the label value of the configmap @ 07/29/23 16:04:59.117
  STEP: Expecting to observe a delete notification for the watched object @ 07/29/23 16:04:59.131
  Jul 29 16:04:59.131: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1112  9df0b4b4-184e-407e-bc67-eb27e86316f6 14246 0 2023-07-29 16:04:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:04:59 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:04:59.132: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1112  9df0b4b4-184e-407e-bc67-eb27e86316f6 14247 0 2023-07-29 16:04:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:04:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:04:59.132: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1112  9df0b4b4-184e-407e-bc67-eb27e86316f6 14248 0 2023-07-29 16:04:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:04:59 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 07/29/23 16:04:59.132
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 07/29/23 16:04:59.147
  STEP: changing the label value of the configmap back @ 07/29/23 16:05:09.151
  STEP: modifying the configmap a third time @ 07/29/23 16:05:09.173
  STEP: deleting the configmap @ 07/29/23 16:05:09.183
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 07/29/23 16:05:09.191
  Jul 29 16:05:09.191: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1112  9df0b4b4-184e-407e-bc67-eb27e86316f6 14302 0 2023-07-29 16:04:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:05:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:05:09.192: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1112  9df0b4b4-184e-407e-bc67-eb27e86316f6 14303 0 2023-07-29 16:04:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:05:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:05:09.192: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1112  9df0b4b4-184e-407e-bc67-eb27e86316f6 14304 0 2023-07-29 16:04:59 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 16:05:09 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:05:09.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1112" for this suite. @ 07/29/23 16:05:09.202
• [10.170 seconds]
------------------------------
SS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 07/29/23 16:05:09.214
  Jul 29 16:05:09.214: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename containers @ 07/29/23 16:05:09.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:05:09.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:05:09.247
  STEP: Creating a pod to test override command @ 07/29/23 16:05:09.254
  STEP: Saw pod success @ 07/29/23 16:05:13.297
  Jul 29 16:05:13.306: INFO: Trying to get logs from node vucheipi7kei-3 pod client-containers-73270a0a-ded1-4511-8525-5eafc7c44454 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:05:13.341
  Jul 29 16:05:13.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6634" for this suite. @ 07/29/23 16:05:13.374
• [4.171 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 07/29/23 16:05:13.389
  Jul 29 16:05:13.389: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:05:13.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:05:13.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:05:13.424
  STEP: creating service endpoint-test2 in namespace services-6395 @ 07/29/23 16:05:13.428
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6395 to expose endpoints map[] @ 07/29/23 16:05:13.448
  Jul 29 16:05:13.472: INFO: successfully validated that service endpoint-test2 in namespace services-6395 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6395 @ 07/29/23 16:05:13.473
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6395 to expose endpoints map[pod1:[80]] @ 07/29/23 16:05:15.519
  Jul 29 16:05:15.537: INFO: successfully validated that service endpoint-test2 in namespace services-6395 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 07/29/23 16:05:15.538
  Jul 29 16:05:15.538: INFO: Creating new exec pod
  Jul 29 16:05:18.563: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-6395 exec execpod8zmzb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 16:05:18.837: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 16:05:18.837: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:05:18.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-6395 exec execpod8zmzb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.38.220 80'
  Jul 29 16:05:19.099: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.38.220 80\nConnection to 10.233.38.220 80 port [tcp/http] succeeded!\n"
  Jul 29 16:05:19.099: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-6395 @ 07/29/23 16:05:19.099
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6395 to expose endpoints map[pod1:[80] pod2:[80]] @ 07/29/23 16:05:21.133
  Jul 29 16:05:21.152: INFO: successfully validated that service endpoint-test2 in namespace services-6395 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 07/29/23 16:05:21.152
  Jul 29 16:05:22.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-6395 exec execpod8zmzb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 16:05:22.429: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 16:05:22.429: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:05:22.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-6395 exec execpod8zmzb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.38.220 80'
  Jul 29 16:05:22.661: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.38.220 80\nConnection to 10.233.38.220 80 port [tcp/http] succeeded!\n"
  Jul 29 16:05:22.661: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6395 @ 07/29/23 16:05:22.661
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6395 to expose endpoints map[pod2:[80]] @ 07/29/23 16:05:22.696
  Jul 29 16:05:23.777: INFO: successfully validated that service endpoint-test2 in namespace services-6395 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 07/29/23 16:05:23.777
  Jul 29 16:05:24.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-6395 exec execpod8zmzb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 16:05:25.034: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 16:05:25.034: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:05:25.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-6395 exec execpod8zmzb -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.38.220 80'
  Jul 29 16:05:25.256: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.38.220 80\nConnection to 10.233.38.220 80 port [tcp/http] succeeded!\n"
  Jul 29 16:05:25.256: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-6395 @ 07/29/23 16:05:25.256
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6395 to expose endpoints map[] @ 07/29/23 16:05:25.306
  Jul 29 16:05:25.326: INFO: successfully validated that service endpoint-test2 in namespace services-6395 exposes endpoints map[]
  Jul 29 16:05:25.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6395" for this suite. @ 07/29/23 16:05:25.363
• [11.986 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 07/29/23 16:05:25.379
  Jul 29 16:05:25.379: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 16:05:25.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:05:25.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:05:25.424
  Jul 29 16:05:25.471: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 16:05:30.478: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 16:05:30.478
  STEP: Scaling up "test-rs" replicaset  @ 07/29/23 16:05:30.478
  Jul 29 16:05:30.492: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 07/29/23 16:05:30.493
  W0729 16:05:30.527029      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 16:05:30.542: INFO: observed ReplicaSet test-rs in namespace replicaset-4865 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 16:05:30.568: INFO: observed ReplicaSet test-rs in namespace replicaset-4865 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 16:05:30.599: INFO: observed ReplicaSet test-rs in namespace replicaset-4865 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 16:05:30.613: INFO: observed ReplicaSet test-rs in namespace replicaset-4865 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 16:05:32.374: INFO: observed ReplicaSet test-rs in namespace replicaset-4865 with ReadyReplicas 2, AvailableReplicas 2
  Jul 29 16:05:32.593: INFO: observed Replicaset test-rs in namespace replicaset-4865 with ReadyReplicas 3 found true
  Jul 29 16:05:32.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4865" for this suite. @ 07/29/23 16:05:32.61
• [7.242 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 07/29/23 16:05:32.625
  Jul 29 16:05:32.625: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename watch @ 07/29/23 16:05:32.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:05:32.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:05:32.652
  STEP: getting a starting resourceVersion @ 07/29/23 16:05:32.655
  STEP: starting a background goroutine to produce watch events @ 07/29/23 16:05:32.66
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 07/29/23 16:05:32.66
  Jul 29 16:05:35.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-8785" for this suite. @ 07/29/23 16:05:35.436
• [2.865 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 07/29/23 16:05:35.497
  Jul 29 16:05:35.497: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-webhook @ 07/29/23 16:05:35.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:05:35.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:05:35.531
  STEP: Setting up server cert @ 07/29/23 16:05:35.536
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/29/23 16:05:36.781
  STEP: Deploying the custom resource conversion webhook pod @ 07/29/23 16:05:36.791
  STEP: Wait for the deployment to be ready @ 07/29/23 16:05:36.815
  Jul 29 16:05:36.829: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 16:05:38.847
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:05:38.863
  Jul 29 16:05:39.863: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 29 16:05:39.874: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Creating a v1 custom resource @ 07/29/23 16:05:42.65
  STEP: Create a v2 custom resource @ 07/29/23 16:05:42.679
  STEP: List CRs in v1 @ 07/29/23 16:05:42.879
  STEP: List CRs in v2 @ 07/29/23 16:05:42.887
  Jul 29 16:05:42.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6018" for this suite. @ 07/29/23 16:05:43.528
• [8.042 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 07/29/23 16:05:43.562
  Jul 29 16:05:43.562: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 16:05:43.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:05:43.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:05:43.598
  STEP: Performing setup for networking test in namespace pod-network-test-4202 @ 07/29/23 16:05:43.604
  STEP: creating a selector @ 07/29/23 16:05:43.604
  STEP: Creating the service pods in kubernetes @ 07/29/23 16:05:43.604
  Jul 29 16:05:43.604: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 16:06:05.889
  Jul 29 16:06:07.948: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 16:06:07.948: INFO: Going to poll 10.233.66.233 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 16:06:07.954: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.233:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4202 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:06:07.954: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:06:07.958: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:06:07.958: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4202/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.233%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 16:06:08.104: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 29 16:06:08.104: INFO: Going to poll 10.233.64.151 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 16:06:08.111: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.151:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4202 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:06:08.111: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:06:08.112: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:06:08.112: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4202/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.151%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 16:06:08.208: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 29 16:06:08.209: INFO: Going to poll 10.233.65.79 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 16:06:08.214: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.79:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4202 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:06:08.214: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:06:08.215: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:06:08.216: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-4202/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.79%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 16:06:08.343: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 29 16:06:08.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4202" for this suite. @ 07/29/23 16:06:08.353
• [24.801 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 07/29/23 16:06:08.364
  Jul 29 16:06:08.365: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:06:08.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:08.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:08.394
  STEP: Creating a ResourceQuota with best effort scope @ 07/29/23 16:06:08.401
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 16:06:08.409
  STEP: Creating a ResourceQuota with not best effort scope @ 07/29/23 16:06:10.418
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 16:06:10.426
  STEP: Creating a best-effort pod @ 07/29/23 16:06:12.433
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 07/29/23 16:06:12.453
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 07/29/23 16:06:14.465
  STEP: Deleting the pod @ 07/29/23 16:06:16.473
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 16:06:16.493
  STEP: Creating a not best-effort pod @ 07/29/23 16:06:18.502
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 07/29/23 16:06:18.521
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 07/29/23 16:06:20.528
  STEP: Deleting the pod @ 07/29/23 16:06:22.537
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 16:06:22.56
  Jul 29 16:06:24.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6222" for this suite. @ 07/29/23 16:06:24.578
• [16.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 07/29/23 16:06:24.592
  Jul 29 16:06:24.592: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:06:24.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:24.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:24.624
  STEP: Setting up server cert @ 07/29/23 16:06:24.657
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:06:25.695
  STEP: Deploying the webhook pod @ 07/29/23 16:06:25.705
  STEP: Wait for the deployment to be ready @ 07/29/23 16:06:25.73
  Jul 29 16:06:25.748: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 16:06:27.779
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:06:27.806
  Jul 29 16:06:28.806: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/29/23 16:06:28.815
  STEP: create a pod @ 07/29/23 16:06:28.852
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 07/29/23 16:06:30.878
  Jul 29 16:06:30.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=webhook-4317 attach --namespace=webhook-4317 to-be-attached-pod -i -c=container1'
  Jul 29 16:06:31.074: INFO: rc: 1
  Jul 29 16:06:31.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4317" for this suite. @ 07/29/23 16:06:31.175
  STEP: Destroying namespace "webhook-markers-5965" for this suite. @ 07/29/23 16:06:31.193
• [6.619 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 07/29/23 16:06:31.229
  Jul 29 16:06:31.229: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:06:31.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:31.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:31.259
  Jul 29 16:06:33.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:06:33.302: INFO: Deleting pod "var-expansion-64d3cd04-af62-4891-b7e5-19a8250b51cc" in namespace "var-expansion-9592"
  Jul 29 16:06:33.319: INFO: Wait up to 5m0s for pod "var-expansion-64d3cd04-af62-4891-b7e5-19a8250b51cc" to be fully deleted
  STEP: Destroying namespace "var-expansion-9592" for this suite. @ 07/29/23 16:06:35.337
• [4.119 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 07/29/23 16:06:35.351
  Jul 29 16:06:35.351: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:06:35.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:35.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:35.38
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:06:35.384
  STEP: Saw pod success @ 07/29/23 16:06:39.425
  Jul 29 16:06:39.432: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-81e74f60-66b0-4818-83d8-5c73bc1abce9 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:06:39.448
  Jul 29 16:06:39.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6097" for this suite. @ 07/29/23 16:06:39.489
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 07/29/23 16:06:39.511
  Jul 29 16:06:39.511: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:06:39.514
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:39.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:39.553
  STEP: Creating projection with secret that has name projected-secret-test-04714956-1023-44eb-9cec-fcccf2177541 @ 07/29/23 16:06:39.559
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:06:39.567
  STEP: Saw pod success @ 07/29/23 16:06:43.627
  Jul 29 16:06:43.632: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-secrets-85225bce-2043-492e-b672-932a8a6cc8ba container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:06:43.641
  Jul 29 16:06:43.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-345" for this suite. @ 07/29/23 16:06:43.673
• [4.174 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 07/29/23 16:06:43.69
  Jul 29 16:06:43.690: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:06:43.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:43.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:43.728
  STEP: starting the proxy server @ 07/29/23 16:06:43.732
  Jul 29 16:06:43.732: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-1895 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 07/29/23 16:06:43.831
  Jul 29 16:06:43.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1895" for this suite. @ 07/29/23 16:06:43.859
• [0.183 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 07/29/23 16:06:43.875
  Jul 29 16:06:43.876: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:06:43.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:43.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:43.904
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:06:43.909
  STEP: Saw pod success @ 07/29/23 16:06:47.944
  Jul 29 16:06:47.949: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-cef25b25-c228-4f3f-aeb3-1cd6c97b178d container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:06:47.958
  Jul 29 16:06:47.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2314" for this suite. @ 07/29/23 16:06:47.988
• [4.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 07/29/23 16:06:48.002
  Jul 29 16:06:48.002: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:06:48.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:48.035
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:48.038
  STEP: Creating projection with secret that has name projected-secret-test-1d482192-b639-44fd-8a74-c1a6312fd58c @ 07/29/23 16:06:48.044
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:06:48.051
  STEP: Saw pod success @ 07/29/23 16:06:52.089
  Jul 29 16:06:52.095: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-secrets-8e149e4e-7ae3-4406-9643-4f181b618d4b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:06:52.11
  Jul 29 16:06:52.137: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1618" for this suite. @ 07/29/23 16:06:52.145
• [4.155 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 07/29/23 16:06:52.166
  Jul 29 16:06:52.167: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename job @ 07/29/23 16:06:52.169
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:06:52.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:06:52.196
  STEP: Creating a job @ 07/29/23 16:06:52.201
  STEP: Ensuring active pods == parallelism @ 07/29/23 16:06:52.213
  STEP: delete a job @ 07/29/23 16:06:54.222
  STEP: deleting Job.batch foo in namespace job-5418, will wait for the garbage collector to delete the pods @ 07/29/23 16:06:54.222
  Jul 29 16:06:54.294: INFO: Deleting Job.batch foo took: 16.077133ms
  Jul 29 16:06:54.396: INFO: Terminating Job.batch foo pods took: 101.768538ms
  STEP: Ensuring job was deleted @ 07/29/23 16:07:25.997
  Jul 29 16:07:26.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5418" for this suite. @ 07/29/23 16:07:26.018
• [33.869 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 07/29/23 16:07:26.043
  Jul 29 16:07:26.043: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 16:07:26.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:26.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:26.076
  STEP: Performing setup for networking test in namespace pod-network-test-7512 @ 07/29/23 16:07:26.08
  STEP: creating a selector @ 07/29/23 16:07:26.081
  STEP: Creating the service pods in kubernetes @ 07/29/23 16:07:26.081
  Jul 29 16:07:26.081: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 16:07:48.356
  Jul 29 16:07:50.427: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 16:07:50.428: INFO: Going to poll 10.233.66.115 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 16:07:50.434: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.115 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7512 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:07:50.434: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:07:50.436: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:07:50.436: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7512/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.115+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 16:07:51.562: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 29 16:07:51.562: INFO: Going to poll 10.233.64.126 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 16:07:51.568: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.126 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7512 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:07:51.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:07:51.570: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:07:51.571: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7512/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.126+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 16:07:52.715: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 29 16:07:52.715: INFO: Going to poll 10.233.65.171 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 16:07:52.721: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7512 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:07:52.722: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:07:52.723: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:07:52.724: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7512/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 16:07:53.832: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 29 16:07:53.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7512" for this suite. @ 07/29/23 16:07:53.845
• [27.816 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 07/29/23 16:07:53.861
  Jul 29 16:07:53.861: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:07:53.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:53.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:53.898
  STEP: Setting up server cert @ 07/29/23 16:07:53.941
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:07:54.868
  STEP: Deploying the webhook pod @ 07/29/23 16:07:54.881
  STEP: Wait for the deployment to be ready @ 07/29/23 16:07:54.904
  Jul 29 16:07:54.914: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 16:07:56.938
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:07:56.966
  Jul 29 16:07:57.966: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 07/29/23 16:07:57.974
  STEP: create a namespace for the webhook @ 07/29/23 16:07:58.009
  STEP: create a configmap should be unconditionally rejected by the webhook @ 07/29/23 16:07:58.033
  Jul 29 16:07:58.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8148" for this suite. @ 07/29/23 16:07:58.188
  STEP: Destroying namespace "webhook-markers-8691" for this suite. @ 07/29/23 16:07:58.202
  STEP: Destroying namespace "fail-closed-namespace-273" for this suite. @ 07/29/23 16:07:58.213
• [4.362 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 07/29/23 16:07:58.229
  Jul 29 16:07:58.229: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-webhook @ 07/29/23 16:07:58.231
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:58.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:58.262
  STEP: Setting up server cert @ 07/29/23 16:07:58.267
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/29/23 16:07:59.28
  STEP: Deploying the custom resource conversion webhook pod @ 07/29/23 16:07:59.297
  STEP: Wait for the deployment to be ready @ 07/29/23 16:07:59.335
  Jul 29 16:07:59.412: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 16:08:01.443
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:08:01.458
  Jul 29 16:08:02.459: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 29 16:08:02.477: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Creating a v1 custom resource @ 07/29/23 16:08:05.553
  STEP: v2 custom resource should be converted @ 07/29/23 16:08:05.568
  Jul 29 16:08:05.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-4291" for this suite. @ 07/29/23 16:08:06.235
• [8.031 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 07/29/23 16:08:06.261
  Jul 29 16:08:06.261: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:08:06.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:06.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:06.323
  STEP: Creating a pod to test substitution in container's args @ 07/29/23 16:08:06.33
  STEP: Saw pod success @ 07/29/23 16:08:10.37
  Jul 29 16:08:10.377: INFO: Trying to get logs from node vucheipi7kei-3 pod var-expansion-037d11d7-5da1-4456-be71-f9bf943f12da container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:08:10.39
  Jul 29 16:08:10.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1761" for this suite. @ 07/29/23 16:08:10.444
• [4.196 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 07/29/23 16:08:10.457
  Jul 29 16:08:10.457: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:08:10.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:10.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:10.491
  Jul 29 16:08:10.496: INFO: Creating simple deployment test-new-deployment
  Jul 29 16:08:10.532: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 07/29/23 16:08:12.563
  STEP: updating a scale subresource @ 07/29/23 16:08:12.57
  STEP: verifying the deployment Spec.Replicas was modified @ 07/29/23 16:08:12.58
  STEP: Patch a scale subresource @ 07/29/23 16:08:12.585
  Jul 29 16:08:12.629: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-5415  4126b3d5-1bde-42b7-9b9c-f7ff7fbfffc3 15801 3 2023-07-29 16:08:10 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-29 16:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004905a88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:08:12 +0000 UTC,LastTransitionTime:2023-07-29 16:08:12 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-07-29 16:08:12 +0000 UTC,LastTransitionTime:2023-07-29 16:08:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 16:08:12.652: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-5415  1e544bfe-4418-4e01-b9a1-115e333666d8 15807 2 2023-07-29 16:08:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 4126b3d5-1bde-42b7-9b9c-f7ff7fbfffc3 0xc004739bc7 0xc004739bc8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4126b3d5-1bde-42b7-9b9c-f7ff7fbfffc3\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:08:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004739c68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:08:12.668: INFO: Pod "test-new-deployment-67bd4bf6dc-9zj5v" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-9zj5v test-new-deployment-67bd4bf6dc- deployment-5415  31969806-1bb2-4eef-8cfe-707d9dd9d879 15804 0 2023-07-29 16:08:12 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 1e544bfe-4418-4e01-b9a1-115e333666d8 0xc004c12157 0xc004c12158}] [] [{kube-controller-manager Update v1 2023-07-29 16:08:12 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e544bfe-4418-4e01-b9a1-115e333666d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jf7zh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jf7zh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:08:12 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 16:08:12.669: INFO: Pod "test-new-deployment-67bd4bf6dc-chvbp" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-chvbp test-new-deployment-67bd4bf6dc- deployment-5415  5032dc2e-28c2-458d-9f99-f741647d993e 15796 0 2023-07-29 16:08:10 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 1e544bfe-4418-4e01-b9a1-115e333666d8 0xc004c122c0 0xc004c122c1}] [] [{kube-controller-manager Update v1 2023-07-29 16:08:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1e544bfe-4418-4e01-b9a1-115e333666d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:08:12 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rmc9h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rmc9h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:08:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:08:12 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:08:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:10.233.65.94,StartTime:2023-07-29 16:08:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:08:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://fff3d4fbeec2b56cc6478d0e400f1d8e61fab24092753cfec4fab9ec7054e112,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.94,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 16:08:12.670: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5415" for this suite. @ 07/29/23 16:08:12.699
• [2.259 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 07/29/23 16:08:12.72
  Jul 29 16:08:12.720: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:08:12.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:12.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:12.795
  STEP: Creating a pod to test downward api env vars @ 07/29/23 16:08:12.799
  STEP: Saw pod success @ 07/29/23 16:08:16.848
  Jul 29 16:08:16.853: INFO: Trying to get logs from node vucheipi7kei-3 pod downward-api-de6e8a3e-1b96-4c86-8e19-67175763604d container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:08:16.868
  Jul 29 16:08:16.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2584" for this suite. @ 07/29/23 16:08:16.907
• [4.199 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 07/29/23 16:08:16.925
  Jul 29 16:08:16.926: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:08:16.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:16.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:16.958
  STEP: Setting up server cert @ 07/29/23 16:08:16.993
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:08:17.591
  STEP: Deploying the webhook pod @ 07/29/23 16:08:17.604
  STEP: Wait for the deployment to be ready @ 07/29/23 16:08:17.628
  Jul 29 16:08:17.675: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 16:08:19.695
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:08:19.717
  Jul 29 16:08:20.719: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 07/29/23 16:08:20.724
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 16:08:20.725
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 07/29/23 16:08:20.761
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 07/29/23 16:08:21.783
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 16:08:21.784
  STEP: Having no error when timeout is longer than webhook latency @ 07/29/23 16:08:22.832
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 16:08:22.832
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 07/29/23 16:08:27.902
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 16:08:27.902
  Jul 29 16:08:32.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4659" for this suite. @ 07/29/23 16:08:33.143
  STEP: Destroying namespace "webhook-markers-3186" for this suite. @ 07/29/23 16:08:33.165
• [16.252 seconds]
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 07/29/23 16:08:33.178
  Jul 29 16:08:33.178: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 16:08:33.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:33.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:33.227
  Jul 29 16:08:35.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5516" for this suite. @ 07/29/23 16:08:35.358
• [2.192 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 07/29/23 16:08:35.372
  Jul 29 16:08:35.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:08:35.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:35.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:35.407
  STEP: Creating service test in namespace statefulset-8483 @ 07/29/23 16:08:35.413
  STEP: Creating a new StatefulSet @ 07/29/23 16:08:35.426
  Jul 29 16:08:35.444: INFO: Found 0 stateful pods, waiting for 3
  Jul 29 16:08:45.453: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:08:45.453: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:08:45.453: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/29/23 16:08:45.47
  Jul 29 16:08:45.496: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/29/23 16:08:45.496
  STEP: Not applying an update when the partition is greater than the number of replicas @ 07/29/23 16:08:55.565
  STEP: Performing a canary update @ 07/29/23 16:08:55.565
  Jul 29 16:08:55.595: INFO: Updating stateful set ss2
  Jul 29 16:08:55.656: INFO: Waiting for Pod statefulset-8483/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 07/29/23 16:09:05.712
  Jul 29 16:09:05.831: INFO: Found 1 stateful pods, waiting for 3
  Jul 29 16:09:15.845: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:09:15.846: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:09:15.846: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 07/29/23 16:09:15.859
  Jul 29 16:09:15.891: INFO: Updating stateful set ss2
  Jul 29 16:09:15.905: INFO: Waiting for Pod statefulset-8483/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul 29 16:09:25.974: INFO: Updating stateful set ss2
  Jul 29 16:09:25.995: INFO: Waiting for StatefulSet statefulset-8483/ss2 to complete update
  Jul 29 16:09:25.995: INFO: Waiting for Pod statefulset-8483/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul 29 16:09:36.019: INFO: Waiting for StatefulSet statefulset-8483/ss2 to complete update
  Jul 29 16:09:36.019: INFO: Waiting for Pod statefulset-8483/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul 29 16:09:46.022: INFO: Deleting all statefulset in ns statefulset-8483
  Jul 29 16:09:46.030: INFO: Scaling statefulset ss2 to 0
  Jul 29 16:10:26.065: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:10:26.072: INFO: Deleting statefulset ss2
  Jul 29 16:10:26.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8483" for this suite. @ 07/29/23 16:10:26.101
• [110.745 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 07/29/23 16:10:26.122
  Jul 29 16:10:26.123: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename taint-single-pod @ 07/29/23 16:10:26.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:10:26.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:10:26.158
  Jul 29 16:10:26.164: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 16:11:26.208: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:11:26.216: INFO: Starting informer...
  STEP: Starting pod... @ 07/29/23 16:11:26.217
  Jul 29 16:11:26.451: INFO: Pod is running on vucheipi7kei-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/29/23 16:11:26.451
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 16:11:26.475
  STEP: Waiting short time to make sure Pod is queued for deletion @ 07/29/23 16:11:26.482
  Jul 29 16:11:26.482: INFO: Pod wasn't evicted. Proceeding
  Jul 29 16:11:26.482: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 16:11:26.505
  STEP: Waiting some time to make sure that toleration time passed. @ 07/29/23 16:11:26.511
  Jul 29 16:12:41.511: INFO: Pod wasn't evicted. Test successful
  Jul 29 16:12:41.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-9797" for this suite. @ 07/29/23 16:12:41.531
• [135.424 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 07/29/23 16:12:41.555
  Jul 29 16:12:41.555: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename init-container @ 07/29/23 16:12:41.558
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:41.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:41.591
  STEP: creating the pod @ 07/29/23 16:12:41.596
  Jul 29 16:12:41.596: INFO: PodSpec: initContainers in spec.initContainers
  Jul 29 16:12:45.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2999" for this suite. @ 07/29/23 16:12:45.269
• [3.729 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 07/29/23 16:12:45.29
  Jul 29 16:12:45.290: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:12:45.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:45.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:45.321
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:12:45.327
  STEP: Saw pod success @ 07/29/23 16:12:49.368
  Jul 29 16:12:49.375: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-8a4d9d38-4315-4e26-920f-287cb2d05130 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:12:49.407
  Jul 29 16:12:49.434: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7134" for this suite. @ 07/29/23 16:12:49.443
• [4.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 07/29/23 16:12:49.474
  Jul 29 16:12:49.474: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 16:12:49.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:49.541
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:49.547
  Jul 29 16:12:49.553: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  W0729 16:12:52.357728      14 warnings.go:70] unknown field "alpha"
  W0729 16:12:52.357803      14 warnings.go:70] unknown field "beta"
  W0729 16:12:52.357814      14 warnings.go:70] unknown field "delta"
  W0729 16:12:52.357823      14 warnings.go:70] unknown field "epsilon"
  W0729 16:12:52.357832      14 warnings.go:70] unknown field "gamma"
  Jul 29 16:12:52.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4048" for this suite. @ 07/29/23 16:12:52.953
• [3.489 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 07/29/23 16:12:52.968
  Jul 29 16:12:52.968: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename taint-multiple-pods @ 07/29/23 16:12:52.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:53.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:53.007
  Jul 29 16:12:53.012: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 16:13:53.061: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:13:53.069: INFO: Starting informer...
  STEP: Starting pods... @ 07/29/23 16:13:53.07
  Jul 29 16:13:53.307: INFO: Pod1 is running on vucheipi7kei-3. Tainting Node
  Jul 29 16:13:55.551: INFO: Pod2 is running on vucheipi7kei-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/29/23 16:13:55.552
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 16:13:55.574
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 07/29/23 16:13:55.582
  Jul 29 16:14:01.610: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Jul 29 16:14:21.741: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jul 29 16:14:21.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 16:14:21.778
  STEP: Destroying namespace "taint-multiple-pods-1469" for this suite. @ 07/29/23 16:14:21.786
• [88.832 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 07/29/23 16:14:21.807
  Jul 29 16:14:21.807: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:14:21.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:21.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:21.845
  Jul 29 16:14:21.852: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jul 29 16:14:21.866: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 16:14:26.875: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 16:14:26.876
  Jul 29 16:14:26.877: INFO: Creating deployment "test-rolling-update-deployment"
  Jul 29 16:14:26.889: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jul 29 16:14:26.917: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Jul 29 16:14:28.934: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jul 29 16:14:28.942: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jul 29 16:14:28.974: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9826  d09635e0-2bc2-47b3-a0dd-011b0afa09c4 17363 1 2023-07-29 16:14:26 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-29 16:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:14:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d72d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:14:26 +0000 UTC,LastTransitionTime:2023-07-29 16:14:26 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-07-29 16:14:28 +0000 UTC,LastTransitionTime:2023-07-29 16:14:26 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 16:14:28.980: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-9826  3c904c86-b7c6-4a08-9fb2-40055beba52d 17352 1 2023-07-29 16:14:26 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment d09635e0-2bc2-47b3-a0dd-011b0afa09c4 0xc004d73277 0xc004d73278}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d09635e0-2bc2-47b3-a0dd-011b0afa09c4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:14:28 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d73328 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:14:28.980: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jul 29 16:14:28.981: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9826  c9ed4a66-5b6d-4aea-8506-466e9aac0a44 17362 2 2023-07-29 16:14:21 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment d09635e0-2bc2-47b3-a0dd-011b0afa09c4 0xc004d73147 0xc004d73148}] [] [{e2e.test Update apps/v1 2023-07-29 16:14:21 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:14:28 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d09635e0-2bc2-47b3-a0dd-011b0afa09c4\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:14:28 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d73208 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:14:28.987: INFO: Pod "test-rolling-update-deployment-656d657cd8-vdlwm" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-vdlwm test-rolling-update-deployment-656d657cd8- deployment-9826  64d7cb0e-df78-4aaa-b54d-895b51b48204 17351 0 2023-07-29 16:14:26 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 3c904c86-b7c6-4a08-9fb2-40055beba52d 0xc004d73797 0xc004d73798}] [] [{kube-controller-manager Update v1 2023-07-29 16:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c904c86-b7c6-4a08-9fb2-40055beba52d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:14:28 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.219\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8d6d5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8d6d5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:14:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:14:28 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:14:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:10.233.65.219,StartTime:2023-07-29 16:14:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:14:27 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://ae70f5e4e2a00b20a21b35206422bdbb17ce86f8a51798cd33eedcb9eef18c3e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.219,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 16:14:28.988: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9826" for this suite. @ 07/29/23 16:14:28.996
• [7.201 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 07/29/23 16:14:29.013
  Jul 29 16:14:29.013: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 16:14:29.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:29.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:29.052
  Jul 29 16:14:29.058: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 07/29/23 16:14:30.968
  Jul 29 16:14:30.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 create -f -'
  Jul 29 16:14:32.350: INFO: stderr: ""
  Jul 29 16:14:32.350: INFO: stdout: "e2e-test-crd-publish-openapi-567-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 29 16:14:32.351: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 delete e2e-test-crd-publish-openapi-567-crds test-foo'
  Jul 29 16:14:32.485: INFO: stderr: ""
  Jul 29 16:14:32.485: INFO: stdout: "e2e-test-crd-publish-openapi-567-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jul 29 16:14:32.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 apply -f -'
  Jul 29 16:14:32.927: INFO: stderr: ""
  Jul 29 16:14:32.927: INFO: stdout: "e2e-test-crd-publish-openapi-567-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 29 16:14:32.927: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 delete e2e-test-crd-publish-openapi-567-crds test-foo'
  Jul 29 16:14:33.207: INFO: stderr: ""
  Jul 29 16:14:33.207: INFO: stdout: "e2e-test-crd-publish-openapi-567-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 07/29/23 16:14:33.207
  Jul 29 16:14:33.207: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 create -f -'
  Jul 29 16:14:34.605: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 07/29/23 16:14:34.605
  Jul 29 16:14:34.605: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 create -f -'
  Jul 29 16:14:35.065: INFO: rc: 1
  Jul 29 16:14:35.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 apply -f -'
  Jul 29 16:14:35.532: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 07/29/23 16:14:35.532
  Jul 29 16:14:35.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 create -f -'
  Jul 29 16:14:36.014: INFO: rc: 1
  Jul 29 16:14:36.014: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 --namespace=crd-publish-openapi-209 apply -f -'
  Jul 29 16:14:36.454: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 07/29/23 16:14:36.454
  Jul 29 16:14:36.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 explain e2e-test-crd-publish-openapi-567-crds'
  Jul 29 16:14:36.886: INFO: stderr: ""
  Jul 29 16:14:36.886: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-567-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 07/29/23 16:14:36.886
  Jul 29 16:14:36.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 explain e2e-test-crd-publish-openapi-567-crds.metadata'
  Jul 29 16:14:37.348: INFO: stderr: ""
  Jul 29 16:14:37.348: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-567-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jul 29 16:14:37.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 explain e2e-test-crd-publish-openapi-567-crds.spec'
  Jul 29 16:14:37.759: INFO: stderr: ""
  Jul 29 16:14:37.759: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-567-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jul 29 16:14:37.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 explain e2e-test-crd-publish-openapi-567-crds.spec.bars'
  Jul 29 16:14:38.163: INFO: stderr: ""
  Jul 29 16:14:38.164: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-567-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 07/29/23 16:14:38.164
  Jul 29 16:14:38.164: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-209 explain e2e-test-crd-publish-openapi-567-crds.spec.bars2'
  Jul 29 16:14:38.557: INFO: rc: 1
  Jul 29 16:14:40.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-209" for this suite. @ 07/29/23 16:14:40.408
• [11.406 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 07/29/23 16:14:40.421
  Jul 29 16:14:40.421: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename init-container @ 07/29/23 16:14:40.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:40.458
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:40.464
  STEP: creating the pod @ 07/29/23 16:14:40.469
  Jul 29 16:14:40.469: INFO: PodSpec: initContainers in spec.initContainers
  Jul 29 16:14:44.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5836" for this suite. @ 07/29/23 16:14:44.902
• [4.491 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 07/29/23 16:14:44.915
  Jul 29 16:14:44.915: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:14:44.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:44.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:44.965
  STEP: creating service in namespace services-2873 @ 07/29/23 16:14:44.971
  STEP: creating service affinity-nodeport in namespace services-2873 @ 07/29/23 16:14:44.972
  STEP: creating replication controller affinity-nodeport in namespace services-2873 @ 07/29/23 16:14:44.998
  I0729 16:14:45.015785      14 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-2873, replica count: 3
  I0729 16:14:48.071729      14 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:14:48.096: INFO: Creating new exec pod
  Jul 29 16:14:51.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-2873 exec execpod-affinityzvsxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jul 29 16:14:51.431: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jul 29 16:14:51.431: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:14:51.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-2873 exec execpod-affinityzvsxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.52.23 80'
  Jul 29 16:14:51.677: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.52.23 80\nConnection to 10.233.52.23 80 port [tcp/http] succeeded!\n"
  Jul 29 16:14:51.677: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:14:51.678: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-2873 exec execpod-affinityzvsxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.88 30543'
  Jul 29 16:14:51.914: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.88 30543\nConnection to 192.168.121.88 30543 port [tcp/*] succeeded!\n"
  Jul 29 16:14:51.914: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:14:51.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-2873 exec execpod-affinityzvsxp -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.77 30543'
  Jul 29 16:14:52.122: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.77 30543\nConnection to 192.168.121.77 30543 port [tcp/*] succeeded!\n"
  Jul 29 16:14:52.122: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:14:52.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-2873 exec execpod-affinityzvsxp -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.88:30543/ ; done'
  Jul 29 16:14:52.543: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30543/\n"
  Jul 29 16:14:52.543: INFO: stdout: "\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst\naffinity-nodeport-5scst"
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Received response from host: affinity-nodeport-5scst
  Jul 29 16:14:52.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:14:52.552: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-2873, will wait for the garbage collector to delete the pods @ 07/29/23 16:14:52.577
  Jul 29 16:14:52.657: INFO: Deleting ReplicationController affinity-nodeport took: 15.609099ms
  Jul 29 16:14:52.758: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.17882ms
  STEP: Destroying namespace "services-2873" for this suite. @ 07/29/23 16:14:55.004
• [10.100 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 07/29/23 16:14:55.016
  Jul 29 16:14:55.016: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:14:55.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:55.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:55.052
  STEP: Creating configMap with name configmap-test-upd-f6a8f12a-e8bb-4148-af26-2c281e0d6d82 @ 07/29/23 16:14:55.063
  STEP: Creating the pod @ 07/29/23 16:14:55.07
  STEP: Updating configmap configmap-test-upd-f6a8f12a-e8bb-4148-af26-2c281e0d6d82 @ 07/29/23 16:14:57.146
  STEP: waiting to observe update in volume @ 07/29/23 16:14:57.154
  Jul 29 16:16:09.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3185" for this suite. @ 07/29/23 16:16:09.927
• [74.924 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 07/29/23 16:16:09.943
  Jul 29 16:16:09.943: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:16:09.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:09.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:09.983
  STEP: Creating service test in namespace statefulset-3781 @ 07/29/23 16:16:09.987
  STEP: Creating a new StatefulSet @ 07/29/23 16:16:09.998
  Jul 29 16:16:10.015: INFO: Found 0 stateful pods, waiting for 3
  Jul 29 16:16:20.023: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:16:20.024: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:16:20.024: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:16:20.042: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-3781 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:16:20.341: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:16:20.341: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:16:20.341: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/29/23 16:16:30.37
  Jul 29 16:16:30.401: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/29/23 16:16:30.401
  STEP: Updating Pods in reverse ordinal order @ 07/29/23 16:16:40.454
  Jul 29 16:16:40.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-3781 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:16:40.718: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:16:40.718: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:16:40.718: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 07/29/23 16:16:50.769
  Jul 29 16:16:50.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-3781 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:16:51.033: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:16:51.034: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:16:51.034: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:17:01.089: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 07/29/23 16:17:11.121
  Jul 29 16:17:11.128: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-3781 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:17:11.402: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:17:11.402: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:17:11.402: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:17:21.448: INFO: Deleting all statefulset in ns statefulset-3781
  Jul 29 16:17:21.453: INFO: Scaling statefulset ss2 to 0
  Jul 29 16:17:31.489: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:17:31.496: INFO: Deleting statefulset ss2
  Jul 29 16:17:31.526: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3781" for this suite. @ 07/29/23 16:17:31.538
• [81.612 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 07/29/23 16:17:31.557
  Jul 29 16:17:31.557: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 16:17:31.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:31.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:31.59
  STEP: Updating Namespace "namespaces-7881" @ 07/29/23 16:17:31.595
  Jul 29 16:17:31.610: INFO: Namespace "namespaces-7881" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"4165bb08-2af4-4df8-8e08-20c77064136a", "kubernetes.io/metadata.name":"namespaces-7881", "namespaces-7881":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jul 29 16:17:31.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7881" for this suite. @ 07/29/23 16:17:31.622
• [0.081 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 07/29/23 16:17:31.644
  Jul 29 16:17:31.644: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:17:31.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:31.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:31.677
  STEP: Creating a ResourceQuota @ 07/29/23 16:17:31.681
  STEP: Getting a ResourceQuota @ 07/29/23 16:17:31.691
  STEP: Listing all ResourceQuotas with LabelSelector @ 07/29/23 16:17:31.709
  STEP: Patching the ResourceQuota @ 07/29/23 16:17:31.714
  STEP: Deleting a Collection of ResourceQuotas @ 07/29/23 16:17:31.725
  STEP: Verifying the deleted ResourceQuota @ 07/29/23 16:17:31.738
  Jul 29 16:17:31.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5910" for this suite. @ 07/29/23 16:17:31.775
• [0.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 07/29/23 16:17:31.794
  Jul 29 16:17:31.794: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 16:17:31.796
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:31.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:31.83
  Jul 29 16:17:31.834: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 16:17:31.857: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:17:31.862: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-1 before test
  Jul 29 16:17:31.878: INFO: cilium-lk6kf from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.878: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: cilium-node-init-gs57c from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.878: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: kube-addon-manager-vucheipi7kei-1 from kube-system started at 2023-07-29 15:23:00 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.878: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: kube-apiserver-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.878: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: kube-controller-manager-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.878: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: kube-proxy-qkz2r from kube-system started at 2023-07-29 15:14:13 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.878: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: kube-scheduler-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.878: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-4kwpr from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 16:17:31.878: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:17:31.878: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-2 before test
  Jul 29 16:17:31.898: INFO: cilium-cqdq2 from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: cilium-node-init-5p29d from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: coredns-5d78c9869d-67zm5 from kube-system started at 2023-07-29 15:24:06 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: coredns-5d78c9869d-kpth8 from kube-system started at 2023-07-29 15:24:06 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: kube-addon-manager-vucheipi7kei-2 from kube-system started at 2023-07-29 15:23:00 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: kube-apiserver-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: kube-controller-manager-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: kube-proxy-n67bc from kube-system started at 2023-07-29 15:14:48 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: kube-scheduler-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-hpqzb from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 16:17:31.898: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:17:31.898: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-3 before test
  Jul 29 16:17:31.918: INFO: cilium-9924s from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.918: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:17:31.919: INFO: cilium-node-init-ndt4w from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.919: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:17:31.919: INFO: cilium-operator-64cdf5fc9d-wj2rs from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.920: INFO: 	Container cilium-operator ready: true, restart count 0
  Jul 29 16:17:31.920: INFO: kube-proxy-szdbr from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.920: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:17:31.920: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (1 container statuses recorded)
  Jul 29 16:17:31.921: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 16:17:31.921: INFO: sonobuoy-e2e-job-e435975ae918422f from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 16:17:31.921: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 16:17:31.921: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:17:31.921: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-rwzlm from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 16:17:31.922: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:17:31.922: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 16:17:31.923
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 16:17:33.961
  STEP: Trying to apply a random label on the found node. @ 07/29/23 16:17:33.983
  STEP: verifying the node has the label kubernetes.io/e2e-68da19a6-a971-4372-88a1-ef42344b8f98 95 @ 07/29/23 16:17:34.005
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 07/29/23 16:17:34.016
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.129 on the node which pod4 resides and expect not scheduled @ 07/29/23 16:17:36.049
  STEP: removing the label kubernetes.io/e2e-68da19a6-a971-4372-88a1-ef42344b8f98 off the node vucheipi7kei-3 @ 07/29/23 16:22:36.061
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-68da19a6-a971-4372-88a1-ef42344b8f98 @ 07/29/23 16:22:36.09
  Jul 29 16:22:36.102: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1606" for this suite. @ 07/29/23 16:22:36.12
• [304.346 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 07/29/23 16:22:36.143
  Jul 29 16:22:36.143: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:22:36.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:22:36.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:22:36.197
  STEP: creating service in namespace services-4661 @ 07/29/23 16:22:36.208
  STEP: creating service affinity-nodeport-transition in namespace services-4661 @ 07/29/23 16:22:36.209
  STEP: creating replication controller affinity-nodeport-transition in namespace services-4661 @ 07/29/23 16:22:36.24
  I0729 16:22:36.258466      14 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-4661, replica count: 3
  I0729 16:22:39.311022      14 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 16:22:42.313765      14 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:22:42.339: INFO: Creating new exec pod
  Jul 29 16:22:45.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4661 exec execpod-affinityws845 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jul 29 16:22:45.714: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jul 29 16:22:45.714: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:22:45.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4661 exec execpod-affinityws845 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.60.66 80'
  Jul 29 16:22:45.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.60.66 80\nConnection to 10.233.60.66 80 port [tcp/http] succeeded!\n"
  Jul 29 16:22:45.943: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:22:45.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4661 exec execpod-affinityws845 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.77 30877'
  Jul 29 16:22:46.211: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.77 30877\nConnection to 192.168.121.77 30877 port [tcp/*] succeeded!\n"
  Jul 29 16:22:46.212: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:22:46.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4661 exec execpod-affinityws845 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.88 30877'
  Jul 29 16:22:46.448: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.88 30877\nConnection to 192.168.121.88 30877 port [tcp/*] succeeded!\n"
  Jul 29 16:22:46.448: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:22:46.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4661 exec execpod-affinityws845 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.88:30877/ ; done'
  Jul 29 16:22:47.031: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n"
  Jul 29 16:22:47.031: INFO: stdout: "\naffinity-nodeport-transition-wbbps\naffinity-nodeport-transition-wbbps\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-qvzs4\naffinity-nodeport-transition-wbbps\naffinity-nodeport-transition-qvzs4\naffinity-nodeport-transition-qvzs4\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-qvzs4\naffinity-nodeport-transition-qvzs4\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-qvzs4\naffinity-nodeport-transition-qvzs4\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54"
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-wbbps
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-wbbps
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-qvzs4
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-wbbps
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-qvzs4
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-qvzs4
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-qvzs4
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-qvzs4
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-qvzs4
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-qvzs4
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.031: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4661 exec execpod-affinityws845 -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.88:30877/ ; done'
  Jul 29 16:22:47.549: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.88:30877/\n"
  Jul 29 16:22:47.549: INFO: stdout: "\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54\naffinity-nodeport-transition-cpj54"
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Received response from host: affinity-nodeport-transition-cpj54
  Jul 29 16:22:47.549: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:22:47.558: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4661, will wait for the garbage collector to delete the pods @ 07/29/23 16:22:47.579
  Jul 29 16:22:47.657: INFO: Deleting ReplicationController affinity-nodeport-transition took: 15.959482ms
  Jul 29 16:22:47.758: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.308271ms
  STEP: Destroying namespace "services-4661" for this suite. @ 07/29/23 16:22:49.909
• [13.779 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 07/29/23 16:22:49.923
  Jul 29 16:22:49.923: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:22:49.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:22:49.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:22:49.967
  STEP: creating secret secrets-9964/secret-test-07dbfac7-7a03-4750-bba8-6e3eb476e365 @ 07/29/23 16:22:49.972
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:22:49.98
  STEP: Saw pod success @ 07/29/23 16:22:54.021
  Jul 29 16:22:54.031: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-3bc723c0-afa3-4e12-a6ae-9a227b46ee50 container env-test: <nil>
  STEP: delete the pod @ 07/29/23 16:22:54.064
  Jul 29 16:22:54.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9964" for this suite. @ 07/29/23 16:22:54.094
• [4.183 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 07/29/23 16:22:54.106
  Jul 29 16:22:54.106: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 16:22:54.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:22:54.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:22:54.19
  STEP: Creating namespace "e2e-ns-l8zst" @ 07/29/23 16:22:54.195
  Jul 29 16:22:54.219: INFO: Namespace "e2e-ns-l8zst-3293" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-l8zst-3293" @ 07/29/23 16:22:54.219
  Jul 29 16:22:54.234: INFO: Namespace "e2e-ns-l8zst-3293" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-l8zst-3293" @ 07/29/23 16:22:54.234
  Jul 29 16:22:54.246: INFO: Namespace "e2e-ns-l8zst-3293" has []v1.FinalizerName{"kubernetes"}
  Jul 29 16:22:54.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6086" for this suite. @ 07/29/23 16:22:54.254
  STEP: Destroying namespace "e2e-ns-l8zst-3293" for this suite. @ 07/29/23 16:22:54.263
• [0.165 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 07/29/23 16:22:54.277
  Jul 29 16:22:54.277: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/29/23 16:22:54.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:22:54.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:22:54.315
  STEP: creating @ 07/29/23 16:22:54.322
  STEP: getting @ 07/29/23 16:22:54.357
  STEP: listing in namespace @ 07/29/23 16:22:54.366
  STEP: patching @ 07/29/23 16:22:54.372
  STEP: deleting @ 07/29/23 16:22:54.392
  Jul 29 16:22:54.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8826" for this suite. @ 07/29/23 16:22:54.429
• [0.161 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 07/29/23 16:22:54.441
  Jul 29 16:22:54.442: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:22:54.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:22:54.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:22:54.481
  STEP: creating service nodeport-test with type=NodePort in namespace services-4030 @ 07/29/23 16:22:54.485
  STEP: creating replication controller nodeport-test in namespace services-4030 @ 07/29/23 16:22:54.514
  I0729 16:22:54.530026      14 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-4030, replica count: 2
  I0729 16:22:57.581424      14 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:22:57.581: INFO: Creating new exec pod
  Jul 29 16:23:00.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4030 exec execpod8jxjk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul 29 16:23:00.903: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 29 16:23:00.903: INFO: stdout: ""
  Jul 29 16:23:01.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4030 exec execpod8jxjk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul 29 16:23:02.131: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 29 16:23:02.131: INFO: stdout: ""
  Jul 29 16:23:02.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4030 exec execpod8jxjk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul 29 16:23:03.115: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 29 16:23:03.115: INFO: stdout: "nodeport-test-4wst8"
  Jul 29 16:23:03.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4030 exec execpod8jxjk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.33.65 80'
  Jul 29 16:23:03.367: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.33.65 80\nConnection to 10.233.33.65 80 port [tcp/http] succeeded!\n"
  Jul 29 16:23:03.367: INFO: stdout: "nodeport-test-7t758"
  Jul 29 16:23:03.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4030 exec execpod8jxjk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.77 30451'
  Jul 29 16:23:03.582: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.77 30451\nConnection to 192.168.121.77 30451 port [tcp/*] succeeded!\n"
  Jul 29 16:23:03.582: INFO: stdout: "nodeport-test-7t758"
  Jul 29 16:23:03.583: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4030 exec execpod8jxjk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.88 30451'
  Jul 29 16:23:03.828: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.88 30451\nConnection to 192.168.121.88 30451 port [tcp/*] succeeded!\n"
  Jul 29 16:23:03.828: INFO: stdout: ""
  Jul 29 16:23:04.829: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-4030 exec execpod8jxjk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.88 30451'
  Jul 29 16:23:05.089: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.88 30451\nConnection to 192.168.121.88 30451 port [tcp/*] succeeded!\n"
  Jul 29 16:23:05.089: INFO: stdout: "nodeport-test-7t758"
  Jul 29 16:23:05.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4030" for this suite. @ 07/29/23 16:23:05.098
• [10.671 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 07/29/23 16:23:05.116
  Jul 29 16:23:05.116: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename gc @ 07/29/23 16:23:05.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:23:05.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:23:05.149
  STEP: create the rc @ 07/29/23 16:23:05.159
  W0729 16:23:05.167442      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/29/23 16:23:11.239
  STEP: wait for the rc to be deleted @ 07/29/23 16:23:11.492
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 07/29/23 16:23:16.81
  STEP: Gathering metrics @ 07/29/23 16:23:46.85
  Jul 29 16:23:47.055: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 16:23:47.056: INFO: Deleting pod "simpletest.rc-2hlmw" in namespace "gc-8233"
  Jul 29 16:23:47.079: INFO: Deleting pod "simpletest.rc-2mht9" in namespace "gc-8233"
  Jul 29 16:23:47.164: INFO: Deleting pod "simpletest.rc-2whpq" in namespace "gc-8233"
  Jul 29 16:23:47.227: INFO: Deleting pod "simpletest.rc-2zfft" in namespace "gc-8233"
  Jul 29 16:23:47.305: INFO: Deleting pod "simpletest.rc-47h84" in namespace "gc-8233"
  Jul 29 16:23:47.347: INFO: Deleting pod "simpletest.rc-4bbdz" in namespace "gc-8233"
  Jul 29 16:23:47.396: INFO: Deleting pod "simpletest.rc-4g7rm" in namespace "gc-8233"
  Jul 29 16:23:47.434: INFO: Deleting pod "simpletest.rc-4hccd" in namespace "gc-8233"
  Jul 29 16:23:47.520: INFO: Deleting pod "simpletest.rc-4kzvf" in namespace "gc-8233"
  Jul 29 16:23:47.564: INFO: Deleting pod "simpletest.rc-54sss" in namespace "gc-8233"
  Jul 29 16:23:47.633: INFO: Deleting pod "simpletest.rc-58crl" in namespace "gc-8233"
  Jul 29 16:23:47.757: INFO: Deleting pod "simpletest.rc-5gmpl" in namespace "gc-8233"
  Jul 29 16:23:47.830: INFO: Deleting pod "simpletest.rc-62r5f" in namespace "gc-8233"
  Jul 29 16:23:47.898: INFO: Deleting pod "simpletest.rc-6bqt4" in namespace "gc-8233"
  Jul 29 16:23:47.939: INFO: Deleting pod "simpletest.rc-6sl4w" in namespace "gc-8233"
  Jul 29 16:23:47.986: INFO: Deleting pod "simpletest.rc-6v977" in namespace "gc-8233"
  Jul 29 16:23:48.096: INFO: Deleting pod "simpletest.rc-6wtpx" in namespace "gc-8233"
  Jul 29 16:23:48.252: INFO: Deleting pod "simpletest.rc-6xh64" in namespace "gc-8233"
  Jul 29 16:23:48.320: INFO: Deleting pod "simpletest.rc-7gk4s" in namespace "gc-8233"
  Jul 29 16:23:48.421: INFO: Deleting pod "simpletest.rc-7jz44" in namespace "gc-8233"
  Jul 29 16:23:48.477: INFO: Deleting pod "simpletest.rc-7k6z8" in namespace "gc-8233"
  Jul 29 16:23:48.549: INFO: Deleting pod "simpletest.rc-7s9zs" in namespace "gc-8233"
  Jul 29 16:23:48.609: INFO: Deleting pod "simpletest.rc-82srt" in namespace "gc-8233"
  Jul 29 16:23:48.677: INFO: Deleting pod "simpletest.rc-84d2x" in namespace "gc-8233"
  Jul 29 16:23:48.828: INFO: Deleting pod "simpletest.rc-86dlt" in namespace "gc-8233"
  Jul 29 16:23:48.940: INFO: Deleting pod "simpletest.rc-88k56" in namespace "gc-8233"
  Jul 29 16:23:48.998: INFO: Deleting pod "simpletest.rc-896nx" in namespace "gc-8233"
  Jul 29 16:23:49.032: INFO: Deleting pod "simpletest.rc-8dcbn" in namespace "gc-8233"
  Jul 29 16:23:49.089: INFO: Deleting pod "simpletest.rc-8zfpb" in namespace "gc-8233"
  Jul 29 16:23:49.250: INFO: Deleting pod "simpletest.rc-97qwk" in namespace "gc-8233"
  Jul 29 16:23:49.421: INFO: Deleting pod "simpletest.rc-9cp2b" in namespace "gc-8233"
  Jul 29 16:23:49.545: INFO: Deleting pod "simpletest.rc-9g7t4" in namespace "gc-8233"
  Jul 29 16:23:49.986: INFO: Deleting pod "simpletest.rc-9tm8p" in namespace "gc-8233"
  Jul 29 16:23:50.117: INFO: Deleting pod "simpletest.rc-9wljl" in namespace "gc-8233"
  Jul 29 16:23:50.163: INFO: Deleting pod "simpletest.rc-b45vk" in namespace "gc-8233"
  Jul 29 16:23:50.297: INFO: Deleting pod "simpletest.rc-b6gjx" in namespace "gc-8233"
  Jul 29 16:23:50.370: INFO: Deleting pod "simpletest.rc-cf2zl" in namespace "gc-8233"
  Jul 29 16:23:50.567: INFO: Deleting pod "simpletest.rc-crlh8" in namespace "gc-8233"
  Jul 29 16:23:50.648: INFO: Deleting pod "simpletest.rc-d8skj" in namespace "gc-8233"
  Jul 29 16:23:50.744: INFO: Deleting pod "simpletest.rc-dl8sr" in namespace "gc-8233"
  Jul 29 16:23:50.826: INFO: Deleting pod "simpletest.rc-fdjsq" in namespace "gc-8233"
  Jul 29 16:23:50.915: INFO: Deleting pod "simpletest.rc-ffnbl" in namespace "gc-8233"
  Jul 29 16:23:51.035: INFO: Deleting pod "simpletest.rc-fr682" in namespace "gc-8233"
  Jul 29 16:23:51.108: INFO: Deleting pod "simpletest.rc-fwrqq" in namespace "gc-8233"
  Jul 29 16:23:51.199: INFO: Deleting pod "simpletest.rc-ghskj" in namespace "gc-8233"
  Jul 29 16:23:51.254: INFO: Deleting pod "simpletest.rc-h72wf" in namespace "gc-8233"
  Jul 29 16:23:51.331: INFO: Deleting pod "simpletest.rc-hnzb7" in namespace "gc-8233"
  Jul 29 16:23:51.372: INFO: Deleting pod "simpletest.rc-hvqcz" in namespace "gc-8233"
  Jul 29 16:23:51.444: INFO: Deleting pod "simpletest.rc-j5vp7" in namespace "gc-8233"
  Jul 29 16:23:51.576: INFO: Deleting pod "simpletest.rc-jxfq9" in namespace "gc-8233"
  Jul 29 16:23:51.635: INFO: Deleting pod "simpletest.rc-k8jss" in namespace "gc-8233"
  Jul 29 16:23:51.703: INFO: Deleting pod "simpletest.rc-k9h9q" in namespace "gc-8233"
  Jul 29 16:23:51.833: INFO: Deleting pod "simpletest.rc-kpjvj" in namespace "gc-8233"
  Jul 29 16:23:51.964: INFO: Deleting pod "simpletest.rc-l6b2t" in namespace "gc-8233"
  Jul 29 16:23:52.032: INFO: Deleting pod "simpletest.rc-ljtl5" in namespace "gc-8233"
  Jul 29 16:23:52.104: INFO: Deleting pod "simpletest.rc-lkdzt" in namespace "gc-8233"
  Jul 29 16:23:52.172: INFO: Deleting pod "simpletest.rc-ltknp" in namespace "gc-8233"
  Jul 29 16:23:52.252: INFO: Deleting pod "simpletest.rc-m4m2m" in namespace "gc-8233"
  Jul 29 16:23:52.314: INFO: Deleting pod "simpletest.rc-m968p" in namespace "gc-8233"
  Jul 29 16:23:52.377: INFO: Deleting pod "simpletest.rc-mdpmk" in namespace "gc-8233"
  Jul 29 16:23:52.569: INFO: Deleting pod "simpletest.rc-mflvh" in namespace "gc-8233"
  Jul 29 16:23:52.647: INFO: Deleting pod "simpletest.rc-mhcnj" in namespace "gc-8233"
  Jul 29 16:23:52.709: INFO: Deleting pod "simpletest.rc-mwbd8" in namespace "gc-8233"
  Jul 29 16:23:52.757: INFO: Deleting pod "simpletest.rc-n2hdt" in namespace "gc-8233"
  Jul 29 16:23:52.864: INFO: Deleting pod "simpletest.rc-n4bl5" in namespace "gc-8233"
  Jul 29 16:23:52.931: INFO: Deleting pod "simpletest.rc-nmbsz" in namespace "gc-8233"
  Jul 29 16:23:52.991: INFO: Deleting pod "simpletest.rc-pbf4b" in namespace "gc-8233"
  Jul 29 16:23:53.062: INFO: Deleting pod "simpletest.rc-ptd44" in namespace "gc-8233"
  Jul 29 16:23:53.179: INFO: Deleting pod "simpletest.rc-pttbl" in namespace "gc-8233"
  Jul 29 16:23:53.247: INFO: Deleting pod "simpletest.rc-pwrdv" in namespace "gc-8233"
  Jul 29 16:23:53.303: INFO: Deleting pod "simpletest.rc-qldvh" in namespace "gc-8233"
  Jul 29 16:23:53.380: INFO: Deleting pod "simpletest.rc-qp9nc" in namespace "gc-8233"
  Jul 29 16:23:53.446: INFO: Deleting pod "simpletest.rc-rbh9k" in namespace "gc-8233"
  Jul 29 16:23:53.522: INFO: Deleting pod "simpletest.rc-rpmfh" in namespace "gc-8233"
  Jul 29 16:23:53.764: INFO: Deleting pod "simpletest.rc-rsm7q" in namespace "gc-8233"
  Jul 29 16:23:53.835: INFO: Deleting pod "simpletest.rc-sb4z6" in namespace "gc-8233"
  Jul 29 16:23:53.909: INFO: Deleting pod "simpletest.rc-scw8r" in namespace "gc-8233"
  Jul 29 16:23:53.981: INFO: Deleting pod "simpletest.rc-sqrj4" in namespace "gc-8233"
  Jul 29 16:23:54.019: INFO: Deleting pod "simpletest.rc-ssdpz" in namespace "gc-8233"
  Jul 29 16:23:54.074: INFO: Deleting pod "simpletest.rc-t8pqb" in namespace "gc-8233"
  Jul 29 16:23:54.155: INFO: Deleting pod "simpletest.rc-tb8gd" in namespace "gc-8233"
  Jul 29 16:23:54.204: INFO: Deleting pod "simpletest.rc-tqffq" in namespace "gc-8233"
  Jul 29 16:23:54.312: INFO: Deleting pod "simpletest.rc-tzws2" in namespace "gc-8233"
  Jul 29 16:23:54.363: INFO: Deleting pod "simpletest.rc-vlpgm" in namespace "gc-8233"
  Jul 29 16:23:54.439: INFO: Deleting pod "simpletest.rc-vqpqd" in namespace "gc-8233"
  Jul 29 16:23:54.524: INFO: Deleting pod "simpletest.rc-vtch6" in namespace "gc-8233"
  Jul 29 16:23:54.648: INFO: Deleting pod "simpletest.rc-vvtnw" in namespace "gc-8233"
  Jul 29 16:23:54.714: INFO: Deleting pod "simpletest.rc-w6nhj" in namespace "gc-8233"
  Jul 29 16:23:54.749: INFO: Deleting pod "simpletest.rc-wd6ls" in namespace "gc-8233"
  Jul 29 16:23:54.833: INFO: Deleting pod "simpletest.rc-wdtfj" in namespace "gc-8233"
  Jul 29 16:23:54.973: INFO: Deleting pod "simpletest.rc-wk262" in namespace "gc-8233"
  Jul 29 16:23:55.055: INFO: Deleting pod "simpletest.rc-wnwvr" in namespace "gc-8233"
  Jul 29 16:23:55.095: INFO: Deleting pod "simpletest.rc-wq46k" in namespace "gc-8233"
  Jul 29 16:23:55.167: INFO: Deleting pod "simpletest.rc-x4vpk" in namespace "gc-8233"
  Jul 29 16:23:55.248: INFO: Deleting pod "simpletest.rc-xnpk8" in namespace "gc-8233"
  Jul 29 16:23:55.343: INFO: Deleting pod "simpletest.rc-xnsks" in namespace "gc-8233"
  Jul 29 16:23:55.381: INFO: Deleting pod "simpletest.rc-z7dhx" in namespace "gc-8233"
  Jul 29 16:23:55.426: INFO: Deleting pod "simpletest.rc-z7ptz" in namespace "gc-8233"
  Jul 29 16:23:55.525: INFO: Deleting pod "simpletest.rc-zdw52" in namespace "gc-8233"
  Jul 29 16:23:55.576: INFO: Deleting pod "simpletest.rc-zndxt" in namespace "gc-8233"
  Jul 29 16:23:55.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8233" for this suite. @ 07/29/23 16:23:55.697
• [50.607 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 07/29/23 16:23:55.73
  Jul 29 16:23:55.730: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:23:55.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:23:55.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:23:55.855
  STEP: creating Agnhost RC @ 07/29/23 16:23:55.86
  Jul 29 16:23:55.860: INFO: namespace kubectl-697
  Jul 29 16:23:55.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-697 create -f -'
  Jul 29 16:23:56.799: INFO: stderr: ""
  Jul 29 16:23:56.799: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 16:23:56.799
  Jul 29 16:23:57.943: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:23:57.943: INFO: Found 0 / 1
  Jul 29 16:23:58.807: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:23:58.807: INFO: Found 1 / 1
  Jul 29 16:23:58.807: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 29 16:23:58.814: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:23:58.814: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 16:23:58.814: INFO: wait on agnhost-primary startup in kubectl-697 
  Jul 29 16:23:58.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-697 logs agnhost-primary-smpst agnhost-primary'
  Jul 29 16:23:58.967: INFO: stderr: ""
  Jul 29 16:23:58.967: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 07/29/23 16:23:58.967
  Jul 29 16:23:58.967: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-697 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jul 29 16:23:59.137: INFO: stderr: ""
  Jul 29 16:23:59.137: INFO: stdout: "service/rm2 exposed\n"
  Jul 29 16:23:59.149: INFO: Service rm2 in namespace kubectl-697 found.
  STEP: exposing service @ 07/29/23 16:24:01.161
  Jul 29 16:24:01.161: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-697 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jul 29 16:24:01.345: INFO: stderr: ""
  Jul 29 16:24:01.345: INFO: stdout: "service/rm3 exposed\n"
  Jul 29 16:24:01.358: INFO: Service rm3 in namespace kubectl-697 found.
  Jul 29 16:24:03.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-697" for this suite. @ 07/29/23 16:24:03.38
• [7.660 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 07/29/23 16:24:03.392
  Jul 29 16:24:03.392: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename disruption @ 07/29/23 16:24:03.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:03.429
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:03.433
  STEP: Creating a kubernetes client @ 07/29/23 16:24:03.438
  Jul 29 16:24:03.438: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename disruption-2 @ 07/29/23 16:24:03.44
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:03.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:03.47
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:24:03.483
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:24:05.52
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:24:07.54
  STEP: listing a collection of PDBs across all namespaces @ 07/29/23 16:24:07.551
  STEP: listing a collection of PDBs in namespace disruption-2878 @ 07/29/23 16:24:07.557
  STEP: deleting a collection of PDBs @ 07/29/23 16:24:07.565
  STEP: Waiting for the PDB collection to be deleted @ 07/29/23 16:24:07.584
  Jul 29 16:24:07.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:24:07.599: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5873" for this suite. @ 07/29/23 16:24:07.609
  STEP: Destroying namespace "disruption-2878" for this suite. @ 07/29/23 16:24:07.621
• [4.243 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 07/29/23 16:24:07.636
  Jul 29 16:24:07.636: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename containers @ 07/29/23 16:24:07.639
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:07.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:07.671
  Jul 29 16:24:09.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1612" for this suite. @ 07/29/23 16:24:09.727
• [2.103 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 07/29/23 16:24:09.743
  Jul 29 16:24:09.743: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 16:24:09.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:09.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:09.778
  STEP: creating a ServiceAccount @ 07/29/23 16:24:09.784
  STEP: watching for the ServiceAccount to be added @ 07/29/23 16:24:09.798
  STEP: patching the ServiceAccount @ 07/29/23 16:24:09.805
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 07/29/23 16:24:09.815
  STEP: deleting the ServiceAccount @ 07/29/23 16:24:09.823
  Jul 29 16:24:09.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6078" for this suite. @ 07/29/23 16:24:09.866
• [0.135 seconds]
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 07/29/23 16:24:09.877
  Jul 29 16:24:09.877: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 16:24:09.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:09.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:09.914
  Jul 29 16:24:13.958: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6882" for this suite. @ 07/29/23 16:24:13.969
• [4.103 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 07/29/23 16:24:13.984
  Jul 29 16:24:13.984: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:24:13.986
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:14.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:14.024
  STEP: Creating pod test-grpc-1fc8dbfa-45dd-40ee-b951-f8c7364c4501 in namespace container-probe-1494 @ 07/29/23 16:24:14.027
  Jul 29 16:24:16.053: INFO: Started pod test-grpc-1fc8dbfa-45dd-40ee-b951-f8c7364c4501 in namespace container-probe-1494
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:24:16.053
  Jul 29 16:24:16.058: INFO: Initial restart count of pod test-grpc-1fc8dbfa-45dd-40ee-b951-f8c7364c4501 is 0
  Jul 29 16:25:30.410: INFO: Restart count of pod container-probe-1494/test-grpc-1fc8dbfa-45dd-40ee-b951-f8c7364c4501 is now 1 (1m14.351809394s elapsed)
  Jul 29 16:25:30.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:25:30.421
  STEP: Destroying namespace "container-probe-1494" for this suite. @ 07/29/23 16:25:30.444
• [76.476 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 07/29/23 16:25:30.461
  Jul 29 16:25:30.461: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:25:30.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:25:30.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:25:30.501
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 16:25:30.506
  Jul 29 16:25:30.507: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5554 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 29 16:25:30.663: INFO: stderr: ""
  Jul 29 16:25:30.663: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 07/29/23 16:25:30.664
  Jul 29 16:25:30.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5554 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jul 29 16:25:30.839: INFO: stderr: ""
  Jul 29 16:25:30.839: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 16:25:30.839
  Jul 29 16:25:30.846: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5554 delete pods e2e-test-httpd-pod'
  Jul 29 16:25:32.905: INFO: stderr: ""
  Jul 29 16:25:32.905: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 16:25:32.905: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5554" for this suite. @ 07/29/23 16:25:32.915
• [2.467 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 07/29/23 16:25:32.935
  Jul 29 16:25:32.935: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:25:32.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:25:32.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:25:32.971
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-7726 @ 07/29/23 16:25:32.977
  STEP: changing the ExternalName service to type=NodePort @ 07/29/23 16:25:32.989
  STEP: creating replication controller externalname-service in namespace services-7726 @ 07/29/23 16:25:33.022
  I0729 16:25:33.044379      14 runners.go:194] Created replication controller with name: externalname-service, namespace: services-7726, replica count: 2
  I0729 16:25:36.097770      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:25:36.098: INFO: Creating new exec pod
  Jul 29 16:25:39.140: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-7726 exec execpod5jtk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 29 16:25:39.421: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 29 16:25:39.421: INFO: stdout: ""
  Jul 29 16:25:40.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-7726 exec execpod5jtk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 29 16:25:40.677: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 29 16:25:40.677: INFO: stdout: ""
  Jul 29 16:25:41.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-7726 exec execpod5jtk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 29 16:25:41.684: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 29 16:25:41.684: INFO: stdout: "externalname-service-nlhr5"
  Jul 29 16:25:41.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-7726 exec execpod5jtk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.62.31 80'
  Jul 29 16:25:41.915: INFO: stderr: "+ nc -v -t -w 2 10.233.62.31 80\n+ echo hostName\nConnection to 10.233.62.31 80 port [tcp/http] succeeded!\n"
  Jul 29 16:25:41.915: INFO: stdout: "externalname-service-2wmsr"
  Jul 29 16:25:41.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-7726 exec execpod5jtk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.77 31681'
  Jul 29 16:25:42.159: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.77 31681\nConnection to 192.168.121.77 31681 port [tcp/*] succeeded!\n"
  Jul 29 16:25:42.159: INFO: stdout: "externalname-service-nlhr5"
  Jul 29 16:25:42.159: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-7726 exec execpod5jtk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.129 31681'
  Jul 29 16:25:42.386: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.129 31681\nConnection to 192.168.121.129 31681 port [tcp/*] succeeded!\n"
  Jul 29 16:25:42.386: INFO: stdout: ""
  Jul 29 16:25:43.387: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-7726 exec execpod5jtk8 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.129 31681'
  Jul 29 16:25:43.629: INFO: stderr: "+ nc -v -t -w 2 192.168.121.129 31681\n+ echo hostName\nConnection to 192.168.121.129 31681 port [tcp/*] succeeded!\n"
  Jul 29 16:25:43.629: INFO: stdout: "externalname-service-2wmsr"
  Jul 29 16:25:43.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:25:43.639: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-7726" for this suite. @ 07/29/23 16:25:43.683
• [10.757 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 07/29/23 16:25:43.695
  Jul 29 16:25:43.695: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:25:43.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:25:43.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:25:43.776
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/29/23 16:25:43.783
  STEP: Saw pod success @ 07/29/23 16:25:47.835
  Jul 29 16:25:47.841: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-9b1b58c9-7c0f-410a-a121-1dafcb17d6f7 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:25:47.868
  Jul 29 16:25:47.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3805" for this suite. @ 07/29/23 16:25:47.905
• [4.228 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 07/29/23 16:25:47.926
  Jul 29 16:25:47.926: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename containers @ 07/29/23 16:25:47.929
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:25:47.956
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:25:47.962
  STEP: Creating a pod to test override arguments @ 07/29/23 16:25:47.967
  STEP: Saw pod success @ 07/29/23 16:25:52.006
  Jul 29 16:25:52.010: INFO: Trying to get logs from node vucheipi7kei-3 pod client-containers-a05a6eea-3fab-46f4-b48e-5a0e411f17b0 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:25:52.02
  Jul 29 16:25:52.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-2714" for this suite. @ 07/29/23 16:25:52.057
• [4.142 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 07/29/23 16:25:52.07
  Jul 29 16:25:52.070: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 16:25:52.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:25:52.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:25:52.109
  STEP: apply creating a deployment @ 07/29/23 16:25:52.113
  Jul 29 16:25:52.116: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4397" for this suite. @ 07/29/23 16:25:52.141
• [0.079 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 07/29/23 16:25:52.154
  Jul 29 16:25:52.154: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:25:52.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:25:52.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:25:52.187
  STEP: creating all guestbook components @ 07/29/23 16:25:52.192
  Jul 29 16:25:52.192: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jul 29 16:25:52.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 create -f -'
  Jul 29 16:25:53.423: INFO: stderr: ""
  Jul 29 16:25:53.423: INFO: stdout: "service/agnhost-replica created\n"
  Jul 29 16:25:53.423: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jul 29 16:25:53.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 create -f -'
  Jul 29 16:25:53.988: INFO: stderr: ""
  Jul 29 16:25:53.988: INFO: stdout: "service/agnhost-primary created\n"
  Jul 29 16:25:53.988: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jul 29 16:25:53.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 create -f -'
  Jul 29 16:25:54.494: INFO: stderr: ""
  Jul 29 16:25:54.494: INFO: stdout: "service/frontend created\n"
  Jul 29 16:25:54.495: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jul 29 16:25:54.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 create -f -'
  Jul 29 16:25:54.992: INFO: stderr: ""
  Jul 29 16:25:54.992: INFO: stdout: "deployment.apps/frontend created\n"
  Jul 29 16:25:54.993: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 29 16:25:54.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 create -f -'
  Jul 29 16:25:55.529: INFO: stderr: ""
  Jul 29 16:25:55.530: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jul 29 16:25:55.530: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 29 16:25:55.536: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 create -f -'
  Jul 29 16:25:56.417: INFO: stderr: ""
  Jul 29 16:25:56.417: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 07/29/23 16:25:56.417
  Jul 29 16:25:56.417: INFO: Waiting for all frontend pods to be Running.
  Jul 29 16:26:01.469: INFO: Waiting for frontend to serve content.
  Jul 29 16:26:01.534: INFO: Trying to add a new entry to the guestbook.
  Jul 29 16:26:01.559: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 07/29/23 16:26:01.573
  Jul 29 16:26:01.574: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 delete --grace-period=0 --force -f -'
  Jul 29 16:26:01.758: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:26:01.758: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:26:01.759
  Jul 29 16:26:01.760: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 delete --grace-period=0 --force -f -'
  Jul 29 16:26:01.945: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:26:01.945: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:26:01.945
  Jul 29 16:26:01.945: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 delete --grace-period=0 --force -f -'
  Jul 29 16:26:02.096: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:26:02.097: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:26:02.097
  Jul 29 16:26:02.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 delete --grace-period=0 --force -f -'
  Jul 29 16:26:02.234: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:26:02.235: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:26:02.235
  Jul 29 16:26:02.235: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 delete --grace-period=0 --force -f -'
  Jul 29 16:26:02.417: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:26:02.417: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:26:02.417
  Jul 29 16:26:02.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-729 delete --grace-period=0 --force -f -'
  Jul 29 16:26:02.639: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:26:02.639: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jul 29 16:26:02.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-729" for this suite. @ 07/29/23 16:26:02.649
• [10.521 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 07/29/23 16:26:02.678
  Jul 29 16:26:02.678: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename events @ 07/29/23 16:26:02.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:02.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:02.765
  STEP: Create set of events @ 07/29/23 16:26:02.777
  STEP: get a list of Events with a label in the current namespace @ 07/29/23 16:26:02.812
  STEP: delete a list of events @ 07/29/23 16:26:02.829
  Jul 29 16:26:02.829: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/29/23 16:26:02.939
  Jul 29 16:26:02.947: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6486" for this suite. @ 07/29/23 16:26:02.96
• [0.301 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 07/29/23 16:26:02.982
  Jul 29 16:26:02.982: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:26:02.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:03.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:03.025
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:26:03.044
  STEP: Saw pod success @ 07/29/23 16:26:07.089
  Jul 29 16:26:07.095: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-eca0ea09-961a-4427-a599-ca99cbd9e574 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:26:07.106
  Jul 29 16:26:07.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-571" for this suite. @ 07/29/23 16:26:07.143
• [4.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 07/29/23 16:26:07.16
  Jul 29 16:26:07.160: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 16:26:07.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:07.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:07.197
  STEP: Creating a test namespace @ 07/29/23 16:26:07.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:07.223
  STEP: Creating a service in the namespace @ 07/29/23 16:26:07.227
  STEP: Deleting the namespace @ 07/29/23 16:26:07.251
  STEP: Waiting for the namespace to be removed. @ 07/29/23 16:26:07.267
  STEP: Recreating the namespace @ 07/29/23 16:26:13.277
  STEP: Verifying there is no service in the namespace @ 07/29/23 16:26:13.337
  Jul 29 16:26:13.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-5578" for this suite. @ 07/29/23 16:26:13.358
  STEP: Destroying namespace "nsdeletetest-3504" for this suite. @ 07/29/23 16:26:13.37
  Jul 29 16:26:13.374: INFO: Namespace nsdeletetest-3504 was already deleted
  STEP: Destroying namespace "nsdeletetest-4797" for this suite. @ 07/29/23 16:26:13.375
• [6.228 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 07/29/23 16:26:13.402
  Jul 29 16:26:13.402: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:26:13.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:13.43
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:13.435
  Jul 29 16:26:13.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-2359 create -f -'
  Jul 29 16:26:14.105: INFO: stderr: ""
  Jul 29 16:26:14.105: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jul 29 16:26:14.106: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-2359 create -f -'
  Jul 29 16:26:14.751: INFO: stderr: ""
  Jul 29 16:26:14.751: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 16:26:14.751
  Jul 29 16:26:15.771: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:26:15.771: INFO: Found 1 / 1
  Jul 29 16:26:15.771: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 29 16:26:15.781: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:26:15.781: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 16:26:15.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-2359 describe pod agnhost-primary-drn99'
  Jul 29 16:26:15.948: INFO: stderr: ""
  Jul 29 16:26:15.948: INFO: stdout: "Name:             agnhost-primary-drn99\nNamespace:        kubectl-2359\nPriority:         0\nService Account:  default\nNode:             vucheipi7kei-3/192.168.121.129\nStart Time:       Sat, 29 Jul 2023 16:26:14 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.65.134\nIPs:\n  IP:           10.233.65.134\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://182ec78dd759f91d902894fa81233676ebe986b4b4b1c7652f4d1efe1feb6842\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 29 Jul 2023 16:26:15 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-qhzrc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-qhzrc:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2359/agnhost-primary-drn99 to vucheipi7kei-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    0s    kubelet            Created container agnhost-primary\n  Normal  Started    0s    kubelet            Started container agnhost-primary\n"
  Jul 29 16:26:15.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-2359 describe rc agnhost-primary'
  Jul 29 16:26:16.101: INFO: stderr: ""
  Jul 29 16:26:16.101: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2359\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-drn99\n"
  Jul 29 16:26:16.101: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-2359 describe service agnhost-primary'
  Jul 29 16:26:16.239: INFO: stderr: ""
  Jul 29 16:26:16.239: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2359\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.15.107\nIPs:               10.233.15.107\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.65.134:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jul 29 16:26:16.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-2359 describe node vucheipi7kei-1'
  Jul 29 16:26:16.429: INFO: stderr: ""
  Jul 29 16:26:16.429: INFO: stdout: "Name:               vucheipi7kei-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=vucheipi7kei-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 29 Jul 2023 15:13:57 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  vucheipi7kei-1\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 29 Jul 2023 16:26:12 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 29 Jul 2023 15:24:17 +0000   Sat, 29 Jul 2023 15:24:17 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Sat, 29 Jul 2023 16:25:04 +0000   Sat, 29 Jul 2023 15:13:48 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 29 Jul 2023 16:25:04 +0000   Sat, 29 Jul 2023 15:13:48 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 29 Jul 2023 16:25:04 +0000   Sat, 29 Jul 2023 15:13:48 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 29 Jul 2023 16:25:04 +0000   Sat, 29 Jul 2023 15:15:21 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.88\n  Hostname:    vucheipi7kei-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8127912Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3278248Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 1f5bcf27befe4bba99c2a632912b50e2\n  System UUID:                1f5bcf27-befe-4bba-99c2-a632912b50e2\n  Boot ID:                    852fcb23-f2c5-4309-9eaf-c7f250e74f97\n  Kernel Version:             5.19.0-50-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.1\n  Kubelet Version:            v1.27.4\n  Kube-Proxy Version:         v1.27.4\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (8 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-lk6kf                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         62m\n  kube-system                 cilium-node-init-gs57c                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         62m\n  kube-system                 kube-addon-manager-vucheipi7kei-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         63m\n  kube-system                 kube-apiserver-vucheipi7kei-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         72m\n  kube-system                 kube-controller-manager-vucheipi7kei-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         72m\n  kube-system                 kube-proxy-qkz2r                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         72m\n  kube-system                 kube-scheduler-vucheipi7kei-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         72m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-4kwpr    0 (0%)        0 (0%)      0 (0%)           0 (0%)         61m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                755m (47%)  0 (0%)\n  memory             250Mi (7%)  0 (0%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  Jul 29 16:26:16.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-2359 describe namespace kubectl-2359'
  Jul 29 16:26:16.579: INFO: stderr: ""
  Jul 29 16:26:16.579: INFO: stdout: "Name:         kubectl-2359\nLabels:       e2e-framework=kubectl\n              e2e-run=4165bb08-2af4-4df8-8e08-20c77064136a\n              kubernetes.io/metadata.name=kubectl-2359\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jul 29 16:26:16.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2359" for this suite. @ 07/29/23 16:26:16.589
• [3.197 seconds]
------------------------------
S
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 07/29/23 16:26:16.599
  Jul 29 16:26:16.599: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 16:26:16.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:16.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:16.633
  STEP: Create set of pod templates @ 07/29/23 16:26:16.636
  Jul 29 16:26:16.645: INFO: created test-podtemplate-1
  Jul 29 16:26:16.652: INFO: created test-podtemplate-2
  Jul 29 16:26:16.659: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 07/29/23 16:26:16.659
  STEP: delete collection of pod templates @ 07/29/23 16:26:16.664
  Jul 29 16:26:16.665: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 07/29/23 16:26:16.701
  Jul 29 16:26:16.701: INFO: requesting list of pod templates to confirm quantity
  Jul 29 16:26:16.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8683" for this suite. @ 07/29/23 16:26:16.719
• [0.142 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 07/29/23 16:26:16.742
  Jul 29 16:26:16.742: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:26:16.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:16.774
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:16.779
  STEP: Creating service test in namespace statefulset-328 @ 07/29/23 16:26:16.785
  STEP: Looking for a node to schedule stateful set and pod @ 07/29/23 16:26:16.796
  STEP: Creating pod with conflicting port in namespace statefulset-328 @ 07/29/23 16:26:16.807
  STEP: Waiting until pod test-pod will start running in namespace statefulset-328 @ 07/29/23 16:26:16.822
  STEP: Creating statefulset with conflicting port in namespace statefulset-328 @ 07/29/23 16:26:18.844
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-328 @ 07/29/23 16:26:18.854
  Jul 29 16:26:18.920: INFO: Observed stateful pod in namespace: statefulset-328, name: ss-0, uid: 187a425e-686b-45aa-a053-51a540203d25, status phase: Pending. Waiting for statefulset controller to delete.
  Jul 29 16:26:18.936: INFO: Observed stateful pod in namespace: statefulset-328, name: ss-0, uid: 187a425e-686b-45aa-a053-51a540203d25, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 29 16:26:18.981: INFO: Observed stateful pod in namespace: statefulset-328, name: ss-0, uid: 187a425e-686b-45aa-a053-51a540203d25, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 29 16:26:18.986: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-328
  STEP: Removing pod with conflicting port in namespace statefulset-328 @ 07/29/23 16:26:18.987
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-328 and will be in running state @ 07/29/23 16:26:19.013
  Jul 29 16:26:21.032: INFO: Deleting all statefulset in ns statefulset-328
  Jul 29 16:26:21.038: INFO: Scaling statefulset ss to 0
  Jul 29 16:26:31.068: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:26:31.073: INFO: Deleting statefulset ss
  Jul 29 16:26:31.095: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-328" for this suite. @ 07/29/23 16:26:31.106
• [14.380 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 07/29/23 16:26:31.124
  Jul 29 16:26:31.124: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:26:31.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:31.154
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:31.158
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:26:31.169
  STEP: Saw pod success @ 07/29/23 16:26:35.219
  Jul 29 16:26:35.225: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-e64bc0f0-ef76-41d1-9904-edfc2e2783a5 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:26:35.237
  Jul 29 16:26:35.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7281" for this suite. @ 07/29/23 16:26:35.28
• [4.172 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 07/29/23 16:26:35.3
  Jul 29 16:26:35.300: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename events @ 07/29/23 16:26:35.303
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:35.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:35.351
  STEP: Create set of events @ 07/29/23 16:26:35.357
  Jul 29 16:26:35.367: INFO: created test-event-1
  Jul 29 16:26:35.378: INFO: created test-event-2
  Jul 29 16:26:35.389: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 07/29/23 16:26:35.39
  STEP: delete collection of events @ 07/29/23 16:26:35.398
  Jul 29 16:26:35.398: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/29/23 16:26:35.443
  Jul 29 16:26:35.443: INFO: requesting list of events to confirm quantity
  Jul 29 16:26:35.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7635" for this suite. @ 07/29/23 16:26:35.464
• [0.179 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 07/29/23 16:26:35.484
  Jul 29 16:26:35.484: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:26:35.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:35.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:35.522
  STEP: Creating secret with name secret-test-1b686fb0-cde5-487e-b21d-43d873ae1b81 @ 07/29/23 16:26:35.527
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:26:35.537
  STEP: Saw pod success @ 07/29/23 16:26:39.582
  Jul 29 16:26:39.589: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-secrets-d6875197-cdc2-4918-849a-be819fbc1fa8 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:26:39.605
  Jul 29 16:26:39.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-365" for this suite. @ 07/29/23 16:26:39.641
• [4.189 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 07/29/23 16:26:39.674
  Jul 29 16:26:39.674: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename proxy @ 07/29/23 16:26:39.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:39.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:39.707
  STEP: starting an echo server on multiple ports @ 07/29/23 16:26:39.728
  STEP: creating replication controller proxy-service-jg27f in namespace proxy-8829 @ 07/29/23 16:26:39.728
  I0729 16:26:39.747673      14 runners.go:194] Created replication controller with name: proxy-service-jg27f, namespace: proxy-8829, replica count: 1
  I0729 16:26:40.799240      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 16:26:41.799779      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:42.800512      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:43.801297      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:44.802653      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:45.805096      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:46.806627      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:47.806865      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:48.807852      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:49.808200      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:26:50.809210      14 runners.go:194] proxy-service-jg27f Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:26:50.816: INFO: setup took 11.105439395s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 07/29/23 16:26:50.816
  Jul 29 16:26:50.834: INFO: (0) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 17.334154ms)
  Jul 29 16:26:50.839: INFO: (0) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 22.078868ms)
  Jul 29 16:26:50.843: INFO: (0) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 25.745388ms)
  Jul 29 16:26:50.849: INFO: (0) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 32.569705ms)
  Jul 29 16:26:50.850: INFO: (0) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 32.922622ms)
  Jul 29 16:26:50.853: INFO: (0) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 35.613109ms)
  Jul 29 16:26:50.853: INFO: (0) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 35.99121ms)
  Jul 29 16:26:50.853: INFO: (0) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 36.397323ms)
  Jul 29 16:26:50.853: INFO: (0) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 35.790332ms)
  Jul 29 16:26:50.854: INFO: (0) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 36.393338ms)
  Jul 29 16:26:50.854: INFO: (0) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 36.446372ms)
  Jul 29 16:26:50.854: INFO: (0) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 36.762776ms)
  Jul 29 16:26:50.857: INFO: (0) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 40.26747ms)
  Jul 29 16:26:50.857: INFO: (0) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 40.512107ms)
  Jul 29 16:26:50.858: INFO: (0) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 40.000945ms)
  Jul 29 16:26:50.858: INFO: (0) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 40.651097ms)
  Jul 29 16:26:50.867: INFO: (1) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 8.813499ms)
  Jul 29 16:26:50.870: INFO: (1) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 10.467733ms)
  Jul 29 16:26:50.870: INFO: (1) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 10.882842ms)
  Jul 29 16:26:50.870: INFO: (1) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 11.156939ms)
  Jul 29 16:26:50.871: INFO: (1) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 12.826616ms)
  Jul 29 16:26:50.871: INFO: (1) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 11.992802ms)
  Jul 29 16:26:50.871: INFO: (1) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 12.425193ms)
  Jul 29 16:26:50.874: INFO: (1) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 15.900074ms)
  Jul 29 16:26:50.875: INFO: (1) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 16.182217ms)
  Jul 29 16:26:50.876: INFO: (1) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 17.323993ms)
  Jul 29 16:26:50.876: INFO: (1) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 16.921806ms)
  Jul 29 16:26:50.876: INFO: (1) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 16.963872ms)
  Jul 29 16:26:50.877: INFO: (1) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 18.053108ms)
  Jul 29 16:26:50.878: INFO: (1) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 19.627615ms)
  Jul 29 16:26:50.878: INFO: (1) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 19.263967ms)
  Jul 29 16:26:50.878: INFO: (1) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 19.370994ms)
  Jul 29 16:26:50.892: INFO: (2) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 13.869885ms)
  Jul 29 16:26:50.895: INFO: (2) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 15.692518ms)
  Jul 29 16:26:50.895: INFO: (2) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 16.500323ms)
  Jul 29 16:26:50.896: INFO: (2) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 16.862568ms)
  Jul 29 16:26:50.896: INFO: (2) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 16.818667ms)
  Jul 29 16:26:50.896: INFO: (2) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 16.913652ms)
  Jul 29 16:26:50.896: INFO: (2) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 17.501888ms)
  Jul 29 16:26:50.896: INFO: (2) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 17.25582ms)
  Jul 29 16:26:50.902: INFO: (2) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 22.604672ms)
  Jul 29 16:26:50.902: INFO: (2) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 22.768358ms)
  Jul 29 16:26:50.902: INFO: (2) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 23.746595ms)
  Jul 29 16:26:50.902: INFO: (2) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 22.733833ms)
  Jul 29 16:26:50.902: INFO: (2) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 23.374115ms)
  Jul 29 16:26:50.903: INFO: (2) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 23.923065ms)
  Jul 29 16:26:50.903: INFO: (2) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 24.598142ms)
  Jul 29 16:26:50.904: INFO: (2) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 24.903772ms)
  Jul 29 16:26:50.914: INFO: (3) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 9.458094ms)
  Jul 29 16:26:50.921: INFO: (3) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 15.294198ms)
  Jul 29 16:26:50.921: INFO: (3) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 15.943624ms)
  Jul 29 16:26:50.922: INFO: (3) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 17.048616ms)
  Jul 29 16:26:50.924: INFO: (3) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 19.145717ms)
  Jul 29 16:26:50.924: INFO: (3) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 19.209564ms)
  Jul 29 16:26:50.926: INFO: (3) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 20.30573ms)
  Jul 29 16:26:50.927: INFO: (3) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 21.770748ms)
  Jul 29 16:26:50.927: INFO: (3) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 21.835752ms)
  Jul 29 16:26:50.928: INFO: (3) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 22.137932ms)
  Jul 29 16:26:50.928: INFO: (3) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 22.596885ms)
  Jul 29 16:26:50.928: INFO: (3) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 22.482111ms)
  Jul 29 16:26:50.928: INFO: (3) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 23.138332ms)
  Jul 29 16:26:50.930: INFO: (3) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 24.15522ms)
  Jul 29 16:26:50.930: INFO: (3) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 24.628617ms)
  Jul 29 16:26:50.930: INFO: (3) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 24.43147ms)
  Jul 29 16:26:50.942: INFO: (4) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 11.643859ms)
  Jul 29 16:26:50.943: INFO: (4) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 12.651294ms)
  Jul 29 16:26:50.944: INFO: (4) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 12.419178ms)
  Jul 29 16:26:50.944: INFO: (4) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 13.278283ms)
  Jul 29 16:26:50.947: INFO: (4) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 15.271211ms)
  Jul 29 16:26:50.947: INFO: (4) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 16.208167ms)
  Jul 29 16:26:50.947: INFO: (4) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 15.953759ms)
  Jul 29 16:26:50.947: INFO: (4) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 16.721184ms)
  Jul 29 16:26:50.948: INFO: (4) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 17.389723ms)
  Jul 29 16:26:50.948: INFO: (4) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 16.914571ms)
  Jul 29 16:26:50.953: INFO: (4) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 21.621456ms)
  Jul 29 16:26:50.953: INFO: (4) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 21.847616ms)
  Jul 29 16:26:50.953: INFO: (4) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 22.501727ms)
  Jul 29 16:26:50.953: INFO: (4) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 22.755191ms)
  Jul 29 16:26:50.953: INFO: (4) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 22.665952ms)
  Jul 29 16:26:50.953: INFO: (4) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 22.16859ms)
  Jul 29 16:26:50.966: INFO: (5) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 11.697162ms)
  Jul 29 16:26:50.969: INFO: (5) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 14.557131ms)
  Jul 29 16:26:50.969: INFO: (5) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 14.443274ms)
  Jul 29 16:26:50.973: INFO: (5) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 18.18446ms)
  Jul 29 16:26:50.973: INFO: (5) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 18.428428ms)
  Jul 29 16:26:50.973: INFO: (5) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 18.628654ms)
  Jul 29 16:26:50.973: INFO: (5) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 18.468418ms)
  Jul 29 16:26:50.973: INFO: (5) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 19.14353ms)
  Jul 29 16:26:50.973: INFO: (5) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 18.762143ms)
  Jul 29 16:26:50.974: INFO: (5) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 18.49917ms)
  Jul 29 16:26:50.977: INFO: (5) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 21.939807ms)
  Jul 29 16:26:50.977: INFO: (5) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 21.778107ms)
  Jul 29 16:26:50.977: INFO: (5) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 22.081848ms)
  Jul 29 16:26:50.978: INFO: (5) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 22.901025ms)
  Jul 29 16:26:50.978: INFO: (5) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 23.072229ms)
  Jul 29 16:26:50.981: INFO: (5) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 25.954103ms)
  Jul 29 16:26:50.991: INFO: (6) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 9.343255ms)
  Jul 29 16:26:50.993: INFO: (6) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 11.015971ms)
  Jul 29 16:26:50.993: INFO: (6) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 11.130582ms)
  Jul 29 16:26:50.996: INFO: (6) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 13.314586ms)
  Jul 29 16:26:50.999: INFO: (6) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 16.660721ms)
  Jul 29 16:26:50.999: INFO: (6) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 17.173102ms)
  Jul 29 16:26:51.000: INFO: (6) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 17.302139ms)
  Jul 29 16:26:51.001: INFO: (6) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 18.881753ms)
  Jul 29 16:26:51.001: INFO: (6) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 19.000687ms)
  Jul 29 16:26:51.001: INFO: (6) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 19.294471ms)
  Jul 29 16:26:51.001: INFO: (6) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 19.429296ms)
  Jul 29 16:26:51.001: INFO: (6) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 18.913052ms)
  Jul 29 16:26:51.001: INFO: (6) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 19.17003ms)
  Jul 29 16:26:51.002: INFO: (6) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 19.256758ms)
  Jul 29 16:26:51.002: INFO: (6) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 20.248654ms)
  Jul 29 16:26:51.002: INFO: (6) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 20.482337ms)
  Jul 29 16:26:51.022: INFO: (7) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 15.804892ms)
  Jul 29 16:26:51.022: INFO: (7) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 15.821536ms)
  Jul 29 16:26:51.023: INFO: (7) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 16.457752ms)
  Jul 29 16:26:51.023: INFO: (7) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 16.600505ms)
  Jul 29 16:26:51.024: INFO: (7) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 17.314182ms)
  Jul 29 16:26:51.025: INFO: (7) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 18.051475ms)
  Jul 29 16:26:51.025: INFO: (7) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 18.898268ms)
  Jul 29 16:26:51.026: INFO: (7) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 19.080361ms)
  Jul 29 16:26:51.028: INFO: (7) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 20.833412ms)
  Jul 29 16:26:51.029: INFO: (7) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 21.934621ms)
  Jul 29 16:26:51.030: INFO: (7) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 23.415421ms)
  Jul 29 16:26:51.031: INFO: (7) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 23.948519ms)
  Jul 29 16:26:51.031: INFO: (7) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 23.662132ms)
  Jul 29 16:26:51.031: INFO: (7) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 23.916129ms)
  Jul 29 16:26:51.031: INFO: (7) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 24.452022ms)
  Jul 29 16:26:51.032: INFO: (7) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 25.051957ms)
  Jul 29 16:26:51.038: INFO: (8) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 6.739703ms)
  Jul 29 16:26:51.045: INFO: (8) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 12.20231ms)
  Jul 29 16:26:51.046: INFO: (8) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 13.775802ms)
  Jul 29 16:26:51.047: INFO: (8) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 14.3921ms)
  Jul 29 16:26:51.047: INFO: (8) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 15.135778ms)
  Jul 29 16:26:51.047: INFO: (8) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 14.928174ms)
  Jul 29 16:26:51.049: INFO: (8) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 15.346909ms)
  Jul 29 16:26:51.049: INFO: (8) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 16.066273ms)
  Jul 29 16:26:51.050: INFO: (8) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 16.012034ms)
  Jul 29 16:26:51.050: INFO: (8) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 16.218882ms)
  Jul 29 16:26:51.050: INFO: (8) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 18.224572ms)
  Jul 29 16:26:51.052: INFO: (8) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 19.568581ms)
  Jul 29 16:26:51.055: INFO: (8) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 22.66831ms)
  Jul 29 16:26:51.056: INFO: (8) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 21.940393ms)
  Jul 29 16:26:51.057: INFO: (8) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 23.225938ms)
  Jul 29 16:26:51.060: INFO: (8) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 26.034764ms)
  Jul 29 16:26:51.068: INFO: (9) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 7.762618ms)
  Jul 29 16:26:51.069: INFO: (9) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 9.179128ms)
  Jul 29 16:26:51.069: INFO: (9) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 9.111427ms)
  Jul 29 16:26:51.070: INFO: (9) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 9.721052ms)
  Jul 29 16:26:51.072: INFO: (9) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 11.592174ms)
  Jul 29 16:26:51.077: INFO: (9) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 16.25393ms)
  Jul 29 16:26:51.079: INFO: (9) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 17.942421ms)
  Jul 29 16:26:51.077: INFO: (9) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 16.701194ms)
  Jul 29 16:26:51.077: INFO: (9) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 17.425422ms)
  Jul 29 16:26:51.079: INFO: (9) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 18.951007ms)
  Jul 29 16:26:51.079: INFO: (9) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 19.076934ms)
  Jul 29 16:26:51.079: INFO: (9) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 18.795085ms)
  Jul 29 16:26:51.080: INFO: (9) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 19.447738ms)
  Jul 29 16:26:51.081: INFO: (9) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 19.95176ms)
  Jul 29 16:26:51.084: INFO: (9) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 23.69756ms)
  Jul 29 16:26:51.084: INFO: (9) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 23.88428ms)
  Jul 29 16:26:51.098: INFO: (10) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 13.280692ms)
  Jul 29 16:26:51.100: INFO: (10) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 14.948118ms)
  Jul 29 16:26:51.100: INFO: (10) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 15.236244ms)
  Jul 29 16:26:51.100: INFO: (10) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 15.302571ms)
  Jul 29 16:26:51.101: INFO: (10) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 15.926268ms)
  Jul 29 16:26:51.101: INFO: (10) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 16.679826ms)
  Jul 29 16:26:51.101: INFO: (10) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 16.394943ms)
  Jul 29 16:26:51.101: INFO: (10) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 16.727283ms)
  Jul 29 16:26:51.101: INFO: (10) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 16.537933ms)
  Jul 29 16:26:51.107: INFO: (10) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 22.402851ms)
  Jul 29 16:26:51.108: INFO: (10) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 22.617365ms)
  Jul 29 16:26:51.109: INFO: (10) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 23.745991ms)
  Jul 29 16:26:51.109: INFO: (10) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 24.54381ms)
  Jul 29 16:26:51.110: INFO: (10) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 25.068903ms)
  Jul 29 16:26:51.110: INFO: (10) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 25.325082ms)
  Jul 29 16:26:51.112: INFO: (10) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 26.521054ms)
  Jul 29 16:26:51.123: INFO: (11) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 10.940144ms)
  Jul 29 16:26:51.124: INFO: (11) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 12.457778ms)
  Jul 29 16:26:51.125: INFO: (11) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 12.856262ms)
  Jul 29 16:26:51.125: INFO: (11) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 12.671395ms)
  Jul 29 16:26:51.126: INFO: (11) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 13.996907ms)
  Jul 29 16:26:51.128: INFO: (11) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 16.079882ms)
  Jul 29 16:26:51.129: INFO: (11) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 16.551255ms)
  Jul 29 16:26:51.130: INFO: (11) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 17.761032ms)
  Jul 29 16:26:51.132: INFO: (11) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 19.655388ms)
  Jul 29 16:26:51.134: INFO: (11) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 21.512836ms)
  Jul 29 16:26:51.136: INFO: (11) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 23.049219ms)
  Jul 29 16:26:51.136: INFO: (11) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 23.0859ms)
  Jul 29 16:26:51.136: INFO: (11) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 23.45666ms)
  Jul 29 16:26:51.137: INFO: (11) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 24.222756ms)
  Jul 29 16:26:51.137: INFO: (11) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 25.158336ms)
  Jul 29 16:26:51.138: INFO: (11) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 25.946627ms)
  Jul 29 16:26:51.149: INFO: (12) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 10.747088ms)
  Jul 29 16:26:51.154: INFO: (12) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 15.373248ms)
  Jul 29 16:26:51.155: INFO: (12) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 16.15931ms)
  Jul 29 16:26:51.155: INFO: (12) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 16.158673ms)
  Jul 29 16:26:51.157: INFO: (12) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 18.5149ms)
  Jul 29 16:26:51.158: INFO: (12) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 19.573611ms)
  Jul 29 16:26:51.158: INFO: (12) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 19.923804ms)
  Jul 29 16:26:51.158: INFO: (12) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 19.858357ms)
  Jul 29 16:26:51.158: INFO: (12) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 19.930624ms)
  Jul 29 16:26:51.159: INFO: (12) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 20.212892ms)
  Jul 29 16:26:51.159: INFO: (12) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 19.980321ms)
  Jul 29 16:26:51.159: INFO: (12) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 20.913097ms)
  Jul 29 16:26:51.159: INFO: (12) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 20.745614ms)
  Jul 29 16:26:51.159: INFO: (12) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 21.431856ms)
  Jul 29 16:26:51.164: INFO: (12) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 25.30312ms)
  Jul 29 16:26:51.164: INFO: (12) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 25.477846ms)
  Jul 29 16:26:51.173: INFO: (13) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 8.440436ms)
  Jul 29 16:26:51.175: INFO: (13) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 9.921854ms)
  Jul 29 16:26:51.178: INFO: (13) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 13.163128ms)
  Jul 29 16:26:51.178: INFO: (13) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 12.973728ms)
  Jul 29 16:26:51.179: INFO: (13) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 14.247344ms)
  Jul 29 16:26:51.180: INFO: (13) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 14.868992ms)
  Jul 29 16:26:51.181: INFO: (13) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 16.268619ms)
  Jul 29 16:26:51.182: INFO: (13) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 16.472019ms)
  Jul 29 16:26:51.183: INFO: (13) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 17.777436ms)
  Jul 29 16:26:51.183: INFO: (13) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 17.462129ms)
  Jul 29 16:26:51.183: INFO: (13) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 17.578378ms)
  Jul 29 16:26:51.183: INFO: (13) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 17.467125ms)
  Jul 29 16:26:51.184: INFO: (13) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 18.986141ms)
  Jul 29 16:26:51.185: INFO: (13) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 19.420556ms)
  Jul 29 16:26:51.185: INFO: (13) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 19.944123ms)
  Jul 29 16:26:51.187: INFO: (13) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 21.891452ms)
  Jul 29 16:26:51.197: INFO: (14) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 9.082583ms)
  Jul 29 16:26:51.197: INFO: (14) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 9.453811ms)
  Jul 29 16:26:51.199: INFO: (14) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 11.34703ms)
  Jul 29 16:26:51.202: INFO: (14) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 13.347092ms)
  Jul 29 16:26:51.205: INFO: (14) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 17.404517ms)
  Jul 29 16:26:51.205: INFO: (14) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 17.368252ms)
  Jul 29 16:26:51.205: INFO: (14) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 17.781204ms)
  Jul 29 16:26:51.206: INFO: (14) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 18.371453ms)
  Jul 29 16:26:51.207: INFO: (14) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 19.152356ms)
  Jul 29 16:26:51.209: INFO: (14) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 20.796843ms)
  Jul 29 16:26:51.211: INFO: (14) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 22.898832ms)
  Jul 29 16:26:51.213: INFO: (14) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 24.296517ms)
  Jul 29 16:26:51.214: INFO: (14) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 25.655335ms)
  Jul 29 16:26:51.215: INFO: (14) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 26.39898ms)
  Jul 29 16:26:51.215: INFO: (14) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 26.405876ms)
  Jul 29 16:26:51.216: INFO: (14) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 27.27303ms)
  Jul 29 16:26:51.227: INFO: (15) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 11.438024ms)
  Jul 29 16:26:51.228: INFO: (15) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 12.363661ms)
  Jul 29 16:26:51.230: INFO: (15) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 13.908404ms)
  Jul 29 16:26:51.233: INFO: (15) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 16.9209ms)
  Jul 29 16:26:51.233: INFO: (15) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 16.638285ms)
  Jul 29 16:26:51.233: INFO: (15) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 16.951577ms)
  Jul 29 16:26:51.234: INFO: (15) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 17.881464ms)
  Jul 29 16:26:51.240: INFO: (15) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 23.754731ms)
  Jul 29 16:26:51.243: INFO: (15) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 27.046719ms)
  Jul 29 16:26:51.244: INFO: (15) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 27.286663ms)
  Jul 29 16:26:51.245: INFO: (15) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 28.486529ms)
  Jul 29 16:26:51.245: INFO: (15) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 28.15977ms)
  Jul 29 16:26:51.245: INFO: (15) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 28.730068ms)
  Jul 29 16:26:51.245: INFO: (15) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 28.556231ms)
  Jul 29 16:26:51.245: INFO: (15) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 28.511233ms)
  Jul 29 16:26:51.247: INFO: (15) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 30.248077ms)
  Jul 29 16:26:51.260: INFO: (16) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 13.698754ms)
  Jul 29 16:26:51.263: INFO: (16) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 16.4172ms)
  Jul 29 16:26:51.264: INFO: (16) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 17.076833ms)
  Jul 29 16:26:51.264: INFO: (16) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 17.01845ms)
  Jul 29 16:26:51.265: INFO: (16) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 17.561438ms)
  Jul 29 16:26:51.265: INFO: (16) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 17.794842ms)
  Jul 29 16:26:51.265: INFO: (16) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 18.25482ms)
  Jul 29 16:26:51.265: INFO: (16) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 18.041489ms)
  Jul 29 16:26:51.266: INFO: (16) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 18.87558ms)
  Jul 29 16:26:51.268: INFO: (16) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 21.119346ms)
  Jul 29 16:26:51.268: INFO: (16) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 21.029936ms)
  Jul 29 16:26:51.270: INFO: (16) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 22.499443ms)
  Jul 29 16:26:51.270: INFO: (16) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 22.391699ms)
  Jul 29 16:26:51.271: INFO: (16) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 23.570895ms)
  Jul 29 16:26:51.271: INFO: (16) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 23.717289ms)
  Jul 29 16:26:51.271: INFO: (16) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 23.662995ms)
  Jul 29 16:26:51.284: INFO: (17) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 11.855929ms)
  Jul 29 16:26:51.288: INFO: (17) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 15.714235ms)
  Jul 29 16:26:51.290: INFO: (17) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 17.759305ms)
  Jul 29 16:26:51.292: INFO: (17) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 19.308995ms)
  Jul 29 16:26:51.292: INFO: (17) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 19.795044ms)
  Jul 29 16:26:51.292: INFO: (17) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 19.461788ms)
  Jul 29 16:26:51.292: INFO: (17) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 20.225658ms)
  Jul 29 16:26:51.292: INFO: (17) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 20.021357ms)
  Jul 29 16:26:51.299: INFO: (17) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 26.553209ms)
  Jul 29 16:26:51.300: INFO: (17) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 27.538041ms)
  Jul 29 16:26:51.300: INFO: (17) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 27.415735ms)
  Jul 29 16:26:51.301: INFO: (17) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 28.153323ms)
  Jul 29 16:26:51.301: INFO: (17) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 28.781886ms)
  Jul 29 16:26:51.301: INFO: (17) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 27.994171ms)
  Jul 29 16:26:51.302: INFO: (17) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 29.129062ms)
  Jul 29 16:26:51.303: INFO: (17) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 30.087335ms)
  Jul 29 16:26:51.311: INFO: (18) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 7.605894ms)
  Jul 29 16:26:51.313: INFO: (18) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 9.709516ms)
  Jul 29 16:26:51.313: INFO: (18) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 9.721403ms)
  Jul 29 16:26:51.314: INFO: (18) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 9.980681ms)
  Jul 29 16:26:51.320: INFO: (18) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 16.678701ms)
  Jul 29 16:26:51.320: INFO: (18) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 16.326269ms)
  Jul 29 16:26:51.327: INFO: (18) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 23.161429ms)
  Jul 29 16:26:51.327: INFO: (18) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 23.664119ms)
  Jul 29 16:26:51.327: INFO: (18) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 23.739675ms)
  Jul 29 16:26:51.328: INFO: (18) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 23.704083ms)
  Jul 29 16:26:51.328: INFO: (18) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 24.987946ms)
  Jul 29 16:26:51.329: INFO: (18) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 25.313219ms)
  Jul 29 16:26:51.330: INFO: (18) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 26.131855ms)
  Jul 29 16:26:51.330: INFO: (18) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 26.050889ms)
  Jul 29 16:26:51.330: INFO: (18) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 26.165461ms)
  Jul 29 16:26:51.330: INFO: (18) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 26.386198ms)
  Jul 29 16:26:51.341: INFO: (19) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 10.358546ms)
  Jul 29 16:26:51.344: INFO: (19) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">... (200; 13.128299ms)
  Jul 29 16:26:51.344: INFO: (19) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:1080/proxy/rewriteme">test<... (200; 13.024592ms)
  Jul 29 16:26:51.344: INFO: (19) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname2/proxy/: bar (200; 14.176971ms)
  Jul 29 16:26:51.354: INFO: (19) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:443/proxy/tlsrewritem... (200; 23.258414ms)
  Jul 29 16:26:51.354: INFO: (19) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:162/proxy/: bar (200; 23.237431ms)
  Jul 29 16:26:51.354: INFO: (19) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/: <a href="/api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j/proxy/rewriteme">test</a> (200; 22.954315ms)
  Jul 29 16:26:51.354: INFO: (19) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname2/proxy/: tls qux (200; 23.527073ms)
  Jul 29 16:26:51.364: INFO: (19) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname1/proxy/: foo (200; 32.893068ms)
  Jul 29 16:26:51.364: INFO: (19) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:462/proxy/: tls qux (200; 32.473248ms)
  Jul 29 16:26:51.364: INFO: (19) /api/v1/namespaces/proxy-8829/pods/https:proxy-service-jg27f-5vd7j:460/proxy/: tls baz (200; 32.648512ms)
  Jul 29 16:26:51.364: INFO: (19) /api/v1/namespaces/proxy-8829/services/http:proxy-service-jg27f:portname2/proxy/: bar (200; 33.455839ms)
  Jul 29 16:26:51.364: INFO: (19) /api/v1/namespaces/proxy-8829/services/proxy-service-jg27f:portname1/proxy/: foo (200; 33.05745ms)
  Jul 29 16:26:51.366: INFO: (19) /api/v1/namespaces/proxy-8829/pods/http:proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 35.012934ms)
  Jul 29 16:26:51.366: INFO: (19) /api/v1/namespaces/proxy-8829/pods/proxy-service-jg27f-5vd7j:160/proxy/: foo (200; 35.754038ms)
  Jul 29 16:26:51.367: INFO: (19) /api/v1/namespaces/proxy-8829/services/https:proxy-service-jg27f:tlsportname1/proxy/: tls baz (200; 36.611095ms)
  Jul 29 16:26:51.368: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-jg27f in namespace proxy-8829, will wait for the garbage collector to delete the pods @ 07/29/23 16:26:51.383
  Jul 29 16:26:51.473: INFO: Deleting ReplicationController proxy-service-jg27f took: 16.085016ms
  Jul 29 16:26:51.573: INFO: Terminating ReplicationController proxy-service-jg27f pods took: 100.227883ms
  STEP: Destroying namespace "proxy-8829" for this suite. @ 07/29/23 16:26:52.476
• [12.815 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 07/29/23 16:26:52.493
  Jul 29 16:26:52.493: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:26:52.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:52.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:52.526
  STEP: creating a collection of services @ 07/29/23 16:26:52.531
  Jul 29 16:26:52.531: INFO: Creating e2e-svc-a-rp65w
  Jul 29 16:26:52.551: INFO: Creating e2e-svc-b-27zqb
  Jul 29 16:26:52.580: INFO: Creating e2e-svc-c-pdggj
  STEP: deleting service collection @ 07/29/23 16:26:52.606
  Jul 29 16:26:52.662: INFO: Collection of services has been deleted
  Jul 29 16:26:52.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4981" for this suite. @ 07/29/23 16:26:52.674
• [0.198 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 07/29/23 16:26:52.692
  Jul 29 16:26:52.692: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 16:26:52.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:52.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:52.725
  STEP: Creating a cronjob @ 07/29/23 16:26:52.729
  STEP: Ensuring more than one job is running at a time @ 07/29/23 16:26:52.738
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 07/29/23 16:28:00.756
  STEP: Removing cronjob @ 07/29/23 16:28:00.763
  Jul 29 16:28:00.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9995" for this suite. @ 07/29/23 16:28:00.788
• [68.114 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 07/29/23 16:28:00.825
  Jul 29 16:28:00.825: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:28:00.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:28:00.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:28:00.958
  STEP: Create a pod @ 07/29/23 16:28:00.967
  STEP: patching /status @ 07/29/23 16:28:03.003
  Jul 29 16:28:03.018: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jul 29 16:28:03.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3594" for this suite. @ 07/29/23 16:28:03.033
• [2.219 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 07/29/23 16:28:03.072
  Jul 29 16:28:03.073: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 16:28:03.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:28:03.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:28:03.116
  STEP: Creating a cronjob @ 07/29/23 16:28:03.122
  STEP: creating @ 07/29/23 16:28:03.123
  STEP: getting @ 07/29/23 16:28:03.134
  STEP: listing @ 07/29/23 16:28:03.139
  STEP: watching @ 07/29/23 16:28:03.145
  Jul 29 16:28:03.145: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 16:28:03.146
  STEP: cluster-wide watching @ 07/29/23 16:28:03.152
  Jul 29 16:28:03.152: INFO: starting watch
  STEP: patching @ 07/29/23 16:28:03.153
  STEP: updating @ 07/29/23 16:28:03.165
  Jul 29 16:28:03.177: INFO: waiting for watch events with expected annotations
  Jul 29 16:28:03.178: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/29/23 16:28:03.178
  STEP: updating /status @ 07/29/23 16:28:03.193
  STEP: get /status @ 07/29/23 16:28:03.207
  STEP: deleting @ 07/29/23 16:28:03.213
  STEP: deleting a collection @ 07/29/23 16:28:03.238
  Jul 29 16:28:03.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-700" for this suite. @ 07/29/23 16:28:03.266
• [0.204 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 07/29/23 16:28:03.284
  Jul 29 16:28:03.284: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:28:03.286
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:28:03.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:28:03.319
  STEP: Creating pod liveness-0e4c555c-96f3-41b3-9da2-b6ef1f674bd0 in namespace container-probe-9112 @ 07/29/23 16:28:03.325
  Jul 29 16:28:05.360: INFO: Started pod liveness-0e4c555c-96f3-41b3-9da2-b6ef1f674bd0 in namespace container-probe-9112
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:28:05.36
  Jul 29 16:28:05.366: INFO: Initial restart count of pod liveness-0e4c555c-96f3-41b3-9da2-b6ef1f674bd0 is 0
  Jul 29 16:28:25.463: INFO: Restart count of pod container-probe-9112/liveness-0e4c555c-96f3-41b3-9da2-b6ef1f674bd0 is now 1 (20.096545955s elapsed)
  Jul 29 16:28:25.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:28:25.472
  STEP: Destroying namespace "container-probe-9112" for this suite. @ 07/29/23 16:28:25.545
• [22.276 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 07/29/23 16:28:25.561
  Jul 29 16:28:25.561: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-watch @ 07/29/23 16:28:25.564
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:28:25.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:28:25.607
  Jul 29 16:28:25.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Creating first CR  @ 07/29/23 16:28:28.272
  Jul 29 16:28:28.281: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:28:28Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:28:28Z]] name:name1 resourceVersion:23340 uid:f05a9ecd-3aeb-4851-ab61-25cadccca427] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 07/29/23 16:28:38.285
  Jul 29 16:28:38.301: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:28:38Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:28:38Z]] name:name2 resourceVersion:23381 uid:b49ed7b8-6160-4f3b-8739-5f6af7cb0860] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 07/29/23 16:28:48.302
  Jul 29 16:28:48.314: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:28:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:28:48Z]] name:name1 resourceVersion:23406 uid:f05a9ecd-3aeb-4851-ab61-25cadccca427] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 07/29/23 16:28:58.317
  Jul 29 16:28:58.333: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:28:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:28:58Z]] name:name2 resourceVersion:23430 uid:b49ed7b8-6160-4f3b-8739-5f6af7cb0860] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 07/29/23 16:29:08.334
  Jul 29 16:29:08.355: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:28:28Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:28:48Z]] name:name1 resourceVersion:23453 uid:f05a9ecd-3aeb-4851-ab61-25cadccca427] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 07/29/23 16:29:18.358
  Jul 29 16:29:18.375: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:28:38Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:28:58Z]] name:name2 resourceVersion:23476 uid:b49ed7b8-6160-4f3b-8739-5f6af7cb0860] num:map[num1:9223372036854775807 num2:1000000]]}
  Jul 29 16:29:28.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-7058" for this suite. @ 07/29/23 16:29:28.911
• [63.359 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 07/29/23 16:29:28.922
  Jul 29 16:29:28.922: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 16:29:28.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:28.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:28.952
  Jul 29 16:29:29.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3204" for this suite. @ 07/29/23 16:29:29.01
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 07/29/23 16:29:29.02
  Jul 29 16:29:29.020: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 16:29:29.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:29.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:29.057
  STEP: creating a ReplicationController @ 07/29/23 16:29:29.069
  STEP: waiting for RC to be added @ 07/29/23 16:29:29.079
  STEP: waiting for available Replicas @ 07/29/23 16:29:29.079
  STEP: patching ReplicationController @ 07/29/23 16:29:30.987
  STEP: waiting for RC to be modified @ 07/29/23 16:29:31.004
  STEP: patching ReplicationController status @ 07/29/23 16:29:31.004
  STEP: waiting for RC to be modified @ 07/29/23 16:29:31.015
  STEP: waiting for available Replicas @ 07/29/23 16:29:31.016
  STEP: fetching ReplicationController status @ 07/29/23 16:29:31.023
  STEP: patching ReplicationController scale @ 07/29/23 16:29:31.028
  STEP: waiting for RC to be modified @ 07/29/23 16:29:31.039
  STEP: waiting for ReplicationController's scale to be the max amount @ 07/29/23 16:29:31.039
  STEP: fetching ReplicationController; ensuring that it's patched @ 07/29/23 16:29:32.937
  STEP: updating ReplicationController status @ 07/29/23 16:29:32.944
  STEP: waiting for RC to be modified @ 07/29/23 16:29:32.96
  STEP: listing all ReplicationControllers @ 07/29/23 16:29:32.961
  STEP: checking that ReplicationController has expected values @ 07/29/23 16:29:32.969
  STEP: deleting ReplicationControllers by collection @ 07/29/23 16:29:32.969
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 07/29/23 16:29:32.991
  Jul 29 16:29:33.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 16:29:33.074409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-3498" for this suite. @ 07/29/23 16:29:33.082
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 07/29/23 16:29:33.104
  Jul 29 16:29:33.104: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:29:33.107
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:33.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:33.139
  STEP: Creating service test in namespace statefulset-4930 @ 07/29/23 16:29:33.144
  Jul 29 16:29:33.173: INFO: Found 0 stateful pods, waiting for 1
  E0729 16:29:34.076563      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:35.077378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:36.077513      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:37.078156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:38.078612      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:39.078938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:40.079657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:41.079859      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:42.080284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:43.081611      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:29:43.182: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 07/29/23 16:29:43.197
  W0729 16:29:43.211897      14 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 16:29:43.230: INFO: Found 1 stateful pods, waiting for 2
  E0729 16:29:44.081721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:45.082934      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:46.083150      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:47.083262      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:48.083409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:49.083784      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:50.084751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:51.085287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:52.085318      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:53.086175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:29:53.242: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:29:53.242: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 07/29/23 16:29:53.264
  STEP: Delete all of the StatefulSets @ 07/29/23 16:29:53.273
  STEP: Verify that StatefulSets have been deleted @ 07/29/23 16:29:53.296
  Jul 29 16:29:53.308: INFO: Deleting all statefulset in ns statefulset-4930
  Jul 29 16:29:53.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4930" for this suite. @ 07/29/23 16:29:53.381
• [20.295 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 07/29/23 16:29:53.401
  Jul 29 16:29:53.402: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:29:53.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:53.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:53.501
  STEP: Creating the pod @ 07/29/23 16:29:53.517
  E0729 16:29:54.126725      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:55.087539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:56.088741      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:29:56.281: INFO: Successfully updated pod "labelsupdate718354a6-057f-4e3c-8797-3dc8ef0167b8"
  E0729 16:29:57.088871      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:29:58.091081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:29:58.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4979" for this suite. @ 07/29/23 16:29:58.355
• [4.968 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 07/29/23 16:29:58.373
  Jul 29 16:29:58.373: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:29:58.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:58.407
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:58.414
  STEP: Starting the proxy @ 07/29/23 16:29:58.42
  Jul 29 16:29:58.422: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-1227 proxy --unix-socket=/tmp/kubectl-proxy-unix2092631677/test'
  STEP: retrieving proxy /api/ output @ 07/29/23 16:29:58.57
  Jul 29 16:29:58.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1227" for this suite. @ 07/29/23 16:29:58.583
• [0.228 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 07/29/23 16:29:58.602
  Jul 29 16:29:58.602: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:29:58.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:58.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:58.647
  STEP: Creating a pod to test downward api env vars @ 07/29/23 16:29:58.654
  E0729 16:29:59.091017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:00.091598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:01.092400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:02.092709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:30:02.696
  Jul 29 16:30:02.703: INFO: Trying to get logs from node vucheipi7kei-3 pod downward-api-8e549b22-f053-432a-b5e4-93e33a251f0d container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:30:02.72
  Jul 29 16:30:02.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6612" for this suite. @ 07/29/23 16:30:02.766
• [4.179 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 07/29/23 16:30:02.785
  Jul 29 16:30:02.785: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:30:02.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:02.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:02.836
  STEP: creating the pod @ 07/29/23 16:30:02.84
  Jul 29 16:30:02.841: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-7277 create -f -'
  E0729 16:30:03.093397      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:04.093552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:04.796: INFO: stderr: ""
  Jul 29 16:30:04.796: INFO: stdout: "pod/pause created\n"
  E0729 16:30:05.093675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:06.093975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 07/29/23 16:30:06.816
  Jul 29 16:30:06.816: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-7277 label pods pause testing-label=testing-label-value'
  Jul 29 16:30:06.974: INFO: stderr: ""
  Jul 29 16:30:06.974: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 07/29/23 16:30:06.974
  Jul 29 16:30:06.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-7277 get pod pause -L testing-label'
  E0729 16:30:07.094125      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:07.114: INFO: stderr: ""
  Jul 29 16:30:07.114: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 07/29/23 16:30:07.114
  Jul 29 16:30:07.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-7277 label pods pause testing-label-'
  Jul 29 16:30:07.258: INFO: stderr: ""
  Jul 29 16:30:07.258: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 07/29/23 16:30:07.258
  Jul 29 16:30:07.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-7277 get pod pause -L testing-label'
  Jul 29 16:30:07.405: INFO: stderr: ""
  Jul 29 16:30:07.405: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 07/29/23 16:30:07.405
  Jul 29 16:30:07.405: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-7277 delete --grace-period=0 --force -f -'
  Jul 29 16:30:07.575: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:30:07.575: INFO: stdout: "pod \"pause\" force deleted\n"
  Jul 29 16:30:07.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-7277 get rc,svc -l name=pause --no-headers'
  Jul 29 16:30:07.790: INFO: stderr: "No resources found in kubectl-7277 namespace.\n"
  Jul 29 16:30:07.790: INFO: stdout: ""
  Jul 29 16:30:07.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-7277 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 16:30:07.956: INFO: stderr: ""
  Jul 29 16:30:07.956: INFO: stdout: ""
  Jul 29 16:30:07.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7277" for this suite. @ 07/29/23 16:30:07.97
• [5.196 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 07/29/23 16:30:07.981
  Jul 29 16:30:07.981: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:30:07.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:08.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:08.015
  STEP: Creating secret with name secret-test-41c303a1-96fe-4fd5-965a-cb9d2e163b75 @ 07/29/23 16:30:08.022
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:30:08.034
  E0729 16:30:08.094319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:09.094813      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:10.094695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:11.095217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:30:12.069
  Jul 29 16:30:12.074: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-secrets-2c6c0865-08a6-474d-beb2-a044a46d5ab8 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:30:12.085
  E0729 16:30:12.095946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:12.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9524" for this suite. @ 07/29/23 16:30:12.125
• [4.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 07/29/23 16:30:12.139
  Jul 29 16:30:12.139: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 16:30:12.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:12.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:12.18
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 07/29/23 16:30:12.185
  Jul 29 16:30:12.187: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 16:30:13.096200      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:14.096807      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:14.190: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 16:30:15.097892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:16.098871      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:17.099794      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:18.100277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:19.100395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:20.101160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:21.101192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:22.102328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:23.102245      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:23.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8711" for this suite. @ 07/29/23 16:30:23.379
• [11.255 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 07/29/23 16:30:23.402
  Jul 29 16:30:23.403: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:30:23.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:23.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:23.439
  STEP: Creating the pod @ 07/29/23 16:30:23.444
  E0729 16:30:24.102460      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:25.103103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:26.007: INFO: Successfully updated pod "annotationupdateef6fe925-3bf5-45cd-8225-fa888fbeac00"
  E0729 16:30:26.104081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:27.104394      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:28.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5968" for this suite. @ 07/29/23 16:30:28.046
• [4.657 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 07/29/23 16:30:28.064
  Jul 29 16:30:28.064: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:30:28.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:28.102
  E0729 16:30:28.104121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:28.108
  Jul 29 16:30:28.112: INFO: Creating deployment "test-recreate-deployment"
  Jul 29 16:30:28.121: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jul 29 16:30:28.133: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  E0729 16:30:29.104632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:30.104812      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:30.148: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jul 29 16:30:30.154: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jul 29 16:30:30.168: INFO: Updating deployment test-recreate-deployment
  Jul 29 16:30:30.168: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jul 29 16:30:30.340: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2468  12d39332-a0a4-4fa7-a33d-ada211d76b96 24056 2 2023-07-29 16:30:28 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 16:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f56808 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-29 16:30:30 +0000 UTC,LastTransitionTime:2023-07-29 16:30:30 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-07-29 16:30:30 +0000 UTC,LastTransitionTime:2023-07-29 16:30:28 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 29 16:30:30.346: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-2468  24574fc6-4126-4934-9718-d226e987e780 24054 1 2023-07-29 16:30:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 12d39332-a0a4-4fa7-a33d-ada211d76b96 0xc004f56c07 0xc004f56c08}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12d39332-a0a4-4fa7-a33d-ada211d76b96\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:30:30 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f57268 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:30:30.346: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jul 29 16:30:30.346: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-2468  a76b1bad-8493-4681-83d7-cc889f248eec 24044 2 2023-07-29 16:30:28 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 12d39332-a0a4-4fa7-a33d-ada211d76b96 0xc004f573a7 0xc004f573a8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"12d39332-a0a4-4fa7-a33d-ada211d76b96\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:30:30 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004f57468 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:30:30.352: INFO: Pod "test-recreate-deployment-54757ffd6c-xx48r" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-xx48r test-recreate-deployment-54757ffd6c- deployment-2468  b3822abd-e10a-4124-ad92-af5d1bfd0499 24055 0 2023-07-29 16:30:30 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 24574fc6-4126-4934-9718-d226e987e780 0xc00358e007 0xc00358e008}] [] [{kube-controller-manager Update v1 2023-07-29 16:30:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24574fc6-4126-4934-9718-d226e987e780\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:30:30 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lznb7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lznb7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:30 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:30:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:,StartTime:2023-07-29 16:30:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 16:30:30.353: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2468" for this suite. @ 07/29/23 16:30:30.36
• [2.305 seconds]
------------------------------
SSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 07/29/23 16:30:30.372
  Jul 29 16:30:30.373: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 16:30:30.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:30.41
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:30.419
  Jul 29 16:30:30.426: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 16:30:30.441: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:30:30.445: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-1 before test
  Jul 29 16:30:30.457: INFO: cilium-lk6kf from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.457: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: cilium-node-init-gs57c from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.457: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: kube-addon-manager-vucheipi7kei-1 from kube-system started at 2023-07-29 15:23:00 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.457: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: kube-apiserver-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.457: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: kube-controller-manager-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.457: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: kube-proxy-qkz2r from kube-system started at 2023-07-29 15:14:13 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.457: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: kube-scheduler-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.457: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-4kwpr from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 16:30:30.457: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:30:30.457: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-2 before test
  Jul 29 16:30:30.471: INFO: cilium-cqdq2 from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.471: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:30:30.471: INFO: cilium-node-init-5p29d from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.471: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:30:30.472: INFO: coredns-5d78c9869d-67zm5 from kube-system started at 2023-07-29 15:24:06 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.472: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:30:30.472: INFO: coredns-5d78c9869d-kpth8 from kube-system started at 2023-07-29 15:24:06 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.472: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:30:30.472: INFO: kube-addon-manager-vucheipi7kei-2 from kube-system started at 2023-07-29 15:23:00 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.473: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:30:30.474: INFO: kube-apiserver-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.474: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:30:30.474: INFO: kube-controller-manager-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.475: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:30:30.475: INFO: kube-proxy-n67bc from kube-system started at 2023-07-29 15:14:48 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.476: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:30:30.476: INFO: kube-scheduler-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.476: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:30:30.477: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-hpqzb from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 16:30:30.477: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:30:30.478: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:30:30.478: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-3 before test
  Jul 29 16:30:30.494: INFO: test-recreate-deployment-54757ffd6c-xx48r from deployment-2468 started at 2023-07-29 16:30:30 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.495: INFO: 	Container httpd ready: false, restart count 0
  Jul 29 16:30:30.495: INFO: annotationupdateef6fe925-3bf5-45cd-8225-fa888fbeac00 from downward-api-5968 started at 2023-07-29 16:30:23 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.495: INFO: 	Container client-container ready: true, restart count 0
  Jul 29 16:30:30.495: INFO: cilium-9924s from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.496: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:30:30.496: INFO: cilium-node-init-ndt4w from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.496: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:30:30.496: INFO: cilium-operator-64cdf5fc9d-wj2rs from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.496: INFO: 	Container cilium-operator ready: true, restart count 0
  Jul 29 16:30:30.497: INFO: kube-proxy-szdbr from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.497: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:30:30.498: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (1 container statuses recorded)
  Jul 29 16:30:30.498: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 16:30:30.498: INFO: sonobuoy-e2e-job-e435975ae918422f from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 16:30:30.498: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 16:30:30.499: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:30:30.499: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-rwzlm from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 16:30:30.499: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:30:30.499: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 07/29/23 16:30:30.5
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.1776639bb6cfd675], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 07/29/23 16:30:30.579
  E0729 16:30:31.105028      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:31.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7886" for this suite. @ 07/29/23 16:30:31.58
• [1.221 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 07/29/23 16:30:31.595
  Jul 29 16:30:31.595: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:30:31.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:31.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:31.627
  STEP: Creating configMap with name configmap-projected-all-test-volume-ce5352fb-a6f9-4aa9-a2e8-2e52ddc29bdb @ 07/29/23 16:30:31.632
  STEP: Creating secret with name secret-projected-all-test-volume-c0081443-e6e6-4102-82f2-d320a574e533 @ 07/29/23 16:30:31.642
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 07/29/23 16:30:31.657
  E0729 16:30:32.105618      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:33.106509      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:34.106796      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:35.106801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:30:35.692
  Jul 29 16:30:35.700: INFO: Trying to get logs from node vucheipi7kei-3 pod projected-volume-f5e8b5f6-79a2-4a24-9cbb-0619a6fe563a container projected-all-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:30:35.724
  Jul 29 16:30:35.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5387" for this suite. @ 07/29/23 16:30:35.781
• [4.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 07/29/23 16:30:35.802
  Jul 29 16:30:35.802: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename watch @ 07/29/23 16:30:35.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:35.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:35.835
  STEP: creating a watch on configmaps @ 07/29/23 16:30:35.84
  STEP: creating a new configmap @ 07/29/23 16:30:35.844
  STEP: modifying the configmap once @ 07/29/23 16:30:35.853
  STEP: closing the watch once it receives two notifications @ 07/29/23 16:30:35.864
  Jul 29 16:30:35.865: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3327  00724c63-db22-45a2-8988-d0dd13439df8 24131 0 2023-07-29 16:30:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 16:30:35 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:30:35.865: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3327  00724c63-db22-45a2-8988-d0dd13439df8 24132 0 2023-07-29 16:30:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 16:30:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 07/29/23 16:30:35.866
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 07/29/23 16:30:35.882
  STEP: deleting the configmap @ 07/29/23 16:30:35.885
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 07/29/23 16:30:35.899
  Jul 29 16:30:35.900: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3327  00724c63-db22-45a2-8988-d0dd13439df8 24133 0 2023-07-29 16:30:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 16:30:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:30:35.900: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-3327  00724c63-db22-45a2-8988-d0dd13439df8 24134 0 2023-07-29 16:30:35 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 16:30:35 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:30:35.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3327" for this suite. @ 07/29/23 16:30:35.91
• [0.125 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 07/29/23 16:30:35.928
  Jul 29 16:30:35.928: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:30:35.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:35.961
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:35.971
  STEP: Creating secret with name secret-test-6e5770a4-11f0-430a-bc74-48066213c564 @ 07/29/23 16:30:35.976
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:30:35.985
  E0729 16:30:36.107764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:37.108626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:38.109488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:39.109748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:30:40.024
  Jul 29 16:30:40.028: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-secrets-446de315-1c62-4de9-992c-d5ae300bbe1d container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:30:40.042
  Jul 29 16:30:40.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2134" for this suite. @ 07/29/23 16:30:40.081
• [4.162 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 07/29/23 16:30:40.093
  Jul 29 16:30:40.093: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:30:40.096
  E0729 16:30:40.111148      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:40.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:40.13
  STEP: creating a secret @ 07/29/23 16:30:40.134
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 07/29/23 16:30:40.141
  STEP: patching the secret @ 07/29/23 16:30:40.148
  STEP: deleting the secret using a LabelSelector @ 07/29/23 16:30:40.16
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 07/29/23 16:30:40.17
  Jul 29 16:30:40.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5584" for this suite. @ 07/29/23 16:30:40.183
• [0.101 seconds]
------------------------------
S
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 07/29/23 16:30:40.195
  Jul 29 16:30:40.196: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename ingressclass @ 07/29/23 16:30:40.198
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:40.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:40.232
  STEP: getting /apis @ 07/29/23 16:30:40.236
  STEP: getting /apis/networking.k8s.io @ 07/29/23 16:30:40.244
  STEP: getting /apis/networking.k8s.iov1 @ 07/29/23 16:30:40.246
  STEP: creating @ 07/29/23 16:30:40.249
  STEP: getting @ 07/29/23 16:30:40.279
  STEP: listing @ 07/29/23 16:30:40.282
  STEP: watching @ 07/29/23 16:30:40.286
  Jul 29 16:30:40.287: INFO: starting watch
  STEP: patching @ 07/29/23 16:30:40.288
  STEP: updating @ 07/29/23 16:30:40.301
  Jul 29 16:30:40.310: INFO: waiting for watch events with expected annotations
  Jul 29 16:30:40.311: INFO: saw patched and updated annotations
  STEP: deleting @ 07/29/23 16:30:40.311
  STEP: deleting a collection @ 07/29/23 16:30:40.333
  Jul 29 16:30:40.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-2983" for this suite. @ 07/29/23 16:30:40.371
• [0.187 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 07/29/23 16:30:40.383
  Jul 29 16:30:40.383: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 16:30:40.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:40.411
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:40.415
  E0729 16:30:41.112239      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:42.113389      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:43.114684      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:44.114495      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:44.470: INFO: Got logs for pod "busybox-privileged-false-860da54b-28e7-4466-b496-dddbd6097e1c": "ip: RTNETLINK answers: Operation not permitted\n"
  Jul 29 16:30:44.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-3799" for this suite. @ 07/29/23 16:30:44.479
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 07/29/23 16:30:44.493
  Jul 29 16:30:44.493: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:30:44.495
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:44.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:44.523
  STEP: creating pod @ 07/29/23 16:30:44.527
  E0729 16:30:45.115497      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:46.115656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:46.569: INFO: Pod pod-hostip-ec4ac682-c6ca-43a7-9296-80d18cdd6462 has hostIP: 192.168.121.129
  Jul 29 16:30:46.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3673" for this suite. @ 07/29/23 16:30:46.577
• [2.095 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 07/29/23 16:30:46.595
  Jul 29 16:30:46.595: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:30:46.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:46.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:46.633
  Jul 29 16:30:46.637: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: creating the pod @ 07/29/23 16:30:46.638
  STEP: submitting the pod to kubernetes @ 07/29/23 16:30:46.639
  E0729 16:30:47.116648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:48.117532      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:48.702: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6225" for this suite. @ 07/29/23 16:30:48.71
• [2.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 07/29/23 16:30:48.734
  Jul 29 16:30:48.734: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 16:30:48.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:48.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:48.771
  Jul 29 16:30:48.779: INFO: Got root ca configmap in namespace "svcaccounts-5447"
  Jul 29 16:30:48.791: INFO: Deleted root ca configmap in namespace "svcaccounts-5447"
  E0729 16:30:49.118360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 07/29/23 16:30:49.292
  Jul 29 16:30:49.299: INFO: Recreated root ca configmap in namespace "svcaccounts-5447"
  Jul 29 16:30:49.310: INFO: Updated root ca configmap in namespace "svcaccounts-5447"
  STEP: waiting for the root ca configmap reconciled @ 07/29/23 16:30:49.81
  Jul 29 16:30:49.816: INFO: Reconciled root ca configmap in namespace "svcaccounts-5447"
  Jul 29 16:30:49.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5447" for this suite. @ 07/29/23 16:30:49.824
• [1.103 seconds]
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 07/29/23 16:30:49.837
  Jul 29 16:30:49.837: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename job @ 07/29/23 16:30:49.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:49.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:49.869
  STEP: Creating a job @ 07/29/23 16:30:49.873
  STEP: Ensuring active pods == parallelism @ 07/29/23 16:30:49.88
  E0729 16:30:50.118650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:51.118889      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 07/29/23 16:30:51.892
  E0729 16:30:52.119863      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:52.438: INFO: Successfully updated pod "adopt-release-ms4cb"
  STEP: Checking that the Job readopts the Pod @ 07/29/23 16:30:52.438
  E0729 16:30:53.121147      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:54.121187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 07/29/23 16:30:54.462
  Jul 29 16:30:54.985: INFO: Successfully updated pod "adopt-release-ms4cb"
  STEP: Checking that the Job releases the Pod @ 07/29/23 16:30:54.986
  E0729 16:30:55.122046      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:56.122876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:57.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7477" for this suite. @ 07/29/23 16:30:57.013
• [7.188 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 07/29/23 16:30:57.029
  Jul 29 16:30:57.029: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:30:57.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:57.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:57.071
  STEP: Creating pod test-webserver-6008e767-05a6-4fde-a3ff-bf1690b7f667 in namespace container-probe-3626 @ 07/29/23 16:30:57.077
  E0729 16:30:57.123292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:30:58.124094      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:30:59.108: INFO: Started pod test-webserver-6008e767-05a6-4fde-a3ff-bf1690b7f667 in namespace container-probe-3626
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:30:59.108
  Jul 29 16:30:59.113: INFO: Initial restart count of pod test-webserver-6008e767-05a6-4fde-a3ff-bf1690b7f667 is 0
  E0729 16:30:59.124074      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:00.125191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:01.125984      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:02.129840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:03.130914      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:04.131422      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:05.132305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:06.133261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:07.133776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:08.134150      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:09.134831      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:10.135029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:11.135484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:12.136097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:13.136647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:14.137178      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:15.137912      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:16.138331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:17.138632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:18.138871      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:19.140405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:20.140656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:21.140946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:22.141646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:23.142111      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:24.142792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:25.143204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:26.144200      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:27.144834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:28.145266      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:29.146096      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:30.146970      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:31.147579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:32.147838      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:33.148219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:34.148614      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:35.149231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:36.149612      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:37.149850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:38.150189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:39.150169      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:40.150832      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:41.150982      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:42.151713      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:43.163112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:44.152861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:45.153211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:46.153489      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:47.154258      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:48.154923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:49.154961      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:50.155992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:51.156014      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:52.156310      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:53.156552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:54.156758      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:55.157307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:56.157519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:57.157795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:58.157630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:31:59.158288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:00.158665      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:01.159291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:02.159482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:03.160292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:04.160644      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:05.161601      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:06.161656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:07.162006      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:08.162688      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:09.163106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:10.163776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:11.163948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:12.164007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:13.164261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:14.164448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:15.165909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:16.166614      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:17.166966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:18.168301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:19.168753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:20.169435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:21.169997      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:22.169695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:23.170118      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:24.170862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:25.170869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:26.172422      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:27.172107      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:28.172625      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:29.172838      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:30.173996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:31.174422      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:32.174564      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:33.175020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:34.175375      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:35.175976      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:36.176310      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:37.176362      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:38.176904      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:39.177219      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:40.177491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:41.177892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:42.178403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:43.178305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:44.178844      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:45.179411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:46.180283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:47.180826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:48.181493      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:49.183351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:50.183590      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:51.183875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:52.184186      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:53.185019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:54.185304      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:55.186110      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:56.186415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:57.186684      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:58.186722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:32:59.188432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:00.189229      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:01.190272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:02.190388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:03.191313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:04.192234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:05.193344      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:06.194434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:07.195218      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:08.196271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:09.196319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:10.196694      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:11.197571      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:12.197867      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:13.198204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:14.198804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:15.199162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:16.199577      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:17.199737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:18.199833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:19.199946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:20.200569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:21.201624      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:22.202759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:23.203545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:24.203740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:25.204178      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:26.204937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:27.205584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:28.206412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:29.207392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:30.208217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:31.208144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:32.208314      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:33.208684      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:34.209124      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:35.209753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:36.209919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:37.210265      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:38.211034      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:39.211063      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:40.211974      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:41.212251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:42.212409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:43.212881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:44.213290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:45.213297      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:46.214345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:47.215043      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:48.216246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:49.216519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:50.216651      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:51.217103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:52.218088      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:53.218864      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:54.219640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:55.220388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:56.221083      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:57.221553      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:58.222472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:33:59.222626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:00.223609      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:01.224748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:02.225868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:03.226337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:04.227363      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:05.227868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:06.228298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:07.229070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:08.229579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:09.230189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:10.230345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:11.230848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:12.231234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:13.231416      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:14.231778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:15.232040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:16.231948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:17.232259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:18.233673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:19.233914      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:20.233981      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:21.234118      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:22.234672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:23.235939      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:24.236565      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:25.237224      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:26.237759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:27.238009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:28.238138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:29.238525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:30.238946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:31.239179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:32.239892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:33.240041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:34.241308      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:35.241595      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:36.242284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:37.243172      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:38.243673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:39.243947      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:40.245947      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:41.246861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:42.246315      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:43.246789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:44.247023      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:45.247952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:46.248214      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:47.248495      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:48.252053      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:49.249768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:50.249967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:51.250233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:52.250728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:53.251001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:54.251676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:55.252536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:56.253133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:57.253430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:58.254639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:34:59.254726      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:00.256339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:35:00.313: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:35:00.329
  STEP: Destroying namespace "container-probe-3626" for this suite. @ 07/29/23 16:35:00.356
• [243.369 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 07/29/23 16:35:00.401
  Jul 29 16:35:00.401: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:35:00.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:00.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:00.444
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:35:00.448
  E0729 16:35:01.255256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:02.256378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:03.256601      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:04.256887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:35:04.487
  Jul 29 16:35:04.493: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-659b73f2-1283-494e-8548-73bf993d726a container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:35:04.528
  Jul 29 16:35:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3220" for this suite. @ 07/29/23 16:35:04.569
• [4.182 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 07/29/23 16:35:04.585
  Jul 29 16:35:04.585: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:35:04.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:04.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:04.621
  STEP: Creating projection with secret that has name secret-emptykey-test-805a4763-cddf-4922-b78f-7d41099b8e3c @ 07/29/23 16:35:04.626
  Jul 29 16:35:04.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3757" for this suite. @ 07/29/23 16:35:04.636
• [0.064 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 07/29/23 16:35:04.651
  Jul 29 16:35:04.651: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:35:04.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:04.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:04.68
  STEP: Creating Pod @ 07/29/23 16:35:04.684
  E0729 16:35:05.257741      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:06.258705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 07/29/23 16:35:06.721
  Jul 29 16:35:06.721: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-6721 PodName:pod-sharedvolume-28698a24-4116-4c12-9bf0-4d0e5b6c0fb4 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:35:06.721: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:35:06.723: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:35:06.723: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-6721/pods/pod-sharedvolume-28698a24-4116-4c12-9bf0-4d0e5b6c0fb4/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jul 29 16:35:06.817: INFO: Exec stderr: ""
  Jul 29 16:35:06.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6721" for this suite. @ 07/29/23 16:35:06.829
• [2.188 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 07/29/23 16:35:06.841
  Jul 29 16:35:06.841: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:35:06.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:06.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:06.882
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/29/23 16:35:06.893
  E0729 16:35:07.258895      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:08.260089      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:09.259862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:10.260423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:35:10.938
  Jul 29 16:35:10.946: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-8c160b65-6f9e-467a-8317-8a546406a8ce container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:35:10.959
  Jul 29 16:35:10.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7130" for this suite. @ 07/29/23 16:35:10.994
• [4.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 07/29/23 16:35:11.02
  Jul 29 16:35:11.020: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:35:11.022
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:11.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:11.061
  STEP: Creating configMap with name configmap-test-volume-d8dccd9a-038a-4ef7-a019-13ddbf58cb8c @ 07/29/23 16:35:11.066
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:35:11.074
  E0729 16:35:11.260924      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:12.260964      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:13.261796      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:14.262804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:35:15.112
  Jul 29 16:35:15.119: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-30297612-2628-4bdb-a874-0d2f3a87b1a9 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:35:15.135
  Jul 29 16:35:15.163: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2800" for this suite. @ 07/29/23 16:35:15.175
• [4.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 07/29/23 16:35:15.194
  Jul 29 16:35:15.194: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:35:15.195
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:15.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:15.227
  STEP: Counting existing ResourceQuota @ 07/29/23 16:35:15.231
  E0729 16:35:15.262880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:16.263320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:17.264177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:18.264569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:19.265415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 16:35:20.237
  STEP: Ensuring resource quota status is calculated @ 07/29/23 16:35:20.249
  E0729 16:35:20.266459      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:21.266627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:35:22.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 16:35:22.266516      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "resourcequota-4548" for this suite. @ 07/29/23 16:35:22.266
• [7.083 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 07/29/23 16:35:22.278
  Jul 29 16:35:22.278: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename disruption @ 07/29/23 16:35:22.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:22.311
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:22.319
  STEP: Creating a pdb that targets all three pods in a test replica set @ 07/29/23 16:35:22.324
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:35:22.332
  E0729 16:35:23.278994      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:24.270521      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 07/29/23 16:35:24.367
  STEP: Waiting for all pods to be running @ 07/29/23 16:35:24.367
  Jul 29 16:35:24.374: INFO: pods: 0 < 3
  E0729 16:35:25.271562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:26.272301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/29/23 16:35:26.385
  STEP: Updating the pdb to allow a pod to be evicted @ 07/29/23 16:35:26.406
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:35:26.422
  E0729 16:35:27.272290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:28.273685      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/29/23 16:35:28.447
  STEP: Waiting for all pods to be running @ 07/29/23 16:35:28.448
  STEP: Waiting for the pdb to observed all healthy pods @ 07/29/23 16:35:28.457
  STEP: Patching the pdb to disallow a pod to be evicted @ 07/29/23 16:35:28.517
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:35:28.588
  STEP: Waiting for all pods to be running @ 07/29/23 16:35:28.605
  Jul 29 16:35:28.614: INFO: running pods: 2 < 3
  E0729 16:35:29.274561      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:30.274745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/29/23 16:35:30.625
  STEP: Deleting the pdb to allow a pod to be evicted @ 07/29/23 16:35:30.647
  STEP: Waiting for the pdb to be deleted @ 07/29/23 16:35:30.658
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/29/23 16:35:30.669
  STEP: Waiting for all pods to be running @ 07/29/23 16:35:30.67
  Jul 29 16:35:30.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-7057" for this suite. @ 07/29/23 16:35:30.717
• [8.469 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 07/29/23 16:35:30.753
  Jul 29 16:35:30.753: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 16:35:30.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:30.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:30.799
  Jul 29 16:35:30.806: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 16:35:31.274858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:32.276208      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:33.276183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:34.276840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:35:34.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-8471" for this suite. @ 07/29/23 16:35:34.3
• [3.563 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 07/29/23 16:35:34.319
  Jul 29 16:35:34.319: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:35:34.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:34.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:34.359
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/29/23 16:35:34.362
  E0729 16:35:35.277805      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:36.278564      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:37.278807      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:38.278945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:35:38.402
  Jul 29 16:35:38.408: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-a2d1bca3-6403-4ec1-a71c-98207fb36956 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:35:38.421
  Jul 29 16:35:38.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9233" for this suite. @ 07/29/23 16:35:38.455
• [4.145 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 07/29/23 16:35:38.468
  Jul 29 16:35:38.469: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:35:38.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:35:38.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:35:38.504
  STEP: Creating service test in namespace statefulset-566 @ 07/29/23 16:35:38.51
  STEP: Creating stateful set ss in namespace statefulset-566 @ 07/29/23 16:35:38.532
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-566 @ 07/29/23 16:35:38.542
  Jul 29 16:35:38.547: INFO: Found 0 stateful pods, waiting for 1
  E0729 16:35:39.279144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:40.279809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:41.280159      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:42.280475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:43.280655      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:44.281352      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:45.282208      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:46.283025      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:47.283358      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:48.283161      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:35:48.555: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 07/29/23 16:35:48.555
  Jul 29 16:35:48.562: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-566 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:35:48.827: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:35:48.827: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:35:48.827: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:35:48.834: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0729 16:35:49.284196      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:50.285108      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:51.285383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:52.285539      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:53.286315      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:54.286386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:55.287227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:56.287608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:57.288249      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:35:58.288414      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:35:58.846: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:35:58.846: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:35:58.887: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999389s
  E0729 16:35:59.288935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:35:59.896: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.988445924s
  E0729 16:36:00.289672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:00.907: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.978378579s
  E0729 16:36:01.290473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:01.917: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.968325994s
  E0729 16:36:02.290937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:02.927: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.958323141s
  E0729 16:36:03.291930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:03.937: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.948461434s
  E0729 16:36:04.292484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:04.947: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.938292773s
  E0729 16:36:05.292729      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:05.957: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.927799597s
  E0729 16:36:06.293133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:06.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.918355849s
  E0729 16:36:07.293627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:07.980: INFO: Verifying statefulset ss doesn't scale past 3 for another 906.603393ms
  E0729 16:36:08.294157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-566 @ 07/29/23 16:36:08.981
  Jul 29 16:36:08.992: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-566 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:36:09.256: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:36:09.256: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:36:09.256: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:36:09.256: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-566 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0729 16:36:09.294069      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:09.510: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 29 16:36:09.510: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:36:09.510: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:36:09.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-566 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:36:09.789: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 29 16:36:09.789: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:36:09.789: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:36:09.799: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  E0729 16:36:10.294629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:11.295248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:12.295346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:13.295909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:14.296145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:15.297230      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:16.297647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:17.298429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:18.298995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:19.299461      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:19.807: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:36:19.807: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:36:19.807: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 07/29/23 16:36:19.807
  Jul 29 16:36:19.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-566 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:36:20.111: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:36:20.111: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:36:20.111: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:36:20.111: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-566 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  E0729 16:36:20.300275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:20.398: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:36:20.398: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:36:20.399: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:36:20.399: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=statefulset-566 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:36:20.692: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:36:20.692: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:36:20.692: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:36:20.692: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:36:20.700: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0729 16:36:21.300339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:22.300904      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:23.303048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:24.301346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:25.301316      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:26.301798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:27.302427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:28.302583      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:29.302835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:30.303990      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:30.719: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:36:30.719: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:36:30.720: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:36:30.756: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Jul 29 16:36:30.757: INFO: ss-0  vucheipi7kei-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:35:38 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:36:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:36:20 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:35:38 +0000 UTC  }]
  Jul 29 16:36:30.757: INFO: ss-1  vucheipi7kei-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:35:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:36:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:36:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:35:58 +0000 UTC  }]
  Jul 29 16:36:30.758: INFO: ss-2  vucheipi7kei-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:35:58 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:36:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:36:21 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:35:58 +0000 UTC  }]
  Jul 29 16:36:30.758: INFO: 
  Jul 29 16:36:30.758: INFO: StatefulSet ss has not reached scale 0, at 3
  E0729 16:36:31.304289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:31.768: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Jul 29 16:36:31.769: INFO: ss-0  vucheipi7kei-3  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:35:38 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:36:20 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:36:20 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:35:38 +0000 UTC  }]
  Jul 29 16:36:31.769: INFO: 
  Jul 29 16:36:31.769: INFO: StatefulSet ss has not reached scale 0, at 1
  E0729 16:36:32.304742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:32.778: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.977899089s
  E0729 16:36:33.305434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:33.785: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.969327312s
  E0729 16:36:34.306591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:34.795: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.961551224s
  E0729 16:36:35.307688      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:35.800: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.952273481s
  E0729 16:36:36.308151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:36.808: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.946407306s
  E0729 16:36:37.308519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:37.821: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.938466465s
  E0729 16:36:38.308623      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:38.831: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.92603751s
  E0729 16:36:39.308979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:39.840: INFO: Verifying statefulset ss doesn't scale past 0 for another 916.002924ms
  E0729 16:36:40.309185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-566 @ 07/29/23 16:36:40.841
  Jul 29 16:36:40.853: INFO: Scaling statefulset ss to 0
  Jul 29 16:36:40.876: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:36:40.882: INFO: Deleting all statefulset in ns statefulset-566
  Jul 29 16:36:40.887: INFO: Scaling statefulset ss to 0
  Jul 29 16:36:40.912: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:36:40.916: INFO: Deleting statefulset ss
  Jul 29 16:36:40.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-566" for this suite. @ 07/29/23 16:36:40.948
• [62.495 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 07/29/23 16:36:40.964
  Jul 29 16:36:40.964: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:36:40.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:36:40.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:36:41.001
  STEP: Creating secret with name secret-test-68ac8c53-0380-4ba8-b8ca-88bc9777fce5 @ 07/29/23 16:36:41.005
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:36:41.013
  E0729 16:36:41.309738      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:42.310897      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:43.311062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:44.311586      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:36:45.051
  Jul 29 16:36:45.057: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-secrets-c92a4849-8708-4d35-b275-33ec691b965c container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:36:45.08
  Jul 29 16:36:45.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9634" for this suite. @ 07/29/23 16:36:45.114
• [4.160 seconds]
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 07/29/23 16:36:45.124
  Jul 29 16:36:45.124: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 16:36:45.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:36:45.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:36:45.152
  STEP: create the container @ 07/29/23 16:36:45.156
  W0729 16:36:45.166921      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 16:36:45.167
  E0729 16:36:45.312357      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:46.312891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:47.312903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/29/23 16:36:48.197
  STEP: the container should be terminated @ 07/29/23 16:36:48.203
  STEP: the termination message should be set @ 07/29/23 16:36:48.203
  Jul 29 16:36:48.203: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/29/23 16:36:48.203
  Jul 29 16:36:48.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8425" for this suite. @ 07/29/23 16:36:48.25
• [3.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 07/29/23 16:36:48.269
  Jul 29 16:36:48.269: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 16:36:48.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:36:48.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:36:48.304
  E0729 16:36:48.313587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 07/29/23 16:36:48.324
  E0729 16:36:49.313966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:50.313924      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:51.315432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:52.315055      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:52.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8651" for this suite. @ 07/29/23 16:36:52.371
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 07/29/23 16:36:52.383
  Jul 29 16:36:52.384: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename dns @ 07/29/23 16:36:52.385
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:36:52.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:36:52.417
  STEP: Creating a test headless service @ 07/29/23 16:36:52.425
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6270.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6270.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6270.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6270.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6270.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6270.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6270.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6270.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6270.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6270.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 56.24.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.24.56_udp@PTR;check="$$(dig +tcp +noall +answer +search 56.24.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.24.56_tcp@PTR;sleep 1; done
   @ 07/29/23 16:36:52.462
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6270.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6270.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6270.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6270.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6270.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6270.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6270.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6270.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6270.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6270.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 56.24.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.24.56_udp@PTR;check="$$(dig +tcp +noall +answer +search 56.24.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.24.56_tcp@PTR;sleep 1; done
   @ 07/29/23 16:36:52.463
  STEP: creating a pod to probe DNS @ 07/29/23 16:36:52.463
  STEP: submitting the pod to kubernetes @ 07/29/23 16:36:52.463
  E0729 16:36:53.315418      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:54.316024      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 16:36:54.511
  STEP: looking for the results for each expected name from probers @ 07/29/23 16:36:54.517
  Jul 29 16:36:54.529: INFO: Unable to read wheezy_udp@dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:54.535: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:54.541: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:54.548: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:54.572: INFO: Unable to read jessie_udp@dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:54.578: INFO: Unable to read jessie_tcp@dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:54.583: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:54.588: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:54.611: INFO: Lookups using dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384 failed for: [wheezy_udp@dns-test-service.dns-6270.svc.cluster.local wheezy_tcp@dns-test-service.dns-6270.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_udp@dns-test-service.dns-6270.svc.cluster.local jessie_tcp@dns-test-service.dns-6270.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local]

  E0729 16:36:55.316862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:56.317383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:57.317604      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:58.318350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:36:59.319074      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:36:59.636: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:59.643: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:59.697: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:59.704: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:36:59.728: INFO: Lookups using dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local]

  E0729 16:37:00.321337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:01.322009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:02.323027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:03.323025      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:04.322734      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:37:04.637: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:04.646: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:04.699: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:04.706: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:04.731: INFO: Lookups using dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local]

  E0729 16:37:05.322575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:06.323391      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:07.323307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:08.324392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:09.324680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:37:09.627: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:09.669: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:09.711: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:09.717: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:09.738: INFO: Lookups using dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local]

  E0729 16:37:10.325495      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:11.327309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:12.327001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:13.327157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:14.327531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:37:14.636: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:14.645: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:14.724: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:14.731: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:14.756: INFO: Lookups using dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local]

  E0729 16:37:15.328920      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:16.328869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:17.328899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:18.329078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:19.329228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:37:19.635: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:19.646: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:19.719: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:19.739: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local from pod dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384: the server could not find the requested resource (get pods dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384)
  Jul 29 16:37:19.798: INFO: Lookups using dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384 failed for: [wheezy_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6270.svc.cluster.local]

  E0729 16:37:20.330967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:21.330859      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:22.330916      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:23.331648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:24.332896      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:37:24.736: INFO: DNS probes using dns-6270/dns-test-a62a80e8-ae33-494d-a4b9-7f22a8f19384 succeeded

  Jul 29 16:37:24.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:37:24.744
  STEP: deleting the test service @ 07/29/23 16:37:24.768
  STEP: deleting the test headless service @ 07/29/23 16:37:24.831
  STEP: Destroying namespace "dns-6270" for this suite. @ 07/29/23 16:37:24.872
• [32.510 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 07/29/23 16:37:24.895
  Jul 29 16:37:24.895: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:37:24.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:24.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:24.959
  E0729 16:37:25.333919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:26.334157      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:37:26.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:37:27.011: INFO: Deleting pod "var-expansion-69d6b87a-6180-4b64-9993-96fbc0f3072e" in namespace "var-expansion-9701"
  Jul 29 16:37:27.024: INFO: Wait up to 5m0s for pod "var-expansion-69d6b87a-6180-4b64-9993-96fbc0f3072e" to be fully deleted
  E0729 16:37:27.334630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:28.335041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-9701" for this suite. @ 07/29/23 16:37:29.045
• [4.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 07/29/23 16:37:29.059
  Jul 29 16:37:29.059: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sysctl @ 07/29/23 16:37:29.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:29.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:29.094
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 07/29/23 16:37:29.1
  STEP: Watching for error events or started pod @ 07/29/23 16:37:29.118
  E0729 16:37:29.336251      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:30.336795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 07/29/23 16:37:31.127
  E0729 16:37:31.337576      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:32.338353      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 07/29/23 16:37:33.148
  STEP: Getting logs from the pod @ 07/29/23 16:37:33.149
  STEP: Checking that the sysctl is actually updated @ 07/29/23 16:37:33.161
  Jul 29 16:37:33.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-316" for this suite. @ 07/29/23 16:37:33.17
• [4.128 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 07/29/23 16:37:33.187
  Jul 29 16:37:33.187: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:37:33.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:33.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:33.231
  E0729 16:37:33.338646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:34.339288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:35.339364      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:36.340060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:37.340840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:38.341642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:39.342763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:40.343829      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:41.345400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:42.345669      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:43.346849      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:44.347383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:45.347365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:46.348140      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:47.348684      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:48.349290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:49.349050      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:50.350275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:51.350265      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:52.351057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:53.352291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:54.353075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:55.353561      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:37:55.379: INFO: Container started at 2023-07-29 16:37:34 +0000 UTC, pod became ready at 2023-07-29 16:37:53 +0000 UTC
  Jul 29 16:37:55.380: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-9071" for this suite. @ 07/29/23 16:37:55.391
• [22.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 07/29/23 16:37:55.411
  Jul 29 16:37:55.411: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:37:55.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:55.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:55.454
  STEP: Creating configMap with name cm-test-opt-del-de2a7fa0-08ae-438c-9006-a42d04ec0360 @ 07/29/23 16:37:55.473
  STEP: Creating configMap with name cm-test-opt-upd-df183ffb-f871-4bd6-9bb3-fbad34abea50 @ 07/29/23 16:37:55.484
  STEP: Creating the pod @ 07/29/23 16:37:55.491
  E0729 16:37:56.354380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:57.354402      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-de2a7fa0-08ae-438c-9006-a42d04ec0360 @ 07/29/23 16:37:57.581
  STEP: Updating configmap cm-test-opt-upd-df183ffb-f871-4bd6-9bb3-fbad34abea50 @ 07/29/23 16:37:57.593
  STEP: Creating configMap with name cm-test-opt-create-c202be57-a74e-4bce-afc3-502966d18c13 @ 07/29/23 16:37:57.602
  STEP: waiting to observe update in volume @ 07/29/23 16:37:57.608
  E0729 16:37:58.354705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:37:59.355287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:37:59.659: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1923" for this suite. @ 07/29/23 16:37:59.671
• [4.274 seconds]
------------------------------
SSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 07/29/23 16:37:59.686
  Jul 29 16:37:59.686: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename events @ 07/29/23 16:37:59.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:59.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:59.734
  STEP: creating a test event @ 07/29/23 16:37:59.738
  STEP: listing events in all namespaces @ 07/29/23 16:37:59.749
  STEP: listing events in test namespace @ 07/29/23 16:37:59.757
  STEP: listing events with field selection filtering on source @ 07/29/23 16:37:59.775
  STEP: listing events with field selection filtering on reportingController @ 07/29/23 16:37:59.787
  STEP: getting the test event @ 07/29/23 16:37:59.792
  STEP: patching the test event @ 07/29/23 16:37:59.797
  STEP: getting the test event @ 07/29/23 16:37:59.817
  STEP: updating the test event @ 07/29/23 16:37:59.824
  STEP: getting the test event @ 07/29/23 16:37:59.833
  STEP: deleting the test event @ 07/29/23 16:37:59.839
  STEP: listing events in all namespaces @ 07/29/23 16:37:59.85
  STEP: listing events in test namespace @ 07/29/23 16:37:59.855
  Jul 29 16:37:59.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-6768" for this suite. @ 07/29/23 16:37:59.869
• [0.192 seconds]
------------------------------
SSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 07/29/23 16:37:59.88
  Jul 29 16:37:59.880: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename certificates @ 07/29/23 16:37:59.883
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:59.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:59.914
  E0729 16:38:00.355545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting /apis @ 07/29/23 16:38:01.044
  STEP: getting /apis/certificates.k8s.io @ 07/29/23 16:38:01.052
  STEP: getting /apis/certificates.k8s.io/v1 @ 07/29/23 16:38:01.054
  STEP: creating @ 07/29/23 16:38:01.057
  STEP: getting @ 07/29/23 16:38:01.083
  STEP: listing @ 07/29/23 16:38:01.088
  STEP: watching @ 07/29/23 16:38:01.097
  Jul 29 16:38:01.098: INFO: starting watch
  STEP: patching @ 07/29/23 16:38:01.1
  STEP: updating @ 07/29/23 16:38:01.11
  Jul 29 16:38:01.121: INFO: waiting for watch events with expected annotations
  Jul 29 16:38:01.121: INFO: saw patched and updated annotations
  STEP: getting /approval @ 07/29/23 16:38:01.121
  STEP: patching /approval @ 07/29/23 16:38:01.129
  STEP: updating /approval @ 07/29/23 16:38:01.141
  STEP: getting /status @ 07/29/23 16:38:01.153
  STEP: patching /status @ 07/29/23 16:38:01.16
  STEP: updating /status @ 07/29/23 16:38:01.173
  STEP: deleting @ 07/29/23 16:38:01.188
  STEP: deleting a collection @ 07/29/23 16:38:01.211
  Jul 29 16:38:01.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-3802" for this suite. @ 07/29/23 16:38:01.246
• [1.376 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 07/29/23 16:38:01.257
  Jul 29 16:38:01.258: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:38:01.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:01.287
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:01.292
  STEP: Creating pod busybox-f9eba1c2-6e9d-4788-aeb6-997e58df901c in namespace container-probe-5870 @ 07/29/23 16:38:01.296
  E0729 16:38:01.355994      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:02.356510      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:38:03.339: INFO: Started pod busybox-f9eba1c2-6e9d-4788-aeb6-997e58df901c in namespace container-probe-5870
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:38:03.339
  Jul 29 16:38:03.343: INFO: Initial restart count of pod busybox-f9eba1c2-6e9d-4788-aeb6-997e58df901c is 0
  E0729 16:38:03.357090      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:04.358015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:05.358699      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:06.359449      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:07.360307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:08.361034      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:09.361979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:10.362854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:11.363091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:12.363184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:13.364376      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:14.363797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:15.364420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:16.364743      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:17.364798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:18.365004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:19.365243      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:20.365640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:21.366005      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:22.366081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:23.368181      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:24.368329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:25.369405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:26.369698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:27.370209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:28.370979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:29.371384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:30.371578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:31.372254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:32.372506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:33.372606      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:34.372946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:35.373055      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:36.373409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:37.373738      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:38.373776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:39.374016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:40.375029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:41.375183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:42.375637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:43.376551      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:44.376790      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:45.376883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:46.377134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:47.377222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:48.377770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:49.377815      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:50.377825      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:51.378313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:52.378935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:53.380838      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:38:53.634: INFO: Restart count of pod container-probe-5870/busybox-f9eba1c2-6e9d-4788-aeb6-997e58df901c is now 1 (50.291217011s elapsed)
  Jul 29 16:38:53.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:38:53.643
  STEP: Destroying namespace "container-probe-5870" for this suite. @ 07/29/23 16:38:53.688
• [52.454 seconds]
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 07/29/23 16:38:53.712
  Jul 29 16:38:53.713: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:38:53.715
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:53.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:53.753
  Jul 29 16:38:53.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6795" for this suite. @ 07/29/23 16:38:53.84
• [0.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 07/29/23 16:38:53.856
  Jul 29 16:38:53.856: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename limitrange @ 07/29/23 16:38:53.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:53.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:53.891
  STEP: Creating LimitRange "e2e-limitrange-9vkvh" in namespace "limitrange-5524" @ 07/29/23 16:38:53.895
  STEP: Creating another limitRange in another namespace @ 07/29/23 16:38:53.903
  Jul 29 16:38:53.926: INFO: Namespace "e2e-limitrange-9vkvh-9085" created
  Jul 29 16:38:53.927: INFO: Creating LimitRange "e2e-limitrange-9vkvh" in namespace "e2e-limitrange-9vkvh-9085"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-9vkvh" @ 07/29/23 16:38:53.935
  Jul 29 16:38:53.942: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-9vkvh" in "limitrange-5524" namespace @ 07/29/23 16:38:53.943
  Jul 29 16:38:53.959: INFO: LimitRange "e2e-limitrange-9vkvh" has been patched
  STEP: Delete LimitRange "e2e-limitrange-9vkvh" by Collection with labelSelector: "e2e-limitrange-9vkvh=patched" @ 07/29/23 16:38:53.96
  STEP: Confirm that the limitRange "e2e-limitrange-9vkvh" has been deleted @ 07/29/23 16:38:53.973
  Jul 29 16:38:53.973: INFO: Requesting list of LimitRange to confirm quantity
  Jul 29 16:38:53.980: INFO: Found 0 LimitRange with label "e2e-limitrange-9vkvh=patched"
  Jul 29 16:38:53.980: INFO: LimitRange "e2e-limitrange-9vkvh" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-9vkvh" @ 07/29/23 16:38:53.981
  Jul 29 16:38:53.991: INFO: Found 1 limitRange
  Jul 29 16:38:53.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5524" for this suite. @ 07/29/23 16:38:53.999
  STEP: Destroying namespace "e2e-limitrange-9vkvh-9085" for this suite. @ 07/29/23 16:38:54.013
• [0.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 07/29/23 16:38:54.032
  Jul 29 16:38:54.032: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:38:54.034
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:54.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:54.069
  STEP: Creating configMap with name cm-test-opt-del-403728df-1475-44eb-9cc0-35ace9927b95 @ 07/29/23 16:38:54.079
  STEP: Creating configMap with name cm-test-opt-upd-38db639b-4ad5-4a1d-ad49-54f5e0c03423 @ 07/29/23 16:38:54.089
  STEP: Creating the pod @ 07/29/23 16:38:54.097
  E0729 16:38:54.383659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:55.385146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-403728df-1475-44eb-9cc0-35ace9927b95 @ 07/29/23 16:38:56.192
  STEP: Updating configmap cm-test-opt-upd-38db639b-4ad5-4a1d-ad49-54f5e0c03423 @ 07/29/23 16:38:56.204
  STEP: Creating configMap with name cm-test-opt-create-3a3cf95f-00cb-4ea4-9e0d-5c78d1871aa3 @ 07/29/23 16:38:56.216
  STEP: waiting to observe update in volume @ 07/29/23 16:38:56.227
  E0729 16:38:56.385742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:57.386078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:58.386233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:38:59.386382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:00.387439      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:01.387272      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:02.388256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:03.388307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:04.389411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:05.389550      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:06.390279      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:07.390965      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:08.392388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:09.392580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:10.393768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:11.393872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:12.394708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:13.395146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:14.395367      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:15.395641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:16.396577      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:17.396898      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:18.396941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:19.397338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:20.397569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:21.398284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:22.398458      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:23.399110      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:24.399051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:25.399735      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:26.399666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:27.400533      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:28.400747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:29.401183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:30.403409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:31.402879      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:32.402993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:33.403775      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:34.404259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:35.404411      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:36.404537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:37.404720      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:38.405202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:39.405498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:40.405494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:41.406088      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:42.406511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:43.407455      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:44.407792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:45.407839      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:46.407978      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:47.408640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:48.409041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:49.409420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:50.410050      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:51.410045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:52.410542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:53.410837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:54.411476      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:55.412042      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:56.412231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:57.412940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:58.413609      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:39:59.414531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:00.415427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:01.415723      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:02.415834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:03.416901      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:04.417617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:05.417819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:06.418435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:07.418786      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:08.419400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:09.420563      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:10.421580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:11.421760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:12.422547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:13.008: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6011" for this suite. @ 07/29/23 16:40:13.018
• [79.002 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 07/29/23 16:40:13.036
  Jul 29 16:40:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:40:13.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:40:13.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:40:13.083
  STEP: Creating configMap with name configmap-test-volume-map-7a4152ca-9f3e-4a11-8bed-370e7ae4588f @ 07/29/23 16:40:13.088
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:40:13.095
  E0729 16:40:13.422297      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:14.422717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:15.423015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:16.429521      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:40:17.135
  Jul 29 16:40:17.143: INFO: Trying to get logs from node vucheipi7kei-1 pod pod-configmaps-12ae5891-4315-4488-902f-82e84417a538 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:40:17.177
  Jul 29 16:40:17.204: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9376" for this suite. @ 07/29/23 16:40:17.215
• [4.193 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 07/29/23 16:40:17.234
  Jul 29 16:40:17.234: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:40:17.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:40:17.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:40:17.276
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:40:17.28
  E0729 16:40:17.426507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:18.434931      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:19.428584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:20.429549      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:40:21.325
  Jul 29 16:40:21.345: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-d1e9df42-2082-4f77-b076-f40f6798ba08 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:40:21.364
  Jul 29 16:40:21.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9922" for this suite. @ 07/29/23 16:40:21.406
• [4.195 seconds]
------------------------------
SS  E0729 16:40:21.429344      14 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 07/29/23 16:40:21.441
  Jul 29 16:40:21.441: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:40:21.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:40:21.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:40:21.489
  STEP: creating a replication controller @ 07/29/23 16:40:21.497
  Jul 29 16:40:21.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 create -f -'
  E0729 16:40:22.429780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:22.483: INFO: stderr: ""
  Jul 29 16:40:22.484: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 16:40:22.485
  Jul 29 16:40:22.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:40:22.669: INFO: stderr: ""
  Jul 29 16:40:22.669: INFO: stdout: "update-demo-nautilus-cs7tg update-demo-nautilus-s88hq "
  Jul 29 16:40:22.670: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-cs7tg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:40:22.823: INFO: stderr: ""
  Jul 29 16:40:22.823: INFO: stdout: ""
  Jul 29 16:40:22.823: INFO: update-demo-nautilus-cs7tg is created but not running
  E0729 16:40:23.429946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:24.431607      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:25.431681      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:26.432046      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:27.432246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:27.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:40:27.987: INFO: stderr: ""
  Jul 29 16:40:27.987: INFO: stdout: "update-demo-nautilus-cs7tg update-demo-nautilus-s88hq "
  Jul 29 16:40:27.988: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-cs7tg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:40:28.180: INFO: stderr: ""
  Jul 29 16:40:28.180: INFO: stdout: ""
  Jul 29 16:40:28.180: INFO: update-demo-nautilus-cs7tg is created but not running
  E0729 16:40:28.432510      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:29.432657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:30.432858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:31.433405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:32.434608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:33.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:40:33.360: INFO: stderr: ""
  Jul 29 16:40:33.360: INFO: stdout: "update-demo-nautilus-cs7tg update-demo-nautilus-s88hq "
  Jul 29 16:40:33.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-cs7tg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0729 16:40:33.434758      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:33.552: INFO: stderr: ""
  Jul 29 16:40:33.552: INFO: stdout: "true"
  Jul 29 16:40:33.552: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-cs7tg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 16:40:33.721: INFO: stderr: ""
  Jul 29 16:40:33.721: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 16:40:33.721: INFO: validating pod update-demo-nautilus-cs7tg
  Jul 29 16:40:33.735: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 16:40:33.736: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 16:40:33.736: INFO: update-demo-nautilus-cs7tg is verified up and running
  Jul 29 16:40:33.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-s88hq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:40:33.874: INFO: stderr: ""
  Jul 29 16:40:33.874: INFO: stdout: ""
  Jul 29 16:40:33.874: INFO: update-demo-nautilus-s88hq is created but not running
  E0729 16:40:34.435437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:35.436476      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:36.436642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:37.437099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:38.437137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:38.875: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:40:39.036: INFO: stderr: ""
  Jul 29 16:40:39.036: INFO: stdout: "update-demo-nautilus-cs7tg update-demo-nautilus-s88hq "
  Jul 29 16:40:39.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-cs7tg -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:40:39.176: INFO: stderr: ""
  Jul 29 16:40:39.176: INFO: stdout: "true"
  Jul 29 16:40:39.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-cs7tg -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 16:40:39.307: INFO: stderr: ""
  Jul 29 16:40:39.307: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 16:40:39.307: INFO: validating pod update-demo-nautilus-cs7tg
  Jul 29 16:40:39.314: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 16:40:39.314: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 16:40:39.314: INFO: update-demo-nautilus-cs7tg is verified up and running
  Jul 29 16:40:39.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-s88hq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0729 16:40:39.437632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:39.450: INFO: stderr: ""
  Jul 29 16:40:39.450: INFO: stdout: "true"
  Jul 29 16:40:39.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-s88hq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 16:40:39.604: INFO: stderr: ""
  Jul 29 16:40:39.604: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 16:40:39.604: INFO: validating pod update-demo-nautilus-s88hq
  Jul 29 16:40:39.624: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 16:40:39.624: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 16:40:39.624: INFO: update-demo-nautilus-s88hq is verified up and running
  STEP: scaling down the replication controller @ 07/29/23 16:40:39.624
  Jul 29 16:40:39.645: INFO: scanned /root for discovery docs: <nil>
  Jul 29 16:40:39.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0729 16:40:40.438492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:40.887: INFO: stderr: ""
  Jul 29 16:40:40.887: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 16:40:40.887
  Jul 29 16:40:40.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:40:41.031: INFO: stderr: ""
  Jul 29 16:40:41.031: INFO: stdout: "update-demo-nautilus-cs7tg update-demo-nautilus-s88hq "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 07/29/23 16:40:41.031
  E0729 16:40:41.439849      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:42.439874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:43.440295      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:44.440300      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:45.441799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:46.033: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:40:46.172: INFO: stderr: ""
  Jul 29 16:40:46.172: INFO: stdout: "update-demo-nautilus-s88hq "
  Jul 29 16:40:46.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-s88hq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:40:46.298: INFO: stderr: ""
  Jul 29 16:40:46.298: INFO: stdout: "true"
  Jul 29 16:40:46.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-s88hq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 16:40:46.433: INFO: stderr: ""
  Jul 29 16:40:46.433: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 16:40:46.433: INFO: validating pod update-demo-nautilus-s88hq
  Jul 29 16:40:46.441: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 16:40:46.441: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 16:40:46.441: INFO: update-demo-nautilus-s88hq is verified up and running
  STEP: scaling up the replication controller @ 07/29/23 16:40:46.441
  E0729 16:40:46.442240      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:46.452: INFO: scanned /root for discovery docs: <nil>
  Jul 29 16:40:46.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0729 16:40:47.443271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:47.659: INFO: stderr: ""
  Jul 29 16:40:47.659: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 16:40:47.659
  Jul 29 16:40:47.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:40:47.850: INFO: stderr: ""
  Jul 29 16:40:47.850: INFO: stdout: "update-demo-nautilus-r45wl update-demo-nautilus-s88hq "
  Jul 29 16:40:47.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-r45wl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:40:48.003: INFO: stderr: ""
  Jul 29 16:40:48.003: INFO: stdout: ""
  Jul 29 16:40:48.003: INFO: update-demo-nautilus-r45wl is created but not running
  E0729 16:40:48.444008      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:49.444383      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:50.444641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:51.444889      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:52.445234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:53.004: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:40:53.167: INFO: stderr: ""
  Jul 29 16:40:53.167: INFO: stdout: "update-demo-nautilus-r45wl update-demo-nautilus-s88hq "
  Jul 29 16:40:53.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-r45wl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:40:53.313: INFO: stderr: ""
  Jul 29 16:40:53.313: INFO: stdout: "true"
  Jul 29 16:40:53.313: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-r45wl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0729 16:40:53.445188      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:40:53.472: INFO: stderr: ""
  Jul 29 16:40:53.472: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 16:40:53.472: INFO: validating pod update-demo-nautilus-r45wl
  Jul 29 16:40:53.495: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 16:40:53.495: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 16:40:53.495: INFO: update-demo-nautilus-r45wl is verified up and running
  Jul 29 16:40:53.495: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-s88hq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:40:53.652: INFO: stderr: ""
  Jul 29 16:40:53.652: INFO: stdout: "true"
  Jul 29 16:40:53.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods update-demo-nautilus-s88hq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 16:40:53.803: INFO: stderr: ""
  Jul 29 16:40:53.803: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 16:40:53.803: INFO: validating pod update-demo-nautilus-s88hq
  Jul 29 16:40:53.814: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 16:40:53.814: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 16:40:53.814: INFO: update-demo-nautilus-s88hq is verified up and running
  STEP: using delete to clean up resources @ 07/29/23 16:40:53.814
  Jul 29 16:40:53.815: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 delete --grace-period=0 --force -f -'
  Jul 29 16:40:53.975: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:40:53.975: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 29 16:40:53.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get rc,svc -l name=update-demo --no-headers'
  Jul 29 16:40:54.171: INFO: stderr: "No resources found in kubectl-4346 namespace.\n"
  Jul 29 16:40:54.171: INFO: stdout: ""
  Jul 29 16:40:54.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-4346 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 16:40:54.365: INFO: stderr: ""
  Jul 29 16:40:54.365: INFO: stdout: ""
  Jul 29 16:40:54.365: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4346" for this suite. @ 07/29/23 16:40:54.381
• [32.955 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 07/29/23 16:40:54.397
  Jul 29 16:40:54.397: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:40:54.399
  E0729 16:40:54.446067      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:40:54.447
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:40:54.454
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:40:54.459
  E0729 16:40:55.446833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:56.447467      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:57.447457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:40:58.447636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:40:58.496
  Jul 29 16:40:58.502: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-6fee65aa-5e33-42c1-8fa2-fe46db7320eb container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:40:58.516
  Jul 29 16:40:58.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9818" for this suite. @ 07/29/23 16:40:58.554
• [4.168 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 07/29/23 16:40:58.568
  Jul 29 16:40:58.568: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:40:58.569
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:40:58.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:40:58.606
  STEP: Creating secret with name s-test-opt-del-ce6499bc-5202-471c-84b8-e796f4869da5 @ 07/29/23 16:40:58.618
  STEP: Creating secret with name s-test-opt-upd-49995211-469f-4661-91cd-c604462a00ae @ 07/29/23 16:40:58.627
  STEP: Creating the pod @ 07/29/23 16:40:58.632
  E0729 16:40:59.448669      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:00.449238      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-ce6499bc-5202-471c-84b8-e796f4869da5 @ 07/29/23 16:41:00.709
  STEP: Updating secret s-test-opt-upd-49995211-469f-4661-91cd-c604462a00ae @ 07/29/23 16:41:00.719
  STEP: Creating secret with name s-test-opt-create-9d142ce1-cded-4b05-ba8d-4df44115859e @ 07/29/23 16:41:00.728
  STEP: waiting to observe update in volume @ 07/29/23 16:41:00.736
  E0729 16:41:01.449705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:02.449744      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:03.450016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:04.450636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:05.450861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:06.451274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:07.452203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:08.452621      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:09.453737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:10.454432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:11.454588      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:12.454859      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:13.455490      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:14.455789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:15.456566      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:16.457360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:17.456836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:18.457161      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:19.457877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:20.457972      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:21.458412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:22.458603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:23.458940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:24.459530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:25.459466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:26.459909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:27.460187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:28.460481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:29.461074      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:30.462151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:31.462342      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:32.462795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:33.462737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:34.463386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:35.463709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:36.463881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:37.464610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:38.464748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:39.465721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:40.466712      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:41.467524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:42.467695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:43.468596      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:44.469045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:45.469888      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:46.470221      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:47.471311      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:48.471509      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:49.471595      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:50.472329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:51.472572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:52.472436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:53.473316      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:54.473791      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:55.474381      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:56.475130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:57.476119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:58.476255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:41:59.477297      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:00.478329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:01.478701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:02.479351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:03.480290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:04.481071      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:05.484980      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:06.482880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:07.483140      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:08.483630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:09.484662      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:10.485548      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:11.485823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:12.486045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:13.486673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:14.487442      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:15.488149      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:16.488640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:17.489560      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:18.490143      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:19.490409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:20.490584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:21.491365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:22.490991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:23.491792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:42:23.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1927" for this suite. @ 07/29/23 16:42:23.633
• [85.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 07/29/23 16:42:23.652
  Jul 29 16:42:23.652: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:42:23.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:23.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:23.701
  STEP: fetching services @ 07/29/23 16:42:23.705
  Jul 29 16:42:23.713: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-7335" for this suite. @ 07/29/23 16:42:23.727
• [0.087 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 07/29/23 16:42:23.74
  Jul 29 16:42:23.740: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:42:23.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:23.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:23.806
  STEP: Setting up server cert @ 07/29/23 16:42:23.848
  E0729 16:42:24.491624      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:42:24.782
  STEP: Deploying the webhook pod @ 07/29/23 16:42:24.799
  STEP: Wait for the deployment to be ready @ 07/29/23 16:42:24.824
  Jul 29 16:42:24.853: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 16:42:25.492097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:26.492733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 16:42:26.874
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:42:26.897
  E0729 16:42:27.493835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:42:27.897: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 16:42:27.908: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-5543-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 16:42:28.432
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/29/23 16:42:28.473
  E0729 16:42:28.494338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:29.495364      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:30.496005      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:42:30.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6312" for this suite. @ 07/29/23 16:42:31.373
  STEP: Destroying namespace "webhook-markers-7426" for this suite. @ 07/29/23 16:42:31.39
• [7.668 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 07/29/23 16:42:31.41
  Jul 29 16:42:31.410: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:42:31.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:31.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:31.451
  E0729 16:42:31.496830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 07/29/23 16:42:31.505
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:42:31.902
  STEP: Deploying the webhook pod @ 07/29/23 16:42:31.911
  STEP: Wait for the deployment to be ready @ 07/29/23 16:42:31.929
  Jul 29 16:42:31.944: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 16:42:32.529835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:33.529852      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 16:42:33.963
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:42:33.987
  E0729 16:42:34.531582      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:42:34.988: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 07/29/23 16:42:34.998
  Jul 29 16:42:35.028: INFO: Waiting for webhook configuration to be ready...
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 16:42:35.146
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 07/29/23 16:42:35.166
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 16:42:35.183
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 07/29/23 16:42:35.197
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 16:42:35.208
  Jul 29 16:42:35.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1918" for this suite. @ 07/29/23 16:42:35.332
  STEP: Destroying namespace "webhook-markers-4923" for this suite. @ 07/29/23 16:42:35.341
• [3.944 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 07/29/23 16:42:35.369
  Jul 29 16:42:35.369: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:42:35.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:35.401
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:35.407
  STEP: Creating the pod @ 07/29/23 16:42:35.413
  E0729 16:42:35.532508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:36.533057      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:37.533706      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:42:37.994: INFO: Successfully updated pod "annotationupdatee7efa21d-b11f-4015-af14-0c68641e98b0"
  E0729 16:42:38.533731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:39.533993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:42:40.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1363" for this suite. @ 07/29/23 16:42:40.035
• [4.685 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 07/29/23 16:42:40.067
  Jul 29 16:42:40.067: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:42:40.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:40.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:40.108
  STEP: Creating a pod to test downward api env vars @ 07/29/23 16:42:40.111
  E0729 16:42:40.535821      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:41.535346      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:42.535360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:43.535620      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:42:44.148
  Jul 29 16:42:44.154: INFO: Trying to get logs from node vucheipi7kei-3 pod downward-api-f865981c-4571-4a92-8d4c-7b0c0e87fe9b container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:42:44.169
  Jul 29 16:42:44.189: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-987" for this suite. @ 07/29/23 16:42:44.196
• [4.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 07/29/23 16:42:44.21
  Jul 29 16:42:44.210: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:42:44.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:44.242
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:44.246
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/29/23 16:42:44.252
  E0729 16:42:44.536451      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:45.537709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:46.538799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:47.539408      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:42:48.291
  Jul 29 16:42:48.298: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-a8c0c6e0-1823-4a30-b5d7-6e9e09d4284d container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:42:48.314
  Jul 29 16:42:48.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3142" for this suite. @ 07/29/23 16:42:48.364
• [4.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 07/29/23 16:42:48.382
  Jul 29 16:42:48.382: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 16:42:48.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:48.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:48.419
  Jul 29 16:42:48.424: INFO: Creating ReplicaSet my-hostname-basic-f4b72f6a-1c6c-4e2c-85d3-3e825fd38a43
  Jul 29 16:42:48.443: INFO: Pod name my-hostname-basic-f4b72f6a-1c6c-4e2c-85d3-3e825fd38a43: Found 0 pods out of 1
  E0729 16:42:48.539919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:49.540705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:50.541119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:51.541789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:52.542064      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:42:53.449: INFO: Pod name my-hostname-basic-f4b72f6a-1c6c-4e2c-85d3-3e825fd38a43: Found 1 pods out of 1
  Jul 29 16:42:53.449: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-f4b72f6a-1c6c-4e2c-85d3-3e825fd38a43" is running
  Jul 29 16:42:53.452: INFO: Pod "my-hostname-basic-f4b72f6a-1c6c-4e2c-85d3-3e825fd38a43-4t67n" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:42:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:42:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:42:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:42:48 +0000 UTC Reason: Message:}])
  Jul 29 16:42:53.452: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/29/23 16:42:53.452
  Jul 29 16:42:53.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7619" for this suite. @ 07/29/23 16:42:53.48
  E0729 16:42:53.543002      14 retrywatcher.go:130] "Watch failed" err="context canceled"
• [5.167 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 07/29/23 16:42:53.55
  Jul 29 16:42:53.550: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:42:53.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:53.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:53.591
  STEP: creating the pod @ 07/29/23 16:42:53.599
  STEP: setting up watch @ 07/29/23 16:42:53.6
  STEP: submitting the pod to kubernetes @ 07/29/23 16:42:53.709
  STEP: verifying the pod is in kubernetes @ 07/29/23 16:42:53.729
  STEP: verifying pod creation was observed @ 07/29/23 16:42:53.748
  E0729 16:42:54.542770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:55.542831      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 07/29/23 16:42:55.788
  STEP: verifying pod deletion was observed @ 07/29/23 16:42:55.801
  E0729 16:42:56.543766      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:57.544190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:42:58.544651      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:42:58.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7099" for this suite. @ 07/29/23 16:42:58.761
• [5.222 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 07/29/23 16:42:58.774
  Jul 29 16:42:58.774: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 16:42:58.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:58.816
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:58.823
  Jul 29 16:42:58.836: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 16:42:59.545696      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:00.546165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:01.546973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 16:43:01.685
  Jul 29 16:43:01.685: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-7430 --namespace=crd-publish-openapi-7430 create -f -'
  E0729 16:43:02.546669      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:43:03.221: INFO: stderr: ""
  Jul 29 16:43:03.221: INFO: stdout: "e2e-test-crd-publish-openapi-4281-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 29 16:43:03.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-7430 --namespace=crd-publish-openapi-7430 delete e2e-test-crd-publish-openapi-4281-crds test-cr'
  Jul 29 16:43:03.466: INFO: stderr: ""
  Jul 29 16:43:03.466: INFO: stdout: "e2e-test-crd-publish-openapi-4281-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jul 29 16:43:03.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-7430 --namespace=crd-publish-openapi-7430 apply -f -'
  E0729 16:43:03.547666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:43:03.943: INFO: stderr: ""
  Jul 29 16:43:03.943: INFO: stdout: "e2e-test-crd-publish-openapi-4281-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 29 16:43:03.943: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-7430 --namespace=crd-publish-openapi-7430 delete e2e-test-crd-publish-openapi-4281-crds test-cr'
  Jul 29 16:43:04.089: INFO: stderr: ""
  Jul 29 16:43:04.089: INFO: stdout: "e2e-test-crd-publish-openapi-4281-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/29/23 16:43:04.089
  Jul 29 16:43:04.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-7430 explain e2e-test-crd-publish-openapi-4281-crds'
  Jul 29 16:43:04.523: INFO: stderr: ""
  Jul 29 16:43:04.523: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-4281-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0729 16:43:04.548046      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:05.548301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:06.549355      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:43:06.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7430" for this suite. @ 07/29/23 16:43:06.671
• [7.912 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 07/29/23 16:43:06.687
  Jul 29 16:43:06.687: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 16:43:06.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:06.715
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:06.719
  Jul 29 16:43:06.724: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 16:43:07.550254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:43:07.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1997" for this suite. @ 07/29/23 16:43:07.781
• [1.109 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 07/29/23 16:43:07.796
  Jul 29 16:43:07.797: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 16:43:07.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:07.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:07.835
  E0729 16:43:08.550492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:09.550488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:10.550691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:11.550850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:43:11.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8360" for this suite. @ 07/29/23 16:43:11.897
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 07/29/23 16:43:11.92
  Jul 29 16:43:11.920: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename disruption @ 07/29/23 16:43:11.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:11.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:11.955
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:43:11.968
  E0729 16:43:12.551193      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:13.551697      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating PodDisruptionBudget status @ 07/29/23 16:43:13.981
  STEP: Waiting for all pods to be running @ 07/29/23 16:43:13.996
  Jul 29 16:43:14.004: INFO: running pods: 0 < 1
  E0729 16:43:14.552525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:15.552852      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 07/29/23 16:43:16.016
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:43:16.038
  STEP: Patching PodDisruptionBudget status @ 07/29/23 16:43:16.055
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:43:16.073
  Jul 29 16:43:16.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3950" for this suite. @ 07/29/23 16:43:16.085
• [4.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 07/29/23 16:43:16.096
  Jul 29 16:43:16.096: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 16:43:16.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:16.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:16.13
  STEP: Read namespace status @ 07/29/23 16:43:16.135
  Jul 29 16:43:16.140: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 07/29/23 16:43:16.14
  Jul 29 16:43:16.149: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 07/29/23 16:43:16.149
  Jul 29 16:43:16.164: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jul 29 16:43:16.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2214" for this suite. @ 07/29/23 16:43:16.174
• [0.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 07/29/23 16:43:16.217
  Jul 29 16:43:16.217: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:43:16.219
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:16.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:16.25
  STEP: Creating pod liveness-7c8fc593-d4c4-41e2-8133-d535aae8363d in namespace container-probe-6410 @ 07/29/23 16:43:16.254
  E0729 16:43:16.552998      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:17.553024      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:43:18.282: INFO: Started pod liveness-7c8fc593-d4c4-41e2-8133-d535aae8363d in namespace container-probe-6410
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:43:18.282
  Jul 29 16:43:18.287: INFO: Initial restart count of pod liveness-7c8fc593-d4c4-41e2-8133-d535aae8363d is 0
  E0729 16:43:18.554003      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:19.555038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:20.555448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:21.556099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:22.556556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:23.556944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:24.557785      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:25.558402      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:26.559111      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:27.559530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:28.559436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:29.560479      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:30.560166      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:31.561030      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:32.560392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:33.560824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:34.561716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:35.561576      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:36.562263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:37.562409      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:43:38.379: INFO: Restart count of pod container-probe-6410/liveness-7c8fc593-d4c4-41e2-8133-d535aae8363d is now 1 (20.091190912s elapsed)
  E0729 16:43:38.562884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:39.563164      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:40.563772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:41.563903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:42.564933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:43.565036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:44.566189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:45.566211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:46.566989      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:47.567949      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:48.568712      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:49.568919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:50.569944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:51.570671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:52.571487      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:53.571551      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:54.572739      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:55.573648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:56.574519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:57.575240      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:43:58.469: INFO: Restart count of pod container-probe-6410/liveness-7c8fc593-d4c4-41e2-8133-d535aae8363d is now 2 (40.182001183s elapsed)
  E0729 16:43:58.576248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:43:59.576547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:00.577293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:01.577739      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:02.578438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:03.578400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:04.578950      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:05.579782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:06.580048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:07.580723      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:08.580656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:09.581658      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:10.582027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:11.582352      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:12.582692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:13.583376      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:14.584211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:15.584226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:16.584369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:17.585262      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:44:18.559: INFO: Restart count of pod container-probe-6410/liveness-7c8fc593-d4c4-41e2-8133-d535aae8363d is now 3 (1m0.271831302s elapsed)
  E0729 16:44:18.585768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:19.586055      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:20.587171      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:21.587466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:22.587917      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:23.588639      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:24.589167      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:25.590319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:26.591015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:27.591389      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:28.592137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:29.592641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:30.593693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:31.593842      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:32.593868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:33.594227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:34.594237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:35.594442      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:36.595123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:37.596572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:38.596634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:44:38.646: INFO: Restart count of pod container-probe-6410/liveness-7c8fc593-d4c4-41e2-8133-d535aae8363d is now 4 (1m20.35816161s elapsed)
  E0729 16:44:39.597401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:40.597963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:41.598374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:42.598385      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:43.598421      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:44.598685      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:45.598975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:46.599290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:47.599300      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:48.600237      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:49.600370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:50.600674      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:51.600828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:52.601134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:53.601662      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:54.602431      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:55.602303      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:56.603092      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:57.603131      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:58.603908      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:44:59.604150      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:00.604654      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:01.604988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:02.605905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:03.606345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:04.606579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:05.607328      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:06.608641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:07.608822      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:08.608954      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:09.609147      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:10.609351      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:11.609685      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:12.610189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:13.611082      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:14.611118      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:15.612115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:16.613848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:17.614222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:18.614944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:19.615835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:20.615876      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:21.616097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:22.616952      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:23.617170      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:24.617882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:25.618755      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:26.619121      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:27.619276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:28.619866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:29.620033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:30.620281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:31.620521      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:32.621683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:33.621749      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:34.622065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:35.622621      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:36.622678      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:37.622842      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:38.623410      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:39.623757      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:40.624095      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:45:40.933: INFO: Restart count of pod container-probe-6410/liveness-7c8fc593-d4c4-41e2-8133-d535aae8363d is now 5 (2m22.645138789s elapsed)
  Jul 29 16:45:40.934: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:45:40.942
  STEP: Destroying namespace "container-probe-6410" for this suite. @ 07/29/23 16:45:40.963
• [144.756 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 07/29/23 16:45:40.975
  Jul 29 16:45:40.975: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:45:40.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:45:41.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:45:41.018
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 16:45:41.024
  Jul 29 16:45:41.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-9284 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 29 16:45:41.171: INFO: stderr: ""
  Jul 29 16:45:41.171: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 07/29/23 16:45:41.171
  E0729 16:45:41.624830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:42.625980      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:43.626142      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:44.626649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:45.626680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/29/23 16:45:46.223
  Jul 29 16:45:46.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-9284 get pod e2e-test-httpd-pod -o json'
  Jul 29 16:45:46.380: INFO: stderr: ""
  Jul 29 16:45:46.380: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-29T16:45:41Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9284\",\n        \"resourceVersion\": \"28160\",\n        \"uid\": \"e8f7e8f4-a5dd-4474-968e-4bcbc19117dd\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-r49sw\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"vucheipi7kei-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-r49sw\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:45:41Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:45:42Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:45:42Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T16:45:41Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://b3286cbb84a59d89962510a5cab7bff0a890499e0742cc0c62ff09397387d95e\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-29T16:45:42Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.129\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.65.78\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.65.78\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-29T16:45:41Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 07/29/23 16:45:46.38
  Jul 29 16:45:46.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-9284 replace -f -'
  E0729 16:45:46.627502      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:45:47.231: INFO: stderr: ""
  Jul 29 16:45:47.231: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 07/29/23 16:45:47.231
  Jul 29 16:45:47.240: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-9284 delete pods e2e-test-httpd-pod'
  E0729 16:45:47.627927      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:48.628869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:45:49.432: INFO: stderr: ""
  Jul 29 16:45:49.432: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 16:45:49.432: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9284" for this suite. @ 07/29/23 16:45:49.443
• [8.496 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 07/29/23 16:45:49.483
  Jul 29 16:45:49.483: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename endpointslicemirroring @ 07/29/23 16:45:49.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:45:49.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:45:49.579
  STEP: mirroring a new custom Endpoint @ 07/29/23 16:45:49.601
  Jul 29 16:45:49.617: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0729 16:45:49.629991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:50.630175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 07/29/23 16:45:51.625
  E0729 16:45:51.631403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:45:51.638: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0729 16:45:52.631499      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:53.631685      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 07/29/23 16:45:53.667
  Jul 29 16:45:53.687: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0729 16:45:54.632407      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:55.633329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:45:55.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-7849" for this suite. @ 07/29/23 16:45:55.705
• [6.231 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 07/29/23 16:45:55.715
  Jul 29 16:45:55.715: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:45:55.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:45:55.74
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:45:55.759
  STEP: creating a Pod with a static label @ 07/29/23 16:45:55.792
  STEP: watching for Pod to be ready @ 07/29/23 16:45:55.807
  Jul 29 16:45:55.811: INFO: observed Pod pod-test in namespace pods-1041 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jul 29 16:45:55.816: INFO: observed Pod pod-test in namespace pods-1041 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:55 +0000 UTC  }]
  Jul 29 16:45:55.839: INFO: observed Pod pod-test in namespace pods-1041 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:55 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:55 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:55 +0000 UTC  }]
  E0729 16:45:56.633646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:45:57.402: INFO: Found Pod pod-test in namespace pods-1041 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:55 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:57 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:57 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:45:55 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 07/29/23 16:45:57.407
  STEP: getting the Pod and ensuring that it's patched @ 07/29/23 16:45:57.422
  STEP: replacing the Pod's status Ready condition to False @ 07/29/23 16:45:57.436
  STEP: check the Pod again to ensure its Ready conditions are False @ 07/29/23 16:45:57.459
  STEP: deleting the Pod via a Collection with a LabelSelector @ 07/29/23 16:45:57.459
  STEP: watching for the Pod to be deleted @ 07/29/23 16:45:57.471
  Jul 29 16:45:57.478: INFO: observed event type MODIFIED
  E0729 16:45:57.634257      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:45:58.634930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:45:59.418: INFO: observed event type MODIFIED
  Jul 29 16:45:59.548: INFO: observed event type MODIFIED
  E0729 16:45:59.635976      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:00.423: INFO: observed event type MODIFIED
  Jul 29 16:46:00.462: INFO: observed event type MODIFIED
  Jul 29 16:46:00.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1041" for this suite. @ 07/29/23 16:46:00.483
• [4.780 seconds]
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 07/29/23 16:46:00.496
  Jul 29 16:46:00.496: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:46:00.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:00.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:00.532
  STEP: Creating configMap configmap-1461/configmap-test-3400dc84-c53a-49c3-81a2-02ed72a79ffe @ 07/29/23 16:46:00.541
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:46:00.55
  E0729 16:46:00.636666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:01.637205      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:02.637883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:03.638554      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:46:04.578
  Jul 29 16:46:04.583: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-07a12013-2454-407d-8896-1fde4933b5a2 container env-test: <nil>
  STEP: delete the pod @ 07/29/23 16:46:04.615
  Jul 29 16:46:04.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 16:46:04.638941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "configmap-1461" for this suite. @ 07/29/23 16:46:04.648
• [4.170 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 07/29/23 16:46:04.675
  Jul 29 16:46:04.675: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename job @ 07/29/23 16:46:04.676
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:04.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:04.713
  STEP: Creating a job @ 07/29/23 16:46:04.717
  STEP: Ensure pods equal to parallelism count is attached to the job @ 07/29/23 16:46:04.726
  E0729 16:46:05.639589      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:06.639950      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 07/29/23 16:46:06.739
  STEP: updating /status @ 07/29/23 16:46:06.754
  STEP: get /status @ 07/29/23 16:46:06.8
  Jul 29 16:46:06.809: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5442" for this suite. @ 07/29/23 16:46:06.819
• [2.156 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 07/29/23 16:46:06.832
  Jul 29 16:46:06.832: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 16:46:06.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:06.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:06.864
  E0729 16:46:07.640245      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:08.640632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:09.640632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:10.641742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:10.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1029" for this suite. @ 07/29/23 16:46:10.908
• [4.089 seconds]
------------------------------
SSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 07/29/23 16:46:10.922
  Jul 29 16:46:10.922: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:46:10.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:10.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:10.954
  STEP: Creating configMap that has name configmap-test-emptyKey-84d81a97-3f87-4c41-a721-5fba3fe5ef82 @ 07/29/23 16:46:10.957
  Jul 29 16:46:10.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-978" for this suite. @ 07/29/23 16:46:10.968
• [0.057 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 07/29/23 16:46:10.981
  Jul 29 16:46:10.981: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 16:46:10.984
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:11.009
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:11.014
  STEP: Create a ReplicaSet @ 07/29/23 16:46:11.018
  STEP: Verify that the required pods have come up @ 07/29/23 16:46:11.037
  Jul 29 16:46:11.044: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0729 16:46:11.642210      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:12.642403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:13.643076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:14.643189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:15.643388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:16.051: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 07/29/23 16:46:16.051
  Jul 29 16:46:16.060: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 07/29/23 16:46:16.06
  STEP: DeleteCollection of the ReplicaSets @ 07/29/23 16:46:16.065
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 07/29/23 16:46:16.08
  Jul 29 16:46:16.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9097" for this suite. @ 07/29/23 16:46:16.092
• [5.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 07/29/23 16:46:16.112
  Jul 29 16:46:16.112: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:46:16.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:16.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:16.231
  STEP: creating the pod @ 07/29/23 16:46:16.239
  STEP: submitting the pod to kubernetes @ 07/29/23 16:46:16.239
  W0729 16:46:16.258278      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0729 16:46:16.644162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:17.644216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 07/29/23 16:46:18.294
  STEP: updating the pod @ 07/29/23 16:46:18.3
  E0729 16:46:18.645025      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:18.820: INFO: Successfully updated pod "pod-update-activedeadlineseconds-e37767a0-1b4d-408d-8ba7-fa6b36305fa7"
  E0729 16:46:19.645724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:20.645831      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:21.649429      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:22.649797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:22.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5482" for this suite. @ 07/29/23 16:46:22.868
• [6.767 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 07/29/23 16:46:22.889
  Jul 29 16:46:22.890: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:46:22.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:22.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:22.929
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8244 @ 07/29/23 16:46:22.935
  STEP: changing the ExternalName service to type=ClusterIP @ 07/29/23 16:46:22.946
  STEP: creating replication controller externalname-service in namespace services-8244 @ 07/29/23 16:46:22.989
  I0729 16:46:23.005951      14 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8244, replica count: 2
  E0729 16:46:23.650401      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:24.650958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:25.651204      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 16:46:26.058344      14 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:46:26.058: INFO: Creating new exec pod
  E0729 16:46:26.660735      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:27.653801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:28.654857      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:29.094: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-8244 exec execpodcldw4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 29 16:46:29.384: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 29 16:46:29.384: INFO: stdout: "externalname-service-k66gb"
  Jul 29 16:46:29.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-8244 exec execpodcldw4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.89 80'
  Jul 29 16:46:29.649: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.89 80\nConnection to 10.233.7.89 80 port [tcp/http] succeeded!\n"
  Jul 29 16:46:29.649: INFO: stdout: "externalname-service-k66gb"
  Jul 29 16:46:29.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 16:46:29.655325      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:29.657: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-8244" for this suite. @ 07/29/23 16:46:29.707
• [6.839 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 07/29/23 16:46:29.73
  Jul 29 16:46:29.730: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 16:46:29.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:29.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:29.796
  Jul 29 16:46:29.844: INFO: Create a RollingUpdate DaemonSet
  Jul 29 16:46:29.853: INFO: Check that daemon pods launch on every node of the cluster
  Jul 29 16:46:29.868: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:46:29.868: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 16:46:30.655905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:30.885: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:46:30.886: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 16:46:31.655984      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:31.887: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 16:46:31.887: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  E0729 16:46:32.657107      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:32.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 16:46:32.889: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jul 29 16:46:32.889: INFO: Update the DaemonSet to trigger a rollout
  Jul 29 16:46:32.904: INFO: Updating DaemonSet daemon-set
  E0729 16:46:33.657324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:33.952: INFO: Roll back the DaemonSet before rollout is complete
  Jul 29 16:46:33.966: INFO: Updating DaemonSet daemon-set
  Jul 29 16:46:33.966: INFO: Make sure DaemonSet rollback is complete
  Jul 29 16:46:33.975: INFO: Wrong image for pod: daemon-set-f55hz. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jul 29 16:46:33.975: INFO: Pod daemon-set-f55hz is not available
  E0729 16:46:34.658163      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:35.658882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:36.659078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:37.659946      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:38.660294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:39.660806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:40.009: INFO: Pod daemon-set-gqhvl is not available
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 16:46:40.033
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9161, will wait for the garbage collector to delete the pods @ 07/29/23 16:46:40.034
  Jul 29 16:46:40.101: INFO: Deleting DaemonSet.extensions daemon-set took: 11.862784ms
  Jul 29 16:46:40.202: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.931077ms
  E0729 16:46:40.660853      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:41.661101      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:42.412: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:46:42.412: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 16:46:42.419: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28802"},"items":null}

  Jul 29 16:46:42.432: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28802"},"items":null}

  Jul 29 16:46:42.460: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9161" for this suite. @ 07/29/23 16:46:42.469
• [12.753 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 07/29/23 16:46:42.485
  Jul 29 16:46:42.485: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 16:46:42.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:42.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:42.557
  STEP: Creating a pod to test service account token:  @ 07/29/23 16:46:42.561
  E0729 16:46:42.661336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:43.661647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:44.662657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:45.662683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:46:46.601
  Jul 29 16:46:46.607: INFO: Trying to get logs from node vucheipi7kei-3 pod test-pod-5dfc8d07-729b-435e-8978-46275b47edfa container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:46:46.618
  Jul 29 16:46:46.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8542" for this suite. @ 07/29/23 16:46:46.649
• [4.173 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 07/29/23 16:46:46.659
  Jul 29 16:46:46.659: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename dns @ 07/29/23 16:46:46.662
  E0729 16:46:46.662842      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:46.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:46.69
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/29/23 16:46:46.694
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/29/23 16:46:46.694
  STEP: creating a pod to probe DNS @ 07/29/23 16:46:46.695
  STEP: submitting the pod to kubernetes @ 07/29/23 16:46:46.695
  E0729 16:46:47.663317      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:48.664388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:49.663966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:50.665014      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 16:46:50.734
  STEP: looking for the results for each expected name from probers @ 07/29/23 16:46:50.742
  Jul 29 16:46:50.779: INFO: DNS probes using dns-9656/dns-test-b3058ef5-1a99-47f8-88a3-fc6695bbda56 succeeded

  Jul 29 16:46:50.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:46:50.788
  STEP: Destroying namespace "dns-9656" for this suite. @ 07/29/23 16:46:50.854
• [4.205 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 07/29/23 16:46:50.869
  Jul 29 16:46:50.869: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:46:50.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:46:50.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:46:50.921
  STEP: Creating pod test-grpc-c5d0a83e-f1a6-4747-89f0-8bdca24a0780 in namespace container-probe-8515 @ 07/29/23 16:46:50.925
  E0729 16:46:51.665834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:52.665875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:46:52.950: INFO: Started pod test-grpc-c5d0a83e-f1a6-4747-89f0-8bdca24a0780 in namespace container-probe-8515
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:46:52.95
  Jul 29 16:46:52.956: INFO: Initial restart count of pod test-grpc-c5d0a83e-f1a6-4747-89f0-8bdca24a0780 is 0
  E0729 16:46:53.665814      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:54.666349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:55.666768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:56.667598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:57.668765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:58.669242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:46:59.669316      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:00.669519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:01.669616      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:02.670045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:03.670434      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:04.670334      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:05.670676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:06.672189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:07.672349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:08.672762      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:09.673417      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:10.674038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:11.674379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:12.675052      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:13.675999      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:14.676299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:15.677533      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:16.678394      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:17.678782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:18.679495      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:19.679619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:20.679895      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:21.681705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:22.682371      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:23.682988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:24.683172      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:25.683378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:26.683837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:27.684024      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:28.684531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:29.684979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:30.685940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:31.688078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:32.686652      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:33.687558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:34.687670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:35.688814      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:36.689080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:37.689191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:38.689519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:39.689617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:40.689748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:41.689805      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:42.690264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:43.690306      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:44.690626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:45.691100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:46.691256      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:47.691541      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:48.691742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:49.692365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:50.692753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:51.692765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:52.692999      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:53.693683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:54.693800      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:55.693956      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:56.694264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:57.694428      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:58.694646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:47:59.696319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:00.695893      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:01.696031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:02.696427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:03.697186      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:04.697732      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:05.697869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:06.698305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:07.700577      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:08.699593      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:09.700146      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:10.700264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:11.700578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:12.701112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:13.702063      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:14.702231      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:15.703624      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:16.703960      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:17.704426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:18.704586      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:19.705531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:20.705806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:21.706879      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:22.707584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:23.708374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:24.709162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:25.709919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:26.710513      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:27.710799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:28.713397      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:29.712709      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:30.719352      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:31.717184      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:32.716048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:33.717069      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:34.717778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:35.717973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:36.718915      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:37.719918      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:38.719943      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:39.720838      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:40.721942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:41.722912      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:42.723102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:43.724165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:44.724656      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:45.725407      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:46.726283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:47.726444      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:48.726981      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:49.727582      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:50.727830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:51.728947      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:52.729078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:53.729209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:54.729945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:55.729818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:56.730078      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:57.731657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:58.730614      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:48:59.731565      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:00.732140      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:01.732760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:02.733294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:03.734092      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:04.734577      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:05.734773      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:06.737648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:07.737285      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:08.737788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:09.738796      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:10.739501      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:11.740482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:12.741038      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:13.741255      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:14.741742      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:15.742228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:16.743044      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:17.743249      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:18.743799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:19.744178      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:20.744943      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:21.746051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:22.746689      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:23.747692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:24.748215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:25.749075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:26.749473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:27.753780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:28.754050      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:29.754634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:30.754875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:31.755878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:32.756488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:33.757592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:34.758469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:35.758602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:36.759217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:37.760259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:38.760918      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:39.761324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:40.761701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:41.762653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:42.762970      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:43.763123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:44.763427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:45.763441      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:46.764502      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:47.764719      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:48.765097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:49.765528      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:50.765772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:51.766422      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:52.766937      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:53.766872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:54.767430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:55.767412      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:56.767519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:57.767818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:58.768120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:49:59.768365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:00.768561      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:01.769156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:02.769673      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:03.770043      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:04.770872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:05.772007      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:06.772496      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:07.772475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:08.772796      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:09.773423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:10.773700      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:11.773740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:12.774877      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:13.775453      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:14.776761      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:15.776887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:16.777280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:17.777495      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:18.777701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:19.778051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:20.778183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:21.778872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:22.779507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:23.780170      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:24.781119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:25.780636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:26.780820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:27.781334      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:28.781797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:29.781788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:30.781983      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:31.782100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:32.782386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:33.782906      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:34.783260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:35.783276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:36.783431      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:37.783691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:38.783806      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:39.783955      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:40.784001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:41.784544      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:42.784705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:43.785451      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:44.786435      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:45.786557      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:46.787815      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:47.788211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:48.788530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:49.788598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:50.788990      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:51.789045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:52.789687      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:53.790293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:50:54.081: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:50:54.092
  STEP: Destroying namespace "container-probe-8515" for this suite. @ 07/29/23 16:50:54.12
• [243.273 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 07/29/23 16:50:54.174
  Jul 29 16:50:54.175: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename gc @ 07/29/23 16:50:54.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:50:54.205
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:50:54.209
  STEP: create the deployment @ 07/29/23 16:50:54.214
  W0729 16:50:54.224394      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/29/23 16:50:54.224
  STEP: delete the deployment @ 07/29/23 16:50:54.742
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 07/29/23 16:50:54.756
  E0729 16:50:54.790350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/29/23 16:50:55.315
  Jul 29 16:50:55.496: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 16:50:55.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8872" for this suite. @ 07/29/23 16:50:55.516
• [1.356 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 07/29/23 16:50:55.534
  Jul 29 16:50:55.534: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 16:50:55.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:50:55.57
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:50:55.575
  STEP: create the container @ 07/29/23 16:50:55.58
  W0729 16:50:55.603694      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 16:50:55.603
  E0729 16:50:55.790646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:56.791307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:57.791969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/29/23 16:50:58.683
  STEP: the container should be terminated @ 07/29/23 16:50:58.689
  STEP: the termination message should be set @ 07/29/23 16:50:58.689
  Jul 29 16:50:58.690: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 07/29/23 16:50:58.69
  Jul 29 16:50:58.714: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8423" for this suite. @ 07/29/23 16:50:58.728
• [3.204 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 07/29/23 16:50:58.741
  Jul 29 16:50:58.741: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 16:50:58.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:50:58.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:50:58.773
  Jul 29 16:50:58.776: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 16:50:58.792147      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:50:59.792473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:00.793153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0729 16:51:01.547442      14 warnings.go:70] unknown field "alpha"
  W0729 16:51:01.547653      14 warnings.go:70] unknown field "beta"
  W0729 16:51:01.547679      14 warnings.go:70] unknown field "delta"
  W0729 16:51:01.547687      14 warnings.go:70] unknown field "epsilon"
  W0729 16:51:01.547695      14 warnings.go:70] unknown field "gamma"
  E0729 16:51:01.793652      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:51:02.110: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1914" for this suite. @ 07/29/23 16:51:02.138
• [3.408 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 07/29/23 16:51:02.156
  Jul 29 16:51:02.157: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 16:51:02.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:02.19
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:02.197
  STEP: Creating a test namespace @ 07/29/23 16:51:02.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:02.238
  STEP: Creating a pod in the namespace @ 07/29/23 16:51:02.242
  STEP: Waiting for the pod to have running status @ 07/29/23 16:51:02.256
  E0729 16:51:02.794025      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:03.794041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the namespace @ 07/29/23 16:51:04.289
  STEP: Waiting for the namespace to be removed. @ 07/29/23 16:51:04.3
  E0729 16:51:04.794428      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:05.794415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:06.796772      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:07.796948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:08.797392      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:09.798102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:10.798887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:11.799203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:12.800233      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:13.800466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:14.800731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Recreating the namespace @ 07/29/23 16:51:15.307
  STEP: Verifying there are no pods in the namespace @ 07/29/23 16:51:15.334
  Jul 29 16:51:15.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2757" for this suite. @ 07/29/23 16:51:15.347
  STEP: Destroying namespace "nsdeletetest-9505" for this suite. @ 07/29/23 16:51:15.356
  Jul 29 16:51:15.363: INFO: Namespace nsdeletetest-9505 was already deleted
  STEP: Destroying namespace "nsdeletetest-5501" for this suite. @ 07/29/23 16:51:15.363
• [13.214 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 07/29/23 16:51:15.375
  Jul 29 16:51:15.375: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:51:15.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:15.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:15.403
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-6519 @ 07/29/23 16:51:15.407
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/29/23 16:51:15.428
  STEP: creating service externalsvc in namespace services-6519 @ 07/29/23 16:51:15.428
  STEP: creating replication controller externalsvc in namespace services-6519 @ 07/29/23 16:51:15.456
  I0729 16:51:15.469635      14 runners.go:194] Created replication controller with name: externalsvc, namespace: services-6519, replica count: 2
  E0729 16:51:15.800989      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:16.801849      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:17.802164      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 16:51:18.522715      14 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 07/29/23 16:51:18.529
  Jul 29 16:51:18.560: INFO: Creating new exec pod
  E0729 16:51:18.802671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:19.803608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:51:20.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-6519 exec execpodfbwkf -- /bin/sh -x -c nslookup nodeport-service.services-6519.svc.cluster.local'
  E0729 16:51:20.803883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:51:20.980: INFO: stderr: "+ nslookup nodeport-service.services-6519.svc.cluster.local\n"
  Jul 29 16:51:20.980: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-6519.svc.cluster.local\tcanonical name = externalsvc.services-6519.svc.cluster.local.\nName:\texternalsvc.services-6519.svc.cluster.local\nAddress: 10.233.8.170\n\n"
  Jul 29 16:51:20.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-6519, will wait for the garbage collector to delete the pods @ 07/29/23 16:51:20.987
  Jul 29 16:51:21.054: INFO: Deleting ReplicationController externalsvc took: 11.424535ms
  Jul 29 16:51:21.155: INFO: Terminating ReplicationController externalsvc pods took: 100.917577ms
  E0729 16:51:21.804002      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:22.804836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:51:23.489: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-6519" for this suite. @ 07/29/23 16:51:23.513
• [8.154 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 07/29/23 16:51:23.53
  Jul 29 16:51:23.530: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:51:23.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:23.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:23.564
  STEP: Creating configMap with name projected-configmap-test-volume-map-79da7c53-d6f7-4809-acc3-a2558df02b59 @ 07/29/23 16:51:23.568
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:51:23.576
  E0729 16:51:23.805430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:24.805860      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:25.807515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:26.807363      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:51:27.618
  Jul 29 16:51:27.625: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-configmaps-a7ebc50e-6615-439a-a151-39669a5734cc container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:51:27.659
  Jul 29 16:51:27.688: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2441" for this suite. @ 07/29/23 16:51:27.697
• [4.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 07/29/23 16:51:27.722
  Jul 29 16:51:27.723: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 16:51:27.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:27.771
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:27.777
  E0729 16:51:27.807776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:51:27.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4203" for this suite. @ 07/29/23 16:51:27.838
• [0.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 07/29/23 16:51:27.853
  Jul 29 16:51:27.853: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 16:51:27.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:27.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:27.891
  STEP: Deleting RuntimeClass runtimeclass-3198-delete-me @ 07/29/23 16:51:27.903
  STEP: Waiting for the RuntimeClass to disappear @ 07/29/23 16:51:27.912
  Jul 29 16:51:27.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3198" for this suite. @ 07/29/23 16:51:27.936
• [0.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 07/29/23 16:51:27.962
  Jul 29 16:51:27.962: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename ingress @ 07/29/23 16:51:27.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:28.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:28.008
  STEP: getting /apis @ 07/29/23 16:51:28.017
  STEP: getting /apis/networking.k8s.io @ 07/29/23 16:51:28.026
  STEP: getting /apis/networking.k8s.iov1 @ 07/29/23 16:51:28.028
  STEP: creating @ 07/29/23 16:51:28.03
  STEP: getting @ 07/29/23 16:51:28.056
  STEP: listing @ 07/29/23 16:51:28.061
  STEP: watching @ 07/29/23 16:51:28.066
  Jul 29 16:51:28.066: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 16:51:28.067
  STEP: cluster-wide watching @ 07/29/23 16:51:28.074
  Jul 29 16:51:28.074: INFO: starting watch
  STEP: patching @ 07/29/23 16:51:28.076
  STEP: updating @ 07/29/23 16:51:28.084
  Jul 29 16:51:28.098: INFO: waiting for watch events with expected annotations
  Jul 29 16:51:28.099: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/29/23 16:51:28.099
  STEP: updating /status @ 07/29/23 16:51:28.109
  STEP: get /status @ 07/29/23 16:51:28.123
  STEP: deleting @ 07/29/23 16:51:28.128
  STEP: deleting a collection @ 07/29/23 16:51:28.158
  Jul 29 16:51:28.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-5006" for this suite. @ 07/29/23 16:51:28.195
• [0.244 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 07/29/23 16:51:28.206
  Jul 29 16:51:28.206: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:51:28.208
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:28.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:28.244
  Jul 29 16:51:28.254: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6026" for this suite. @ 07/29/23 16:51:28.263
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 07/29/23 16:51:28.282
  Jul 29 16:51:28.282: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename containers @ 07/29/23 16:51:28.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:28.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:28.317
  STEP: Creating a pod to test override all @ 07/29/23 16:51:28.322
  E0729 16:51:28.808494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:29.809301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:30.809214      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:31.809406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:51:32.365
  Jul 29 16:51:32.371: INFO: Trying to get logs from node vucheipi7kei-3 pod client-containers-542ed92f-ee67-4764-81c0-beaa4e1eb6a1 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:51:32.383
  Jul 29 16:51:32.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7592" for this suite. @ 07/29/23 16:51:32.42
• [4.147 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 07/29/23 16:51:32.433
  Jul 29 16:51:32.434: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 16:51:32.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:32.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:32.466
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 07/29/23 16:51:32.499
  E0729 16:51:32.809922      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:33.810332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:34.810740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:35.811931      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:36.812831      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:37.813145      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:38.813221      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:39.814060      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:40.815139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:41.816089      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:42.816170      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:43.816530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:44.817337      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:45.818462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:46.818921      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:47.819201      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:48.820031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:49.820166      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:50.820862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 07/29/23 16:51:51.727
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 07/29/23 16:51:51.733
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 07/29/23 16:51:51.774
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 07/29/23 16:51:51.775
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 07/29/23 16:51:51.815
  E0729 16:51:51.821723      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:52.821833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:53.822413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:54.822325      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 07/29/23 16:51:54.849
  E0729 16:51:55.822492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 07/29/23 16:51:55.866
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 07/29/23 16:51:55.876
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 07/29/23 16:51:55.876
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 07/29/23 16:51:55.92
  E0729 16:51:56.822642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 07/29/23 16:51:56.939
  E0729 16:51:57.822995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:51:58.823065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 07/29/23 16:51:58.963
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 07/29/23 16:51:58.975
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 07/29/23 16:51:58.975
  Jul 29 16:51:59.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1224" for this suite. @ 07/29/23 16:51:59.028
• [26.611 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 07/29/23 16:51:59.049
  Jul 29 16:51:59.049: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename dns @ 07/29/23 16:51:59.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:51:59.098
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:51:59.105
  STEP: Creating a test headless service @ 07/29/23 16:51:59.11
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1155.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-1155.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 07/29/23 16:51:59.122
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-1155.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-1155.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 07/29/23 16:51:59.122
  STEP: creating a pod to probe DNS @ 07/29/23 16:51:59.122
  STEP: submitting the pod to kubernetes @ 07/29/23 16:51:59.123
  E0729 16:51:59.825817      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:00.825967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 16:52:01.152
  STEP: looking for the results for each expected name from probers @ 07/29/23 16:52:01.16
  Jul 29 16:52:01.187: INFO: DNS probes using dns-1155/dns-test-af2aa815-eaf8-4099-baed-05d33c50b216 succeeded

  Jul 29 16:52:01.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:52:01.198
  STEP: deleting the test headless service @ 07/29/23 16:52:01.228
  STEP: Destroying namespace "dns-1155" for this suite. @ 07/29/23 16:52:01.251
• [2.216 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 07/29/23 16:52:01.27
  Jul 29 16:52:01.270: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename server-version @ 07/29/23 16:52:01.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:52:01.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:52:01.303
  STEP: Request ServerVersion @ 07/29/23 16:52:01.307
  STEP: Confirm major version @ 07/29/23 16:52:01.311
  Jul 29 16:52:01.311: INFO: Major version: 1
  STEP: Confirm minor version @ 07/29/23 16:52:01.311
  Jul 29 16:52:01.311: INFO: cleanMinorVersion: 27
  Jul 29 16:52:01.312: INFO: Minor version: 27
  Jul 29 16:52:01.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-1886" for this suite. @ 07/29/23 16:52:01.32
• [0.058 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 07/29/23 16:52:01.33
  Jul 29 16:52:01.330: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:52:01.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:52:01.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:52:01.364
  STEP: Creating secret with name secret-test-7973027b-27b4-4262-9c8b-876e2e5ceb59 @ 07/29/23 16:52:01.371
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:52:01.381
  E0729 16:52:01.826765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:02.827309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:03.827780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:04.828092      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:52:05.42
  Jul 29 16:52:05.428: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-secrets-4032beda-d98e-475b-80fe-431f5f8ab5ec container secret-env-test: <nil>
  STEP: delete the pod @ 07/29/23 16:52:05.44
  Jul 29 16:52:05.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6335" for this suite. @ 07/29/23 16:52:05.521
• [4.204 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 07/29/23 16:52:05.535
  Jul 29 16:52:05.535: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:52:05.537
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:52:05.567
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:52:05.573
  STEP: Counting existing ResourceQuota @ 07/29/23 16:52:05.578
  E0729 16:52:05.828882      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:06.829855      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:07.829988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:08.830719      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:09.831778      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 16:52:10.584
  STEP: Ensuring resource quota status is calculated @ 07/29/23 16:52:10.596
  E0729 16:52:10.832385      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:11.832764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 07/29/23 16:52:12.608
  STEP: Ensuring resource quota status captures replicaset creation @ 07/29/23 16:52:12.632
  E0729 16:52:12.832962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:13.833726      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 07/29/23 16:52:14.642
  STEP: Ensuring resource quota status released usage @ 07/29/23 16:52:14.653
  E0729 16:52:14.833781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:15.834036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:52:16.671: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7571" for this suite. @ 07/29/23 16:52:16.679
• [11.164 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 07/29/23 16:52:16.7
  Jul 29 16:52:16.700: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:52:16.706
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:52:16.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:52:16.742
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:52:16.748
  E0729 16:52:16.835085      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:17.836036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:18.836958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:19.837367      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:52:20.79
  Jul 29 16:52:20.797: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-d5083ef5-61d3-4b13-8867-11be1f7e0660 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:52:20.809
  E0729 16:52:20.837705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:52:20.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2241" for this suite. @ 07/29/23 16:52:20.852
• [4.164 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 07/29/23 16:52:20.87
  Jul 29 16:52:20.871: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 16:52:20.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:52:20.911
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:52:20.916
  STEP: Performing setup for networking test in namespace pod-network-test-3606 @ 07/29/23 16:52:20.919
  STEP: creating a selector @ 07/29/23 16:52:20.92
  STEP: Creating the service pods in kubernetes @ 07/29/23 16:52:20.92
  Jul 29 16:52:20.920: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0729 16:52:21.848338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:22.838753      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:23.838962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:24.839223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:25.840387      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:26.841099      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:27.841537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:28.841670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:29.842366      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:30.842788      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:31.843215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:32.843637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:33.844347      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:34.844268      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:35.844700      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:36.844910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:37.845222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:38.845418      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:39.846045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:40.846841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:41.846649      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:42.847336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 07/29/23 16:52:43.122
  E0729 16:52:43.847452      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:44.847945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:52:45.159: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 16:52:45.159: INFO: Breadth first check of 10.233.66.116 on host 192.168.121.88...
  Jul 29 16:52:45.166: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.238:9080/dial?request=hostname&protocol=http&host=10.233.66.116&port=8083&tries=1'] Namespace:pod-network-test-3606 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:52:45.166: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:52:45.167: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:52:45.168: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3606/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.116%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:52:45.304: INFO: Waiting for responses: map[]
  Jul 29 16:52:45.304: INFO: reached 10.233.66.116 after 0/1 tries
  Jul 29 16:52:45.304: INFO: Breadth first check of 10.233.64.38 on host 192.168.121.77...
  Jul 29 16:52:45.311: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.238:9080/dial?request=hostname&protocol=http&host=10.233.64.38&port=8083&tries=1'] Namespace:pod-network-test-3606 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:52:45.311: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:52:45.313: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:52:45.314: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3606/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.38%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:52:45.430: INFO: Waiting for responses: map[]
  Jul 29 16:52:45.430: INFO: reached 10.233.64.38 after 0/1 tries
  Jul 29 16:52:45.430: INFO: Breadth first check of 10.233.65.47 on host 192.168.121.129...
  Jul 29 16:52:45.438: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.65.238:9080/dial?request=hostname&protocol=http&host=10.233.65.47&port=8083&tries=1'] Namespace:pod-network-test-3606 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:52:45.438: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:52:45.439: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:52:45.439: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-3606/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.65.238%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.47%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:52:45.556: INFO: Waiting for responses: map[]
  Jul 29 16:52:45.557: INFO: reached 10.233.65.47 after 0/1 tries
  Jul 29 16:52:45.557: INFO: Going to retry 0 out of 3 pods....
  Jul 29 16:52:45.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-3606" for this suite. @ 07/29/23 16:52:45.567
• [24.709 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 07/29/23 16:52:45.581
  Jul 29 16:52:45.581: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:52:45.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:52:45.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:52:45.648
  STEP: Setting up server cert @ 07/29/23 16:52:45.692
  E0729 16:52:45.847999      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:52:46.178
  STEP: Deploying the webhook pod @ 07/29/23 16:52:46.195
  STEP: Wait for the deployment to be ready @ 07/29/23 16:52:46.214
  Jul 29 16:52:46.229: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 16:52:46.848552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:47.848659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 16:52:48.249
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:52:48.274
  E0729 16:52:48.849800      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:52:49.274: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 07/29/23 16:52:49.284
  STEP: create a pod that should be updated by the webhook @ 07/29/23 16:52:49.317
  Jul 29 16:52:49.349: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7102" for this suite. @ 07/29/23 16:52:49.483
  STEP: Destroying namespace "webhook-markers-3523" for this suite. @ 07/29/23 16:52:49.565
• [4.026 seconds]
------------------------------
SSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 07/29/23 16:52:49.608
  Jul 29 16:52:49.613: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename hostport @ 07/29/23 16:52:49.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:52:49.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:52:49.731
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 07/29/23 16:52:49.742
  E0729 16:52:49.852042      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:50.853432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:51.856056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:52.856498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.88 on the node which pod1 resides and expect scheduled @ 07/29/23 16:52:53.783
  E0729 16:52:53.857481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:54.857804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:55.858787      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:56.858964      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.88 but use UDP protocol on the node which pod2 resides @ 07/29/23 16:52:57.823
  E0729 16:52:57.859795      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:58.860220      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:52:59.861019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:00.861245      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:01.861872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:02.862297      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:03.862616      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 07/29/23 16:53:03.893
  Jul 29 16:53:03.894: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.88 http://127.0.0.1:54323/hostname] Namespace:hostport-4606 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:53:03.894: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:53:03.895: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:53:03.895: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4606/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.88+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.88, port: 54323 @ 07/29/23 16:53:04.062
  Jul 29 16:53:04.062: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.88:54323/hostname] Namespace:hostport-4606 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:53:04.062: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:53:04.064: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:53:04.064: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4606/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.88%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.88, port: 54323 UDP @ 07/29/23 16:53:04.187
  Jul 29 16:53:04.187: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.88 54323] Namespace:hostport-4606 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:53:04.188: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:53:04.189: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:53:04.190: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-4606/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.88+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0729 16:53:04.862647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:05.863230      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:06.866009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:07.866247      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:08.866307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:53:09.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-4606" for this suite. @ 07/29/23 16:53:09.305
• [19.708 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 07/29/23 16:53:09.322
  Jul 29 16:53:09.322: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename conformance-tests @ 07/29/23 16:53:09.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:09.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:09.357
  STEP: Getting node addresses @ 07/29/23 16:53:09.362
  Jul 29 16:53:09.363: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jul 29 16:53:09.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-1645" for this suite. @ 07/29/23 16:53:09.379
• [0.066 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 07/29/23 16:53:09.391
  Jul 29 16:53:09.391: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:53:09.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:09.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:09.423
  STEP: Setting up server cert @ 07/29/23 16:53:09.47
  E0729 16:53:09.873754      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:53:10.768
  STEP: Deploying the webhook pod @ 07/29/23 16:53:10.776
  STEP: Wait for the deployment to be ready @ 07/29/23 16:53:10.793
  Jul 29 16:53:10.804: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0729 16:53:10.874094      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:11.874271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 16:53:12.825
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:53:12.844
  E0729 16:53:12.875537      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:53:13.845: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 16:53:13.854: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 16:53:13.876809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-702-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 16:53:14.377
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/29/23 16:53:14.406
  E0729 16:53:14.877660      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:15.877724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:53:16.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 16:53:16.878243      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-33" for this suite. @ 07/29/23 16:53:17.342
  STEP: Destroying namespace "webhook-markers-8241" for this suite. @ 07/29/23 16:53:17.358
• [7.980 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 07/29/23 16:53:17.395
  Jul 29 16:53:17.395: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename security-context @ 07/29/23 16:53:17.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:17.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:17.44
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/29/23 16:53:17.449
  E0729 16:53:17.878424      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:18.878634      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:19.879692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:20.880249      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:53:21.535
  Jul 29 16:53:21.556: INFO: Trying to get logs from node vucheipi7kei-3 pod security-context-387da04b-50f0-487d-9553-2054a8ce1609 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:53:21.571
  Jul 29 16:53:21.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3627" for this suite. @ 07/29/23 16:53:21.629
• [4.246 seconds]
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 07/29/23 16:53:21.641
  Jul 29 16:53:21.641: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 16:53:21.643
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:21.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:21.672
  Jul 29 16:53:21.704: INFO: created pod
  E0729 16:53:21.880298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:22.881105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:23.882278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:24.882403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:53:25.732
  E0729 16:53:25.883008      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:26.883115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:27.883456      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:28.883593      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:29.883808      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:30.883890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:31.884172      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:32.884323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:33.884489      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:34.885924      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:35.885992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:36.886222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:37.886399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:38.886722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:39.886824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:40.887258      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:41.887505      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:42.887725      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:43.887966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:44.888368      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:45.888612      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:46.888858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:47.889202      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:48.889379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:49.889767      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:50.890106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:51.890329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:52.890499      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:53.890609      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:54.891558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:53:55.732: INFO: polling logs
  Jul 29 16:53:55.775: INFO: Pod logs: 
  I0729 16:53:22.675072       1 log.go:198] OK: Got token
  I0729 16:53:22.675581       1 log.go:198] validating with in-cluster discovery
  I0729 16:53:22.677100       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0729 16:53:22.677176       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1840:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690650201, NotBefore:1690649601, IssuedAt:1690649601, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1840", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"820fb4f3-75f5-48b2-b0f8-d596f27020c6"}}}
  I0729 16:53:22.702259       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0729 16:53:22.714009       1 log.go:198] OK: Validated signature on JWT
  I0729 16:53:22.714289       1 log.go:198] OK: Got valid claims from token!
  I0729 16:53:22.714348       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1840:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690650201, NotBefore:1690649601, IssuedAt:1690649601, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1840", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"820fb4f3-75f5-48b2-b0f8-d596f27020c6"}}}

  Jul 29 16:53:55.775: INFO: completed pod
  Jul 29 16:53:55.795: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1840" for this suite. @ 07/29/23 16:53:55.803
• [34.174 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 07/29/23 16:53:55.817
  Jul 29 16:53:55.817: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:53:55.819
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:55.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:55.856
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 16:53:55.863
  Jul 29 16:53:55.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-6333 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  E0729 16:53:55.892561      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:53:56.022: INFO: stderr: ""
  Jul 29 16:53:56.023: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/29/23 16:53:56.023
  Jul 29 16:53:56.034: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-6333 delete pods e2e-test-httpd-pod'
  E0729 16:53:56.892650      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:57.892886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:53:58.578: INFO: stderr: ""
  Jul 29 16:53:58.578: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 16:53:58.578: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6333" for this suite. @ 07/29/23 16:53:58.587
• [2.787 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 07/29/23 16:53:58.605
  Jul 29 16:53:58.605: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename job @ 07/29/23 16:53:58.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:58.641
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:58.646
  STEP: Creating a job @ 07/29/23 16:53:58.65
  STEP: Ensuring job reaches completions @ 07/29/23 16:53:58.661
  E0729 16:53:58.893887      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:53:59.893981      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:00.894043      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:01.894317      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:02.896075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:03.896098      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:04.896697      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:05.897716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:06.898459      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:07.900151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:08.900883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:09.900969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:10.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9042" for this suite. @ 07/29/23 16:54:10.682
• [12.090 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 07/29/23 16:54:10.695
  Jul 29 16:54:10.695: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 16:54:10.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:10.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:10.745
  STEP: Creating replication controller my-hostname-basic-884a47c3-5c62-4487-8012-c88f0d862265 @ 07/29/23 16:54:10.756
  Jul 29 16:54:10.771: INFO: Pod name my-hostname-basic-884a47c3-5c62-4487-8012-c88f0d862265: Found 0 pods out of 1
  E0729 16:54:10.901597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:11.901621      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:12.904123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:13.902983      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:14.903798      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:15.778: INFO: Pod name my-hostname-basic-884a47c3-5c62-4487-8012-c88f0d862265: Found 1 pods out of 1
  Jul 29 16:54:15.778: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-884a47c3-5c62-4487-8012-c88f0d862265" are running
  Jul 29 16:54:15.785: INFO: Pod "my-hostname-basic-884a47c3-5c62-4487-8012-c88f0d862265-2c2mv" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:54:10 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:54:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:54:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:54:10 +0000 UTC Reason: Message:}])
  Jul 29 16:54:15.786: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/29/23 16:54:15.786
  Jul 29 16:54:15.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9717" for this suite. @ 07/29/23 16:54:15.823
• [5.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 07/29/23 16:54:15.84
  Jul 29 16:54:15.840: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 16:54:15.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:15.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:15.883
  STEP: fetching the /apis discovery document @ 07/29/23 16:54:15.888
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 07/29/23 16:54:15.891
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 07/29/23 16:54:15.892
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 07/29/23 16:54:15.892
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 07/29/23 16:54:15.894
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 07/29/23 16:54:15.894
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 07/29/23 16:54:15.896
  Jul 29 16:54:15.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7832" for this suite. @ 07/29/23 16:54:15.902
  E0729 16:54:15.903782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.089 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 07/29/23 16:54:15.93
  Jul 29 16:54:15.930: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 16:54:15.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:15.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:15.97
  STEP: creating service in namespace services-3623 @ 07/29/23 16:54:15.974
  STEP: creating service affinity-clusterip-transition in namespace services-3623 @ 07/29/23 16:54:15.975
  STEP: creating replication controller affinity-clusterip-transition in namespace services-3623 @ 07/29/23 16:54:15.993
  I0729 16:54:16.002444      14 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-3623, replica count: 3
  E0729 16:54:16.904167      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:17.904965      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:18.905360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 16:54:19.057503      14 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:54:19.072: INFO: Creating new exec pod
  E0729 16:54:19.905861      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:20.906234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:21.906961      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:22.113: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-3623 exec execpod-affinitydmbvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jul 29 16:54:22.475: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jul 29 16:54:22.475: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:54:22.475: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-3623 exec execpod-affinitydmbvv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.50.247 80'
  Jul 29 16:54:22.727: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.50.247 80\nConnection to 10.233.50.247 80 port [tcp/http] succeeded!\n"
  Jul 29 16:54:22.727: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:54:22.749: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-3623 exec execpod-affinitydmbvv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.50.247:80/ ; done'
  E0729 16:54:22.907852      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:23.267: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n"
  Jul 29 16:54:23.267: INFO: stdout: "\naffinity-clusterip-transition-4msds\naffinity-clusterip-transition-4msds\naffinity-clusterip-transition-8tdx5\naffinity-clusterip-transition-4msds\naffinity-clusterip-transition-4msds\naffinity-clusterip-transition-4msds\naffinity-clusterip-transition-4msds\naffinity-clusterip-transition-8tdx5\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-8tdx5\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-8tdx5\naffinity-clusterip-transition-8tdx5"
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-4msds
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-4msds
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-8tdx5
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-4msds
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-4msds
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-4msds
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-4msds
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-8tdx5
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-8tdx5
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-8tdx5
  Jul 29 16:54:23.267: INFO: Received response from host: affinity-clusterip-transition-8tdx5
  Jul 29 16:54:23.287: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-3623 exec execpod-affinitydmbvv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.50.247:80/ ; done'
  Jul 29 16:54:23.743: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.50.247:80/\n"
  Jul 29 16:54:23.743: INFO: stdout: "\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt\naffinity-clusterip-transition-wjjxt"
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Received response from host: affinity-clusterip-transition-wjjxt
  Jul 29 16:54:23.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:54:23.751: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3623, will wait for the garbage collector to delete the pods @ 07/29/23 16:54:23.797
  Jul 29 16:54:23.897: INFO: Deleting ReplicationController affinity-clusterip-transition took: 26.269276ms
  E0729 16:54:23.928194      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:23.998: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.73305ms
  E0729 16:54:24.929020      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:25.929533      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-3623" for this suite. @ 07/29/23 16:54:26.133
• [10.219 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 07/29/23 16:54:26.15
  Jul 29 16:54:26.150: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:54:26.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:26.188
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:26.193
  STEP: Creating a pod to test downward api env vars @ 07/29/23 16:54:26.197
  E0729 16:54:26.930309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:27.931033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:28.931021      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:29.931129      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:54:30.234
  Jul 29 16:54:30.242: INFO: Trying to get logs from node vucheipi7kei-3 pod downward-api-28f9ed85-429b-42db-99c4-15819ec2477f container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:54:30.261
  Jul 29 16:54:30.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5435" for this suite. @ 07/29/23 16:54:30.307
• [4.175 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 07/29/23 16:54:30.331
  Jul 29 16:54:30.331: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:54:30.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:30.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:30.368
  STEP: Creating configMap with name projected-configmap-test-volume-2ee09d41-a25f-4044-8fa3-64ec96c6b186 @ 07/29/23 16:54:30.374
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:54:30.383
  E0729 16:54:30.933680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:31.934117      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:32.934271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:33.934862      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 16:54:34.425
  Jul 29 16:54:34.431: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-configmaps-396d7c51-e5ae-4b75-9bb5-4aa2b1147f96 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:54:34.444
  Jul 29 16:54:34.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4027" for this suite. @ 07/29/23 16:54:34.479
• [4.163 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 07/29/23 16:54:34.497
  Jul 29 16:54:34.497: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename proxy @ 07/29/23 16:54:34.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:34.535
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:34.539
  Jul 29 16:54:34.543: INFO: Creating pod...
  E0729 16:54:34.935433      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:35.936340      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:36.576: INFO: Creating service...
  Jul 29 16:54:36.597: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/pods/agnhost/proxy?method=DELETE
  Jul 29 16:54:36.622: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 16:54:36.623: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/pods/agnhost/proxy?method=OPTIONS
  Jul 29 16:54:36.630: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 16:54:36.630: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/pods/agnhost/proxy?method=PATCH
  Jul 29 16:54:36.637: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 16:54:36.638: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/pods/agnhost/proxy?method=POST
  Jul 29 16:54:36.647: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 16:54:36.647: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/pods/agnhost/proxy?method=PUT
  Jul 29 16:54:36.654: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 16:54:36.654: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/services/e2e-proxy-test-service/proxy?method=DELETE
  Jul 29 16:54:36.666: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 16:54:36.666: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jul 29 16:54:36.677: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 16:54:36.677: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/services/e2e-proxy-test-service/proxy?method=PATCH
  Jul 29 16:54:36.687: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 16:54:36.687: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/services/e2e-proxy-test-service/proxy?method=POST
  Jul 29 16:54:36.695: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 16:54:36.695: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/services/e2e-proxy-test-service/proxy?method=PUT
  Jul 29 16:54:36.707: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 16:54:36.707: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/pods/agnhost/proxy?method=GET
  Jul 29 16:54:36.711: INFO: http.Client request:GET StatusCode:301
  Jul 29 16:54:36.711: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/services/e2e-proxy-test-service/proxy?method=GET
  Jul 29 16:54:36.720: INFO: http.Client request:GET StatusCode:301
  Jul 29 16:54:36.720: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/pods/agnhost/proxy?method=HEAD
  Jul 29 16:54:36.725: INFO: http.Client request:HEAD StatusCode:301
  Jul 29 16:54:36.725: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7751/services/e2e-proxy-test-service/proxy?method=HEAD
  Jul 29 16:54:36.733: INFO: http.Client request:HEAD StatusCode:301
  Jul 29 16:54:36.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7751" for this suite. @ 07/29/23 16:54:36.742
• [2.258 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 07/29/23 16:54:36.76
  Jul 29 16:54:36.760: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:54:36.762
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:36.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:36.803
  STEP: Creating the pod @ 07/29/23 16:54:36.809
  E0729 16:54:36.936648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:37.937624      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:38.937116      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:39.402: INFO: Successfully updated pod "labelsupdated4d85e07-c375-4e11-ae58-f6b33267e6b8"
  E0729 16:54:39.937213      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:40.937979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:41.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5903" for this suite. @ 07/29/23 16:54:41.459
• [4.709 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 07/29/23 16:54:41.481
  Jul 29 16:54:41.481: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 16:54:41.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:41.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:41.566
  E0729 16:54:41.938275      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:42.938353      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:54:43.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3393" for this suite. @ 07/29/23 16:54:43.645
• [2.176 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 07/29/23 16:54:43.658
  Jul 29 16:54:43.658: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:54:43.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:43.702
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:43.707
  STEP: creating the pod @ 07/29/23 16:54:43.71
  STEP: waiting for pod running @ 07/29/23 16:54:43.73
  E0729 16:54:43.938837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:44.939621      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 07/29/23 16:54:45.745
  Jul 29 16:54:45.751: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-9094 PodName:var-expansion-f1490abf-5c1c-463a-ad30-b9a1fb4c7f52 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:54:45.752: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:54:45.755: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:54:45.756: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9094/pods/var-expansion-f1490abf-5c1c-463a-ad30-b9a1fb4c7f52/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 07/29/23 16:54:45.858
  Jul 29 16:54:45.868: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-9094 PodName:var-expansion-f1490abf-5c1c-463a-ad30-b9a1fb4c7f52 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:54:45.868: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 16:54:45.870: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:54:45.870: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-9094/pods/var-expansion-f1490abf-5c1c-463a-ad30-b9a1fb4c7f52/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  E0729 16:54:45.940028      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the annotation value @ 07/29/23 16:54:45.992
  Jul 29 16:54:46.520: INFO: Successfully updated pod "var-expansion-f1490abf-5c1c-463a-ad30-b9a1fb4c7f52"
  STEP: waiting for annotated pod running @ 07/29/23 16:54:46.522
  STEP: deleting the pod gracefully @ 07/29/23 16:54:46.529
  Jul 29 16:54:46.530: INFO: Deleting pod "var-expansion-f1490abf-5c1c-463a-ad30-b9a1fb4c7f52" in namespace "var-expansion-9094"
  Jul 29 16:54:46.549: INFO: Wait up to 5m0s for pod "var-expansion-f1490abf-5c1c-463a-ad30-b9a1fb4c7f52" to be fully deleted
  E0729 16:54:46.940658      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:47.940963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:48.941504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:49.941602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:50.941797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:51.942299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:52.942490      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:53.942774      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:54.943273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:55.943477      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:56.944549      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:57.944850      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:58.945847      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:54:59.946059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:00.946833      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:01.947155      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:02.947866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:03.948076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:04.948525      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:05.948881      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:06.953785      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:07.953675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:08.954629      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:09.955567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:10.955839      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:11.956211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:12.956312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:13.956972      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:14.958371      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:15.958828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:16.959619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:17.959797      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 16:55:18.727: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9094" for this suite. @ 07/29/23 16:55:18.735
• [35.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 07/29/23 16:55:18.756
  Jul 29 16:55:18.756: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename dns @ 07/29/23 16:55:18.758
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:55:18.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:55:18.821
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1833.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-1833.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 07/29/23 16:55:18.826
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-1833.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-1833.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 07/29/23 16:55:18.827
  STEP: creating a pod to probe /etc/hosts @ 07/29/23 16:55:18.827
  STEP: submitting the pod to kubernetes @ 07/29/23 16:55:18.827
  E0729 16:55:18.960299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:19.961048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:20.961558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:21.962072      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 16:55:22.885
  STEP: looking for the results for each expected name from probers @ 07/29/23 16:55:22.894
  Jul 29 16:55:22.924: INFO: DNS probes using dns-1833/dns-test-cfc9db88-b6ef-452b-85a0-2834854b6575 succeeded

  Jul 29 16:55:22.924: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:55:22.933
  E0729 16:55:22.962093      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "dns-1833" for this suite. @ 07/29/23 16:55:22.962
• [4.216 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 07/29/23 16:55:22.974
  Jul 29 16:55:22.974: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 16:55:22.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:55:23.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:55:23.026
  STEP: Creating a ReplaceConcurrent cronjob @ 07/29/23 16:55:23.033
  STEP: Ensuring a job is scheduled @ 07/29/23 16:55:23.042
  E0729 16:55:23.962783      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:24.963468      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:25.964248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:26.964610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:27.964977      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:28.965724      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:29.966552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:30.966646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:31.966754      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:32.967027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:33.967172      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:34.968482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:35.969422      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:36.970295      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:37.969765      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:38.970632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:39.970403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:40.970692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:41.970846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:42.971127      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:43.971415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:44.971878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:45.972087      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:46.972380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:47.972518      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:48.973190      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:49.974075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:50.974387      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:51.974478      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:52.975012      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:53.975885      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:54.975944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:55.976068      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:56.976919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:57.977015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:58.977331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:55:59.977903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:00.977824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/29/23 16:56:01.06
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/29/23 16:56:01.065
  STEP: Ensuring the job is replaced with a new one @ 07/29/23 16:56:01.073
  E0729 16:56:01.978705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:02.978808      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:03.979365      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:04.980197      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:05.980296      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:06.980381      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:07.980651      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:08.980760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:09.981310      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:10.981962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:11.982223      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:12.982630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:13.983499      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:14.984295      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:15.985059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:16.985683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:17.985593      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:18.986307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:19.986954      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:20.987310      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:21.988457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:22.988679      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:23.989513      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:24.989942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:25.990313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:26.991195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:27.991345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:28.992112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:29.993399      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:30.993424      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:31.993636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:32.994665      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:33.995021      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:34.996127      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:35.996319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:36.996295      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:37.997076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:38.998361      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:39.998493      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:40.999230      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:41.999940      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:43.000596      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:44.001160      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:45.001737      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:46.001828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:47.002402      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:48.002672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:49.003180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:50.003932      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:51.004134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:52.004179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:53.005270      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:54.005530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:55.006268      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:56.007301      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:57.007025      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:58.007481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:56:59.008081      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:00.008517      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:01.009332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/29/23 16:57:01.08
  Jul 29 16:57:01.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5451" for this suite. @ 07/29/23 16:57:01.108
• [98.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 07/29/23 16:57:01.128
  Jul 29 16:57:01.128: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 16:57:01.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:57:01.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:57:01.177
  STEP: Creating a ForbidConcurrent cronjob @ 07/29/23 16:57:01.182
  STEP: Ensuring a job is scheduled @ 07/29/23 16:57:01.19
  E0729 16:57:02.009580      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:03.009731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:04.009920      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:05.009987      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:06.010704      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:07.010841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:08.011287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:09.011187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:10.011264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:11.011506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:12.013313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:13.013820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:14.015568      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:15.014547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:16.014572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:17.015436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:18.016485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:19.016698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:20.018603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:21.017568      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:22.017760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:23.018799      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:24.018162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:25.019054      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:26.019287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:27.020102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:28.020310      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:29.022671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:30.023589      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:31.023731      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:32.024088      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:33.024276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:34.025566      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:35.025770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:36.026484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:37.027028      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:38.027108      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:39.027602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:40.028288      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:41.028347      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:42.028706      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:43.029080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:44.029151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:45.029953      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:46.030503      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:47.030848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:48.031177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:49.031349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:50.031398      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:51.031608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:52.032413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:53.032733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:54.033763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:55.034621      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:56.035350      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:57.035575      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:58.035856      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:57:59.037910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:00.038124      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:01.039259      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/29/23 16:58:01.199
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/29/23 16:58:01.206
  STEP: Ensuring no more jobs are scheduled @ 07/29/23 16:58:01.21
  E0729 16:58:02.039991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:03.040215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:04.041423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:05.042109      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:06.042734      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:07.042869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:08.043793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:09.044082      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:10.044370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:11.044764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:12.044870      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:13.044846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:14.045229      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:15.046040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:16.046466      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:17.046958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:18.047552      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:19.047633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:20.048436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:21.049313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:22.049334      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:23.049891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:24.050512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:25.051149      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:26.051885      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:27.052107      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:28.052245      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:29.052554      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:30.052486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:31.052686      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:32.052843      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:33.053152      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:34.053345      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:35.053306      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:36.053501      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:37.054302      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:38.054900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:39.055456      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:40.056637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:41.057339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:42.057603      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:43.057821      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:44.057982      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:45.058034      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:46.058597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:47.059304      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:48.060101      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:49.060240      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:50.060613      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:51.060759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:52.061100      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:53.061698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:54.061942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:55.062129      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:56.062509      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:57.063473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:58.064484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:58:59.064661      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:00.065894      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:01.066845      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:02.067029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:03.067680      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:04.067888      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:05.068027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:06.068287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:07.069009      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:08.069764      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:09.069415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:10.070582      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:11.070690      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:12.070843      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:13.071032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:14.071185      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:15.072143      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:16.072486      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:17.072640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:18.072865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:19.072838      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:20.073369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:21.073751      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:22.074103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:23.074291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:24.074562      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:25.075106      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:26.075423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:27.075507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:28.076128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:29.076336      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:30.076139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:31.077143      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:32.077974      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:33.078417      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:34.078988      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:35.079635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:36.080238      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:37.080264      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:38.080538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:39.080483      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:40.080945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:41.081475      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:42.082003      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:43.082002      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:44.082076      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:45.082780      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:46.083489      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:47.083396      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:48.083592      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:49.083938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:50.084261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:51.084450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:52.084604      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:53.084773      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:54.085061      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:55.085993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:56.086032      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:57.086909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:58.087990      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 16:59:59.088143      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:00.089257      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:01.089957      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:02.090517      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:03.090522      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:04.091283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:05.092215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:06.092199      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:07.092417      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:08.092728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:09.092865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:10.093932      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:11.094001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:12.094579      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:13.094759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:14.094860      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:15.095746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:16.096052      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:17.096781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:18.097044      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:19.097283      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:20.098267      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:21.098120      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:22.098112      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:23.098335      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:24.098730      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:25.098886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:26.099278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:27.099936      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:28.100091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:29.100487      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:30.100703      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:31.101056      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:32.101235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:33.101389      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:34.102181      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:35.101941      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:36.102502      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:37.102558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:38.103073      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:39.103116      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:40.105212      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:41.104144      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:42.104379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:43.104440      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:44.104625      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:45.105138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:46.105748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:47.106035      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:48.106545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:49.106845      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:50.107715      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:51.107819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:52.107995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:53.108854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:54.108667      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:55.109138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:56.109342      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:57.110353      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:58.110507      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:00:59.110597      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:00.111693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:01.111804      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:02.111953      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:03.112281      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:04.112624      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:05.113191      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:06.113704      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:07.113791      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:08.123362      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:09.114035      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:10.115013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:11.115905      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:12.115781      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:13.116360      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:14.117395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:15.118105      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:16.118437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:17.118872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:18.118984      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:19.119953      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:20.120913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:21.121034      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:22.121244      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:23.121506      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:24.121515      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:25.123179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:26.123227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:27.123902      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:28.124529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:29.124767      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:30.125914      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:31.125939      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:32.126523      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:33.142379      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:34.135520      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:35.136114      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:36.136135      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:37.136217      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:38.136211      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:39.139176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:40.138472      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:41.139138      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:42.139129      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:43.139925      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:44.140127      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:45.142886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:46.140693      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:47.141263      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:48.141027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:49.141169      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:50.141845      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:51.142290      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:52.143287      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:53.143782      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:54.143710      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:55.143769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:56.144001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:57.144786      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:58.144992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:01:59.145832      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:00.146595      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:01.147149      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:02.147473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:03.147488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:04.147698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:05.148299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:06.148642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:07.149349      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:08.149636      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:09.150352      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:10.150543      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:11.150678      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:12.150949      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:13.151176      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:14.151446      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:15.152048      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:16.152248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:17.152716      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:18.153064      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:19.153324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:20.153901      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:21.154133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:22.154343      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:23.154608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:24.154824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:25.154935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:26.155132      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:27.155257      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:28.155659      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:29.156319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:30.156498      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:31.156717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:32.156973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:33.157091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:34.157571      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:35.157945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:36.158285      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:37.158384      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:38.158736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:39.159104      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:40.159130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:41.159427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:42.159762      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:43.160014      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:44.161089      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:45.162162      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:46.162327      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:47.162865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:48.163380      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:49.163678      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:50.163807      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:51.163953      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:52.164109      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:53.164886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:54.165708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:55.166018      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:56.166226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:57.166809      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:58.167093      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:02:59.167277      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:00.167745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:01.168033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/29/23 17:03:01.228
  Jul 29 17:03:01.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-1060" for this suite. @ 07/29/23 17:03:01.258
• [360.150 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 07/29/23 17:03:01.279
  Jul 29 17:03:01.279: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:03:01.287
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:01.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:01.349
  STEP: create deployment with httpd image @ 07/29/23 17:03:01.358
  Jul 29 17:03:01.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5617 create -f -'
  Jul 29 17:03:02.159: INFO: stderr: ""
  Jul 29 17:03:02.159: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 07/29/23 17:03:02.159
  Jul 29 17:03:02.160: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5617 diff -f -'
  E0729 17:03:02.170001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:03:02.785: INFO: rc: 1
  Jul 29 17:03:02.801: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5617 delete -f -'
  Jul 29 17:03:03.028: INFO: stderr: ""
  Jul 29 17:03:03.028: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jul 29 17:03:03.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5617" for this suite. @ 07/29/23 17:03:03.056
• [1.800 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 07/29/23 17:03:03.085
  Jul 29 17:03:03.085: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:03:03.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:03.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:03.145
  E0729 17:03:03.169248      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 07/29/23 17:03:03.209
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:03:04.051
  STEP: Deploying the webhook pod @ 07/29/23 17:03:04.066
  STEP: Wait for the deployment to be ready @ 07/29/23 17:03:04.092
  Jul 29 17:03:04.116: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 17:03:04.169694      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:05.170671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:03:06.139
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:03:06.156
  E0729 17:03:06.172271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:03:07.157: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0729 17:03:07.173954      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Listing all of the created validation webhooks @ 07/29/23 17:03:07.255
  STEP: Creating a configMap that should be mutated @ 07/29/23 17:03:07.281
  STEP: Deleting the collection of validation webhooks @ 07/29/23 17:03:07.328
  STEP: Creating a configMap that should not be mutated @ 07/29/23 17:03:07.418
  Jul 29 17:03:07.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1264" for this suite. @ 07/29/23 17:03:07.519
  STEP: Destroying namespace "webhook-markers-5857" for this suite. @ 07/29/23 17:03:07.533
• [4.470 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 07/29/23 17:03:07.558
  Jul 29 17:03:07.558: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 17:03:07.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:07.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:07.602
  STEP: Creating secret with name secret-test-map-280fb600-b840-4a6c-9463-367f605473e4 @ 07/29/23 17:03:07.606
  STEP: Creating a pod to test consume secrets @ 07/29/23 17:03:07.616
  E0729 17:03:08.175926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:09.174488      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:10.175503      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:11.175686      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:03:11.675
  Jul 29 17:03:11.682: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-secrets-ae86481f-eae5-472c-8ad4-9a696cc1de52 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 17:03:11.735
  Jul 29 17:03:11.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3485" for this suite. @ 07/29/23 17:03:11.807
• [4.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 07/29/23 17:03:11.828
  Jul 29 17:03:11.829: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:03:11.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:11.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:11.864
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:03:11.868
  E0729 17:03:12.176641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:13.176869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:14.177396      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:15.177914      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:03:15.917
  Jul 29 17:03:15.924: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-458fd732-3cec-43f4-95b5-bf7a65b6e1af container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:03:15.942
  Jul 29 17:03:15.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2996" for this suite. @ 07/29/23 17:03:15.981
• [4.169 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 07/29/23 17:03:15.999
  Jul 29 17:03:15.999: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:03:16.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:16.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:16.042
  STEP: creating Agnhost RC @ 07/29/23 17:03:16.05
  Jul 29 17:03:16.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-3665 create -f -'
  E0729 17:03:16.178448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:03:16.815: INFO: stderr: ""
  Jul 29 17:03:16.815: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 17:03:16.815
  E0729 17:03:17.178632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:03:17.828: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 17:03:17.829: INFO: Found 1 / 1
  Jul 29 17:03:17.829: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 07/29/23 17:03:17.829
  Jul 29 17:03:17.836: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 17:03:17.837: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 17:03:17.837: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-3665 patch pod agnhost-primary-4rb9w -p {"metadata":{"annotations":{"x":"y"}}}'
  Jul 29 17:03:17.986: INFO: stderr: ""
  Jul 29 17:03:17.986: INFO: stdout: "pod/agnhost-primary-4rb9w patched\n"
  STEP: checking annotations @ 07/29/23 17:03:17.986
  Jul 29 17:03:17.994: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 17:03:17.994: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 17:03:17.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3665" for this suite. @ 07/29/23 17:03:18.002
• [2.014 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 07/29/23 17:03:18.016
  Jul 29 17:03:18.016: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 17:03:18.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:18.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:18.052
  STEP: Creating secret with name secret-test-c09ebac6-17a0-43e7-af47-ad52ff1d87a3 @ 07/29/23 17:03:18.094
  STEP: Creating a pod to test consume secrets @ 07/29/23 17:03:18.101
  E0729 17:03:18.179841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:19.179837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:20.180040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:21.180123      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:03:22.139
  Jul 29 17:03:22.145: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-secrets-4d08adf2-4a66-4b60-9109-871a117e84c6 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 17:03:22.157
  E0729 17:03:22.181209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:03:22.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2405" for this suite. @ 07/29/23 17:03:22.198
  STEP: Destroying namespace "secret-namespace-8443" for this suite. @ 07/29/23 17:03:22.211
• [4.211 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 07/29/23 17:03:22.229
  Jul 29 17:03:22.229: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 17:03:22.23
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:22.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:22.265
  Jul 29 17:03:22.270: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:03:23.181593      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:24.181884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:25.182908      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:26.183485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:27.183969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:28.184402      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:03:28.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2988" for this suite. @ 07/29/23 17:03:28.966
• [6.752 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 07/29/23 17:03:28.989
  Jul 29 17:03:28.989: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename prestop @ 07/29/23 17:03:28.998
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:29.028
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:29.038
  STEP: Creating server pod server in namespace prestop-2038 @ 07/29/23 17:03:29.047
  STEP: Waiting for pods to come up. @ 07/29/23 17:03:29.065
  E0729 17:03:29.185134      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:30.186070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-2038 @ 07/29/23 17:03:31.092
  E0729 17:03:31.186492      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:32.186711      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 07/29/23 17:03:33.139
  E0729 17:03:33.187722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:34.187903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:35.188476      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:36.188417      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:37.189187      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:03:38.163: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jul 29 17:03:38.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 07/29/23 17:03:38.182
  E0729 17:03:38.189845      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "prestop-2038" for this suite. @ 07/29/23 17:03:38.215
• [9.261 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 07/29/23 17:03:38.258
  Jul 29 17:03:38.259: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:03:38.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:38.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:38.325
  STEP: Creating projection with secret that has name projected-secret-test-map-a0b40bd6-adae-4726-adca-3f06e4648ead @ 07/29/23 17:03:38.338
  STEP: Creating a pod to test consume secrets @ 07/29/23 17:03:38.349
  E0729 17:03:39.190179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:40.190958      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:41.192004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:42.191535      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:03:42.438
  Jul 29 17:03:42.443: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-secrets-8430c2ff-f59e-4b56-aa06-d803bb3e3a0c container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 17:03:42.467
  Jul 29 17:03:42.496: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6969" for this suite. @ 07/29/23 17:03:42.504
• [4.256 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 07/29/23 17:03:42.524
  Jul 29 17:03:42.524: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename subpath @ 07/29/23 17:03:42.527
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:42.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:42.56
  STEP: Setting up data @ 07/29/23 17:03:42.563
  STEP: Creating pod pod-subpath-test-configmap-zpr7 @ 07/29/23 17:03:42.581
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 17:03:42.581
  E0729 17:03:43.191926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:44.192139      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:45.192242      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:46.192666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:47.193095      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:48.193326      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:49.193430      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:50.194359      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:51.194699      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:52.195415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:53.195763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:54.196372      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:55.196708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:56.196904      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:57.197072      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:58.197420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:03:59.197599      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:00.197747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:01.198097      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:02.198436      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:03.198606      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:04.198923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:05.199538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:06.200416      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:04:06.715
  Jul 29 17:04:06.724: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-subpath-test-configmap-zpr7 container test-container-subpath-configmap-zpr7: <nil>
  STEP: delete the pod @ 07/29/23 17:04:06.744
  STEP: Deleting pod pod-subpath-test-configmap-zpr7 @ 07/29/23 17:04:06.772
  Jul 29 17:04:06.772: INFO: Deleting pod "pod-subpath-test-configmap-zpr7" in namespace "subpath-1147"
  Jul 29 17:04:06.784: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1147" for this suite. @ 07/29/23 17:04:06.793
• [24.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 07/29/23 17:04:06.811
  Jul 29 17:04:06.811: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 17:04:06.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:06.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:06.853
  E0729 17:04:07.200769      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:08.201180      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:09.201909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:10.202775      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:11.203170      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 07/29/23 17:04:12.001
  E0729 17:04:12.203897      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:13.204140      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:14.206386      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:15.206074      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:16.206405      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 07/29/23 17:04:17.014
  E0729 17:04:17.206851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:18.207573      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:19.207867      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:20.208945      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:21.208996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 07/29/23 17:04:22.032
  E0729 17:04:22.209536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:23.211019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:24.210473      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:25.211234      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:26.211720      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 07/29/23 17:04:27.048
  Jul 29 17:04:27.099: INFO: EndpointSlice for Service endpointslice-1853/example-named-port not found
  E0729 17:04:27.212491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:28.212996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:29.213316      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:30.213817      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:31.214491      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:32.214768      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:33.214933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:34.215252      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:35.216246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:36.216464      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:04:37.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1853" for this suite. @ 07/29/23 17:04:37.129
• [30.334 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 07/29/23 17:04:37.145
  Jul 29 17:04:37.145: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:04:37.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:37.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:37.185
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:04:37.193
  E0729 17:04:37.217324      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:38.218001      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:39.217835      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:40.219664      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:41.220059      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:04:41.246
  Jul 29 17:04:41.254: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-e47029ce-6e71-4901-afb3-89bfefec0eb8 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:04:41.268
  Jul 29 17:04:41.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8056" for this suite. @ 07/29/23 17:04:41.31
• [4.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 07/29/23 17:04:41.333
  Jul 29 17:04:41.333: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:04:41.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:41.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:41.381
  STEP: creating the pod @ 07/29/23 17:04:41.389
  STEP: submitting the pod to kubernetes @ 07/29/23 17:04:41.39
  STEP: verifying QOS class is set on the pod @ 07/29/23 17:04:41.406
  Jul 29 17:04:41.415: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8263" for this suite. @ 07/29/23 17:04:41.44
• [0.133 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 07/29/23 17:04:41.466
  Jul 29 17:04:41.466: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 17:04:41.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:41.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:41.568
  STEP: creating a Namespace @ 07/29/23 17:04:41.575
  STEP: patching the Namespace @ 07/29/23 17:04:41.627
  STEP: get the Namespace and ensuring it has the label @ 07/29/23 17:04:41.675
  Jul 29 17:04:41.684: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1259" for this suite. @ 07/29/23 17:04:41.693
  STEP: Destroying namespace "nspatchtest-f4a27c3a-2676-4de3-925c-51ff54c9dc12-4905" for this suite. @ 07/29/23 17:04:41.708
• [0.257 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 07/29/23 17:04:41.727
  Jul 29 17:04:41.727: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 17:04:41.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:41.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:41.771
  E0729 17:04:42.221024      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:43.221865      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:44.222051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:45.222082      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:04:45.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-2825" for this suite. @ 07/29/23 17:04:45.837
• [4.123 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 07/29/23 17:04:45.852
  Jul 29 17:04:45.853: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename watch @ 07/29/23 17:04:45.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:45.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:45.889
  STEP: creating a watch on configmaps with label A @ 07/29/23 17:04:45.894
  STEP: creating a watch on configmaps with label B @ 07/29/23 17:04:45.899
  STEP: creating a watch on configmaps with label A or B @ 07/29/23 17:04:45.903
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 07/29/23 17:04:45.905
  Jul 29 17:04:45.915: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9858  8809c073-811b-438b-a4c2-9af9028867b8 33557 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:04:45.916: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9858  8809c073-811b-438b-a4c2-9af9028867b8 33557 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 07/29/23 17:04:45.916
  Jul 29 17:04:45.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9858  8809c073-811b-438b-a4c2-9af9028867b8 33558 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:04:45.931: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9858  8809c073-811b-438b-a4c2-9af9028867b8 33558 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 07/29/23 17:04:45.932
  Jul 29 17:04:45.951: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9858  8809c073-811b-438b-a4c2-9af9028867b8 33559 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:04:45.952: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9858  8809c073-811b-438b-a4c2-9af9028867b8 33559 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 07/29/23 17:04:45.952
  Jul 29 17:04:45.968: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9858  8809c073-811b-438b-a4c2-9af9028867b8 33560 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:04:45.969: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-9858  8809c073-811b-438b-a4c2-9af9028867b8 33560 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 07/29/23 17:04:45.969
  Jul 29 17:04:45.980: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9858  0790c4e5-5a9d-4c94-b5a9-0ab4448e7ac3 33561 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:04:45.981: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9858  0790c4e5-5a9d-4c94-b5a9-0ab4448e7ac3 33561 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0729 17:04:46.223569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:47.222619      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:48.222663      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:49.223641      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:50.224542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:51.224657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:52.224712      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:53.224906      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:54.225547      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:55.226647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 07/29/23 17:04:55.982
  Jul 29 17:04:55.998: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9858  0790c4e5-5a9d-4c94-b5a9-0ab4448e7ac3 33618 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:04:55.998: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-9858  0790c4e5-5a9d-4c94-b5a9-0ab4448e7ac3 33618 0 2023-07-29 17:04:45 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 17:04:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0729 17:04:56.227653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:57.228008      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:58.228332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:04:59.228439      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:00.228622      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:01.228866      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:02.229222      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:03.229698      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:04.229984      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:05.230045      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:05.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9858" for this suite. @ 07/29/23 17:05:06.012
• [20.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 07/29/23 17:05:06.035
  Jul 29 17:05:06.035: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 17:05:06.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:05:06.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:05:06.086
  STEP: Creating a simple DaemonSet "daemon-set" @ 07/29/23 17:05:06.135
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 17:05:06.147
  Jul 29 17:05:06.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:05:06.166: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:05:06.230511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:07.196: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:05:07.197: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:05:07.230717      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:08.182: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:05:08.183: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:05:08.230924      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:09.191: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:05:09.191: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 07/29/23 17:05:09.198
  E0729 17:05:09.233973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:09.242: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:05:09.242: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:05:10.234810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:10.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:05:10.263: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:05:11.235018      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:11.261: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:05:11.261: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 07/29/23 17:05:11.261
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 17:05:11.275
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8316, will wait for the garbage collector to delete the pods @ 07/29/23 17:05:11.275
  Jul 29 17:05:11.342: INFO: Deleting DaemonSet.extensions daemon-set took: 10.640366ms
  Jul 29 17:05:11.443: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.140167ms
  E0729 17:05:12.235658      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:13.236631      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:13.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:05:13.252: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 17:05:13.258: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33746"},"items":null}

  Jul 29 17:05:13.265: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33746"},"items":null}

  Jul 29 17:05:13.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8316" for this suite. @ 07/29/23 17:05:13.3
• [7.274 seconds]
------------------------------
SSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 07/29/23 17:05:13.309
  Jul 29 17:05:13.310: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 17:05:13.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:05:13.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:05:13.341
  Jul 29 17:05:13.360: INFO: Endpoints addresses: [192.168.121.77 192.168.121.88] , ports: [6443]
  Jul 29 17:05:13.361: INFO: EndpointSlices addresses: [192.168.121.77 192.168.121.88] , ports: [6443]
  Jul 29 17:05:13.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5632" for this suite. @ 07/29/23 17:05:13.371
• [0.076 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 07/29/23 17:05:13.402
  Jul 29 17:05:13.402: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:05:13.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:05:13.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:05:13.439
  STEP: Counting existing ResourceQuota @ 07/29/23 17:05:13.445
  E0729 17:05:14.237128      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:15.238333      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:16.238524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:17.239538      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:18.239558      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 17:05:18.451
  STEP: Ensuring resource quota status is calculated @ 07/29/23 17:05:18.46
  E0729 17:05:19.239986      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:20.240216      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 07/29/23 17:05:20.469
  STEP: Creating a NodePort Service @ 07/29/23 17:05:20.503
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 07/29/23 17:05:20.54
  STEP: Ensuring resource quota status captures service creation @ 07/29/23 17:05:20.581
  E0729 17:05:21.240325      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:22.240638      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 07/29/23 17:05:22.589
  STEP: Ensuring resource quota status released usage @ 07/29/23 17:05:22.656
  E0729 17:05:23.240771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:24.241289      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:24.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7824" for this suite. @ 07/29/23 17:05:24.672
• [11.286 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 07/29/23 17:05:24.69
  Jul 29 17:05:24.690: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 17:05:24.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:05:24.734
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:05:24.739
  STEP: Creating ReplicationController "e2e-rc-qk7jd" @ 07/29/23 17:05:24.743
  Jul 29 17:05:24.756: INFO: Get Replication Controller "e2e-rc-qk7jd" to confirm replicas
  E0729 17:05:25.241819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:25.763: INFO: Get Replication Controller "e2e-rc-qk7jd" to confirm replicas
  Jul 29 17:05:25.770: INFO: Found 1 replicas for "e2e-rc-qk7jd" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-qk7jd" @ 07/29/23 17:05:25.77
  STEP: Updating a scale subresource @ 07/29/23 17:05:25.776
  STEP: Verifying replicas where modified for replication controller "e2e-rc-qk7jd" @ 07/29/23 17:05:25.788
  Jul 29 17:05:25.788: INFO: Get Replication Controller "e2e-rc-qk7jd" to confirm replicas
  E0729 17:05:26.243103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:26.797: INFO: Get Replication Controller "e2e-rc-qk7jd" to confirm replicas
  Jul 29 17:05:26.803: INFO: Found 2 replicas for "e2e-rc-qk7jd" replication controller
  Jul 29 17:05:26.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6552" for this suite. @ 07/29/23 17:05:26.812
• [2.138 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 07/29/23 17:05:26.837
  Jul 29 17:05:26.837: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl-logs @ 07/29/23 17:05:26.838
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:05:26.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:05:26.896
  STEP: creating an pod @ 07/29/23 17:05:26.9
  Jul 29 17:05:26.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-logs-7501 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jul 29 17:05:27.073: INFO: stderr: ""
  Jul 29 17:05:27.073: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 07/29/23 17:05:27.073
  Jul 29 17:05:27.074: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0729 17:05:27.243690      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:28.244071      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:29.092: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 07/29/23 17:05:29.092
  Jul 29 17:05:29.092: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-logs-7501 logs logs-generator logs-generator'
  E0729 17:05:29.244329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:29.268: INFO: stderr: ""
  Jul 29 17:05:29.268: INFO: stdout: "I0729 17:05:28.065368       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/42m 305\nI0729 17:05:28.265566       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/h9lq 510\nI0729 17:05:28.466125       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/jh5 320\nI0729 17:05:28.665626       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/vwxx 418\nI0729 17:05:28.866066       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/9j4 260\nI0729 17:05:29.065434       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/dzz 479\n"
  STEP: limiting log lines @ 07/29/23 17:05:29.268
  Jul 29 17:05:29.268: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-logs-7501 logs logs-generator logs-generator --tail=1'
  Jul 29 17:05:29.406: INFO: stderr: ""
  Jul 29 17:05:29.406: INFO: stdout: "I0729 17:05:29.268433       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/f6jf 468\n"
  Jul 29 17:05:29.406: INFO: got output "I0729 17:05:29.268433       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/f6jf 468\n"
  STEP: limiting log bytes @ 07/29/23 17:05:29.407
  Jul 29 17:05:29.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-logs-7501 logs logs-generator logs-generator --limit-bytes=1'
  Jul 29 17:05:29.559: INFO: stderr: ""
  Jul 29 17:05:29.559: INFO: stdout: "I"
  Jul 29 17:05:29.559: INFO: got output "I"
  STEP: exposing timestamps @ 07/29/23 17:05:29.559
  Jul 29 17:05:29.559: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-logs-7501 logs logs-generator logs-generator --tail=1 --timestamps'
  Jul 29 17:05:29.713: INFO: stderr: ""
  Jul 29 17:05:29.713: INFO: stdout: "2023-07-29T17:05:29.665528915Z I0729 17:05:29.665401       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/5kd 363\n"
  Jul 29 17:05:29.713: INFO: got output "2023-07-29T17:05:29.665528915Z I0729 17:05:29.665401       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/5kd 363\n"
  STEP: restricting to a time range @ 07/29/23 17:05:29.713
  E0729 17:05:30.244331      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:31.244520      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:32.213: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-logs-7501 logs logs-generator logs-generator --since=1s'
  E0729 17:05:32.244960      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:32.429: INFO: stderr: ""
  Jul 29 17:05:32.429: INFO: stdout: "I0729 17:05:31.465591       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/m9q 305\nI0729 17:05:31.665242       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/h65 249\nI0729 17:05:31.865568       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/vzfx 558\nI0729 17:05:32.065913       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/plf 496\nI0729 17:05:32.265275       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/27k 578\n"
  Jul 29 17:05:32.430: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-logs-7501 logs logs-generator logs-generator --since=24h'
  Jul 29 17:05:32.638: INFO: stderr: ""
  Jul 29 17:05:32.638: INFO: stdout: "I0729 17:05:28.065368       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/42m 305\nI0729 17:05:28.265566       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/h9lq 510\nI0729 17:05:28.466125       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/jh5 320\nI0729 17:05:28.665626       1 logs_generator.go:76] 3 PUT /api/v1/namespaces/default/pods/vwxx 418\nI0729 17:05:28.866066       1 logs_generator.go:76] 4 GET /api/v1/namespaces/kube-system/pods/9j4 260\nI0729 17:05:29.065434       1 logs_generator.go:76] 5 POST /api/v1/namespaces/kube-system/pods/dzz 479\nI0729 17:05:29.268433       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/f6jf 468\nI0729 17:05:29.465868       1 logs_generator.go:76] 7 PUT /api/v1/namespaces/ns/pods/2892 218\nI0729 17:05:29.665401       1 logs_generator.go:76] 8 GET /api/v1/namespaces/default/pods/5kd 363\nI0729 17:05:29.865894       1 logs_generator.go:76] 9 GET /api/v1/namespaces/kube-system/pods/zp5 365\nI0729 17:05:30.065205       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/dqph 507\nI0729 17:05:30.265630       1 logs_generator.go:76] 11 GET /api/v1/namespaces/ns/pods/6rx2 221\nI0729 17:05:30.466119       1 logs_generator.go:76] 12 GET /api/v1/namespaces/kube-system/pods/9t2 228\nI0729 17:05:30.665719       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/kp2 472\nI0729 17:05:30.865232       1 logs_generator.go:76] 14 GET /api/v1/namespaces/kube-system/pods/5jk 414\nI0729 17:05:31.065715       1 logs_generator.go:76] 15 GET /api/v1/namespaces/default/pods/q4p2 225\nI0729 17:05:31.266090       1 logs_generator.go:76] 16 POST /api/v1/namespaces/ns/pods/z426 281\nI0729 17:05:31.465591       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/m9q 305\nI0729 17:05:31.665242       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/kube-system/pods/h65 249\nI0729 17:05:31.865568       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/vzfx 558\nI0729 17:05:32.065913       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/plf 496\nI0729 17:05:32.265275       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/27k 578\nI0729 17:05:32.469079       1 logs_generator.go:76] 22 PUT /api/v1/namespaces/kube-system/pods/8psl 398\n"
  Jul 29 17:05:32.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-logs-7501 delete pod logs-generator'
  E0729 17:05:33.245284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:05:33.476: INFO: stderr: ""
  Jul 29 17:05:33.476: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jul 29 17:05:33.476: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-7501" for this suite. @ 07/29/23 17:05:33.49
• [6.681 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 07/29/23 17:05:33.52
  Jul 29 17:05:33.521: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename init-container @ 07/29/23 17:05:33.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:05:33.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:05:33.564
  STEP: creating the pod @ 07/29/23 17:05:33.57
  Jul 29 17:05:33.571: INFO: PodSpec: initContainers in spec.initContainers
  E0729 17:05:34.246049      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:35.246307      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:36.246453      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:37.246718      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:38.246870      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:39.247153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:40.247705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:41.247947      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:42.248229      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:43.248478      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:44.248748      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:45.249261      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:46.249773      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:47.249944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:48.250441      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:49.250898      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:50.251530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:51.251846      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:52.252177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:53.252602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:54.252672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:55.253151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:56.253728      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:57.253642      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:58.253745      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:05:59.254274      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:00.254822      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:01.255133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:02.255437      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:03.255487      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:04.255819      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:05.255668      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:06.256016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:07.256770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:08.257043      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:09.257730      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:10.258834      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:11.259294      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:12.259758      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:13.260203      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:14.260773      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:15.261602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:16.262232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:17.262708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:18.263051      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:19.263523      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:19.590: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-4b4d6ad6-f161-46cb-938b-865e6424a27a", GenerateName:"", Namespace:"init-container-9683", SelfLink:"", UID:"39f77611-8439-47d7-a7a9-f7670772d429", ResourceVersion:"34070", Generation:0, CreationTimestamp:time.Date(2023, time.July, 29, 17, 5, 33, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"571193156"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 17, 5, 33, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0065fb230), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 17, 6, 19, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0065fb278), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-n5dhn", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc004b04b20), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n5dhn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n5dhn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-n5dhn", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc006adbd48), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"vucheipi7kei-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000328bd0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006adbdd0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc006adbdf0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc006adbdf8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc006adbdfc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc006d0b8b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 17, 5, 33, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 17, 5, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 17, 5, 33, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 17, 5, 33, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.129", PodIP:"10.233.65.194", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.65.194"}}, StartTime:time.Date(2023, time.July, 29, 17, 5, 33, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000328d20)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000328d90)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://20809d069af2e431bc8a64f568b4a714577141e9b05d79b45e726f39e99725e9", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b04ba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004b04b80), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc006adbe74), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jul 29 17:06:19.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9683" for this suite. @ 07/29/23 17:06:19.601
• [46.106 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 07/29/23 17:06:19.63
  Jul 29 17:06:19.630: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:06:19.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:19.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:19.675
  STEP: Counting existing ResourceQuota @ 07/29/23 17:06:19.682
  E0729 17:06:20.264484      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:21.264854      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:22.265170      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:23.265733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:24.265912      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 17:06:24.69
  STEP: Ensuring resource quota status is calculated @ 07/29/23 17:06:24.705
  E0729 17:06:25.266883      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:26.267062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 07/29/23 17:06:26.719
  STEP: Ensuring resource quota status captures replication controller creation @ 07/29/23 17:06:26.744
  E0729 17:06:27.267152      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:28.267738      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 07/29/23 17:06:28.754
  STEP: Ensuring resource quota status released usage @ 07/29/23 17:06:28.766
  E0729 17:06:29.268407      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:30.269050      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:30.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-9694" for this suite. @ 07/29/23 17:06:30.789
• [11.171 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 07/29/23 17:06:30.804
  Jul 29 17:06:30.804: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:06:30.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:30.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:30.841
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:06:30.845
  E0729 17:06:31.269733      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:32.270005      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:33.270115      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:34.270450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:06:34.883
  Jul 29 17:06:34.892: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-08ad9be9-bd69-47e4-a545-9f60f77566ce container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:06:34.905
  Jul 29 17:06:34.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1377" for this suite. @ 07/29/23 17:06:34.954
• [4.165 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 07/29/23 17:06:34.969
  Jul 29 17:06:34.970: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 17:06:34.973
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:35.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:35.009
  Jul 29 17:06:35.014: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:06:35.271229      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:36.271338      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:37.271826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0729 17:06:37.780022      14 warnings.go:70] unknown field "alpha"
  W0729 17:06:37.780290      14 warnings.go:70] unknown field "beta"
  W0729 17:06:37.780474      14 warnings.go:70] unknown field "delta"
  W0729 17:06:37.780658      14 warnings.go:70] unknown field "epsilon"
  W0729 17:06:37.780838      14 warnings.go:70] unknown field "gamma"
  E0729 17:06:38.271591      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:38.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7733" for this suite. @ 07/29/23 17:06:38.358
• [3.405 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 07/29/23 17:06:38.381
  Jul 29 17:06:38.382: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 17:06:38.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:38.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:38.419
  STEP: create the container @ 07/29/23 17:06:38.424
  W0729 17:06:38.440125      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 17:06:38.44
  E0729 17:06:39.271740      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:40.272443      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:41.272305      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/29/23 17:06:41.481
  STEP: the container should be terminated @ 07/29/23 17:06:41.529
  STEP: the termination message should be set @ 07/29/23 17:06:41.529
  Jul 29 17:06:41.529: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 07/29/23 17:06:41.529
  Jul 29 17:06:41.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9406" for this suite. @ 07/29/23 17:06:41.575
• [3.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 07/29/23 17:06:41.596
  Jul 29 17:06:41.596: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 17:06:41.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:41.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:41.646
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-8019 @ 07/29/23 17:06:41.657
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/29/23 17:06:41.679
  STEP: creating service externalsvc in namespace services-8019 @ 07/29/23 17:06:41.68
  STEP: creating replication controller externalsvc in namespace services-8019 @ 07/29/23 17:06:41.717
  I0729 17:06:41.734851      14 runners.go:194] Created replication controller with name: externalsvc, namespace: services-8019, replica count: 2
  E0729 17:06:42.272432      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:43.273339      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:44.273215      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 17:06:44.791140      14 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 07/29/23 17:06:44.799
  Jul 29 17:06:44.823: INFO: Creating new exec pod
  E0729 17:06:45.273378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:46.273889      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:46.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-8019 exec execpodxxcwg -- /bin/sh -x -c nslookup clusterip-service.services-8019.svc.cluster.local'
  Jul 29 17:06:47.215: INFO: stderr: "+ nslookup clusterip-service.services-8019.svc.cluster.local\n"
  Jul 29 17:06:47.215: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-8019.svc.cluster.local\tcanonical name = externalsvc.services-8019.svc.cluster.local.\nName:\texternalsvc.services-8019.svc.cluster.local\nAddress: 10.233.4.53\n\n"
  Jul 29 17:06:47.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-8019, will wait for the garbage collector to delete the pods @ 07/29/23 17:06:47.226
  E0729 17:06:47.274572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:47.302: INFO: Deleting ReplicationController externalsvc took: 19.11744ms
  Jul 29 17:06:47.403: INFO: Terminating ReplicationController externalsvc pods took: 101.235513ms
  E0729 17:06:48.275752      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:06:49.276736      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:49.773: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-8019" for this suite. @ 07/29/23 17:06:49.805
• [8.229 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 07/29/23 17:06:49.85
  Jul 29 17:06:49.851: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename svc-latency @ 07/29/23 17:06:49.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:49.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:49.896
  Jul 29 17:06:49.904: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-9813 @ 07/29/23 17:06:49.907
  I0729 17:06:49.919951      14 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-9813, replica count: 1
  E0729 17:06:50.277920      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 17:06:50.971194      14 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0729 17:06:51.281770      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 17:06:51.972737      14 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 17:06:52.127: INFO: Created: latency-svc-mt7qv
  Jul 29 17:06:52.127: INFO: Got endpoints: latency-svc-mt7qv [54.261099ms]
  Jul 29 17:06:52.171: INFO: Created: latency-svc-4h4wh
  Jul 29 17:06:52.173: INFO: Got endpoints: latency-svc-4h4wh [44.66321ms]
  Jul 29 17:06:52.192: INFO: Created: latency-svc-472hn
  Jul 29 17:06:52.201: INFO: Got endpoints: latency-svc-472hn [72.24495ms]
  Jul 29 17:06:52.211: INFO: Created: latency-svc-x9f4f
  Jul 29 17:06:52.223: INFO: Got endpoints: latency-svc-x9f4f [94.186187ms]
  Jul 29 17:06:52.228: INFO: Created: latency-svc-r6dbk
  Jul 29 17:06:52.244: INFO: Got endpoints: latency-svc-r6dbk [114.622083ms]
  Jul 29 17:06:52.246: INFO: Created: latency-svc-rnl88
  Jul 29 17:06:52.258: INFO: Created: latency-svc-4q8nh
  Jul 29 17:06:52.273: INFO: Got endpoints: latency-svc-rnl88 [143.729375ms]
  Jul 29 17:06:52.277: INFO: Created: latency-svc-kxfsx
  E0729 17:06:52.281648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:52.293: INFO: Created: latency-svc-nlpbx
  Jul 29 17:06:52.299: INFO: Created: latency-svc-vw854
  Jul 29 17:06:52.313: INFO: Got endpoints: latency-svc-4q8nh [183.128112ms]
  Jul 29 17:06:52.314: INFO: Got endpoints: latency-svc-kxfsx [184.100643ms]
  Jul 29 17:06:52.320: INFO: Created: latency-svc-fkhwv
  Jul 29 17:06:52.336: INFO: Created: latency-svc-4bs5p
  Jul 29 17:06:52.344: INFO: Created: latency-svc-n4r9k
  Jul 29 17:06:52.347: INFO: Got endpoints: latency-svc-nlpbx [217.645001ms]
  Jul 29 17:06:52.353: INFO: Got endpoints: latency-svc-vw854 [222.492683ms]
  Jul 29 17:06:52.357: INFO: Created: latency-svc-9bzgf
  Jul 29 17:06:52.378: INFO: Created: latency-svc-722fj
  Jul 29 17:06:52.379: INFO: Got endpoints: latency-svc-fkhwv [249.184177ms]
  Jul 29 17:06:52.379: INFO: Got endpoints: latency-svc-4bs5p [248.128863ms]
  Jul 29 17:06:52.391: INFO: Got endpoints: latency-svc-n4r9k [260.085045ms]
  Jul 29 17:06:52.392: INFO: Got endpoints: latency-svc-9bzgf [260.501236ms]
  Jul 29 17:06:52.405: INFO: Got endpoints: latency-svc-722fj [273.468134ms]
  Jul 29 17:06:52.415: INFO: Created: latency-svc-rmnc6
  Jul 29 17:06:52.423: INFO: Created: latency-svc-gsn5d
  Jul 29 17:06:52.427: INFO: Got endpoints: latency-svc-rmnc6 [298.965571ms]
  Jul 29 17:06:52.443: INFO: Created: latency-svc-42jv9
  Jul 29 17:06:52.455: INFO: Created: latency-svc-82s5h
  Jul 29 17:06:52.465: INFO: Got endpoints: latency-svc-gsn5d [291.540624ms]
  Jul 29 17:06:52.473: INFO: Created: latency-svc-pt5wh
  Jul 29 17:06:52.485: INFO: Created: latency-svc-gmzwc
  Jul 29 17:06:52.488: INFO: Got endpoints: latency-svc-82s5h [264.832692ms]
  Jul 29 17:06:52.488: INFO: Got endpoints: latency-svc-pt5wh [244.304072ms]
  Jul 29 17:06:52.489: INFO: Got endpoints: latency-svc-42jv9 [287.409646ms]
  Jul 29 17:06:52.519: INFO: Created: latency-svc-sncdd
  Jul 29 17:06:52.537: INFO: Created: latency-svc-dzxqk
  Jul 29 17:06:52.540: INFO: Got endpoints: latency-svc-gmzwc [266.242893ms]
  Jul 29 17:06:52.558: INFO: Got endpoints: latency-svc-sncdd [244.439434ms]
  Jul 29 17:06:52.569: INFO: Created: latency-svc-k85kw
  Jul 29 17:06:52.580: INFO: Got endpoints: latency-svc-dzxqk [265.354801ms]
  Jul 29 17:06:52.590: INFO: Created: latency-svc-7g89l
  Jul 29 17:06:52.601: INFO: Got endpoints: latency-svc-k85kw [253.720443ms]
  Jul 29 17:06:52.603: INFO: Created: latency-svc-f9tlz
  Jul 29 17:06:52.610: INFO: Got endpoints: latency-svc-7g89l [256.138451ms]
  Jul 29 17:06:52.627: INFO: Got endpoints: latency-svc-f9tlz [246.528138ms]
  Jul 29 17:06:52.768: INFO: Created: latency-svc-st64c
  Jul 29 17:06:52.773: INFO: Created: latency-svc-p5mrr
  Jul 29 17:06:52.773: INFO: Created: latency-svc-q4zm5
  Jul 29 17:06:52.775: INFO: Created: latency-svc-j8zbl
  Jul 29 17:06:52.778: INFO: Created: latency-svc-5d4g4
  Jul 29 17:06:52.808: INFO: Created: latency-svc-rppgx
  Jul 29 17:06:52.808: INFO: Created: latency-svc-jx7jj
  Jul 29 17:06:52.808: INFO: Created: latency-svc-6v6qd
  Jul 29 17:06:52.808: INFO: Created: latency-svc-9kvwj
  Jul 29 17:06:52.809: INFO: Created: latency-svc-6b252
  Jul 29 17:06:52.811: INFO: Got endpoints: latency-svc-st64c [183.005814ms]
  Jul 29 17:06:52.814: INFO: Created: latency-svc-nzsfj
  Jul 29 17:06:52.818: INFO: Created: latency-svc-nqld5
  Jul 29 17:06:52.829: INFO: Created: latency-svc-d5q2q
  Jul 29 17:06:52.829: INFO: Created: latency-svc-2nxhv
  Jul 29 17:06:52.830: INFO: Created: latency-svc-6lp4v
  Jul 29 17:06:52.834: INFO: Got endpoints: latency-svc-p5mrr [345.59925ms]
  Jul 29 17:06:52.835: INFO: Got endpoints: latency-svc-j8zbl [346.319114ms]
  Jul 29 17:06:52.838: INFO: Got endpoints: latency-svc-q4zm5 [257.627015ms]
  Jul 29 17:06:52.839: INFO: Got endpoints: latency-svc-d5q2q [447.476439ms]
  Jul 29 17:06:52.852: INFO: Created: latency-svc-z42fl
  Jul 29 17:06:52.861: INFO: Got endpoints: latency-svc-2nxhv [395.06589ms]
  Jul 29 17:06:52.868: INFO: Got endpoints: latency-svc-6lp4v [257.436461ms]
  Jul 29 17:06:52.890: INFO: Got endpoints: latency-svc-nqld5 [510.887937ms]
  Jul 29 17:06:52.893: INFO: Got endpoints: latency-svc-nzsfj [488.057915ms]
  Jul 29 17:06:52.894: INFO: Created: latency-svc-bbw2n
  Jul 29 17:06:52.912: INFO: Got endpoints: latency-svc-jx7jj [519.766065ms]
  Jul 29 17:06:52.913: INFO: Got endpoints: latency-svc-6b252 [311.862858ms]
  Jul 29 17:06:52.912: INFO: Got endpoints: latency-svc-rppgx [485.377751ms]
  Jul 29 17:06:52.912: INFO: Got endpoints: latency-svc-6v6qd [422.89559ms]
  Jul 29 17:06:52.935: INFO: Got endpoints: latency-svc-5d4g4 [375.980076ms]
  Jul 29 17:06:52.936: INFO: Got endpoints: latency-svc-z42fl [125.06168ms]
  Jul 29 17:06:52.936: INFO: Got endpoints: latency-svc-bbw2n [101.426843ms]
  Jul 29 17:06:52.936: INFO: Got endpoints: latency-svc-9kvwj [395.735824ms]
  Jul 29 17:06:52.961: INFO: Created: latency-svc-7z4mq
  Jul 29 17:06:52.965: INFO: Created: latency-svc-zk7nh
  Jul 29 17:06:52.970: INFO: Got endpoints: latency-svc-7z4mq [130.034325ms]
  Jul 29 17:06:52.984: INFO: Created: latency-svc-cfj5h
  Jul 29 17:06:52.991: INFO: Got endpoints: latency-svc-zk7nh [155.717538ms]
  Jul 29 17:06:53.017: INFO: Created: latency-svc-4s69n
  Jul 29 17:06:53.018: INFO: Got endpoints: latency-svc-cfj5h [178.88864ms]
  Jul 29 17:06:53.020: INFO: Got endpoints: latency-svc-4s69n [151.85039ms]
  Jul 29 17:06:53.026: INFO: Created: latency-svc-l7kwt
  Jul 29 17:06:53.037: INFO: Created: latency-svc-zwrm6
  Jul 29 17:06:53.045: INFO: Got endpoints: latency-svc-l7kwt [183.541418ms]
  Jul 29 17:06:53.056: INFO: Created: latency-svc-f4hzp
  Jul 29 17:06:53.066: INFO: Created: latency-svc-78vrl
  Jul 29 17:06:53.067: INFO: Got endpoints: latency-svc-zwrm6 [176.918013ms]
  Jul 29 17:06:53.088: INFO: Created: latency-svc-wlsnk
  Jul 29 17:06:53.095: INFO: Created: latency-svc-xs2x8
  Jul 29 17:06:53.106: INFO: Created: latency-svc-c5lht
  Jul 29 17:06:53.114: INFO: Got endpoints: latency-svc-f4hzp [220.400329ms]
  Jul 29 17:06:53.123: INFO: Created: latency-svc-lsf6f
  Jul 29 17:06:53.140: INFO: Created: latency-svc-qk282
  Jul 29 17:06:53.152: INFO: Created: latency-svc-pl5tt
  Jul 29 17:06:53.168: INFO: Created: latency-svc-gntc9
  Jul 29 17:06:53.169: INFO: Got endpoints: latency-svc-78vrl [256.923693ms]
  Jul 29 17:06:53.180: INFO: Created: latency-svc-c9ntw
  Jul 29 17:06:53.201: INFO: Created: latency-svc-vxx7c
  Jul 29 17:06:53.209: INFO: Created: latency-svc-srlvr
  Jul 29 17:06:53.218: INFO: Got endpoints: latency-svc-wlsnk [303.727483ms]
  Jul 29 17:06:53.229: INFO: Created: latency-svc-9lvdf
  Jul 29 17:06:53.243: INFO: Created: latency-svc-fgs26
  Jul 29 17:06:53.261: INFO: Created: latency-svc-7x68r
  Jul 29 17:06:53.269: INFO: Got endpoints: latency-svc-xs2x8 [353.9702ms]
  Jul 29 17:06:53.271: INFO: Created: latency-svc-46j8q
  E0729 17:06:53.282293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:53.287: INFO: Created: latency-svc-mnmhj
  Jul 29 17:06:53.304: INFO: Created: latency-svc-jh8fl
  Jul 29 17:06:53.324: INFO: Got endpoints: latency-svc-c5lht [408.495256ms]
  Jul 29 17:06:53.325: INFO: Created: latency-svc-gbgbg
  Jul 29 17:06:53.345: INFO: Created: latency-svc-qm4qk
  Jul 29 17:06:53.370: INFO: Got endpoints: latency-svc-lsf6f [428.693179ms]
  Jul 29 17:06:53.396: INFO: Created: latency-svc-87cgp
  Jul 29 17:06:53.416: INFO: Got endpoints: latency-svc-qk282 [476.772908ms]
  Jul 29 17:06:53.439: INFO: Created: latency-svc-85w4d
  Jul 29 17:06:53.468: INFO: Got endpoints: latency-svc-pl5tt [525.478036ms]
  Jul 29 17:06:53.502: INFO: Created: latency-svc-2sf5z
  Jul 29 17:06:53.523: INFO: Got endpoints: latency-svc-gntc9 [574.981289ms]
  Jul 29 17:06:53.581: INFO: Got endpoints: latency-svc-c9ntw [610.817618ms]
  Jul 29 17:06:53.598: INFO: Created: latency-svc-b6rhm
  Jul 29 17:06:53.628: INFO: Got endpoints: latency-svc-vxx7c [636.347161ms]
  Jul 29 17:06:53.655: INFO: Created: latency-svc-rvlg4
  Jul 29 17:06:53.666: INFO: Got endpoints: latency-svc-srlvr [648.009658ms]
  Jul 29 17:06:53.685: INFO: Created: latency-svc-6gxnv
  Jul 29 17:06:53.715: INFO: Created: latency-svc-m5mb9
  Jul 29 17:06:53.715: INFO: Got endpoints: latency-svc-9lvdf [695.398434ms]
  Jul 29 17:06:53.739: INFO: Created: latency-svc-tkw7z
  Jul 29 17:06:53.765: INFO: Got endpoints: latency-svc-fgs26 [718.715078ms]
  Jul 29 17:06:53.785: INFO: Created: latency-svc-ggz9l
  Jul 29 17:06:53.816: INFO: Got endpoints: latency-svc-7x68r [747.933438ms]
  Jul 29 17:06:53.840: INFO: Created: latency-svc-pl4xr
  Jul 29 17:06:53.867: INFO: Got endpoints: latency-svc-46j8q [752.013485ms]
  Jul 29 17:06:53.888: INFO: Created: latency-svc-mft5m
  Jul 29 17:06:53.922: INFO: Got endpoints: latency-svc-mnmhj [751.777491ms]
  Jul 29 17:06:53.944: INFO: Created: latency-svc-5t79z
  Jul 29 17:06:53.967: INFO: Got endpoints: latency-svc-jh8fl [748.227136ms]
  Jul 29 17:06:53.993: INFO: Created: latency-svc-lqnf6
  Jul 29 17:06:54.017: INFO: Got endpoints: latency-svc-gbgbg [748.213511ms]
  Jul 29 17:06:54.036: INFO: Created: latency-svc-76cx4
  Jul 29 17:06:54.066: INFO: Got endpoints: latency-svc-qm4qk [742.22277ms]
  Jul 29 17:06:54.084: INFO: Created: latency-svc-hzdfz
  Jul 29 17:06:54.118: INFO: Got endpoints: latency-svc-87cgp [748.086907ms]
  Jul 29 17:06:54.135: INFO: Created: latency-svc-wl5p2
  Jul 29 17:06:54.166: INFO: Got endpoints: latency-svc-85w4d [749.743587ms]
  Jul 29 17:06:54.185: INFO: Created: latency-svc-csr9d
  Jul 29 17:06:54.215: INFO: Got endpoints: latency-svc-2sf5z [747.436851ms]
  Jul 29 17:06:54.231: INFO: Created: latency-svc-4mzss
  Jul 29 17:06:54.266: INFO: Got endpoints: latency-svc-b6rhm [743.536479ms]
  E0729 17:06:54.283062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:54.289: INFO: Created: latency-svc-8fdv8
  Jul 29 17:06:54.319: INFO: Got endpoints: latency-svc-6gxnv [737.844693ms]
  Jul 29 17:06:54.337: INFO: Created: latency-svc-rbcbm
  Jul 29 17:06:54.370: INFO: Got endpoints: latency-svc-rvlg4 [741.983286ms]
  Jul 29 17:06:54.391: INFO: Created: latency-svc-lwgl8
  Jul 29 17:06:54.420: INFO: Got endpoints: latency-svc-m5mb9 [753.320209ms]
  Jul 29 17:06:54.442: INFO: Created: latency-svc-jd6nk
  Jul 29 17:06:54.468: INFO: Got endpoints: latency-svc-tkw7z [752.869897ms]
  Jul 29 17:06:54.489: INFO: Created: latency-svc-t95kl
  Jul 29 17:06:54.518: INFO: Got endpoints: latency-svc-ggz9l [752.811135ms]
  Jul 29 17:06:54.539: INFO: Created: latency-svc-mccrw
  Jul 29 17:06:54.569: INFO: Got endpoints: latency-svc-pl4xr [752.953277ms]
  Jul 29 17:06:54.585: INFO: Created: latency-svc-jj6cs
  Jul 29 17:06:54.618: INFO: Got endpoints: latency-svc-mft5m [750.97964ms]
  Jul 29 17:06:54.633: INFO: Created: latency-svc-n55jh
  Jul 29 17:06:54.666: INFO: Got endpoints: latency-svc-5t79z [744.173354ms]
  Jul 29 17:06:54.683: INFO: Created: latency-svc-jxwxt
  Jul 29 17:06:54.715: INFO: Got endpoints: latency-svc-lqnf6 [747.08226ms]
  Jul 29 17:06:54.733: INFO: Created: latency-svc-zgncv
  Jul 29 17:06:54.766: INFO: Got endpoints: latency-svc-76cx4 [748.922395ms]
  Jul 29 17:06:54.784: INFO: Created: latency-svc-vm8qk
  Jul 29 17:06:54.817: INFO: Got endpoints: latency-svc-hzdfz [750.141014ms]
  Jul 29 17:06:54.832: INFO: Created: latency-svc-nqg42
  Jul 29 17:06:54.866: INFO: Got endpoints: latency-svc-wl5p2 [747.764461ms]
  Jul 29 17:06:54.885: INFO: Created: latency-svc-zvlfr
  Jul 29 17:06:54.916: INFO: Got endpoints: latency-svc-csr9d [749.731066ms]
  Jul 29 17:06:54.932: INFO: Created: latency-svc-4gcr8
  Jul 29 17:06:54.961: INFO: Got endpoints: latency-svc-4mzss [745.755496ms]
  Jul 29 17:06:54.980: INFO: Created: latency-svc-455t9
  Jul 29 17:06:55.017: INFO: Got endpoints: latency-svc-8fdv8 [750.062944ms]
  Jul 29 17:06:55.037: INFO: Created: latency-svc-gb6v9
  Jul 29 17:06:55.067: INFO: Got endpoints: latency-svc-rbcbm [747.830649ms]
  Jul 29 17:06:55.083: INFO: Created: latency-svc-jdc6v
  Jul 29 17:06:55.114: INFO: Got endpoints: latency-svc-lwgl8 [744.069768ms]
  Jul 29 17:06:55.132: INFO: Created: latency-svc-t46cj
  Jul 29 17:06:55.164: INFO: Got endpoints: latency-svc-jd6nk [742.562007ms]
  Jul 29 17:06:55.190: INFO: Created: latency-svc-mngth
  Jul 29 17:06:55.226: INFO: Got endpoints: latency-svc-t95kl [757.20315ms]
  Jul 29 17:06:55.245: INFO: Created: latency-svc-pmnjx
  Jul 29 17:06:55.271: INFO: Got endpoints: latency-svc-mccrw [752.622251ms]
  E0729 17:06:55.283531      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:55.294: INFO: Created: latency-svc-wbwwz
  Jul 29 17:06:55.317: INFO: Got endpoints: latency-svc-jj6cs [747.568228ms]
  Jul 29 17:06:55.341: INFO: Created: latency-svc-z4ff9
  Jul 29 17:06:55.367: INFO: Got endpoints: latency-svc-n55jh [748.632784ms]
  Jul 29 17:06:55.397: INFO: Created: latency-svc-rls5w
  Jul 29 17:06:55.419: INFO: Got endpoints: latency-svc-jxwxt [752.246438ms]
  Jul 29 17:06:55.447: INFO: Created: latency-svc-6mk6q
  Jul 29 17:06:55.472: INFO: Got endpoints: latency-svc-zgncv [757.002598ms]
  Jul 29 17:06:55.491: INFO: Created: latency-svc-zxbkh
  Jul 29 17:06:55.515: INFO: Got endpoints: latency-svc-vm8qk [748.284269ms]
  Jul 29 17:06:55.532: INFO: Created: latency-svc-q96ct
  Jul 29 17:06:55.567: INFO: Got endpoints: latency-svc-nqg42 [749.408632ms]
  Jul 29 17:06:55.611: INFO: Created: latency-svc-g5h9h
  Jul 29 17:06:55.630: INFO: Got endpoints: latency-svc-zvlfr [763.735835ms]
  Jul 29 17:06:55.657: INFO: Created: latency-svc-t249w
  Jul 29 17:06:55.666: INFO: Got endpoints: latency-svc-4gcr8 [750.09925ms]
  Jul 29 17:06:55.690: INFO: Created: latency-svc-8bgst
  Jul 29 17:06:55.736: INFO: Got endpoints: latency-svc-455t9 [774.272172ms]
  Jul 29 17:06:55.768: INFO: Got endpoints: latency-svc-gb6v9 [750.512751ms]
  Jul 29 17:06:55.816: INFO: Got endpoints: latency-svc-jdc6v [749.048931ms]
  Jul 29 17:06:55.850: INFO: Created: latency-svc-prdbj
  Jul 29 17:06:55.880: INFO: Got endpoints: latency-svc-t46cj [765.528439ms]
  Jul 29 17:06:55.896: INFO: Created: latency-svc-z8rt2
  Jul 29 17:06:55.903: INFO: Created: latency-svc-9q47v
  Jul 29 17:06:55.920: INFO: Got endpoints: latency-svc-mngth [755.84936ms]
  Jul 29 17:06:55.922: INFO: Created: latency-svc-9g6rd
  Jul 29 17:06:55.940: INFO: Created: latency-svc-htbkp
  Jul 29 17:06:55.965: INFO: Got endpoints: latency-svc-pmnjx [739.238304ms]
  Jul 29 17:06:55.988: INFO: Created: latency-svc-647hr
  Jul 29 17:06:56.015: INFO: Got endpoints: latency-svc-wbwwz [744.562726ms]
  Jul 29 17:06:56.034: INFO: Created: latency-svc-hr6bz
  Jul 29 17:06:56.069: INFO: Got endpoints: latency-svc-z4ff9 [752.330275ms]
  Jul 29 17:06:56.089: INFO: Created: latency-svc-xvdvv
  Jul 29 17:06:56.120: INFO: Got endpoints: latency-svc-rls5w [753.221661ms]
  Jul 29 17:06:56.138: INFO: Created: latency-svc-89snk
  Jul 29 17:06:56.171: INFO: Got endpoints: latency-svc-6mk6q [752.534215ms]
  Jul 29 17:06:56.192: INFO: Created: latency-svc-9q9cf
  Jul 29 17:06:56.216: INFO: Got endpoints: latency-svc-zxbkh [744.210977ms]
  Jul 29 17:06:56.235: INFO: Created: latency-svc-rqq4c
  Jul 29 17:06:56.266: INFO: Got endpoints: latency-svc-q96ct [750.983473ms]
  E0729 17:06:56.284593      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:56.285: INFO: Created: latency-svc-hpsrj
  Jul 29 17:06:56.315: INFO: Got endpoints: latency-svc-g5h9h [748.703029ms]
  Jul 29 17:06:56.331: INFO: Created: latency-svc-n2sgm
  Jul 29 17:06:56.365: INFO: Got endpoints: latency-svc-t249w [734.986436ms]
  Jul 29 17:06:56.392: INFO: Created: latency-svc-bx626
  Jul 29 17:06:56.415: INFO: Got endpoints: latency-svc-8bgst [748.872124ms]
  Jul 29 17:06:56.438: INFO: Created: latency-svc-x5mtv
  Jul 29 17:06:56.465: INFO: Got endpoints: latency-svc-prdbj [729.203981ms]
  Jul 29 17:06:56.487: INFO: Created: latency-svc-rdzml
  Jul 29 17:06:56.516: INFO: Got endpoints: latency-svc-z8rt2 [747.998236ms]
  Jul 29 17:06:56.536: INFO: Created: latency-svc-wj7x7
  Jul 29 17:06:56.570: INFO: Got endpoints: latency-svc-9q47v [753.738894ms]
  Jul 29 17:06:56.592: INFO: Created: latency-svc-4hpst
  Jul 29 17:06:56.620: INFO: Got endpoints: latency-svc-9g6rd [739.601124ms]
  Jul 29 17:06:56.640: INFO: Created: latency-svc-dknd5
  Jul 29 17:06:56.667: INFO: Got endpoints: latency-svc-htbkp [746.182292ms]
  Jul 29 17:06:56.687: INFO: Created: latency-svc-b2x95
  Jul 29 17:06:56.715: INFO: Got endpoints: latency-svc-647hr [749.743138ms]
  Jul 29 17:06:56.732: INFO: Created: latency-svc-kncww
  Jul 29 17:06:56.769: INFO: Got endpoints: latency-svc-hr6bz [752.992669ms]
  Jul 29 17:06:56.784: INFO: Created: latency-svc-lpprg
  Jul 29 17:06:56.820: INFO: Got endpoints: latency-svc-xvdvv [750.642633ms]
  Jul 29 17:06:56.835: INFO: Created: latency-svc-c7fqb
  Jul 29 17:06:56.872: INFO: Got endpoints: latency-svc-89snk [751.624454ms]
  Jul 29 17:06:56.895: INFO: Created: latency-svc-4f4nb
  Jul 29 17:06:56.918: INFO: Got endpoints: latency-svc-9q9cf [745.889271ms]
  Jul 29 17:06:56.938: INFO: Created: latency-svc-lcjzg
  Jul 29 17:06:56.974: INFO: Got endpoints: latency-svc-rqq4c [757.412503ms]
  Jul 29 17:06:56.991: INFO: Created: latency-svc-2d6lc
  Jul 29 17:06:57.020: INFO: Got endpoints: latency-svc-hpsrj [753.981029ms]
  Jul 29 17:06:57.037: INFO: Created: latency-svc-n9pfq
  Jul 29 17:06:57.066: INFO: Got endpoints: latency-svc-n2sgm [750.800784ms]
  Jul 29 17:06:57.089: INFO: Created: latency-svc-x7kkt
  Jul 29 17:06:57.116: INFO: Got endpoints: latency-svc-bx626 [749.992582ms]
  Jul 29 17:06:57.134: INFO: Created: latency-svc-tmlwj
  Jul 29 17:06:57.170: INFO: Got endpoints: latency-svc-x5mtv [754.215191ms]
  Jul 29 17:06:57.185: INFO: Created: latency-svc-kr85x
  Jul 29 17:06:57.215: INFO: Got endpoints: latency-svc-rdzml [749.301758ms]
  Jul 29 17:06:57.231: INFO: Created: latency-svc-c89j2
  Jul 29 17:06:57.265: INFO: Got endpoints: latency-svc-wj7x7 [748.958598ms]
  E0729 17:06:57.285393      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:57.288: INFO: Created: latency-svc-kdn6v
  Jul 29 17:06:57.318: INFO: Got endpoints: latency-svc-4hpst [748.019206ms]
  Jul 29 17:06:57.334: INFO: Created: latency-svc-2pjhr
  Jul 29 17:06:57.373: INFO: Got endpoints: latency-svc-dknd5 [752.483311ms]
  Jul 29 17:06:57.393: INFO: Created: latency-svc-dmmbj
  Jul 29 17:06:57.415: INFO: Got endpoints: latency-svc-b2x95 [747.904687ms]
  Jul 29 17:06:57.437: INFO: Created: latency-svc-qdnt2
  Jul 29 17:06:57.476: INFO: Got endpoints: latency-svc-kncww [760.680395ms]
  Jul 29 17:06:57.492: INFO: Created: latency-svc-ksz6x
  Jul 29 17:06:57.514: INFO: Got endpoints: latency-svc-lpprg [744.787421ms]
  Jul 29 17:06:57.583: INFO: Got endpoints: latency-svc-c7fqb [761.902969ms]
  Jul 29 17:06:57.593: INFO: Created: latency-svc-r886d
  Jul 29 17:06:57.619: INFO: Got endpoints: latency-svc-4f4nb [747.558228ms]
  Jul 29 17:06:57.626: INFO: Created: latency-svc-nwd5b
  Jul 29 17:06:57.639: INFO: Created: latency-svc-hl22s
  Jul 29 17:06:57.679: INFO: Got endpoints: latency-svc-lcjzg [760.656097ms]
  Jul 29 17:06:57.710: INFO: Created: latency-svc-rlrgr
  Jul 29 17:06:57.716: INFO: Got endpoints: latency-svc-2d6lc [741.591675ms]
  Jul 29 17:06:57.733: INFO: Created: latency-svc-cs8gg
  Jul 29 17:06:57.765: INFO: Got endpoints: latency-svc-n9pfq [744.659999ms]
  Jul 29 17:06:57.794: INFO: Created: latency-svc-589lk
  Jul 29 17:06:57.817: INFO: Got endpoints: latency-svc-x7kkt [750.17646ms]
  Jul 29 17:06:57.835: INFO: Created: latency-svc-st6rf
  Jul 29 17:06:57.868: INFO: Got endpoints: latency-svc-tmlwj [751.738065ms]
  Jul 29 17:06:57.892: INFO: Created: latency-svc-ldflr
  Jul 29 17:06:57.917: INFO: Got endpoints: latency-svc-kr85x [747.001813ms]
  Jul 29 17:06:57.933: INFO: Created: latency-svc-b9gp5
  Jul 29 17:06:57.969: INFO: Got endpoints: latency-svc-c89j2 [753.800977ms]
  Jul 29 17:06:57.985: INFO: Created: latency-svc-qd7rf
  Jul 29 17:06:58.020: INFO: Got endpoints: latency-svc-kdn6v [754.195211ms]
  Jul 29 17:06:58.040: INFO: Created: latency-svc-vsxxk
  Jul 29 17:06:58.067: INFO: Got endpoints: latency-svc-2pjhr [748.801172ms]
  Jul 29 17:06:58.083: INFO: Created: latency-svc-kvwv6
  Jul 29 17:06:58.116: INFO: Got endpoints: latency-svc-dmmbj [742.909775ms]
  Jul 29 17:06:58.131: INFO: Created: latency-svc-zv54v
  Jul 29 17:06:58.167: INFO: Got endpoints: latency-svc-qdnt2 [751.758618ms]
  Jul 29 17:06:58.190: INFO: Created: latency-svc-dstw4
  Jul 29 17:06:58.217: INFO: Got endpoints: latency-svc-ksz6x [740.896604ms]
  Jul 29 17:06:58.236: INFO: Created: latency-svc-ddfrk
  Jul 29 17:06:58.268: INFO: Got endpoints: latency-svc-r886d [753.097779ms]
  E0729 17:06:58.285857      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:58.289: INFO: Created: latency-svc-9shtj
  Jul 29 17:06:58.317: INFO: Got endpoints: latency-svc-nwd5b [733.764919ms]
  Jul 29 17:06:58.334: INFO: Created: latency-svc-tckz6
  Jul 29 17:06:58.368: INFO: Got endpoints: latency-svc-hl22s [747.754233ms]
  Jul 29 17:06:58.393: INFO: Created: latency-svc-tjtbp
  Jul 29 17:06:58.416: INFO: Got endpoints: latency-svc-rlrgr [736.407803ms]
  Jul 29 17:06:58.434: INFO: Created: latency-svc-8jxnr
  Jul 29 17:06:58.464: INFO: Got endpoints: latency-svc-cs8gg [748.817779ms]
  Jul 29 17:06:58.486: INFO: Created: latency-svc-9qsfv
  Jul 29 17:06:58.515: INFO: Got endpoints: latency-svc-589lk [749.633992ms]
  Jul 29 17:06:58.550: INFO: Created: latency-svc-jfgtz
  Jul 29 17:06:58.568: INFO: Got endpoints: latency-svc-st6rf [751.766905ms]
  Jul 29 17:06:58.588: INFO: Created: latency-svc-8w7wf
  Jul 29 17:06:58.615: INFO: Got endpoints: latency-svc-ldflr [747.028068ms]
  Jul 29 17:06:58.642: INFO: Created: latency-svc-q4k8z
  Jul 29 17:06:58.666: INFO: Got endpoints: latency-svc-b9gp5 [749.240015ms]
  Jul 29 17:06:58.685: INFO: Created: latency-svc-rg4r9
  Jul 29 17:06:58.716: INFO: Got endpoints: latency-svc-qd7rf [747.038454ms]
  Jul 29 17:06:58.730: INFO: Created: latency-svc-jhqjr
  Jul 29 17:06:58.764: INFO: Got endpoints: latency-svc-vsxxk [743.501223ms]
  Jul 29 17:06:58.782: INFO: Created: latency-svc-czrlh
  Jul 29 17:06:58.815: INFO: Got endpoints: latency-svc-kvwv6 [747.247141ms]
  Jul 29 17:06:58.831: INFO: Created: latency-svc-b9fl8
  Jul 29 17:06:58.868: INFO: Got endpoints: latency-svc-zv54v [750.086287ms]
  Jul 29 17:06:58.889: INFO: Created: latency-svc-ln6d4
  Jul 29 17:06:58.916: INFO: Got endpoints: latency-svc-dstw4 [748.732315ms]
  Jul 29 17:06:58.936: INFO: Created: latency-svc-2qssd
  Jul 29 17:06:58.968: INFO: Got endpoints: latency-svc-ddfrk [750.086527ms]
  Jul 29 17:06:58.986: INFO: Created: latency-svc-6mclk
  Jul 29 17:06:59.016: INFO: Got endpoints: latency-svc-9shtj [748.208035ms]
  Jul 29 17:06:59.037: INFO: Created: latency-svc-29b94
  Jul 29 17:06:59.066: INFO: Got endpoints: latency-svc-tckz6 [748.769856ms]
  Jul 29 17:06:59.081: INFO: Created: latency-svc-k7cns
  Jul 29 17:06:59.119: INFO: Got endpoints: latency-svc-tjtbp [750.942057ms]
  Jul 29 17:06:59.138: INFO: Created: latency-svc-5lvkn
  Jul 29 17:06:59.163: INFO: Got endpoints: latency-svc-8jxnr [747.301091ms]
  Jul 29 17:06:59.179: INFO: Created: latency-svc-q5x7d
  Jul 29 17:06:59.214: INFO: Got endpoints: latency-svc-9qsfv [746.756233ms]
  Jul 29 17:06:59.236: INFO: Created: latency-svc-b762m
  Jul 29 17:06:59.265: INFO: Got endpoints: latency-svc-jfgtz [749.866245ms]
  Jul 29 17:06:59.280: INFO: Created: latency-svc-4lhv8
  E0729 17:06:59.285903      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:06:59.317: INFO: Got endpoints: latency-svc-8w7wf [747.95253ms]
  Jul 29 17:06:59.331: INFO: Created: latency-svc-rwzh8
  Jul 29 17:06:59.366: INFO: Got endpoints: latency-svc-q4k8z [749.886789ms]
  Jul 29 17:06:59.388: INFO: Created: latency-svc-xzpvx
  Jul 29 17:06:59.416: INFO: Got endpoints: latency-svc-rg4r9 [749.973807ms]
  Jul 29 17:06:59.440: INFO: Created: latency-svc-bn8fs
  Jul 29 17:06:59.466: INFO: Got endpoints: latency-svc-jhqjr [750.283115ms]
  Jul 29 17:06:59.481: INFO: Created: latency-svc-55ns8
  Jul 29 17:06:59.511: INFO: Got endpoints: latency-svc-czrlh [746.44177ms]
  Jul 29 17:06:59.531: INFO: Created: latency-svc-jz7xs
  Jul 29 17:06:59.566: INFO: Got endpoints: latency-svc-b9fl8 [751.772924ms]
  Jul 29 17:06:59.586: INFO: Created: latency-svc-sn6k6
  Jul 29 17:06:59.621: INFO: Got endpoints: latency-svc-ln6d4 [752.986065ms]
  Jul 29 17:06:59.635: INFO: Created: latency-svc-hl77c
  Jul 29 17:06:59.664: INFO: Got endpoints: latency-svc-2qssd [748.08136ms]
  Jul 29 17:06:59.680: INFO: Created: latency-svc-g68ls
  Jul 29 17:06:59.732: INFO: Got endpoints: latency-svc-6mclk [764.46864ms]
  Jul 29 17:06:59.770: INFO: Got endpoints: latency-svc-29b94 [753.063876ms]
  Jul 29 17:06:59.774: INFO: Created: latency-svc-566dz
  Jul 29 17:06:59.782: INFO: Created: latency-svc-tmtxt
  Jul 29 17:06:59.812: INFO: Got endpoints: latency-svc-k7cns [745.807841ms]
  Jul 29 17:06:59.832: INFO: Created: latency-svc-rjnvj
  Jul 29 17:06:59.866: INFO: Got endpoints: latency-svc-5lvkn [746.941623ms]
  Jul 29 17:06:59.883: INFO: Created: latency-svc-r8v2h
  Jul 29 17:06:59.923: INFO: Got endpoints: latency-svc-q5x7d [759.719517ms]
  Jul 29 17:06:59.941: INFO: Created: latency-svc-mprk7
  Jul 29 17:06:59.966: INFO: Got endpoints: latency-svc-b762m [752.015518ms]
  Jul 29 17:07:00.015: INFO: Got endpoints: latency-svc-4lhv8 [749.68838ms]
  Jul 29 17:07:00.065: INFO: Got endpoints: latency-svc-rwzh8 [748.609311ms]
  Jul 29 17:07:00.118: INFO: Got endpoints: latency-svc-xzpvx [751.903206ms]
  Jul 29 17:07:00.165: INFO: Got endpoints: latency-svc-bn8fs [748.076757ms]
  Jul 29 17:07:00.214: INFO: Got endpoints: latency-svc-55ns8 [747.936975ms]
  Jul 29 17:07:00.268: INFO: Got endpoints: latency-svc-jz7xs [756.677874ms]
  E0729 17:07:00.286298      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:00.316: INFO: Got endpoints: latency-svc-sn6k6 [749.817934ms]
  Jul 29 17:07:00.369: INFO: Got endpoints: latency-svc-hl77c [748.676526ms]
  Jul 29 17:07:00.419: INFO: Got endpoints: latency-svc-g68ls [754.848604ms]
  Jul 29 17:07:00.469: INFO: Got endpoints: latency-svc-566dz [736.268368ms]
  Jul 29 17:07:00.522: INFO: Got endpoints: latency-svc-tmtxt [751.942774ms]
  Jul 29 17:07:00.565: INFO: Got endpoints: latency-svc-rjnvj [753.012426ms]
  Jul 29 17:07:00.617: INFO: Got endpoints: latency-svc-r8v2h [749.572989ms]
  Jul 29 17:07:00.667: INFO: Got endpoints: latency-svc-mprk7 [743.751262ms]
  Jul 29 17:07:00.669: INFO: Latencies: [44.66321ms 72.24495ms 94.186187ms 101.426843ms 114.622083ms 125.06168ms 130.034325ms 143.729375ms 151.85039ms 155.717538ms 176.918013ms 178.88864ms 183.005814ms 183.128112ms 183.541418ms 184.100643ms 217.645001ms 220.400329ms 222.492683ms 244.304072ms 244.439434ms 246.528138ms 248.128863ms 249.184177ms 253.720443ms 256.138451ms 256.923693ms 257.436461ms 257.627015ms 260.085045ms 260.501236ms 264.832692ms 265.354801ms 266.242893ms 273.468134ms 287.409646ms 291.540624ms 298.965571ms 303.727483ms 311.862858ms 345.59925ms 346.319114ms 353.9702ms 375.980076ms 395.06589ms 395.735824ms 408.495256ms 422.89559ms 428.693179ms 447.476439ms 476.772908ms 485.377751ms 488.057915ms 510.887937ms 519.766065ms 525.478036ms 574.981289ms 610.817618ms 636.347161ms 648.009658ms 695.398434ms 718.715078ms 729.203981ms 733.764919ms 734.986436ms 736.268368ms 736.407803ms 737.844693ms 739.238304ms 739.601124ms 740.896604ms 741.591675ms 741.983286ms 742.22277ms 742.562007ms 742.909775ms 743.501223ms 743.536479ms 743.751262ms 744.069768ms 744.173354ms 744.210977ms 744.562726ms 744.659999ms 744.787421ms 745.755496ms 745.807841ms 745.889271ms 746.182292ms 746.44177ms 746.756233ms 746.941623ms 747.001813ms 747.028068ms 747.038454ms 747.08226ms 747.247141ms 747.301091ms 747.436851ms 747.558228ms 747.568228ms 747.754233ms 747.764461ms 747.830649ms 747.904687ms 747.933438ms 747.936975ms 747.95253ms 747.998236ms 748.019206ms 748.076757ms 748.08136ms 748.086907ms 748.208035ms 748.213511ms 748.227136ms 748.284269ms 748.609311ms 748.632784ms 748.676526ms 748.703029ms 748.732315ms 748.769856ms 748.801172ms 748.817779ms 748.872124ms 748.922395ms 748.958598ms 749.048931ms 749.240015ms 749.301758ms 749.408632ms 749.572989ms 749.633992ms 749.68838ms 749.731066ms 749.743138ms 749.743587ms 749.817934ms 749.866245ms 749.886789ms 749.973807ms 749.992582ms 750.062944ms 750.086287ms 750.086527ms 750.09925ms 750.141014ms 750.17646ms 750.283115ms 750.512751ms 750.642633ms 750.800784ms 750.942057ms 750.97964ms 750.983473ms 751.624454ms 751.738065ms 751.758618ms 751.766905ms 751.772924ms 751.777491ms 751.903206ms 751.942774ms 752.013485ms 752.015518ms 752.246438ms 752.330275ms 752.483311ms 752.534215ms 752.622251ms 752.811135ms 752.869897ms 752.953277ms 752.986065ms 752.992669ms 753.012426ms 753.063876ms 753.097779ms 753.221661ms 753.320209ms 753.738894ms 753.800977ms 753.981029ms 754.195211ms 754.215191ms 754.848604ms 755.84936ms 756.677874ms 757.002598ms 757.20315ms 757.412503ms 759.719517ms 760.656097ms 760.680395ms 761.902969ms 763.735835ms 764.46864ms 765.528439ms 774.272172ms]
  Jul 29 17:07:00.670: INFO: 50 %ile: 747.568228ms
  Jul 29 17:07:00.670: INFO: 90 %ile: 753.320209ms
  Jul 29 17:07:00.671: INFO: 99 %ile: 765.528439ms
  Jul 29 17:07:00.671: INFO: Total sample count: 200
  Jul 29 17:07:00.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-9813" for this suite. @ 07/29/23 17:07:00.682
• [10.842 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 07/29/23 17:07:00.694
  Jul 29 17:07:00.694: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:07:00.697
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:00.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:00.726
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:07:00.729
  E0729 17:07:01.301598      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:02.290415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:03.290463      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:04.291240      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:07:04.765
  Jul 29 17:07:04.772: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-acedd5b2-ae45-4082-a2ef-3b3bcf9a611e container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:07:04.807
  Jul 29 17:07:04.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5857" for this suite. @ 07/29/23 17:07:04.865
• [4.185 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 07/29/23 17:07:04.881
  Jul 29 17:07:04.882: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 17:07:04.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:04.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:04.92
  E0729 17:07:05.292494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:06.293177      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 07/29/23 17:07:06.979
  Jul 29 17:07:06.980: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3729 pod-service-account-8572d04f-5d32-4fe0-b621-26a126f17244 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  E0729 17:07:07.293485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 07/29/23 17:07:07.298
  Jul 29 17:07:07.298: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3729 pod-service-account-8572d04f-5d32-4fe0-b621-26a126f17244 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 07/29/23 17:07:07.569
  Jul 29 17:07:07.569: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-3729 pod-service-account-8572d04f-5d32-4fe0-b621-26a126f17244 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jul 29 17:07:07.863: INFO: Got root ca configmap in namespace "svcaccounts-3729"
  Jul 29 17:07:07.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3729" for this suite. @ 07/29/23 17:07:07.877
• [3.011 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 07/29/23 17:07:07.895
  Jul 29 17:07:07.896: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename init-container @ 07/29/23 17:07:07.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:07.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:07.972
  STEP: creating the pod @ 07/29/23 17:07:07.981
  Jul 29 17:07:07.981: INFO: PodSpec: initContainers in spec.initContainers
  E0729 17:07:08.293991      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:09.302886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:10.294971      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:11.295950      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:12.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6461" for this suite. @ 07/29/23 17:07:12.136
• [4.263 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 07/29/23 17:07:12.16
  Jul 29 17:07:12.160: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:07:12.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:12.23
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:12.24
  E0729 17:07:12.295836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 07/29/23 17:07:12.332
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:07:13.173
  STEP: Deploying the webhook pod @ 07/29/23 17:07:13.197
  STEP: Wait for the deployment to be ready @ 07/29/23 17:07:13.22
  Jul 29 17:07:13.245: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0729 17:07:13.296785      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:14.296930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:07:15.27
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:07:15.29
  E0729 17:07:15.297278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:16.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0729 17:07:16.297542      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:17.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0729 17:07:17.298524      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:18.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0729 17:07:18.299068      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:19.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0729 17:07:19.299410      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:20.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0729 17:07:20.300044      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:21.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0729 17:07:21.300224      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:22.291: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0729 17:07:22.300640      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:22.301: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9991-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 17:07:22.826
  STEP: Creating a custom resource while v1 is storage version @ 07/29/23 17:07:22.874
  E0729 17:07:23.300987      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:24.301995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 07/29/23 17:07:25.169
  STEP: Patching the custom resource while v2 is storage version @ 07/29/23 17:07:25.197
  Jul 29 17:07:25.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 17:07:25.303196      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-9650" for this suite. @ 07/29/23 17:07:25.924
  STEP: Destroying namespace "webhook-markers-9372" for this suite. @ 07/29/23 17:07:25.936
• [13.790 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 07/29/23 17:07:25.956
  Jul 29 17:07:25.956: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 17:07:25.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:26.022
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:26.029
  Jul 29 17:07:26.077: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 17:07:26.091
  Jul 29 17:07:26.111: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:26.111: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:07:26.303511      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:27.132: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:27.132: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:07:27.304585      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:28.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:07:28.129: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 07/29/23 17:07:28.153
  STEP: Check that daemon pods images are updated. @ 07/29/23 17:07:28.176
  Jul 29 17:07:28.187: INFO: Wrong image for pod: daemon-set-6bhvn. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:07:28.188: INFO: Wrong image for pod: daemon-set-74xxm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:07:28.188: INFO: Wrong image for pod: daemon-set-qvnr7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0729 17:07:28.305369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:29.221: INFO: Wrong image for pod: daemon-set-74xxm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:07:29.221: INFO: Wrong image for pod: daemon-set-qvnr7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0729 17:07:29.305993      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:30.224: INFO: Wrong image for pod: daemon-set-74xxm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:07:30.224: INFO: Wrong image for pod: daemon-set-qvnr7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0729 17:07:30.309103      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:31.234: INFO: Pod daemon-set-5jhb5 is not available
  Jul 29 17:07:31.234: INFO: Wrong image for pod: daemon-set-74xxm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:07:31.234: INFO: Wrong image for pod: daemon-set-qvnr7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0729 17:07:31.309632      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:32.223: INFO: Wrong image for pod: daemon-set-74xxm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0729 17:07:32.310313      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:33.221: INFO: Wrong image for pod: daemon-set-74xxm. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:07:33.221: INFO: Pod daemon-set-vzcxt is not available
  E0729 17:07:33.311393      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:34.223: INFO: Pod daemon-set-cg76f is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 07/29/23 17:07:34.236
  Jul 29 17:07:34.253: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:07:34.253: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  E0729 17:07:34.311928      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:35.276: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:07:35.276: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  E0729 17:07:35.312319      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:36.270: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:07:36.270: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 17:07:36.297
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5532, will wait for the garbage collector to delete the pods @ 07/29/23 17:07:36.298
  E0729 17:07:36.313403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:36.363: INFO: Deleting DaemonSet.extensions daemon-set took: 10.341455ms
  Jul 29 17:07:36.464: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.179469ms
  E0729 17:07:37.313880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:37.872: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:37.872: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 17:07:37.878: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"36655"},"items":null}

  Jul 29 17:07:37.882: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"36655"},"items":null}

  Jul 29 17:07:37.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5532" for this suite. @ 07/29/23 17:07:37.919
• [11.978 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 07/29/23 17:07:37.944
  Jul 29 17:07:37.944: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 17:07:37.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:37.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:37.985
  Jul 29 17:07:37.989: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 17:07:38.004: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 17:07:38.010: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-1 before test
  Jul 29 17:07:38.025: INFO: cilium-lk6kf from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.025: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: cilium-node-init-gs57c from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.025: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: kube-addon-manager-vucheipi7kei-1 from kube-system started at 2023-07-29 15:23:00 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.025: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: kube-apiserver-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.025: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: kube-controller-manager-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.025: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: kube-proxy-qkz2r from kube-system started at 2023-07-29 15:14:13 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.025: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: kube-scheduler-vucheipi7kei-1 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.025: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-4kwpr from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:38.025: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 17:07:38.025: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-2 before test
  Jul 29 17:07:38.043: INFO: cilium-cqdq2 from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.043: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 17:07:38.043: INFO: cilium-node-init-5p29d from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.043: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 17:07:38.044: INFO: coredns-5d78c9869d-67zm5 from kube-system started at 2023-07-29 15:24:06 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.044: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 17:07:38.044: INFO: coredns-5d78c9869d-kpth8 from kube-system started at 2023-07-29 15:24:06 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.044: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 17:07:38.044: INFO: kube-addon-manager-vucheipi7kei-2 from kube-system started at 2023-07-29 15:23:00 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.044: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 17:07:38.045: INFO: kube-apiserver-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.045: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 17:07:38.045: INFO: kube-controller-manager-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.045: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 17:07:38.045: INFO: kube-proxy-n67bc from kube-system started at 2023-07-29 15:14:48 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.045: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 17:07:38.046: INFO: kube-scheduler-vucheipi7kei-2 from kube-system started at 2023-07-29 15:15:11 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.046: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 17:07:38.046: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-hpqzb from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:38.046: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 17:07:38.046: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 17:07:38.046: INFO: 
  Logging pods the apiserver thinks is on node vucheipi7kei-3 before test
  Jul 29 17:07:38.062: INFO: cilium-9924s from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.063: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 17:07:38.063: INFO: cilium-node-init-ndt4w from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.063: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 17:07:38.063: INFO: cilium-operator-64cdf5fc9d-wj2rs from kube-system started at 2023-07-29 15:23:21 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.063: INFO: 	Container cilium-operator ready: true, restart count 0
  Jul 29 17:07:38.063: INFO: kube-proxy-szdbr from kube-system started at 2023-07-29 15:15:22 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.064: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 17:07:38.064: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:24:45 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:38.064: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 17:07:38.064: INFO: sonobuoy-e2e-job-e435975ae918422f from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:38.064: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 17:07:38.065: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 17:07:38.065: INFO: sonobuoy-systemd-logs-daemon-set-4f7033e0cb74484d-rwzlm from sonobuoy started at 2023-07-29 15:24:55 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:38.065: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 17:07:38.065: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 17:07:38.065
  E0729 17:07:38.314485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:39.314457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 17:07:40.114
  STEP: Trying to apply a random label on the found node. @ 07/29/23 17:07:40.157
  STEP: verifying the node has the label kubernetes.io/e2e-9f716655-6a30-455d-b680-2d04b67807e2 42 @ 07/29/23 17:07:40.184
  STEP: Trying to relaunch the pod, now with labels. @ 07/29/23 17:07:40.197
  E0729 17:07:40.316944      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:41.317815      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-9f716655-6a30-455d-b680-2d04b67807e2 off the node vucheipi7kei-3 @ 07/29/23 17:07:42.245
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-9f716655-6a30-455d-b680-2d04b67807e2 @ 07/29/23 17:07:42.268
  Jul 29 17:07:42.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9462" for this suite. @ 07/29/23 17:07:42.295
• [4.368 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  E0729 17:07:42.317942      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a kubernetes client @ 07/29/23 17:07:42.319
  Jul 29 17:07:42.319: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 17:07:42.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:42.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:42.376
  STEP: create the container @ 07/29/23 17:07:42.383
  W0729 17:07:42.401602      14 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 07/29/23 17:07:42.401
  E0729 17:07:43.318249      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:44.318948      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:45.319246      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 07/29/23 17:07:45.437
  STEP: the container should be terminated @ 07/29/23 17:07:45.441
  STEP: the termination message should be set @ 07/29/23 17:07:45.441
  Jul 29 17:07:45.442: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/29/23 17:07:45.442
  Jul 29 17:07:45.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5875" for this suite. @ 07/29/23 17:07:45.534
• [3.235 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 07/29/23 17:07:45.557
  Jul 29 17:07:45.557: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:07:45.559
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:45.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:45.603
  STEP: Setting up server cert @ 07/29/23 17:07:45.689
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:07:46.124
  STEP: Deploying the webhook pod @ 07/29/23 17:07:46.143
  STEP: Wait for the deployment to be ready @ 07/29/23 17:07:46.171
  Jul 29 17:07:46.193: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 17:07:46.319721      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:47.320284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:48.237: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 17, 7, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 7, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 17, 7, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 7, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0729 17:07:48.320165      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:49.320453      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:07:50.247
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:07:50.266
  E0729 17:07:50.322195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:51.266: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 07/29/23 17:07:51.277
  STEP: Creating a custom resource definition that should be denied by the webhook @ 07/29/23 17:07:51.316
  Jul 29 17:07:51.317: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:07:51.321690      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:07:51.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4678" for this suite. @ 07/29/23 17:07:51.478
  STEP: Destroying namespace "webhook-markers-8932" for this suite. @ 07/29/23 17:07:51.511
• [5.969 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 07/29/23 17:07:51.529
  Jul 29 17:07:51.529: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 17:07:51.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:51.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:51.564
  STEP: Creating a pod to test env composition @ 07/29/23 17:07:51.57
  E0729 17:07:52.321996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:53.323072      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:54.323091      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:55.323284      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:07:55.613
  Jul 29 17:07:55.619: INFO: Trying to get logs from node vucheipi7kei-3 pod var-expansion-662b0202-c3c5-430c-b271-ae1e42173b9e container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 17:07:55.633
  Jul 29 17:07:55.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9879" for this suite. @ 07/29/23 17:07:55.672
• [4.156 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 07/29/23 17:07:55.687
  Jul 29 17:07:55.688: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename disruption @ 07/29/23 17:07:55.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:55.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:55.743
  STEP: creating the pdb @ 07/29/23 17:07:55.764
  STEP: Waiting for the pdb to be processed @ 07/29/23 17:07:55.783
  E0729 17:07:56.323358      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:57.323683      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 07/29/23 17:07:57.795
  STEP: Waiting for the pdb to be processed @ 07/29/23 17:07:57.808
  E0729 17:07:58.324587      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:07:59.324713      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 07/29/23 17:07:59.824
  STEP: Waiting for the pdb to be processed @ 07/29/23 17:07:59.847
  E0729 17:08:00.324984      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:01.326260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 07/29/23 17:08:01.875
  Jul 29 17:08:01.880: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9017" for this suite. @ 07/29/23 17:08:01.898
• [6.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 07/29/23 17:08:01.919
  Jul 29 17:08:01.920: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:08:01.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:01.953
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:01.957
  STEP: Creating configMap with name projected-configmap-test-volume-fda1f09f-5dd7-4bc4-a396-28b1b46f1307 @ 07/29/23 17:08:01.962
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:08:01.97
  E0729 17:08:02.326450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:03.327011      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:04.327875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:05.327992      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:08:06.025
  Jul 29 17:08:06.031: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-configmaps-c7130362-ceac-4836-a626-eeecaccde794 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 17:08:06.044
  Jul 29 17:08:06.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3752" for this suite. @ 07/29/23 17:08:06.077
• [4.169 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 07/29/23 17:08:06.091
  Jul 29 17:08:06.091: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 17:08:06.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:06.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:06.123
  STEP: creating an Endpoint @ 07/29/23 17:08:06.134
  STEP: waiting for available Endpoint @ 07/29/23 17:08:06.143
  STEP: listing all Endpoints @ 07/29/23 17:08:06.145
  STEP: updating the Endpoint @ 07/29/23 17:08:06.151
  STEP: fetching the Endpoint @ 07/29/23 17:08:06.16
  STEP: patching the Endpoint @ 07/29/23 17:08:06.166
  STEP: fetching the Endpoint @ 07/29/23 17:08:06.181
  STEP: deleting the Endpoint by Collection @ 07/29/23 17:08:06.187
  STEP: waiting for Endpoint deletion @ 07/29/23 17:08:06.206
  STEP: fetching the Endpoint @ 07/29/23 17:08:06.211
  Jul 29 17:08:06.217: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6012" for this suite. @ 07/29/23 17:08:06.227
• [0.147 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 07/29/23 17:08:06.239
  Jul 29 17:08:06.239: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:08:06.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:06.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:06.273
  STEP: Creating configMap with name configmap-test-upd-c6e0a9b6-09e6-4ae7-a923-d350d6d1247c @ 07/29/23 17:08:06.283
  STEP: Creating the pod @ 07/29/23 17:08:06.293
  E0729 17:08:06.328427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:07.328927      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:08.329938      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 07/29/23 17:08:08.33
  STEP: Waiting for pod with binary data @ 07/29/23 17:08:08.342
  Jul 29 17:08:08.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-668" for this suite. @ 07/29/23 17:08:08.371
• [2.145 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 07/29/23 17:08:08.388
  Jul 29 17:08:08.389: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 07/29/23 17:08:08.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:08.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:08.431
  STEP: Setting up the test @ 07/29/23 17:08:08.437
  STEP: Creating hostNetwork=false pod @ 07/29/23 17:08:08.437
  E0729 17:08:09.330868      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:10.332495      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 07/29/23 17:08:10.474
  E0729 17:08:11.332530      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:12.332900      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 07/29/23 17:08:12.517
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 07/29/23 17:08:12.517
  Jul 29 17:08:12.518: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:12.518: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:12.520: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:12.520: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 17:08:12.668: INFO: Exec stderr: ""
  Jul 29 17:08:12.668: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:12.668: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:12.671: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:12.671: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 17:08:12.778: INFO: Exec stderr: ""
  Jul 29 17:08:12.778: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:12.778: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:12.779: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:12.779: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 17:08:12.893: INFO: Exec stderr: ""
  Jul 29 17:08:12.893: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:12.893: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:12.895: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:12.895: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 17:08:12.991: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 07/29/23 17:08:12.991
  Jul 29 17:08:12.991: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:12.992: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:12.994: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:12.994: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 29 17:08:13.099: INFO: Exec stderr: ""
  Jul 29 17:08:13.099: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:13.099: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:13.102: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:13.102: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 29 17:08:13.207: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 07/29/23 17:08:13.208
  Jul 29 17:08:13.208: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:13.208: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:13.209: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:13.209: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  E0729 17:08:13.332891      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:13.372: INFO: Exec stderr: ""
  Jul 29 17:08:13.372: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:13.372: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:13.376: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:13.376: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 17:08:13.530: INFO: Exec stderr: ""
  Jul 29 17:08:13.530: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:13.530: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:13.533: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:13.533: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 17:08:13.711: INFO: Exec stderr: ""
  Jul 29 17:08:13.712: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-149 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:08:13.712: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:08:13.713: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:08:13.714: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-149/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 17:08:13.828: INFO: Exec stderr: ""
  Jul 29 17:08:13.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-149" for this suite. @ 07/29/23 17:08:13.84
• [5.466 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 07/29/23 17:08:13.861
  Jul 29 17:08:13.861: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:08:13.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:13.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:13.917
  STEP: Setting up server cert @ 07/29/23 17:08:13.965
  E0729 17:08:14.333504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:08:14.977
  STEP: Deploying the webhook pod @ 07/29/23 17:08:14.988
  STEP: Wait for the deployment to be ready @ 07/29/23 17:08:15.01
  Jul 29 17:08:15.027: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 17:08:15.334468      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:16.334734      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:08:17.059
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:08:17.085
  E0729 17:08:17.335175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:18.086: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 07/29/23 17:08:18.095
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 07/29/23 17:08:18.099
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 07/29/23 17:08:18.099
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 07/29/23 17:08:18.099
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 07/29/23 17:08:18.101
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/29/23 17:08:18.101
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/29/23 17:08:18.105
  Jul 29 17:08:18.105: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3208" for this suite. @ 07/29/23 17:08:18.185
  STEP: Destroying namespace "webhook-markers-7977" for this suite. @ 07/29/23 17:08:18.197
• [4.355 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 07/29/23 17:08:18.218
  Jul 29 17:08:18.218: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:08:18.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:18.258
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:18.265
  STEP: Creating configMap configmap-3908/configmap-test-eaf6dd2a-9f93-4bce-979d-2ef769aa735b @ 07/29/23 17:08:18.27
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:08:18.283
  E0729 17:08:18.336457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:19.337750      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:20.338135      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:21.338363      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:22.339041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:08:22.341
  Jul 29 17:08:22.348: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-c3c05e4b-2cf0-4c64-a3ae-1a4d56218a31 container env-test: <nil>
  STEP: delete the pod @ 07/29/23 17:08:22.373
  Jul 29 17:08:22.409: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3908" for this suite. @ 07/29/23 17:08:22.421
• [4.216 seconds]
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 07/29/23 17:08:22.436
  Jul 29 17:08:22.436: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 17:08:22.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:22.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:22.478
  E0729 17:08:23.339801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:24.339627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:24.565: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9617" for this suite. @ 07/29/23 17:08:24.579
• [2.158 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 07/29/23 17:08:24.594
  Jul 29 17:08:24.594: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:08:24.595
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:24.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:24.647
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/29/23 17:08:24.656
  E0729 17:08:25.340156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:26.340441      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:27.340617      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:28.341427      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:08:28.708
  Jul 29 17:08:28.713: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-72847902-9ec2-415d-a136-f10fb86503b9 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:08:28.723
  Jul 29 17:08:28.752: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7019" for this suite. @ 07/29/23 17:08:28.761
• [4.188 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 07/29/23 17:08:28.784
  Jul 29 17:08:28.784: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 17:08:28.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:28.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:28.822
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 07/29/23 17:08:28.827
  E0729 17:08:29.341832      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:30.342902      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 07/29/23 17:08:30.871
  STEP: Then the orphan pod is adopted @ 07/29/23 17:08:30.881
  E0729 17:08:31.343058      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 07/29/23 17:08:31.894
  Jul 29 17:08:31.900: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/29/23 17:08:31.918
  E0729 17:08:32.344155      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:32.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2106" for this suite. @ 07/29/23 17:08:32.947
• [4.177 seconds]
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 07/29/23 17:08:32.963
  Jul 29 17:08:32.963: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:08:32.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:32.997
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:33.004
  STEP: Creating a pod to test downward api env vars @ 07/29/23 17:08:33.014
  E0729 17:08:33.345200      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:34.345911      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:35.347087      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:36.347016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:08:37.062
  Jul 29 17:08:37.069: INFO: Trying to get logs from node vucheipi7kei-3 pod downward-api-f93c78a9-b09c-4278-9784-fc47c86d5765 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 17:08:37.087
  Jul 29 17:08:37.123: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9201" for this suite. @ 07/29/23 17:08:37.136
• [4.192 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 07/29/23 17:08:37.158
  Jul 29 17:08:37.158: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 17:08:37.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:37.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:37.2
  STEP: creating service in namespace services-9781 @ 07/29/23 17:08:37.208
  STEP: creating service affinity-clusterip in namespace services-9781 @ 07/29/23 17:08:37.208
  STEP: creating replication controller affinity-clusterip in namespace services-9781 @ 07/29/23 17:08:37.229
  I0729 17:08:37.244926      14 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-9781, replica count: 3
  E0729 17:08:37.347767      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:38.347982      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:39.348293      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 17:08:40.298853      14 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 17:08:40.316: INFO: Creating new exec pod
  E0729 17:08:40.349400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:41.349926      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:42.350124      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:43.350820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:43.354: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-9781 exec execpod-affinityxf5lc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jul 29 17:08:43.641: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jul 29 17:08:43.641: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 17:08:43.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-9781 exec execpod-affinityxf5lc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.30.90 80'
  Jul 29 17:08:43.943: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.30.90 80\nConnection to 10.233.30.90 80 port [tcp/http] succeeded!\n"
  Jul 29 17:08:43.943: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 17:08:43.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=services-9781 exec execpod-affinityxf5lc -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.30.90:80/ ; done'
  E0729 17:08:44.351403      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:44.352: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.30.90:80/\n"
  Jul 29 17:08:44.352: INFO: stdout: "\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s\naffinity-clusterip-c627s"
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Received response from host: affinity-clusterip-c627s
  Jul 29 17:08:44.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 17:08:44.362: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-9781, will wait for the garbage collector to delete the pods @ 07/29/23 17:08:44.406
  Jul 29 17:08:44.479: INFO: Deleting ReplicationController affinity-clusterip took: 15.964399ms
  Jul 29 17:08:44.580: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.203141ms
  E0729 17:08:45.351521      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:46.352646      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-9781" for this suite. @ 07/29/23 17:08:46.927
• [9.784 seconds]
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 07/29/23 17:08:46.943
  Jul 29 17:08:46.943: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:08:46.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:46.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:46.974
  STEP: creating a ConfigMap @ 07/29/23 17:08:46.98
  STEP: fetching the ConfigMap @ 07/29/23 17:08:46.989
  STEP: patching the ConfigMap @ 07/29/23 17:08:46.995
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 07/29/23 17:08:47.008
  STEP: deleting the ConfigMap by collection with a label selector @ 07/29/23 17:08:47.017
  STEP: listing all ConfigMaps in test namespace @ 07/29/23 17:08:47.033
  Jul 29 17:08:47.039: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-708" for this suite. @ 07/29/23 17:08:47.047
• [0.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 07/29/23 17:08:47.071
  Jul 29 17:08:47.071: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:08:47.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:47.097
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:47.101
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:08:47.106
  E0729 17:08:47.352803      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:48.353872      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:49.354462      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:50.355344      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:08:51.149
  Jul 29 17:08:51.155: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-7a0383fa-3a29-4897-a3aa-aa58356a6880 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:08:51.17
  Jul 29 17:08:51.199: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2854" for this suite. @ 07/29/23 17:08:51.209
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 07/29/23 17:08:51.237
  Jul 29 17:08:51.238: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 17:08:51.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:51.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:51.284
  Jul 29 17:08:51.292: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:08:51.355910      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:52.356043      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:53.356309      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:54.356821      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:54.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5127" for this suite. @ 07/29/23 17:08:54.611
• [3.382 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 07/29/23 17:08:54.622
  Jul 29 17:08:54.622: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:08:54.623
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:54.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:54.658
  STEP: Create set of pods @ 07/29/23 17:08:54.662
  Jul 29 17:08:54.680: INFO: created test-pod-1
  Jul 29 17:08:54.691: INFO: created test-pod-2
  Jul 29 17:08:54.709: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 07/29/23 17:08:54.709
  E0729 17:08:55.380792      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:08:56.381024      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 07/29/23 17:08:56.794
  Jul 29 17:08:56.801: INFO: Pod quantity 3 is different from expected quantity 0
  E0729 17:08:57.382129      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:57.812: INFO: Pod quantity 3 is different from expected quantity 0
  E0729 17:08:58.382820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:08:58.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6856" for this suite. @ 07/29/23 17:08:58.816
• [4.205 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 07/29/23 17:08:58.828
  Jul 29 17:08:58.828: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename secrets @ 07/29/23 17:08:58.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:58.862
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:58.867
  STEP: Creating secret with name s-test-opt-del-660dbd05-9a93-4e06-8442-7a3b6c3eb3c8 @ 07/29/23 17:08:58.882
  STEP: Creating secret with name s-test-opt-upd-1fe03d63-59b4-4279-b246-b7ddb2ca66b9 @ 07/29/23 17:08:58.894
  STEP: Creating the pod @ 07/29/23 17:08:58.902
  E0729 17:08:59.383668      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:00.384645      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-660dbd05-9a93-4e06-8442-7a3b6c3eb3c8 @ 07/29/23 17:09:00.97
  STEP: Updating secret s-test-opt-upd-1fe03d63-59b4-4279-b246-b7ddb2ca66b9 @ 07/29/23 17:09:00.981
  STEP: Creating secret with name s-test-opt-create-3f3ed15f-8ac4-456b-a9cb-8b05c38c731a @ 07/29/23 17:09:00.991
  STEP: waiting to observe update in volume @ 07/29/23 17:09:00.999
  E0729 17:09:01.385722      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:02.386674      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:03.387209      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:04.387446      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:05.388207      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:06.389077      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:07.390291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:08.390253      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:09.390567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:10.390400      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:11.391013      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:12.391118      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:13.391678      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:14.391921      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:15.392501      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:16.393093      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:17.394228      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:18.394692      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:19.395602      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:20.396179      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:21.396836      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:22.397061      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:23.397607      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:24.397886      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:25.397995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:26.398512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:27.399141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:28.399626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:29.400529      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:30.400377      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:31.400676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:32.401296      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:33.401863      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:34.402027      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:35.403276      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:36.402605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:37.403014      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:38.403726      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:39.403973      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:40.404909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:41.405567      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:42.405848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:43.406670      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:44.407004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:45.407954      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:46.408017      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:47.408354      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:48.408545      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:49.409131      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:50.409572      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:51.410304      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:52.410610      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:53.432801      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:54.417818      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:55.418269      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:56.419062      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:57.419015      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:58.419230      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:09:59.419980      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:00.419675      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:01.419851      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:02.420064      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:03.420880      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:04.420972      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:05.421122      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:06.421481      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:07.423111      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:08.422875      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:09.422885      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:10.423677      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:11.423959      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:12.424167      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:13.424648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:14.425327      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:15.426793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:16.426668      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:17.426863      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:18.426963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:19.428080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:20.428089      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:21.428395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:22.429280      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:23.429522      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:24.429777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:25.429790      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:26.430046      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:27.430065      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:28.430450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:29.430759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:10:29.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5331" for this suite. @ 07/29/23 17:10:29.883
• [91.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 07/29/23 17:10:29.911
  Jul 29 17:10:29.912: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:10:29.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:10:29.947
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:10:29.952
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/29/23 17:10:29.957
  E0729 17:10:30.430858      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:31.431533      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:32.431635      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:33.431917      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:10:34.001
  Jul 29 17:10:34.009: INFO: Trying to get logs from node vucheipi7kei-1 pod pod-3da1c202-b5be-4ea0-ad47-df54ad4f6316 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:10:34.042
  Jul 29 17:10:34.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4791" for this suite. @ 07/29/23 17:10:34.091
• [4.194 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 07/29/23 17:10:34.107
  Jul 29 17:10:34.107: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:10:34.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:10:34.141
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:10:34.148
  STEP: Setting up server cert @ 07/29/23 17:10:34.209
  E0729 17:10:34.431935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:10:34.752
  STEP: Deploying the webhook pod @ 07/29/23 17:10:34.772
  STEP: Wait for the deployment to be ready @ 07/29/23 17:10:34.8
  Jul 29 17:10:34.823: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0729 17:10:35.432688      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:36.433031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:10:36.843: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 17, 10, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 10, 34, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 17, 10, 34, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 10, 34, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0729 17:10:37.432913      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:38.433777      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:10:38.852
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:10:38.868
  E0729 17:10:39.433843      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:10:39.869: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/29/23 17:10:39.877
  Jul 29 17:10:39.917: INFO: Waiting for webhook configuration to be ready...
  STEP: create a pod that should be denied by the webhook @ 07/29/23 17:10:40.041
  STEP: create a pod that causes the webhook to hang @ 07/29/23 17:10:40.08
  E0729 17:10:40.434533      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:41.435182      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:42.435916      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:43.436192      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:44.436227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:45.437457      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:46.437614      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:47.438092      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:48.438406      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:49.438563      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 07/29/23 17:10:50.096
  STEP: create a configmap that should be admitted by the webhook @ 07/29/23 17:10:50.147
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/29/23 17:10:50.178
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/29/23 17:10:50.197
  STEP: create a namespace that bypass the webhook @ 07/29/23 17:10:50.208
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 07/29/23 17:10:50.237
  Jul 29 17:10:50.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3836" for this suite. @ 07/29/23 17:10:50.421
  E0729 17:10:50.438981      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-markers-3607" for this suite. @ 07/29/23 17:10:50.452
  STEP: Destroying namespace "exempted-namespace-12" for this suite. @ 07/29/23 17:10:50.479
• [16.385 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 07/29/23 17:10:50.498
  Jul 29 17:10:50.498: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 17:10:50.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:10:50.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:10:50.546
  STEP: Create a pod template @ 07/29/23 17:10:50.555
  STEP: Replace a pod template @ 07/29/23 17:10:50.566
  Jul 29 17:10:50.585: INFO: Found updated podtemplate annotation: "true"

  Jul 29 17:10:50.586: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-4297" for this suite. @ 07/29/23 17:10:50.594
• [0.110 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 07/29/23 17:10:50.609
  Jul 29 17:10:50.609: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename dns @ 07/29/23 17:10:50.612
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:10:50.653
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:10:50.659
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 07/29/23 17:10:50.665
  Jul 29 17:10:50.682: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-4538  baa919ff-ce3c-4b08-9b92-33e07838fb4c 38207 0 2023-07-29 17:10:50 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-29 17:10:50 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmfj8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmfj8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0729 17:10:51.440273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:52.440536      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 07/29/23 17:10:52.707
  Jul 29 17:10:52.707: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-4538 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:10:52.707: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:10:52.709: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:10:52.709: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-4538/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 07/29/23 17:10:52.846
  Jul 29 17:10:52.846: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-4538 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:10:52.846: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  Jul 29 17:10:52.848: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:10:52.848: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-4538/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 17:10:52.990: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 17:10:53.002: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-4538" for this suite. @ 07/29/23 17:10:53.026
• [2.429 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 07/29/23 17:10:53.039
  Jul 29 17:10:53.039: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:10:53.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:10:53.07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:10:53.075
  STEP: Creating a ResourceQuota @ 07/29/23 17:10:53.079
  STEP: Getting a ResourceQuota @ 07/29/23 17:10:53.095
  STEP: Updating a ResourceQuota @ 07/29/23 17:10:53.105
  STEP: Verifying a ResourceQuota was modified @ 07/29/23 17:10:53.113
  STEP: Deleting a ResourceQuota @ 07/29/23 17:10:53.119
  STEP: Verifying the deleted ResourceQuota @ 07/29/23 17:10:53.14
  Jul 29 17:10:53.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1464" for this suite. @ 07/29/23 17:10:53.152
• [0.125 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 07/29/23 17:10:53.164
  Jul 29 17:10:53.164: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 17:10:53.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:10:53.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:10:53.201
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 07/29/23 17:10:53.206
  Jul 29 17:10:53.224: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0729 17:10:53.441130      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:54.441691      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:55.442746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:56.442841      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:57.443080      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:10:58.232: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 17:10:58.232
  STEP: getting scale subresource @ 07/29/23 17:10:58.232
  STEP: updating a scale subresource @ 07/29/23 17:10:58.238
  STEP: verifying the replicaset Spec.Replicas was modified @ 07/29/23 17:10:58.248
  STEP: Patch a scale subresource @ 07/29/23 17:10:58.252
  Jul 29 17:10:58.267: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7417" for this suite. @ 07/29/23 17:10:58.273
• [5.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 07/29/23 17:10:58.291
  Jul 29 17:10:58.291: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 17:10:58.293
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:10:58.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:10:58.343
  E0729 17:10:58.443695      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:10:59.444067      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:11:00.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1003" for this suite. @ 07/29/23 17:11:00.421
• [2.141 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 07/29/23 17:11:00.435
  Jul 29 17:11:00.435: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 17:11:00.437
  E0729 17:11:00.444867      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:11:00.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:11:00.476
  STEP: Creating service test in namespace statefulset-2695 @ 07/29/23 17:11:00.484
  STEP: Creating statefulset ss in namespace statefulset-2695 @ 07/29/23 17:11:00.501
  Jul 29 17:11:00.537: INFO: Found 0 stateful pods, waiting for 1
  E0729 17:11:01.445803      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:02.446387      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:03.447456      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:04.447075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:05.447243      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:06.447443      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:07.447647      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:08.447864      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:09.448117      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:10.448869      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:11:10.547: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 07/29/23 17:11:10.56
  STEP: updating a scale subresource @ 07/29/23 17:11:10.57
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/29/23 17:11:10.581
  STEP: Patch a scale subresource @ 07/29/23 17:11:10.588
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/29/23 17:11:10.608
  Jul 29 17:11:10.621: INFO: Deleting all statefulset in ns statefulset-2695
  Jul 29 17:11:10.641: INFO: Scaling statefulset ss to 0
  E0729 17:11:11.449378      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:12.449686      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:13.450126      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:14.450557      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:15.453041      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:16.451519      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:17.452282      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:18.452471      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:19.452708      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:20.453569      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:11:20.689: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 17:11:20.695: INFO: Deleting statefulset ss
  Jul 29 17:11:20.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2695" for this suite. @ 07/29/23 17:11:20.747
• [20.325 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 07/29/23 17:11:20.76
  Jul 29 17:11:20.760: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename events @ 07/29/23 17:11:20.763
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:11:20.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:11:20.846
  STEP: creating a test event @ 07/29/23 17:11:20.852
  STEP: listing all events in all namespaces @ 07/29/23 17:11:20.86
  STEP: patching the test event @ 07/29/23 17:11:20.87
  STEP: fetching the test event @ 07/29/23 17:11:20.884
  STEP: updating the test event @ 07/29/23 17:11:20.897
  STEP: getting the test event @ 07/29/23 17:11:20.919
  STEP: deleting the test event @ 07/29/23 17:11:20.926
  STEP: listing all events in all namespaces @ 07/29/23 17:11:20.944
  Jul 29 17:11:20.956: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-3054" for this suite. @ 07/29/23 17:11:20.967
• [0.217 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 07/29/23 17:11:20.981
  Jul 29 17:11:20.981: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename subpath @ 07/29/23 17:11:20.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:11:21.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:11:21.011
  STEP: Setting up data @ 07/29/23 17:11:21.015
  STEP: Creating pod pod-subpath-test-downwardapi-wjsv @ 07/29/23 17:11:21.037
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 17:11:21.038
  E0729 17:11:21.454759      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:22.455299      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:23.455975      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:24.456239      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:25.456962      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:26.457892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:27.458095      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:28.458335      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:29.458527      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:30.458556      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:31.458888      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:32.459167      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:33.460183      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:34.460291      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:35.460653      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:36.460892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:37.461131      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:38.461950      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:39.462113      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:40.462793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:41.463227      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:42.463559      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:43.464292      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:44.464407      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:11:45.18
  Jul 29 17:11:45.188: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-subpath-test-downwardapi-wjsv container test-container-subpath-downwardapi-wjsv: <nil>
  STEP: delete the pod @ 07/29/23 17:11:45.207
  STEP: Deleting pod pod-subpath-test-downwardapi-wjsv @ 07/29/23 17:11:45.242
  Jul 29 17:11:45.242: INFO: Deleting pod "pod-subpath-test-downwardapi-wjsv" in namespace "subpath-2579"
  Jul 29 17:11:45.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2579" for this suite. @ 07/29/23 17:11:45.255
• [24.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 07/29/23 17:11:45.273
  Jul 29 17:11:45.273: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:11:45.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:11:45.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:11:45.31
  STEP: Creating a ResourceQuota with terminating scope @ 07/29/23 17:11:45.315
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 17:11:45.324
  E0729 17:11:45.465504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:46.465746      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 07/29/23 17:11:47.335
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 17:11:47.348
  E0729 17:11:47.466535      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:48.466884      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 07/29/23 17:11:49.357
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 07/29/23 17:11:49.386
  E0729 17:11:49.467243      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:50.468156      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 07/29/23 17:11:51.405
  E0729 17:11:51.469095      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:52.468936      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/29/23 17:11:53.414
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 17:11:53.443
  E0729 17:11:53.469763      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:54.470036      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 07/29/23 17:11:55.451
  E0729 17:11:55.471369      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 07/29/23 17:11:55.472
  E0729 17:11:56.470966      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:57.471676      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 07/29/23 17:11:57.483
  E0729 17:11:58.472016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:11:59.472771      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/29/23 17:11:59.493
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 17:11:59.528
  E0729 17:12:00.473262      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:01.473374      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:01.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3021" for this suite. @ 07/29/23 17:12:01.548
• [16.295 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 07/29/23 17:12:01.572
  Jul 29 17:12:01.573: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:12:01.575
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:12:01.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:12:01.635
  STEP: Creating configMap with name configmap-test-volume-85c4e59d-3a7b-4f8d-b4e2-afb67a1129af @ 07/29/23 17:12:01.649
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:12:01.681
  E0729 17:12:02.473810      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:03.474388      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:04.474266      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:05.476389      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:12:05.728
  Jul 29 17:12:05.737: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-77965eaf-75d1-443a-ac53-55caf047be9b container configmap-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 17:12:05.751
  Jul 29 17:12:05.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3195" for this suite. @ 07/29/23 17:12:05.801
• [4.243 seconds]
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 07/29/23 17:12:05.816
  Jul 29 17:12:05.816: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename dns @ 07/29/23 17:12:05.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:12:05.851
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:12:05.857
  STEP: Creating a test headless service @ 07/29/23 17:12:05.862
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2129.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2129.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local;sleep 1; done
   @ 07/29/23 17:12:05.874
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-2129.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-2129.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local;sleep 1; done
   @ 07/29/23 17:12:05.874
  STEP: creating a pod to probe DNS @ 07/29/23 17:12:05.874
  STEP: submitting the pod to kubernetes @ 07/29/23 17:12:05.875
  E0729 17:12:06.477584      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:07.477824      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 07/29/23 17:12:07.922
  STEP: looking for the results for each expected name from probers @ 07/29/23 17:12:07.929
  Jul 29 17:12:07.942: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:07.950: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:07.957: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:07.964: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:07.971: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:07.979: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:07.986: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:07.993: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:07.994: INFO: Lookups using dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local]

  E0729 17:12:08.478332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:09.478808      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:10.479800      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:11.480505      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:12.480789      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:13.008: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:13.017: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:13.025: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:13.033: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:13.043: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:13.051: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:13.063: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:13.075: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:13.076: INFO: Lookups using dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local]

  E0729 17:12:13.482022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:14.483005      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:15.484075      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:16.484278      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:17.484415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:18.004: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:18.015: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:18.025: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:18.034: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:18.042: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:18.048: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:18.057: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:18.064: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:18.064: INFO: Lookups using dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local]

  E0729 17:12:18.485271      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:19.485909      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:20.486070      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:21.486951      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:22.487382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:23.003: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:23.012: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:23.017: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:23.023: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:23.031: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:23.040: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:23.045: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:23.050: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:23.051: INFO: Lookups using dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local]

  E0729 17:12:23.487582      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:24.488170      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:25.488512      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:26.489341      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:27.489935      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:28.004: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:28.013: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:28.023: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:28.031: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:28.039: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:28.047: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:28.055: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:28.062: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:28.063: INFO: Lookups using dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local]

  E0729 17:12:28.490837      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:29.491236      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:30.492813      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:31.492551      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:32.492794      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:33.002: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:33.010: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:33.019: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:33.024: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:33.033: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:33.044: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:33.052: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:33.061: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:33.061: INFO: Lookups using dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local]

  E0729 17:12:33.493004      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:34.493578      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:35.493752      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:36.493995      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:37.494260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:38.009: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:38.021: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:38.032: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:38.040: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:38.064: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:38.091: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:38.102: INFO: Unable to read jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:38.112: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local from pod dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8: the server could not find the requested resource (get pods dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8)
  Jul 29 17:12:38.112: INFO: Lookups using dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local wheezy_udp@dns-test-service-2.dns-2129.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-2129.svc.cluster.local jessie_udp@dns-test-service-2.dns-2129.svc.cluster.local jessie_tcp@dns-test-service-2.dns-2129.svc.cluster.local]

  E0729 17:12:38.494630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:39.494969      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:40.495485      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:41.496196      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:42.498933      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:43.060: INFO: DNS probes using dns-2129/dns-test-e31ef9c6-2ae6-4939-ae36-d0befee238d8 succeeded

  Jul 29 17:12:43.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 17:12:43.069
  STEP: deleting the test headless service @ 07/29/23 17:12:43.115
  STEP: Destroying namespace "dns-2129" for this suite. @ 07/29/23 17:12:43.156
• [37.360 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 07/29/23 17:12:43.177
  Jul 29 17:12:43.177: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:12:43.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:12:43.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:12:43.215
  STEP: set up a multi version CRD @ 07/29/23 17:12:43.222
  Jul 29 17:12:43.223: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:12:43.497413      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:44.497312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:45.498415      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:46.498848      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:47.498998      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 07/29/23 17:12:47.922
  STEP: check the unserved version gets removed @ 07/29/23 17:12:47.968
  E0729 17:12:48.500306      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:49.500996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/29/23 17:12:50.274
  E0729 17:12:50.501141      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:51.501803      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:52.502776      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:53.511637      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:54.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8043" for this suite. @ 07/29/23 17:12:54.477
• [11.312 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 07/29/23 17:12:54.493
  Jul 29 17:12:54.493: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:12:54.496
  E0729 17:12:54.511983      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:12:54.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:12:54.534
  STEP: Setting up server cert @ 07/29/23 17:12:54.568
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:12:55.252
  STEP: Deploying the webhook pod @ 07/29/23 17:12:55.267
  STEP: Wait for the deployment to be ready @ 07/29/23 17:12:55.285
  Jul 29 17:12:55.300: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0729 17:12:55.512232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:12:56.512658      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:12:57.326
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:12:57.345
  E0729 17:12:57.513590      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:58.346: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/29/23 17:12:58.353
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/29/23 17:12:58.385
  STEP: Creating a dummy validating-webhook-configuration object @ 07/29/23 17:12:58.414
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 07/29/23 17:12:58.43
  STEP: Creating a dummy mutating-webhook-configuration object @ 07/29/23 17:12:58.441
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 07/29/23 17:12:58.453
  Jul 29 17:12:58.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 17:12:58.515035      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-715" for this suite. @ 07/29/23 17:12:58.588
  STEP: Destroying namespace "webhook-markers-6563" for this suite. @ 07/29/23 17:12:58.598
• [4.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 07/29/23 17:12:58.63
  Jul 29 17:12:58.630: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 17:12:58.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:12:58.664
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:12:58.668
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 17:12:58.727
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 17:12:58.741
  Jul 29 17:12:58.758: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:12:58.758: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:12:59.519273      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:12:59.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:12:59.778: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:13:00.519395      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:00.778: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 17:13:00.778: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:13:01.519630      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:01.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:13:01.777: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  E0729 17:13:02.519563      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:02.795: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:13:02.795: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 07/29/23 17:13:02.804
  Jul 29 17:13:02.855: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:13:02.855: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  E0729 17:13:03.519790      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:03.882: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:13:03.882: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  E0729 17:13:04.520039      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:04.880: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:13:04.880: INFO: Node vucheipi7kei-2 is running 0 daemon pod, expected 1
  E0729 17:13:05.520370      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:05.874: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:13:05.875: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 17:13:05.883
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4774, will wait for the garbage collector to delete the pods @ 07/29/23 17:13:05.883
  Jul 29 17:13:05.955: INFO: Deleting DaemonSet.extensions daemon-set took: 12.717991ms
  Jul 29 17:13:06.056: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.635178ms
  E0729 17:13:06.521033      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:07.269: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:13:07.269: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 17:13:07.276: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39075"},"items":null}

  Jul 29 17:13:07.283: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39075"},"items":null}

  Jul 29 17:13:07.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4774" for this suite. @ 07/29/23 17:13:07.318
• [8.699 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 07/29/23 17:13:07.331
  Jul 29 17:13:07.331: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 17:13:07.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:07.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:07.374
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 17:13:07.417
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 17:13:07.428
  Jul 29 17:13:07.442: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:13:07.442: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:13:07.523008      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:08.470: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:13:08.470: INFO: Node vucheipi7kei-1 is running 0 daemon pod, expected 1
  E0729 17:13:08.522226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:09.476: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:13:09.476: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 07/29/23 17:13:09.494
  STEP: DeleteCollection of the DaemonSets @ 07/29/23 17:13:09.516
  E0729 17:13:09.522426      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verify that ReplicaSets have been deleted @ 07/29/23 17:13:09.554
  Jul 29 17:13:09.578: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"39123"},"items":null}

  Jul 29 17:13:09.590: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"39126"},"items":[{"metadata":{"name":"daemon-set-hl268","generateName":"daemon-set-","namespace":"daemonsets-7353","uid":"8a315239-0a01-46ca-8e7b-78d53f6ccf5b","resourceVersion":"39114","creationTimestamp":"2023-07-29T17:13:07Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cf8fdcbd-b67d-4659-91a4-2d05ef38bd77","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T17:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf8fdcbd-b67d-4659-91a4-2d05ef38bd77\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T17:13:08Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xzf27","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xzf27","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"vucheipi7kei-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["vucheipi7kei-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:08Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:08Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:07Z"}],"hostIP":"192.168.121.129","podIP":"10.233.65.44","podIPs":[{"ip":"10.233.65.44"}],"startTime":"2023-07-29T17:13:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T17:13:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://b47db6ca554ccc310e2b0e9fc3a5ba1002c154a9a036678c0ff3ba9287e28488","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-qwlch","generateName":"daemon-set-","namespace":"daemonsets-7353","uid":"d40ed51e-87fb-4eb6-9d8e-96e0a5b18c34","resourceVersion":"39124","creationTimestamp":"2023-07-29T17:13:07Z","deletionTimestamp":"2023-07-29T17:13:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cf8fdcbd-b67d-4659-91a4-2d05ef38bd77","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T17:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf8fdcbd-b67d-4659-91a4-2d05ef38bd77\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T17:13:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.175\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-ggnhg","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-ggnhg","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"vucheipi7kei-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["vucheipi7kei-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:07Z"}],"hostIP":"192.168.121.77","podIP":"10.233.64.175","podIPs":[{"ip":"10.233.64.175"}],"startTime":"2023-07-29T17:13:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T17:13:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://cc517519831c510e2906b7b1831de039dd8e80be0448cb97992e11a704a2b05b","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-tng4p","generateName":"daemon-set-","namespace":"daemonsets-7353","uid":"54d0b317-f193-42c8-a84c-20e37990f1b3","resourceVersion":"39126","creationTimestamp":"2023-07-29T17:13:07Z","deletionTimestamp":"2023-07-29T17:13:39Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"cf8fdcbd-b67d-4659-91a4-2d05ef38bd77","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T17:13:07Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"cf8fdcbd-b67d-4659-91a4-2d05ef38bd77\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T17:13:09Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.191\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-vzjh2","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-vzjh2","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"vucheipi7kei-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["vucheipi7kei-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:07Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:09Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:09Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T17:13:07Z"}],"hostIP":"192.168.121.88","podIP":"10.233.66.191","podIPs":[{"ip":"10.233.66.191"}],"startTime":"2023-07-29T17:13:07Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T17:13:08Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://e14545bb9d3c477370aafc79058ac79c6a811f6ada43747e349d4d5aa14248e0","started":true}],"qosClass":"BestEffort"}}]}

  Jul 29 17:13:09.629: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7353" for this suite. @ 07/29/23 17:13:09.664
• [2.351 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 07/29/23 17:13:09.691
  Jul 29 17:13:09.691: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:13:09.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:09.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:09.732
  E0729 17:13:10.523963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:11.523830      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:12.538421      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:13.532102      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:14.532235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:15.533119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:13:15.855
  Jul 29 17:13:15.863: INFO: Trying to get logs from node vucheipi7kei-3 pod client-envvars-e96ef4f9-2271-4e53-8652-7dc86ba89b30 container env3cont: <nil>
  STEP: delete the pod @ 07/29/23 17:13:15.88
  Jul 29 17:13:15.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9262" for this suite. @ 07/29/23 17:13:15.923
• [6.245 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 07/29/23 17:13:15.942
  Jul 29 17:13:15.943: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 17:13:15.945
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:15.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:15.988
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 17:13:16.005
  E0729 17:13:16.534627      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:17.534892      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 07/29/23 17:13:18.054
  E0729 17:13:18.534919      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:19.535195      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 07/29/23 17:13:20.093
  STEP: delete the pod with lifecycle hook @ 07/29/23 17:13:20.107
  E0729 17:13:20.535482      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:21.536450      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:22.135: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9788" for this suite. @ 07/29/23 17:13:22.144
• [6.217 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 07/29/23 17:13:22.161
  Jul 29 17:13:22.161: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 17:13:22.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:22.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:22.207
  Jul 29 17:13:22.213: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  W0729 17:13:22.215667      14 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc005ebf5e0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0729 17:13:22.537489      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:23.537878      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:24.538474      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0729 17:13:24.959238      14 warnings.go:70] unknown field "alpha"
  W0729 17:13:24.959467      14 warnings.go:70] unknown field "beta"
  W0729 17:13:24.959601      14 warnings.go:70] unknown field "delta"
  W0729 17:13:24.959728      14 warnings.go:70] unknown field "epsilon"
  W0729 17:13:24.959851      14 warnings.go:70] unknown field "gamma"
  Jul 29 17:13:25.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 17:13:25.539666      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "field-validation-1880" for this suite. @ 07/29/23 17:13:25.585
• [3.434 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 07/29/23 17:13:25.598
  Jul 29 17:13:25.598: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 17:13:25.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:25.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:25.659
  E0729 17:13:26.539989      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:27.540368      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:27.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3716" for this suite. @ 07/29/23 17:13:27.731
• [2.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 07/29/23 17:13:27.747
  Jul 29 17:13:27.747: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:13:27.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:27.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:27.87
  STEP: Setting up server cert @ 07/29/23 17:13:27.924
  E0729 17:13:28.540382      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:29.540978      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:13:29.737
  STEP: Deploying the webhook pod @ 07/29/23 17:13:29.76
  STEP: Wait for the deployment to be ready @ 07/29/23 17:13:29.78
  Jul 29 17:13:29.803: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 17:13:30.541315      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:31.541657      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:13:31.821
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:13:31.846
  E0729 17:13:32.542086      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:32.847: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 07/29/23 17:13:32.855
  Jul 29 17:13:32.893: INFO: Waiting for webhook configuration to be ready...
  STEP: create a configmap that should be updated by the webhook @ 07/29/23 17:13:33.014
  Jul 29 17:13:33.036: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8978" for this suite. @ 07/29/23 17:13:33.129
  STEP: Destroying namespace "webhook-markers-9592" for this suite. @ 07/29/23 17:13:33.142
• [5.415 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 07/29/23 17:13:33.171
  Jul 29 17:13:33.171: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:13:33.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:33.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:33.238
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/29/23 17:13:33.242
  E0729 17:13:33.542189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:34.543028      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:35.543890      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:36.544671      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:13:37.282
  Jul 29 17:13:37.287: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-b82ab191-efdb-4ac6-a12f-51809f5f1ac5 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:13:37.305
  Jul 29 17:13:37.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1126" for this suite. @ 07/29/23 17:13:37.34
• [4.180 seconds]
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 07/29/23 17:13:37.352
  Jul 29 17:13:37.353: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename gc @ 07/29/23 17:13:37.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:37.384
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:37.389
  STEP: create the rc @ 07/29/23 17:13:37.394
  W0729 17:13:37.403648      14 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0729 17:13:37.545323      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:38.545956      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:39.546448      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:40.547611      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:41.547732      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 07/29/23 17:13:42.412
  STEP: wait for all pods to be garbage collected @ 07/29/23 17:13:42.426
  E0729 17:13:42.547899      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:43.548178      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:44.548985      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:45.550064      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:46.550312      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/29/23 17:13:47.437
  E0729 17:13:47.550348      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:47.614: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 17:13:47.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2975" for this suite. @ 07/29/23 17:13:47.623
• [10.282 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 07/29/23 17:13:47.642
  Jul 29 17:13:47.642: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:13:47.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:47.676
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:47.68
  STEP: Creating configMap with name configmap-test-volume-map-d1394ba6-4f8f-4d3f-8533-041f48ae068c @ 07/29/23 17:13:47.685
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:13:47.693
  E0729 17:13:48.550857      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:49.551332      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:50.551137      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:51.551329      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:13:51.764
  Jul 29 17:13:51.771: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-configmaps-c5f21da2-194a-4fce-8b62-20924c36da32 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 17:13:51.791
  Jul 29 17:13:51.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3699" for this suite. @ 07/29/23 17:13:51.833
• [4.202 seconds]
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 07/29/23 17:13:51.845
  Jul 29 17:13:51.845: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:13:51.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:51.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:51.883
  STEP: creating a replication controller @ 07/29/23 17:13:51.889
  Jul 29 17:13:51.890: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 create -f -'
  E0729 17:13:52.552420      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:52.819: INFO: stderr: ""
  Jul 29 17:13:52.819: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 17:13:52.819
  Jul 29 17:13:52.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 17:13:53.043: INFO: stderr: ""
  Jul 29 17:13:53.043: INFO: stdout: "update-demo-nautilus-f29lc update-demo-nautilus-tnd9v "
  Jul 29 17:13:53.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get pods update-demo-nautilus-f29lc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:13:53.179: INFO: stderr: ""
  Jul 29 17:13:53.179: INFO: stdout: ""
  Jul 29 17:13:53.179: INFO: update-demo-nautilus-f29lc is created but not running
  E0729 17:13:53.553385      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:54.553644      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:55.553828      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:56.554040      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:13:57.554577      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:58.180: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 17:13:58.314: INFO: stderr: ""
  Jul 29 17:13:58.314: INFO: stdout: "update-demo-nautilus-f29lc update-demo-nautilus-tnd9v "
  Jul 29 17:13:58.314: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get pods update-demo-nautilus-f29lc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:13:58.444: INFO: stderr: ""
  Jul 29 17:13:58.444: INFO: stdout: "true"
  Jul 29 17:13:58.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get pods update-demo-nautilus-f29lc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0729 17:13:58.555391      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:13:58.570: INFO: stderr: ""
  Jul 29 17:13:58.570: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 17:13:58.570: INFO: validating pod update-demo-nautilus-f29lc
  Jul 29 17:13:58.594: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 17:13:58.594: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 17:13:58.594: INFO: update-demo-nautilus-f29lc is verified up and running
  Jul 29 17:13:58.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get pods update-demo-nautilus-tnd9v -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:13:58.719: INFO: stderr: ""
  Jul 29 17:13:58.719: INFO: stdout: "true"
  Jul 29 17:13:58.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get pods update-demo-nautilus-tnd9v -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 17:13:58.847: INFO: stderr: ""
  Jul 29 17:13:58.847: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 17:13:58.847: INFO: validating pod update-demo-nautilus-tnd9v
  Jul 29 17:13:58.869: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 17:13:58.869: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 17:13:58.869: INFO: update-demo-nautilus-tnd9v is verified up and running
  STEP: using delete to clean up resources @ 07/29/23 17:13:58.869
  Jul 29 17:13:58.869: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 delete --grace-period=0 --force -f -'
  Jul 29 17:13:59.025: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 17:13:59.025: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 29 17:13:59.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get rc,svc -l name=update-demo --no-headers'
  Jul 29 17:13:59.212: INFO: stderr: "No resources found in kubectl-5998 namespace.\n"
  Jul 29 17:13:59.212: INFO: stdout: ""
  Jul 29 17:13:59.212: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-5998 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 17:13:59.404: INFO: stderr: ""
  Jul 29 17:13:59.404: INFO: stdout: ""
  Jul 29 17:13:59.404: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5998" for this suite. @ 07/29/23 17:13:59.411
• [7.581 seconds]
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 07/29/23 17:13:59.426
  Jul 29 17:13:59.426: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:13:59.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:13:59.459
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:13:59.463
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 07/29/23 17:13:59.469
  E0729 17:13:59.555967      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:00.557153      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:01.558064      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:02.558322      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:14:03.53
  Jul 29 17:14:03.535: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-7afbd1c9-eefe-41b8-b099-856231f02f05 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:14:03.547
  E0729 17:14:03.558874      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:03.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4513" for this suite. @ 07/29/23 17:14:03.581
• [4.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 07/29/23 17:14:03.6
  Jul 29 17:14:03.600: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:14:03.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:14:03.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:14:03.635
  STEP: validating api versions @ 07/29/23 17:14:03.642
  Jul 29 17:14:03.643: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-39 api-versions'
  Jul 29 17:14:03.778: INFO: stderr: ""
  Jul 29 17:14:03.779: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jul 29 17:14:03.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-39" for this suite. @ 07/29/23 17:14:03.792
• [0.204 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 07/29/23 17:14:03.805
  Jul 29 17:14:03.805: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:14:03.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:14:03.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:14:03.842
  Jul 29 17:14:03.848: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:14:04.559508      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:05.560133      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 17:14:05.847
  Jul 29 17:14:05.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-6008 --namespace=crd-publish-openapi-6008 create -f -'
  E0729 17:14:06.560254      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:07.361: INFO: stderr: ""
  Jul 29 17:14:07.361: INFO: stdout: "e2e-test-crd-publish-openapi-8048-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 29 17:14:07.362: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-6008 --namespace=crd-publish-openapi-6008 delete e2e-test-crd-publish-openapi-8048-crds test-cr'
  Jul 29 17:14:07.521: INFO: stderr: ""
  Jul 29 17:14:07.522: INFO: stdout: "e2e-test-crd-publish-openapi-8048-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jul 29 17:14:07.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-6008 --namespace=crd-publish-openapi-6008 apply -f -'
  E0729 17:14:07.561376      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:07.965: INFO: stderr: ""
  Jul 29 17:14:07.965: INFO: stdout: "e2e-test-crd-publish-openapi-8048-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 29 17:14:07.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-6008 --namespace=crd-publish-openapi-6008 delete e2e-test-crd-publish-openapi-8048-crds test-cr'
  Jul 29 17:14:08.112: INFO: stderr: ""
  Jul 29 17:14:08.112: INFO: stdout: "e2e-test-crd-publish-openapi-8048-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/29/23 17:14:08.112
  Jul 29 17:14:08.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-6008 explain e2e-test-crd-publish-openapi-8048-crds'
  E0729 17:14:08.562367      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:09.319: INFO: stderr: ""
  Jul 29 17:14:09.319: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-8048-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0729 17:14:09.562893      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:10.563760      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:11.168: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6008" for this suite. @ 07/29/23 17:14:11.184
• [7.392 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 07/29/23 17:14:11.203
  Jul 29 17:14:11.203: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:14:11.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:14:11.238
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:14:11.242
  STEP: Creating configMap with name projected-configmap-test-volume-map-a81d82a9-dd32-4c21-a207-4d0dd2980ca6 @ 07/29/23 17:14:11.246
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:14:11.253
  E0729 17:14:11.564980      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:12.565236      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:13.565422      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:14.565701      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:14:15.295
  Jul 29 17:14:15.303: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-projected-configmaps-a0f08672-5c9e-4079-ac4e-211caed1d411 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 17:14:15.318
  Jul 29 17:14:15.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8593" for this suite. @ 07/29/23 17:14:15.37
• [4.179 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 07/29/23 17:14:15.391
  Jul 29 17:14:15.391: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:14:15.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:14:15.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:14:15.431
  STEP: Creating a pod to test emptydir volume type on node default medium @ 07/29/23 17:14:15.436
  E0729 17:14:15.566320      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:16.566445      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:17.566614      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:18.566853      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:14:19.484
  Jul 29 17:14:19.492: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-bf7a7daf-73ca-4bde-99d9-04ce69cfab7f container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:14:19.504
  Jul 29 17:14:19.533: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8340" for this suite. @ 07/29/23 17:14:19.541
• [4.160 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 07/29/23 17:14:19.556
  Jul 29 17:14:19.556: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename deployment @ 07/29/23 17:14:19.561
  E0729 17:14:19.567469      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:14:19.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:14:19.594
  Jul 29 17:14:19.598: INFO: Creating deployment "webserver-deployment"
  Jul 29 17:14:19.608: INFO: Waiting for observed generation 1
  E0729 17:14:20.627389      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:21.601079      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:21.662: INFO: Waiting for all required pods to come up
  Jul 29 17:14:21.765: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 07/29/23 17:14:21.765
  E0729 17:14:22.601802      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:23.602151      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:23.872: INFO: Waiting for deployment "webserver-deployment" to complete
  Jul 29 17:14:23.883: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jul 29 17:14:23.899: INFO: Updating deployment webserver-deployment
  Jul 29 17:14:23.899: INFO: Waiting for observed generation 2
  E0729 17:14:24.709226      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:25.604840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:25.912: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jul 29 17:14:25.916: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jul 29 17:14:25.921: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 17:14:25.936: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jul 29 17:14:25.936: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jul 29 17:14:25.942: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 17:14:25.951: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jul 29 17:14:25.951: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jul 29 17:14:25.968: INFO: Updating deployment webserver-deployment
  Jul 29 17:14:25.969: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 17:14:25.983: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jul 29 17:14:26.001: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jul 29 17:14:26.038: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-1527  e6881c51-68d8-4fcf-98eb-4154f79c7a49 40065 3 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-07-29 17:14:25 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc000844568 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 17:14:23 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-07-29 17:14:24 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jul 29 17:14:26.059: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-1527  da85f1cf-7832-4e8d-b13a-1b1f9a515931 40071 3 2023-07-29 17:14:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment e6881c51-68d8-4fcf-98eb-4154f79c7a49 0xc0049c5b07 0xc0049c5b08}] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 17:14:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6881c51-68d8-4fcf-98eb-4154f79c7a49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049c5ba8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 17:14:26.060: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jul 29 17:14:26.060: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-1527  58bde8ca-9592-4e14-8750-ff8a4f0f2226 40066 3 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment e6881c51-68d8-4fcf-98eb-4154f79c7a49 0xc0049c5a17 0xc0049c5a18}] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 17:14:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e6881c51-68d8-4fcf-98eb-4154f79c7a49\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049c5aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 17:14:26.080: INFO: Pod "webserver-deployment-67bd4bf6dc-2p88m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2p88m webserver-deployment-67bd4bf6dc- deployment-1527  5eb8894c-eba2-46a1-8b85-ed4d6dcaadcf 40078 0 2023-07-29 17:14:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc000845267 0xc000845268}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l6cjv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l6cjv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:26 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:25 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:,StartTime:2023-07-29 17:14:26 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.081: INFO: Pod "webserver-deployment-67bd4bf6dc-62j2k" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-62j2k webserver-deployment-67bd4bf6dc- deployment-1527  d8b5ba76-393f-4bc9-824c-8f0e758ab4fa 39980 0 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc0008454a7 0xc0008454a8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.88\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-r88pw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-r88pw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.88,PodIP:10.233.66.88,StartTime:2023-07-29 17:14:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:14:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://07bf8b609bc3b1019a467ddfdbb4b04aea68ceeaded5d3caa06c16d20d70f579,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.88,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.082: INFO: Pod "webserver-deployment-67bd4bf6dc-8c27d" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8c27d webserver-deployment-67bd4bf6dc- deployment-1527  f77bf0a7-9f75-4491-9c4e-0641cb2f3dcf 40083 0 2023-07-29 17:14:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc000845697 0xc000845698}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6b9ml,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6b9ml,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.083: INFO: Pod "webserver-deployment-67bd4bf6dc-8cbk7" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8cbk7 webserver-deployment-67bd4bf6dc- deployment-1527  a7c813af-4cb7-48cc-ac0b-81b516673be8 40084 0 2023-07-29 17:14:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc0008457d7 0xc0008457d8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jk5kx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jk5kx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.083: INFO: Pod "webserver-deployment-67bd4bf6dc-9rtvv" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9rtvv webserver-deployment-67bd4bf6dc- deployment-1527  3170eab3-4004-4be8-a464-9c69c3eaa266 39967 0 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc000845917 0xc000845918}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.189\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k8vmq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k8vmq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.77,PodIP:10.233.64.189,StartTime:2023-07-29 17:14:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:14:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dd32418908b7642b81cdda811387d2125150275aecbcd934b9eacc4094008edf,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.189,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.084: INFO: Pod "webserver-deployment-67bd4bf6dc-dqbx8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dqbx8 webserver-deployment-67bd4bf6dc- deployment-1527  37960b8d-273f-4f7f-a1c5-9f4fae5e53de 40077 0 2023-07-29 17:14:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc000845b07 0xc000845b08}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhvjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhvjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.084: INFO: Pod "webserver-deployment-67bd4bf6dc-gmxrc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gmxrc webserver-deployment-67bd4bf6dc- deployment-1527  35f525e3-689a-404e-b53a-37c0ecf60b22 39974 0 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc000845c70 0xc000845c71}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.252\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2x4x8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2x4x8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.77,PodIP:10.233.64.252,StartTime:2023-07-29 17:14:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:14:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://ef2143102edc58956809314152370cb73fe8dcf6c436323582b8f6d03f0e24da,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.252,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.085: INFO: Pod "webserver-deployment-67bd4bf6dc-jnb8f" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jnb8f webserver-deployment-67bd4bf6dc- deployment-1527  76c8f2b3-9f4a-442d-aec1-27565c1599b7 39955 0 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc000845e57 0xc000845e58}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.200\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2mbds,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2mbds,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:10.233.65.200,StartTime:2023-07-29 17:14:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:14:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3dc9c17bef0476ea81ba5cf9e635d42a36fc89c3e5c766316d426ab822da149c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.200,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.086: INFO: Pod "webserver-deployment-67bd4bf6dc-k9fgr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-k9fgr webserver-deployment-67bd4bf6dc- deployment-1527  6b5ecc8d-0e6a-483a-aba3-a883e3dddbaa 39952 0 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc00499c047 0xc00499c048}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5qp96,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5qp96,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:10.233.65.249,StartTime:2023-07-29 17:14:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:14:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://33a7eaef6bc33712d6711cea5a98ed6ab980ae5625869bbb3b997493df0abf85,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.249,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.087: INFO: Pod "webserver-deployment-67bd4bf6dc-lddlr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lddlr webserver-deployment-67bd4bf6dc- deployment-1527  a2fceed7-fe9b-488f-9c26-9232b74c98eb 40076 0 2023-07-29 17:14:25 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc00499c247 0xc00499c248}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:25 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kst67,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kst67,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.088: INFO: Pod "webserver-deployment-67bd4bf6dc-lj8qv" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lj8qv webserver-deployment-67bd4bf6dc- deployment-1527  7d77e7bd-47ef-4f67-903a-4077f0b58a37 39986 0 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc00499c3b0 0xc00499c3b1}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.114\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sbm99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sbm99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.88,PodIP:10.233.66.114,StartTime:2023-07-29 17:14:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:14:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://271c753613c21057b27b6d003db069f21844c38266b4a3ef710682556b22def2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.114,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.090: INFO: Pod "webserver-deployment-67bd4bf6dc-s72hn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s72hn webserver-deployment-67bd4bf6dc- deployment-1527  e5c1d383-94d8-4738-918d-fbd5c04b26ed 39982 0 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc00499c5b7 0xc00499c5b8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.93\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jmjrk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jmjrk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.88,PodIP:10.233.66.93,StartTime:2023-07-29 17:14:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:14:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://f65d38ee91f3a36b02c3cf5d244c6ea29d5b3d97d318670486937e449f060786,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.93,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.090: INFO: Pod "webserver-deployment-67bd4bf6dc-sxxdl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sxxdl webserver-deployment-67bd4bf6dc- deployment-1527  a4eaa50a-a019-4bdc-bc22-086bae5f0ac8 39970 0 2023-07-29 17:14:19 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 58bde8ca-9592-4e14-8750-ff8a4f0f2226 0xc00499c7d7 0xc00499c7d8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58bde8ca-9592-4e14-8750-ff8a4f0f2226\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d4b86,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d4b86,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.77,PodIP:10.233.64.216,StartTime:2023-07-29 17:14:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:14:21 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://a45fb94b9a0a198ba03a08e347e4b2e2c3e52bc20466ea1353f91489cb8ff278,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.216,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.091: INFO: Pod "webserver-deployment-7b75d79cf5-6hvlb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6hvlb webserver-deployment-7b75d79cf5- deployment-1527  d3514778-74dd-45c4-82aa-bd92d23dcf43 40043 0 2023-07-29 17:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 da85f1cf-7832-4e8d-b13a-1b1f9a515931 0xc00499c9e7 0xc00499c9e8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da85f1cf-7832-4e8d-b13a-1b1f9a515931\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gbstw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gbstw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.88,PodIP:,StartTime:2023-07-29 17:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.091: INFO: Pod "webserver-deployment-7b75d79cf5-845ct" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-845ct webserver-deployment-7b75d79cf5- deployment-1527  5b0b968e-3d39-4d4a-8c3d-138e80e05a20 40010 0 2023-07-29 17:14:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 da85f1cf-7832-4e8d-b13a-1b1f9a515931 0xc00499cbf7 0xc00499cbf8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da85f1cf-7832-4e8d-b13a-1b1f9a515931\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rlhb8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rlhb8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.88,PodIP:,StartTime:2023-07-29 17:14:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.092: INFO: Pod "webserver-deployment-7b75d79cf5-cmxhn" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-cmxhn webserver-deployment-7b75d79cf5- deployment-1527  fd68ed8a-29f3-4ccc-a124-7efae2cde3f0 40029 0 2023-07-29 17:14:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 da85f1cf-7832-4e8d-b13a-1b1f9a515931 0xc00499cde7 0xc00499cde8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da85f1cf-7832-4e8d-b13a-1b1f9a515931\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l89fl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l89fl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:,StartTime:2023-07-29 17:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.092: INFO: Pod "webserver-deployment-7b75d79cf5-qd7pv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-qd7pv webserver-deployment-7b75d79cf5- deployment-1527  99c462aa-0c74-46c9-9f18-da918b41e58d 40000 0 2023-07-29 17:14:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 da85f1cf-7832-4e8d-b13a-1b1f9a515931 0xc00499cfd7 0xc00499cfd8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da85f1cf-7832-4e8d-b13a-1b1f9a515931\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gsr46,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gsr46,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.129,PodIP:,StartTime:2023-07-29 17:14:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.093: INFO: Pod "webserver-deployment-7b75d79cf5-tgdxq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tgdxq webserver-deployment-7b75d79cf5- deployment-1527  2edbb158-3c4b-4324-8eec-d321c7ea573d 40079 0 2023-07-29 17:14:26 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 da85f1cf-7832-4e8d-b13a-1b1f9a515931 0xc00499d1e7 0xc00499d1e8}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:26 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da85f1cf-7832-4e8d-b13a-1b1f9a515931\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6644z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6644z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:26 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.093: INFO: Pod "webserver-deployment-7b75d79cf5-wtcfk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wtcfk webserver-deployment-7b75d79cf5- deployment-1527  862d775d-53ba-4953-a6a6-5ceba7fca128 40012 0 2023-07-29 17:14:23 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 da85f1cf-7832-4e8d-b13a-1b1f9a515931 0xc00499d370 0xc00499d371}] [] [{kube-controller-manager Update v1 2023-07-29 17:14:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"da85f1cf-7832-4e8d-b13a-1b1f9a515931\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:14:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hkmfc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hkmfc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:vucheipi7kei-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:14:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.77,PodIP:,StartTime:2023-07-29 17:14:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:14:26.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1527" for this suite. @ 07/29/23 17:14:26.132
• [6.617 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 07/29/23 17:14:26.175
  Jul 29 17:14:26.175: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:14:26.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:14:26.397
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:14:26.404
  STEP: set up a multi version CRD @ 07/29/23 17:14:26.41
  Jul 29 17:14:26.411: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:14:26.605416      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:27.606132      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:28.606296      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:29.606447      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:30.608477      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 07/29/23 17:14:31.42
  STEP: check the new version name is served @ 07/29/23 17:14:31.457
  E0729 17:14:31.608189      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:32.609423      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:33.609743      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 07/29/23 17:14:33.933
  E0729 17:14:34.610730      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 07/29/23 17:14:34.916
  E0729 17:14:35.611206      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:36.612605      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:37.615626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:38.615823      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:38.679: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9282" for this suite. @ 07/29/23 17:14:38.703
• [12.541 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 07/29/23 17:14:38.724
  Jul 29 17:14:38.724: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename gc @ 07/29/23 17:14:38.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:14:38.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:14:38.768
  STEP: create the rc1 @ 07/29/23 17:14:38.785
  STEP: create the rc2 @ 07/29/23 17:14:38.799
  E0729 17:14:39.616300      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:40.865681      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:41.730031      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:42.826456      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:43.756471      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:44.783451      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 07/29/23 17:14:44.842
  E0729 17:14:45.766260      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:46.780494      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:47.781608      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:48.917504      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:49.866633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:50.882022      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 07/29/23 17:14:51.339
  STEP: wait for the rc to be deleted @ 07/29/23 17:14:51.368
  E0729 17:14:51.869923      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:52.870510      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:53.870713      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:54.870979      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:55.871419      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:14:56.546: INFO: 94 pods remaining
  Jul 29 17:14:56.547: INFO: 70 pods has nil DeletionTimestamp
  Jul 29 17:14:56.547: INFO: 
  E0729 17:14:56.871894      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:57.872016      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:58.876705      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:14:59.878194      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:00.877085      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:01.427: INFO: 82 pods remaining
  Jul 29 17:15:01.427: INFO: 50 pods has nil DeletionTimestamp
  Jul 29 17:15:01.427: INFO: 
  E0729 17:15:01.877793      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:02.877996      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:03.878357      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:04.878840      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:05.879126      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/29/23 17:15:06.396
  Jul 29 17:15:06.563: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 17:15:06.564: INFO: Deleting pod "simpletest-rc-to-be-deleted-2ctj2" in namespace "gc-3302"
  Jul 29 17:15:06.614: INFO: Deleting pod "simpletest-rc-to-be-deleted-2mlvj" in namespace "gc-3302"
  Jul 29 17:15:06.678: INFO: Deleting pod "simpletest-rc-to-be-deleted-2p4zt" in namespace "gc-3302"
  Jul 29 17:15:06.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-447hh" in namespace "gc-3302"
  Jul 29 17:15:06.800: INFO: Deleting pod "simpletest-rc-to-be-deleted-44whj" in namespace "gc-3302"
  E0729 17:15:06.880019      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:06.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h8fg" in namespace "gc-3302"
  Jul 29 17:15:06.995: INFO: Deleting pod "simpletest-rc-to-be-deleted-5n6gt" in namespace "gc-3302"
  Jul 29 17:15:07.081: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ptjj" in namespace "gc-3302"
  Jul 29 17:15:07.159: INFO: Deleting pod "simpletest-rc-to-be-deleted-6bz67" in namespace "gc-3302"
  Jul 29 17:15:07.282: INFO: Deleting pod "simpletest-rc-to-be-deleted-6ggkd" in namespace "gc-3302"
  Jul 29 17:15:07.344: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pl8x" in namespace "gc-3302"
  Jul 29 17:15:07.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-79pkg" in namespace "gc-3302"
  Jul 29 17:15:07.560: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b8f4" in namespace "gc-3302"
  Jul 29 17:15:07.642: INFO: Deleting pod "simpletest-rc-to-be-deleted-8f5x2" in namespace "gc-3302"
  Jul 29 17:15:07.704: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gd6f" in namespace "gc-3302"
  Jul 29 17:15:07.873: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hdd9" in namespace "gc-3302"
  E0729 17:15:07.880003      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:07.998: INFO: Deleting pod "simpletest-rc-to-be-deleted-8vk9k" in namespace "gc-3302"
  Jul 29 17:15:08.069: INFO: Deleting pod "simpletest-rc-to-be-deleted-94289" in namespace "gc-3302"
  Jul 29 17:15:08.129: INFO: Deleting pod "simpletest-rc-to-be-deleted-99t52" in namespace "gc-3302"
  Jul 29 17:15:08.183: INFO: Deleting pod "simpletest-rc-to-be-deleted-9dq88" in namespace "gc-3302"
  Jul 29 17:15:08.229: INFO: Deleting pod "simpletest-rc-to-be-deleted-9k7s2" in namespace "gc-3302"
  Jul 29 17:15:08.310: INFO: Deleting pod "simpletest-rc-to-be-deleted-9t69n" in namespace "gc-3302"
  Jul 29 17:15:08.392: INFO: Deleting pod "simpletest-rc-to-be-deleted-bngp7" in namespace "gc-3302"
  Jul 29 17:15:08.567: INFO: Deleting pod "simpletest-rc-to-be-deleted-c8pc6" in namespace "gc-3302"
  Jul 29 17:15:08.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-chvvw" in namespace "gc-3302"
  Jul 29 17:15:08.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwt6m" in namespace "gc-3302"
  Jul 29 17:15:08.835: INFO: Deleting pod "simpletest-rc-to-be-deleted-czn8q" in namespace "gc-3302"
  E0729 17:15:08.880648      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:08.955: INFO: Deleting pod "simpletest-rc-to-be-deleted-dgpdj" in namespace "gc-3302"
  Jul 29 17:15:09.049: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8h5q" in namespace "gc-3302"
  Jul 29 17:15:09.107: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8kn9" in namespace "gc-3302"
  Jul 29 17:15:09.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-fcd6t" in namespace "gc-3302"
  Jul 29 17:15:09.314: INFO: Deleting pod "simpletest-rc-to-be-deleted-fp9kq" in namespace "gc-3302"
  Jul 29 17:15:09.470: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsjpv" in namespace "gc-3302"
  Jul 29 17:15:09.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-ft5tb" in namespace "gc-3302"
  Jul 29 17:15:09.608: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzrgm" in namespace "gc-3302"
  Jul 29 17:15:09.849: INFO: Deleting pod "simpletest-rc-to-be-deleted-g66m5" in namespace "gc-3302"
  E0729 17:15:09.881654      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:09.914: INFO: Deleting pod "simpletest-rc-to-be-deleted-g8bhp" in namespace "gc-3302"
  Jul 29 17:15:09.990: INFO: Deleting pod "simpletest-rc-to-be-deleted-gq88t" in namespace "gc-3302"
  Jul 29 17:15:10.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-gzzgs" in namespace "gc-3302"
  Jul 29 17:15:10.132: INFO: Deleting pod "simpletest-rc-to-be-deleted-h2zvq" in namespace "gc-3302"
  Jul 29 17:15:10.181: INFO: Deleting pod "simpletest-rc-to-be-deleted-h9z4k" in namespace "gc-3302"
  Jul 29 17:15:10.257: INFO: Deleting pod "simpletest-rc-to-be-deleted-jd7xq" in namespace "gc-3302"
  Jul 29 17:15:10.308: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfczf" in namespace "gc-3302"
  Jul 29 17:15:10.390: INFO: Deleting pod "simpletest-rc-to-be-deleted-jfdpr" in namespace "gc-3302"
  Jul 29 17:15:10.467: INFO: Deleting pod "simpletest-rc-to-be-deleted-k9jdw" in namespace "gc-3302"
  Jul 29 17:15:10.565: INFO: Deleting pod "simpletest-rc-to-be-deleted-kzt79" in namespace "gc-3302"
  Jul 29 17:15:10.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-lgvc5" in namespace "gc-3302"
  Jul 29 17:15:10.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-lhgc4" in namespace "gc-3302"
  Jul 29 17:15:10.703: INFO: Deleting pod "simpletest-rc-to-be-deleted-lj54s" in namespace "gc-3302"
  Jul 29 17:15:10.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-lljgs" in namespace "gc-3302"
  E0729 17:15:10.886002      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:10.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3302" for this suite. @ 07/29/23 17:15:10.958
• [32.295 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 07/29/23 17:15:11.04
  Jul 29 17:15:11.040: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:15:11.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:11.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:11.127
  Jul 29 17:15:11.158: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:15:11.886930      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:12.887029      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:13.887182      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 17:15:14.007
  Jul 29 17:15:14.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-2791 --namespace=crd-publish-openapi-2791 create -f -'
  E0729 17:15:14.887826      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:15.741: INFO: stderr: ""
  Jul 29 17:15:15.741: INFO: stdout: "e2e-test-crd-publish-openapi-2993-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 29 17:15:15.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-2791 --namespace=crd-publish-openapi-2791 delete e2e-test-crd-publish-openapi-2993-crds test-cr'
  E0729 17:15:15.888438      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:15.922: INFO: stderr: ""
  Jul 29 17:15:15.922: INFO: stdout: "e2e-test-crd-publish-openapi-2993-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jul 29 17:15:15.923: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-2791 --namespace=crd-publish-openapi-2791 apply -f -'
  Jul 29 17:15:16.479: INFO: stderr: ""
  Jul 29 17:15:16.479: INFO: stdout: "e2e-test-crd-publish-openapi-2993-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 29 17:15:16.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-2791 --namespace=crd-publish-openapi-2791 delete e2e-test-crd-publish-openapi-2993-crds test-cr'
  Jul 29 17:15:16.667: INFO: stderr: ""
  Jul 29 17:15:16.667: INFO: stdout: "e2e-test-crd-publish-openapi-2993-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 07/29/23 17:15:16.667
  Jul 29 17:15:16.667: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=crd-publish-openapi-2791 explain e2e-test-crd-publish-openapi-2993-crds'
  E0729 17:15:16.888820      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:17.168: INFO: stderr: ""
  Jul 29 17:15:17.168: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-2993-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0729 17:15:17.889168      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:18.889626      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:19.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2791" for this suite. @ 07/29/23 17:15:19.031
• [8.009 seconds]
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 07/29/23 17:15:19.049
  Jul 29 17:15:19.049: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:15:19.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:19.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:19.087
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:15:19.093
  E0729 17:15:19.890633      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:20.891747      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:21.891980      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:22.892750      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:15:23.143
  Jul 29 17:15:23.149: INFO: Trying to get logs from node vucheipi7kei-3 pod downwardapi-volume-12b8f9b6-324f-4282-9d89-3cdbffa81b8e container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:15:23.177
  Jul 29 17:15:23.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7650" for this suite. @ 07/29/23 17:15:23.212
• [4.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 07/29/23 17:15:23.23
  Jul 29 17:15:23.230: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename discovery @ 07/29/23 17:15:23.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:23.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:23.262
  STEP: Setting up server cert @ 07/29/23 17:15:23.269
  E0729 17:15:23.893534      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:24.431: INFO: Checking APIGroup: apiregistration.k8s.io
  Jul 29 17:15:24.434: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jul 29 17:15:24.434: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jul 29 17:15:24.434: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jul 29 17:15:24.434: INFO: Checking APIGroup: apps
  Jul 29 17:15:24.436: INFO: PreferredVersion.GroupVersion: apps/v1
  Jul 29 17:15:24.436: INFO: Versions found [{apps/v1 v1}]
  Jul 29 17:15:24.436: INFO: apps/v1 matches apps/v1
  Jul 29 17:15:24.436: INFO: Checking APIGroup: events.k8s.io
  Jul 29 17:15:24.437: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jul 29 17:15:24.437: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jul 29 17:15:24.437: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jul 29 17:15:24.437: INFO: Checking APIGroup: authentication.k8s.io
  Jul 29 17:15:24.441: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jul 29 17:15:24.441: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jul 29 17:15:24.441: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jul 29 17:15:24.441: INFO: Checking APIGroup: authorization.k8s.io
  Jul 29 17:15:24.442: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jul 29 17:15:24.442: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jul 29 17:15:24.442: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jul 29 17:15:24.442: INFO: Checking APIGroup: autoscaling
  Jul 29 17:15:24.443: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jul 29 17:15:24.443: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jul 29 17:15:24.443: INFO: autoscaling/v2 matches autoscaling/v2
  Jul 29 17:15:24.443: INFO: Checking APIGroup: batch
  Jul 29 17:15:24.446: INFO: PreferredVersion.GroupVersion: batch/v1
  Jul 29 17:15:24.446: INFO: Versions found [{batch/v1 v1}]
  Jul 29 17:15:24.446: INFO: batch/v1 matches batch/v1
  Jul 29 17:15:24.447: INFO: Checking APIGroup: certificates.k8s.io
  Jul 29 17:15:24.449: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jul 29 17:15:24.449: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jul 29 17:15:24.449: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jul 29 17:15:24.449: INFO: Checking APIGroup: networking.k8s.io
  Jul 29 17:15:24.451: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jul 29 17:15:24.451: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jul 29 17:15:24.451: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jul 29 17:15:24.452: INFO: Checking APIGroup: policy
  Jul 29 17:15:24.454: INFO: PreferredVersion.GroupVersion: policy/v1
  Jul 29 17:15:24.454: INFO: Versions found [{policy/v1 v1}]
  Jul 29 17:15:24.454: INFO: policy/v1 matches policy/v1
  Jul 29 17:15:24.454: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jul 29 17:15:24.457: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jul 29 17:15:24.458: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jul 29 17:15:24.458: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jul 29 17:15:24.458: INFO: Checking APIGroup: storage.k8s.io
  Jul 29 17:15:24.460: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jul 29 17:15:24.460: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jul 29 17:15:24.460: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jul 29 17:15:24.460: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jul 29 17:15:24.462: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jul 29 17:15:24.462: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jul 29 17:15:24.462: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jul 29 17:15:24.463: INFO: Checking APIGroup: apiextensions.k8s.io
  Jul 29 17:15:24.464: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jul 29 17:15:24.464: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jul 29 17:15:24.464: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jul 29 17:15:24.464: INFO: Checking APIGroup: scheduling.k8s.io
  Jul 29 17:15:24.466: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jul 29 17:15:24.466: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jul 29 17:15:24.467: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jul 29 17:15:24.467: INFO: Checking APIGroup: coordination.k8s.io
  Jul 29 17:15:24.470: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jul 29 17:15:24.470: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jul 29 17:15:24.470: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jul 29 17:15:24.471: INFO: Checking APIGroup: node.k8s.io
  Jul 29 17:15:24.473: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jul 29 17:15:24.473: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jul 29 17:15:24.473: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jul 29 17:15:24.473: INFO: Checking APIGroup: discovery.k8s.io
  Jul 29 17:15:24.475: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jul 29 17:15:24.475: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jul 29 17:15:24.476: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jul 29 17:15:24.476: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jul 29 17:15:24.478: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jul 29 17:15:24.478: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jul 29 17:15:24.478: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jul 29 17:15:24.478: INFO: Checking APIGroup: cilium.io
  Jul 29 17:15:24.480: INFO: PreferredVersion.GroupVersion: cilium.io/v2
  Jul 29 17:15:24.480: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
  Jul 29 17:15:24.481: INFO: cilium.io/v2 matches cilium.io/v2
  Jul 29 17:15:24.481: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-5696" for this suite. @ 07/29/23 17:15:24.491
• [1.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 07/29/23 17:15:24.502
  Jul 29 17:15:24.502: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename services @ 07/29/23 17:15:24.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:24.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:24.527
  STEP: creating a Service @ 07/29/23 17:15:24.535
  STEP: watching for the Service to be added @ 07/29/23 17:15:24.549
  Jul 29 17:15:24.553: INFO: Found Service test-service-p5s82 in namespace services-4774 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jul 29 17:15:24.553: INFO: Service test-service-p5s82 created
  STEP: Getting /status @ 07/29/23 17:15:24.553
  Jul 29 17:15:24.559: INFO: Service test-service-p5s82 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 07/29/23 17:15:24.559
  STEP: watching for the Service to be patched @ 07/29/23 17:15:24.57
  Jul 29 17:15:24.573: INFO: observed Service test-service-p5s82 in namespace services-4774 with annotations: map[] & LoadBalancer: {[]}
  Jul 29 17:15:24.573: INFO: Found Service test-service-p5s82 in namespace services-4774 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jul 29 17:15:24.573: INFO: Service test-service-p5s82 has service status patched
  STEP: updating the ServiceStatus @ 07/29/23 17:15:24.573
  Jul 29 17:15:24.589: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 07/29/23 17:15:24.589
  Jul 29 17:15:24.593: INFO: Observed Service test-service-p5s82 in namespace services-4774 with annotations: map[] & Conditions: {[]}
  Jul 29 17:15:24.594: INFO: Observed event: &Service{ObjectMeta:{test-service-p5s82  services-4774  af8db3ac-395e-4660-83a4-05b3b9c96272 42619 0 2023-07-29 17:15:24 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-29 17:15:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-29 17:15:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.34.215,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.34.215],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jul 29 17:15:24.595: INFO: Found Service test-service-p5s82 in namespace services-4774 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 17:15:24.596: INFO: Service test-service-p5s82 has service status updated
  STEP: patching the service @ 07/29/23 17:15:24.596
  STEP: watching for the Service to be patched @ 07/29/23 17:15:24.619
  Jul 29 17:15:24.625: INFO: observed Service test-service-p5s82 in namespace services-4774 with labels: map[test-service-static:true]
  Jul 29 17:15:24.625: INFO: observed Service test-service-p5s82 in namespace services-4774 with labels: map[test-service-static:true]
  Jul 29 17:15:24.625: INFO: observed Service test-service-p5s82 in namespace services-4774 with labels: map[test-service-static:true]
  Jul 29 17:15:24.625: INFO: Found Service test-service-p5s82 in namespace services-4774 with labels: map[test-service:patched test-service-static:true]
  Jul 29 17:15:24.625: INFO: Service test-service-p5s82 patched
  STEP: deleting the service @ 07/29/23 17:15:24.625
  STEP: watching for the Service to be deleted @ 07/29/23 17:15:24.652
  Jul 29 17:15:24.655: INFO: Observed event: ADDED
  Jul 29 17:15:24.656: INFO: Observed event: MODIFIED
  Jul 29 17:15:24.657: INFO: Observed event: MODIFIED
  Jul 29 17:15:24.658: INFO: Observed event: MODIFIED
  Jul 29 17:15:24.658: INFO: Found Service test-service-p5s82 in namespace services-4774 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jul 29 17:15:24.659: INFO: Service test-service-p5s82 deleted
  Jul 29 17:15:24.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4774" for this suite. @ 07/29/23 17:15:24.669
• [0.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 07/29/23 17:15:24.688
  Jul 29 17:15:24.688: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:15:24.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:24.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:24.714
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/29/23 17:15:24.718
  E0729 17:15:24.894907      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:25.894963      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:26.896068      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:27.896235      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:15:28.759
  Jul 29 17:15:28.766: INFO: Trying to get logs from node vucheipi7kei-3 pod pod-0ca948f1-21b7-4cb9-9687-d47dc75d8073 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:15:28.789
  Jul 29 17:15:28.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7321" for this suite. @ 07/29/23 17:15:28.827
• [4.153 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 07/29/23 17:15:28.842
  Jul 29 17:15:28.842: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 17:15:28.844
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:28.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:28.872
  Jul 29 17:15:28.877: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  E0729 17:15:28.897118      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:29.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-146" for this suite. @ 07/29/23 17:15:29.473
• [0.700 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 07/29/23 17:15:29.543
  Jul 29 17:15:29.543: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename proxy @ 07/29/23 17:15:29.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:29.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:29.605
  Jul 29 17:15:29.608: INFO: Creating pod...
  E0729 17:15:29.898232      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:30.899119      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:31.674: INFO: Creating service...
  Jul 29 17:15:31.693: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/pods/agnhost/proxy/some/path/with/DELETE
  Jul 29 17:15:31.722: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 17:15:31.723: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/pods/agnhost/proxy/some/path/with/GET
  Jul 29 17:15:31.729: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 29 17:15:31.729: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/pods/agnhost/proxy/some/path/with/HEAD
  Jul 29 17:15:31.734: INFO: http.Client request:HEAD | StatusCode:200
  Jul 29 17:15:31.734: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/pods/agnhost/proxy/some/path/with/OPTIONS
  Jul 29 17:15:31.768: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 17:15:31.768: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/pods/agnhost/proxy/some/path/with/PATCH
  Jul 29 17:15:31.776: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 17:15:31.777: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/pods/agnhost/proxy/some/path/with/POST
  Jul 29 17:15:31.787: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 17:15:31.788: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/pods/agnhost/proxy/some/path/with/PUT
  Jul 29 17:15:31.792: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 17:15:31.792: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/services/test-service/proxy/some/path/with/DELETE
  Jul 29 17:15:31.808: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 17:15:31.808: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/services/test-service/proxy/some/path/with/GET
  Jul 29 17:15:31.817: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 29 17:15:31.817: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/services/test-service/proxy/some/path/with/HEAD
  Jul 29 17:15:31.825: INFO: http.Client request:HEAD | StatusCode:200
  Jul 29 17:15:31.825: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/services/test-service/proxy/some/path/with/OPTIONS
  Jul 29 17:15:31.833: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 17:15:31.833: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/services/test-service/proxy/some/path/with/PATCH
  Jul 29 17:15:31.843: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 17:15:31.843: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/services/test-service/proxy/some/path/with/POST
  Jul 29 17:15:31.852: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 17:15:31.852: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-3521/services/test-service/proxy/some/path/with/PUT
  Jul 29 17:15:31.860: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 17:15:31.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-3521" for this suite. @ 07/29/23 17:15:31.866
• [2.334 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 07/29/23 17:15:31.88
  Jul 29 17:15:31.880: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename gc @ 07/29/23 17:15:31.882
  E0729 17:15:31.899480      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:31.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:31.911
  Jul 29 17:15:31.979: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6642a84e-c53a-402e-8f56-628e6f9f5bd1", Controller:(*bool)(0xc006369216), BlockOwnerDeletion:(*bool)(0xc006369217)}}
  Jul 29 17:15:32.030: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"4e0bd440-511f-4237-b9eb-350b97a791ad", Controller:(*bool)(0xc006408016), BlockOwnerDeletion:(*bool)(0xc006408017)}}
  Jul 29 17:15:32.044: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"053963fd-4093-42ba-b9a6-da3b08ede0c4", Controller:(*bool)(0xc0064082ba), BlockOwnerDeletion:(*bool)(0xc0064082bb)}}
  E0729 17:15:32.907754      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:33.907596      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:34.907672      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:35.907911      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:15:36.908175      14 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:15:37.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7195" for this suite. @ 07/29/23 17:15:37.08
• [5.218 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 07/29/23 17:15:37.109
  Jul 29 17:15:37.109: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:15:37.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:37.147
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:37.152
  STEP: validating cluster-info @ 07/29/23 17:15:37.157
  Jul 29 17:15:37.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-4144220329 --namespace=kubectl-9162 cluster-info'
  Jul 29 17:15:37.375: INFO: stderr: ""
  Jul 29 17:15:37.375: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jul 29 17:15:37.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9162" for this suite. @ 07/29/23 17:15:37.389
• [0.303 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 07/29/23 17:15:37.418
  Jul 29 17:15:37.418: INFO: >>> kubeConfig: /tmp/kubeconfig-4144220329
  STEP: Building a namespace api object, basename lease-test @ 07/29/23 17:15:37.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:37.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:37.47
  Jul 29 17:15:37.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-5085" for this suite. @ 07/29/23 17:15:37.648
• [0.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jul 29 17:15:37.688: INFO: Running AfterSuite actions on node 1
  Jul 29 17:15:37.688: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.095 seconds]
------------------------------

Ran 378 of 7207 Specs in 6602.294 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h50m3.248292386s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

