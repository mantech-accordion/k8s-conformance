  I0511 13:28:14.228107      24 e2e.go:117] Starting e2e run "3f89617c-aa68-4c2a-8d4a-02ca06753a9f" on Ginkgo node 1
  May 11 13:28:14.260: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683811694 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May 11 13:28:14.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:28:14.417: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May 11 13:28:14.482: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May 11 13:28:14.487: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'calico-node' (0 seconds elapsed)
  May 11 13:28:14.488: INFO: 6 / 6 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  May 11 13:28:14.488: INFO: e2e test version: v1.27.1
  May 11 13:28:14.489: INFO: kube-apiserver version: v1.27.1
  May 11 13:28:14.489: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:28:14.497: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.081 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/11/23 13:28:14.674
  May 11 13:28:14.674: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename gc @ 05/11/23 13:28:14.675
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:28:14.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:28:14.695
  STEP: create the rc1 @ 05/11/23 13:28:14.707
  STEP: create the rc2 @ 05/11/23 13:28:14.719
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/11/23 13:28:20.843
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/11/23 13:28:23.049
  STEP: wait for the rc to be deleted @ 05/11/23 13:28:23.084
  May 11 13:28:28.124: INFO: 70 pods remaining
  May 11 13:28:28.124: INFO: 70 pods has nil DeletionTimestamp
  May 11 13:28:28.124: INFO: 
  STEP: Gathering metrics @ 05/11/23 13:28:33.099
  May 11 13:28:33.221: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 11 13:28:33.221: INFO: Deleting pod "simpletest-rc-to-be-deleted-25vht" in namespace "gc-8874"
  May 11 13:28:33.233: INFO: Deleting pod "simpletest-rc-to-be-deleted-27x45" in namespace "gc-8874"
  May 11 13:28:33.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-4ckl4" in namespace "gc-8874"
  May 11 13:28:33.259: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f8wd" in namespace "gc-8874"
  May 11 13:28:33.275: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wq86" in namespace "gc-8874"
  May 11 13:28:33.290: INFO: Deleting pod "simpletest-rc-to-be-deleted-5ns7w" in namespace "gc-8874"
  May 11 13:28:33.303: INFO: Deleting pod "simpletest-rc-to-be-deleted-69ht4" in namespace "gc-8874"
  May 11 13:28:33.319: INFO: Deleting pod "simpletest-rc-to-be-deleted-6f4fs" in namespace "gc-8874"
  May 11 13:28:33.332: INFO: Deleting pod "simpletest-rc-to-be-deleted-6l5bx" in namespace "gc-8874"
  May 11 13:28:33.345: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mv4z" in namespace "gc-8874"
  May 11 13:28:33.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mxjp" in namespace "gc-8874"
  May 11 13:28:33.371: INFO: Deleting pod "simpletest-rc-to-be-deleted-6v9vb" in namespace "gc-8874"
  May 11 13:28:33.387: INFO: Deleting pod "simpletest-rc-to-be-deleted-6x59c" in namespace "gc-8874"
  May 11 13:28:33.404: INFO: Deleting pod "simpletest-rc-to-be-deleted-78fm7" in namespace "gc-8874"
  May 11 13:28:33.425: INFO: Deleting pod "simpletest-rc-to-be-deleted-78hgm" in namespace "gc-8874"
  May 11 13:28:33.435: INFO: Deleting pod "simpletest-rc-to-be-deleted-7nrb4" in namespace "gc-8874"
  May 11 13:28:33.477: INFO: Deleting pod "simpletest-rc-to-be-deleted-7qfsq" in namespace "gc-8874"
  May 11 13:28:33.513: INFO: Deleting pod "simpletest-rc-to-be-deleted-85v5q" in namespace "gc-8874"
  May 11 13:28:33.617: INFO: Deleting pod "simpletest-rc-to-be-deleted-89d98" in namespace "gc-8874"
  May 11 13:28:33.697: INFO: Deleting pod "simpletest-rc-to-be-deleted-8jt8v" in namespace "gc-8874"
  May 11 13:28:33.761: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kvfv" in namespace "gc-8874"
  May 11 13:28:33.805: INFO: Deleting pod "simpletest-rc-to-be-deleted-8rkhm" in namespace "gc-8874"
  May 11 13:28:33.885: INFO: Deleting pod "simpletest-rc-to-be-deleted-8wnw6" in namespace "gc-8874"
  May 11 13:28:33.912: INFO: Deleting pod "simpletest-rc-to-be-deleted-8xmzz" in namespace "gc-8874"
  May 11 13:28:34.026: INFO: Deleting pod "simpletest-rc-to-be-deleted-9mgq6" in namespace "gc-8874"
  May 11 13:28:34.167: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zv2c" in namespace "gc-8874"
  May 11 13:28:34.287: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7mv9" in namespace "gc-8874"
  May 11 13:28:34.331: INFO: Deleting pod "simpletest-rc-to-be-deleted-bdv48" in namespace "gc-8874"
  May 11 13:28:34.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-bf57g" in namespace "gc-8874"
  May 11 13:28:34.381: INFO: Deleting pod "simpletest-rc-to-be-deleted-bflzz" in namespace "gc-8874"
  May 11 13:28:34.409: INFO: Deleting pod "simpletest-rc-to-be-deleted-brhj5" in namespace "gc-8874"
  May 11 13:28:34.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-bv7j4" in namespace "gc-8874"
  May 11 13:28:34.486: INFO: Deleting pod "simpletest-rc-to-be-deleted-c57f9" in namespace "gc-8874"
  May 11 13:28:34.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-cc7w2" in namespace "gc-8874"
  May 11 13:28:34.859: INFO: Deleting pod "simpletest-rc-to-be-deleted-ckz7b" in namespace "gc-8874"
  May 11 13:28:34.930: INFO: Deleting pod "simpletest-rc-to-be-deleted-d48tv" in namespace "gc-8874"
  May 11 13:28:34.966: INFO: Deleting pod "simpletest-rc-to-be-deleted-d95rr" in namespace "gc-8874"
  May 11 13:28:35.068: INFO: Deleting pod "simpletest-rc-to-be-deleted-dk96j" in namespace "gc-8874"
  May 11 13:28:35.119: INFO: Deleting pod "simpletest-rc-to-be-deleted-dll7f" in namespace "gc-8874"
  May 11 13:28:35.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-dnz6n" in namespace "gc-8874"
  May 11 13:28:35.405: INFO: Deleting pod "simpletest-rc-to-be-deleted-dsj88" in namespace "gc-8874"
  May 11 13:28:35.427: INFO: Deleting pod "simpletest-rc-to-be-deleted-dw57b" in namespace "gc-8874"
  May 11 13:28:35.526: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9dqg" in namespace "gc-8874"
  May 11 13:28:35.552: INFO: Deleting pod "simpletest-rc-to-be-deleted-fnckz" in namespace "gc-8874"
  May 11 13:28:35.658: INFO: Deleting pod "simpletest-rc-to-be-deleted-fx9nb" in namespace "gc-8874"
  May 11 13:28:35.679: INFO: Deleting pod "simpletest-rc-to-be-deleted-ggmwr" in namespace "gc-8874"
  May 11 13:28:35.769: INFO: Deleting pod "simpletest-rc-to-be-deleted-gm7r7" in namespace "gc-8874"
  May 11 13:28:35.877: INFO: Deleting pod "simpletest-rc-to-be-deleted-gr8jc" in namespace "gc-8874"
  May 11 13:28:35.896: INFO: Deleting pod "simpletest-rc-to-be-deleted-grjrw" in namespace "gc-8874"
  May 11 13:28:36.013: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtm64" in namespace "gc-8874"
  May 11 13:28:36.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8874" for this suite. @ 05/11/23 13:28:36.176
• [21.632 seconds]
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/11/23 13:28:36.306
  May 11 13:28:36.306: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename security-context-test @ 05/11/23 13:28:36.307
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:28:36.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:28:36.341
  May 11 13:28:40.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-9428" for this suite. @ 05/11/23 13:28:40.374
• [4.073 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/11/23 13:28:40.38
  May 11 13:28:40.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename deployment @ 05/11/23 13:28:40.38
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:28:40.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:28:40.407
  May 11 13:28:40.409: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May 11 13:28:40.416: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 11 13:28:45.421: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/11/23 13:28:45.421
  May 11 13:28:45.421: INFO: Creating deployment "test-rolling-update-deployment"
  May 11 13:28:45.426: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May 11 13:28:45.432: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  May 11 13:28:47.440: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May 11 13:28:47.442: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May 11 13:28:47.451: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8394  931df172-e912-42bc-b589-7f573f9d77ff 7123 1 2023-05-11 13:28:45 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-11 13:28:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:28:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d0c338 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-11 13:28:45 +0000 UTC,LastTransitionTime:2023-05-11 13:28:45 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-11 13:28:46 +0000 UTC,LastTransitionTime:2023-05-11 13:28:45 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 11 13:28:47.453: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-8394  2c576730-7832-42ca-8805-2ac32e82f433 7113 1 2023-05-11 13:28:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 931df172-e912-42bc-b589-7f573f9d77ff 0xc004d0c807 0xc004d0c808}] [] [{kube-controller-manager Update apps/v1 2023-05-11 13:28:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"931df172-e912-42bc-b589-7f573f9d77ff\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:28:46 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d0c8b8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 11 13:28:47.453: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May 11 13:28:47.453: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8394  97c016a1-1310-45a2-8f1c-97acb90347dc 7122 2 2023-05-11 13:28:40 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 931df172-e912-42bc-b589-7f573f9d77ff 0xc004d0c6df 0xc004d0c6f0}] [] [{e2e.test Update apps/v1 2023-05-11 13:28:40 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:28:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"931df172-e912-42bc-b589-7f573f9d77ff\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:28:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004d0c7a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 11 13:28:47.456: INFO: Pod "test-rolling-update-deployment-656d657cd8-xvlqd" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-xvlqd test-rolling-update-deployment-656d657cd8- deployment-8394  1abeb142-d195-4955-8715-1283359af55c 7112 0 2023-05-11 13:28:45 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:24091c397008bcb07cc7dc5fb310a8ed2148e7d3a920e406540414e1c0ad33ad cni.projectcalico.org/podIP:192.168.150.165/32 cni.projectcalico.org/podIPs:192.168.150.165/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 2c576730-7832-42ca-8805-2ac32e82f433 0xc004d0cd17 0xc004d0cd18}] [] [{calico Update v1 2023-05-11 13:28:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 13:28:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2c576730-7832-42ca-8805-2ac32e82f433\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 13:28:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.165\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gmtd7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gmtd7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 13:28:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 13:28:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 13:28:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 13:28:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.165,StartTime:2023-05-11 13:28:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 13:28:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://bff116908d91976d701c5c2a9602429b8dc9a8d8834ba1efeab8fa5d2b60adcf,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.165,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 13:28:47.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8394" for this suite. @ 05/11/23 13:28:47.46
• [7.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/11/23 13:28:47.469
  May 11 13:28:47.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/11/23 13:28:47.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:28:47.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:28:47.487
  STEP: create the container to handle the HTTPGet hook request. @ 05/11/23 13:28:47.493
  STEP: create the pod with lifecycle hook @ 05/11/23 13:28:49.513
  STEP: check poststart hook @ 05/11/23 13:28:51.534
  STEP: delete the pod with lifecycle hook @ 05/11/23 13:28:51.553
  May 11 13:28:53.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-603" for this suite. @ 05/11/23 13:28:53.573
• [6.111 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/11/23 13:28:53.58
  May 11 13:28:53.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:28:53.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:28:53.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:28:53.598
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:28:53.6
  STEP: Saw pod success @ 05/11/23 13:28:57.621
  May 11 13:28:57.624: INFO: Trying to get logs from node macpro-3 pod downwardapi-volume-3a2bf5b2-81b6-44bf-9e6f-9ced1ca87178 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:28:57.638
  May 11 13:28:57.652: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7513" for this suite. @ 05/11/23 13:28:57.656
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/11/23 13:28:57.666
  May 11 13:28:57.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 13:28:57.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:28:57.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:28:57.69
  STEP: creating pod @ 05/11/23 13:28:57.693
  May 11 13:28:59.715: INFO: Pod pod-hostip-c4a5d991-dbc2-4b28-9d5d-cc3c548c1f44 has hostIP: 10.221.188.12
  May 11 13:28:59.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8470" for this suite. @ 05/11/23 13:28:59.721
• [2.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/11/23 13:28:59.727
  May 11 13:28:59.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sysctl @ 05/11/23 13:28:59.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:28:59.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:28:59.745
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/11/23 13:28:59.747
  May 11 13:28:59.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3828" for this suite. @ 05/11/23 13:28:59.758
• [0.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/11/23 13:28:59.764
  May 11 13:28:59.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 13:28:59.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:28:59.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:28:59.783
  STEP: Setting up server cert @ 05/11/23 13:28:59.807
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 13:29:00.103
  STEP: Deploying the webhook pod @ 05/11/23 13:29:00.11
  STEP: Wait for the deployment to be ready @ 05/11/23 13:29:00.12
  May 11 13:29:00.127: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 13:29:02.137
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 13:29:02.15
  May 11 13:29:03.150: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/11/23 13:29:03.153
  STEP: create a pod that should be denied by the webhook @ 05/11/23 13:29:03.176
  STEP: create a pod that causes the webhook to hang @ 05/11/23 13:29:03.189
  STEP: create a configmap that should be denied by the webhook @ 05/11/23 13:29:13.197
  STEP: create a configmap that should be admitted by the webhook @ 05/11/23 13:29:13.223
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/11/23 13:29:13.234
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/11/23 13:29:13.243
  STEP: create a namespace that bypass the webhook @ 05/11/23 13:29:13.247
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/11/23 13:29:13.267
  May 11 13:29:13.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3898" for this suite. @ 05/11/23 13:29:13.366
  STEP: Destroying namespace "webhook-markers-4283" for this suite. @ 05/11/23 13:29:13.374
  STEP: Destroying namespace "exempted-namespace-1196" for this suite. @ 05/11/23 13:29:13.385
• [13.638 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/11/23 13:29:13.402
  May 11 13:29:13.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 13:29:13.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:13.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:13.42
  STEP: Creating secret with name secret-test-0b4ae927-1826-4f86-baa9-d9f86b37432c @ 05/11/23 13:29:13.422
  STEP: Creating a pod to test consume secrets @ 05/11/23 13:29:13.427
  STEP: Saw pod success @ 05/11/23 13:29:17.451
  May 11 13:29:17.453: INFO: Trying to get logs from node macpro-2 pod pod-secrets-acedf548-460c-4a8c-811a-85158db047ea container secret-env-test: <nil>
  STEP: delete the pod @ 05/11/23 13:29:17.458
  May 11 13:29:17.474: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9773" for this suite. @ 05/11/23 13:29:17.478
• [4.081 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/11/23 13:29:17.483
  May 11 13:29:17.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 13:29:17.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:17.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:17.502
  STEP: Setting up server cert @ 05/11/23 13:29:17.524
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 13:29:17.956
  STEP: Deploying the webhook pod @ 05/11/23 13:29:17.961
  STEP: Wait for the deployment to be ready @ 05/11/23 13:29:17.97
  May 11 13:29:17.976: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 13:29:19.988
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 13:29:20
  May 11 13:29:21.001: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 11 13:29:21.005: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1440-crds.webhook.example.com via the AdmissionRegistration API @ 05/11/23 13:29:21.519
  STEP: Creating a custom resource while v1 is storage version @ 05/11/23 13:29:21.534
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/11/23 13:29:23.571
  STEP: Patching the custom resource while v2 is storage version @ 05/11/23 13:29:23.592
  May 11 13:29:23.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3399" for this suite. @ 05/11/23 13:29:24.195
  STEP: Destroying namespace "webhook-markers-6684" for this suite. @ 05/11/23 13:29:24.201
• [6.737 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/11/23 13:29:24.221
  May 11 13:29:24.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replicaset @ 05/11/23 13:29:24.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:24.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:24.238
  STEP: Create a Replicaset @ 05/11/23 13:29:24.242
  STEP: Verify that the required pods have come up. @ 05/11/23 13:29:24.246
  May 11 13:29:24.249: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 11 13:29:29.253: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/11/23 13:29:29.253
  STEP: Getting /status @ 05/11/23 13:29:29.253
  May 11 13:29:29.257: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/11/23 13:29:29.257
  May 11 13:29:29.263: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/11/23 13:29:29.263
  May 11 13:29:29.265: INFO: Observed &ReplicaSet event: ADDED
  May 11 13:29:29.265: INFO: Observed &ReplicaSet event: MODIFIED
  May 11 13:29:29.265: INFO: Observed &ReplicaSet event: MODIFIED
  May 11 13:29:29.265: INFO: Observed &ReplicaSet event: MODIFIED
  May 11 13:29:29.265: INFO: Found replicaset test-rs in namespace replicaset-1012 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 11 13:29:29.265: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/11/23 13:29:29.265
  May 11 13:29:29.265: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 11 13:29:29.270: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/11/23 13:29:29.27
  May 11 13:29:29.272: INFO: Observed &ReplicaSet event: ADDED
  May 11 13:29:29.272: INFO: Observed &ReplicaSet event: MODIFIED
  May 11 13:29:29.272: INFO: Observed &ReplicaSet event: MODIFIED
  May 11 13:29:29.272: INFO: Observed &ReplicaSet event: MODIFIED
  May 11 13:29:29.272: INFO: Observed replicaset test-rs in namespace replicaset-1012 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 11 13:29:29.272: INFO: Observed &ReplicaSet event: MODIFIED
  May 11 13:29:29.272: INFO: Found replicaset test-rs in namespace replicaset-1012 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May 11 13:29:29.272: INFO: Replicaset test-rs has a patched status
  May 11 13:29:29.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1012" for this suite. @ 05/11/23 13:29:29.276
• [5.060 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/11/23 13:29:29.281
  May 11 13:29:29.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:29:29.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:29.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:29.306
  STEP: Creating configMap with name projected-configmap-test-volume-7595ca55-7cbd-4aba-882a-60daeda89d83 @ 05/11/23 13:29:29.308
  STEP: Creating a pod to test consume configMaps @ 05/11/23 13:29:29.314
  STEP: Saw pod success @ 05/11/23 13:29:33.338
  May 11 13:29:33.340: INFO: Trying to get logs from node macpro-3 pod pod-projected-configmaps-8458aa80-dd7d-49ea-96d4-36dbbc18a434 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 13:29:33.345
  May 11 13:29:33.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4267" for this suite. @ 05/11/23 13:29:33.361
• [4.085 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/11/23 13:29:33.367
  May 11 13:29:33.367: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 13:29:33.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:33.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:33.394
  STEP: creating a replication controller @ 05/11/23 13:29:33.396
  May 11 13:29:33.396: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 create -f -'
  May 11 13:29:34.384: INFO: stderr: ""
  May 11 13:29:34.384: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/11/23 13:29:34.384
  May 11 13:29:34.384: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 11 13:29:34.448: INFO: stderr: ""
  May 11 13:29:34.448: INFO: stdout: "update-demo-nautilus-vwz4j update-demo-nautilus-x8gzv "
  May 11 13:29:34.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-vwz4j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:29:34.509: INFO: stderr: ""
  May 11 13:29:34.509: INFO: stdout: ""
  May 11 13:29:34.509: INFO: update-demo-nautilus-vwz4j is created but not running
  May 11 13:29:39.509: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 11 13:29:39.571: INFO: stderr: ""
  May 11 13:29:39.571: INFO: stdout: "update-demo-nautilus-vwz4j update-demo-nautilus-x8gzv "
  May 11 13:29:39.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-vwz4j -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:29:39.628: INFO: stderr: ""
  May 11 13:29:39.628: INFO: stdout: "true"
  May 11 13:29:39.628: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-vwz4j -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 11 13:29:39.687: INFO: stderr: ""
  May 11 13:29:39.687: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 11 13:29:39.687: INFO: validating pod update-demo-nautilus-vwz4j
  May 11 13:29:39.691: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 11 13:29:39.691: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 11 13:29:39.691: INFO: update-demo-nautilus-vwz4j is verified up and running
  May 11 13:29:39.691: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x8gzv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:29:39.755: INFO: stderr: ""
  May 11 13:29:39.755: INFO: stdout: "true"
  May 11 13:29:39.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x8gzv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 11 13:29:39.814: INFO: stderr: ""
  May 11 13:29:39.814: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 11 13:29:39.814: INFO: validating pod update-demo-nautilus-x8gzv
  May 11 13:29:39.818: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 11 13:29:39.818: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 11 13:29:39.818: INFO: update-demo-nautilus-x8gzv is verified up and running
  STEP: scaling down the replication controller @ 05/11/23 13:29:39.818
  May 11 13:29:39.819: INFO: scanned /root for discovery docs: <nil>
  May 11 13:29:39.820: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  May 11 13:29:40.893: INFO: stderr: ""
  May 11 13:29:40.893: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/11/23 13:29:40.893
  May 11 13:29:40.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 11 13:29:40.954: INFO: stderr: ""
  May 11 13:29:40.954: INFO: stdout: "update-demo-nautilus-x8gzv "
  May 11 13:29:40.954: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x8gzv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:29:41.012: INFO: stderr: ""
  May 11 13:29:41.012: INFO: stdout: "true"
  May 11 13:29:41.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x8gzv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 11 13:29:41.071: INFO: stderr: ""
  May 11 13:29:41.071: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 11 13:29:41.071: INFO: validating pod update-demo-nautilus-x8gzv
  May 11 13:29:41.075: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 11 13:29:41.075: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 11 13:29:41.075: INFO: update-demo-nautilus-x8gzv is verified up and running
  STEP: scaling up the replication controller @ 05/11/23 13:29:41.075
  May 11 13:29:41.076: INFO: scanned /root for discovery docs: <nil>
  May 11 13:29:41.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  May 11 13:29:42.152: INFO: stderr: ""
  May 11 13:29:42.152: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/11/23 13:29:42.152
  May 11 13:29:42.153: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 11 13:29:42.232: INFO: stderr: ""
  May 11 13:29:42.232: INFO: stdout: "update-demo-nautilus-x4pfn update-demo-nautilus-x8gzv "
  May 11 13:29:42.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x4pfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:29:42.292: INFO: stderr: ""
  May 11 13:29:42.292: INFO: stdout: ""
  May 11 13:29:42.292: INFO: update-demo-nautilus-x4pfn is created but not running
  May 11 13:29:47.294: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 11 13:29:47.355: INFO: stderr: ""
  May 11 13:29:47.355: INFO: stdout: "update-demo-nautilus-x4pfn update-demo-nautilus-x8gzv "
  May 11 13:29:47.355: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x4pfn -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:29:47.415: INFO: stderr: ""
  May 11 13:29:47.415: INFO: stdout: "true"
  May 11 13:29:47.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x4pfn -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 11 13:29:47.472: INFO: stderr: ""
  May 11 13:29:47.472: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 11 13:29:47.472: INFO: validating pod update-demo-nautilus-x4pfn
  May 11 13:29:47.476: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 11 13:29:47.476: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 11 13:29:47.476: INFO: update-demo-nautilus-x4pfn is verified up and running
  May 11 13:29:47.476: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x8gzv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:29:47.534: INFO: stderr: ""
  May 11 13:29:47.534: INFO: stdout: "true"
  May 11 13:29:47.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods update-demo-nautilus-x8gzv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 11 13:29:47.591: INFO: stderr: ""
  May 11 13:29:47.591: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 11 13:29:47.591: INFO: validating pod update-demo-nautilus-x8gzv
  May 11 13:29:47.595: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 11 13:29:47.595: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 11 13:29:47.595: INFO: update-demo-nautilus-x8gzv is verified up and running
  STEP: using delete to clean up resources @ 05/11/23 13:29:47.595
  May 11 13:29:47.595: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 delete --grace-period=0 --force -f -'
  May 11 13:29:47.656: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 13:29:47.656: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 11 13:29:47.656: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get rc,svc -l name=update-demo --no-headers'
  May 11 13:29:47.723: INFO: stderr: "No resources found in kubectl-8763 namespace.\n"
  May 11 13:29:47.723: INFO: stdout: ""
  May 11 13:29:47.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8763 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 11 13:29:47.790: INFO: stderr: ""
  May 11 13:29:47.790: INFO: stdout: ""
  May 11 13:29:47.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8763" for this suite. @ 05/11/23 13:29:47.794
• [14.434 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/11/23 13:29:47.802
  May 11 13:29:47.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename disruption @ 05/11/23 13:29:47.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:47.83
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:47.833
  STEP: Creating a kubernetes client @ 05/11/23 13:29:47.835
  May 11 13:29:47.835: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename disruption-2 @ 05/11/23 13:29:47.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:47.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:47.852
  STEP: Waiting for the pdb to be processed @ 05/11/23 13:29:47.859
  STEP: Waiting for the pdb to be processed @ 05/11/23 13:29:49.869
  STEP: Waiting for the pdb to be processed @ 05/11/23 13:29:51.882
  STEP: listing a collection of PDBs across all namespaces @ 05/11/23 13:29:53.89
  STEP: listing a collection of PDBs in namespace disruption-5112 @ 05/11/23 13:29:53.897
  STEP: deleting a collection of PDBs @ 05/11/23 13:29:53.9
  STEP: Waiting for the PDB collection to be deleted @ 05/11/23 13:29:53.911
  May 11 13:29:53.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 13:29:53.918: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-5946" for this suite. @ 05/11/23 13:29:53.921
  STEP: Destroying namespace "disruption-5112" for this suite. @ 05/11/23 13:29:53.928
• [6.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/11/23 13:29:53.934
  May 11 13:29:53.934: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename watch @ 05/11/23 13:29:53.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:53.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:53.951
  STEP: creating a new configmap @ 05/11/23 13:29:53.953
  STEP: modifying the configmap once @ 05/11/23 13:29:53.977
  STEP: modifying the configmap a second time @ 05/11/23 13:29:53.984
  STEP: deleting the configmap @ 05/11/23 13:29:53.989
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/11/23 13:29:53.994
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/11/23 13:29:53.995
  May 11 13:29:53.995: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-88  51c91dfa-1cb8-491a-880c-ae21fe74724e 7843 0 2023-05-11 13:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-11 13:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 13:29:53.995: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-88  51c91dfa-1cb8-491a-880c-ae21fe74724e 7844 0 2023-05-11 13:29:53 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-11 13:29:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 13:29:53.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-88" for this suite. @ 05/11/23 13:29:53.999
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/11/23 13:29:54.004
  May 11 13:29:54.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename cronjob @ 05/11/23 13:29:54.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:29:54.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:29:54.02
  STEP: Creating a ReplaceConcurrent cronjob @ 05/11/23 13:29:54.022
  STEP: Ensuring a job is scheduled @ 05/11/23 13:29:54.026
  STEP: Ensuring exactly one is scheduled @ 05/11/23 13:30:02.031
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/11/23 13:30:02.034
  STEP: Ensuring the job is replaced with a new one @ 05/11/23 13:30:02.036
  STEP: Removing cronjob @ 05/11/23 13:31:02.042
  May 11 13:31:02.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-9857" for this suite. @ 05/11/23 13:31:02.051
• [68.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/11/23 13:31:02.057
  May 11 13:31:02.057: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 13:31:02.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:02.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:02.082
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-8036 @ 05/11/23 13:31:02.084
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/11/23 13:31:02.101
  STEP: creating service externalsvc in namespace services-8036 @ 05/11/23 13:31:02.101
  STEP: creating replication controller externalsvc in namespace services-8036 @ 05/11/23 13:31:02.122
  I0511 13:31:02.131254      24 runners.go:194] Created replication controller with name: externalsvc, namespace: services-8036, replica count: 2
  I0511 13:31:05.182067      24 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/11/23 13:31:05.185
  May 11 13:31:05.212: INFO: Creating new exec pod
  May 11 13:31:07.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-8036 exec execpodcd6g4 -- /bin/sh -x -c nslookup nodeport-service.services-8036.svc.cluster.local'
  May 11 13:31:07.381: INFO: stderr: "+ nslookup nodeport-service.services-8036.svc.cluster.local\n"
  May 11 13:31:07.381: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-8036.svc.cluster.local\tcanonical name = externalsvc.services-8036.svc.cluster.local.\nName:\texternalsvc.services-8036.svc.cluster.local\nAddress: 10.99.58.146\n\n"
  May 11 13:31:07.381: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-8036, will wait for the garbage collector to delete the pods @ 05/11/23 13:31:07.385
  May 11 13:31:07.443: INFO: Deleting ReplicationController externalsvc took: 4.98358ms
  May 11 13:31:07.543: INFO: Terminating ReplicationController externalsvc pods took: 100.452267ms
  May 11 13:31:09.804: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-8036" for this suite. @ 05/11/23 13:31:09.824
• [7.774 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/11/23 13:31:09.832
  May 11 13:31:09.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename podtemplate @ 05/11/23 13:31:09.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:09.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:09.852
  May 11 13:31:09.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-8924" for this suite. @ 05/11/23 13:31:09.883
• [0.058 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/11/23 13:31:09.89
  May 11 13:31:09.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-pred @ 05/11/23 13:31:09.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:09.914
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:09.917
  May 11 13:31:09.919: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 11 13:31:09.933: INFO: Waiting for terminating namespaces to be deleted...
  May 11 13:31:09.937: INFO: 
  Logging pods the apiserver thinks is on node macpro-1 before test
  May 11 13:31:09.944: INFO: calico-node-p8f6p from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.944: INFO: 	Container calico-node ready: true, restart count 0
  May 11 13:31:09.944: INFO: kube-proxy-t8w6l from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.944: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 13:31:09.944: INFO: speaker-pq5r2 from metallb-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.944: INFO: 	Container speaker ready: true, restart count 1
  May 11 13:31:09.944: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-9g8bt from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 13:31:09.944: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 13:31:09.944: INFO: 	Container systemd-logs ready: true, restart count 0
  May 11 13:31:09.944: INFO: 
  Logging pods the apiserver thinks is on node macpro-2 before test
  May 11 13:31:09.952: INFO: replace-28063530-5sm2r from cronjob-9857 started at 2023-05-11 13:30:00 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.952: INFO: 	Container c ready: true, restart count 0
  May 11 13:31:09.952: INFO: replace-28063531-mzmpp from cronjob-9857 started at 2023-05-11 13:31:00 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.952: INFO: 	Container c ready: true, restart count 0
  May 11 13:31:09.952: INFO: calico-node-92dft from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.952: INFO: 	Container calico-node ready: true, restart count 0
  May 11 13:31:09.952: INFO: kube-proxy-gl6v8 from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.952: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 13:31:09.952: INFO: speaker-hv6kp from metallb-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.952: INFO: 	Container speaker ready: true, restart count 1
  May 11 13:31:09.952: INFO: execpodcd6g4 from services-8036 started at 2023-05-11 13:31:05 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.952: INFO: 	Container agnhost-container ready: true, restart count 0
  May 11 13:31:09.952: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-6lx9v from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 13:31:09.952: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 13:31:09.952: INFO: 	Container systemd-logs ready: true, restart count 0
  May 11 13:31:09.952: INFO: 
  Logging pods the apiserver thinks is on node macpro-3 before test
  May 11 13:31:09.960: INFO: calico-node-gql49 from kube-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.960: INFO: 	Container calico-node ready: true, restart count 0
  May 11 13:31:09.960: INFO: kube-proxy-w7svn from kube-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.960: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 13:31:09.960: INFO: speaker-qtqrf from metallb-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.960: INFO: 	Container speaker ready: true, restart count 1
  May 11 13:31:09.960: INFO: sonobuoy from sonobuoy started at 2023-05-11 13:28:12 +0000 UTC (1 container statuses recorded)
  May 11 13:31:09.960: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 11 13:31:09.960: INFO: sonobuoy-e2e-job-ae6f9d200d3b496f from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 13:31:09.960: INFO: 	Container e2e ready: true, restart count 0
  May 11 13:31:09.960: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 13:31:09.960: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-kc8bx from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 13:31:09.960: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 13:31:09.960: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node macpro-1 @ 05/11/23 13:31:09.985
  STEP: verifying the node has the label node macpro-2 @ 05/11/23 13:31:10.001
  STEP: verifying the node has the label node macpro-3 @ 05/11/23 13:31:10.022
  May 11 13:31:10.040: INFO: Pod replace-28063530-5sm2r requesting resource cpu=0m on Node macpro-2
  May 11 13:31:10.040: INFO: Pod replace-28063531-mzmpp requesting resource cpu=0m on Node macpro-2
  May 11 13:31:10.040: INFO: Pod calico-node-92dft requesting resource cpu=250m on Node macpro-2
  May 11 13:31:10.040: INFO: Pod calico-node-gql49 requesting resource cpu=250m on Node macpro-3
  May 11 13:31:10.040: INFO: Pod calico-node-p8f6p requesting resource cpu=250m on Node macpro-1
  May 11 13:31:10.040: INFO: Pod kube-proxy-gl6v8 requesting resource cpu=0m on Node macpro-2
  May 11 13:31:10.040: INFO: Pod kube-proxy-t8w6l requesting resource cpu=0m on Node macpro-1
  May 11 13:31:10.040: INFO: Pod kube-proxy-w7svn requesting resource cpu=0m on Node macpro-3
  May 11 13:31:10.040: INFO: Pod speaker-hv6kp requesting resource cpu=0m on Node macpro-2
  May 11 13:31:10.040: INFO: Pod speaker-pq5r2 requesting resource cpu=0m on Node macpro-1
  May 11 13:31:10.040: INFO: Pod speaker-qtqrf requesting resource cpu=0m on Node macpro-3
  May 11 13:31:10.040: INFO: Pod execpodcd6g4 requesting resource cpu=0m on Node macpro-2
  May 11 13:31:10.040: INFO: Pod sonobuoy requesting resource cpu=0m on Node macpro-3
  May 11 13:31:10.040: INFO: Pod sonobuoy-e2e-job-ae6f9d200d3b496f requesting resource cpu=0m on Node macpro-3
  May 11 13:31:10.040: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-6lx9v requesting resource cpu=0m on Node macpro-2
  May 11 13:31:10.040: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-9g8bt requesting resource cpu=0m on Node macpro-1
  May 11 13:31:10.040: INFO: Pod sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-kc8bx requesting resource cpu=0m on Node macpro-3
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/11/23 13:31:10.04
  May 11 13:31:10.040: INFO: Creating a pod which consumes cpu=8225m on Node macpro-1
  May 11 13:31:10.050: INFO: Creating a pod which consumes cpu=16625m on Node macpro-2
  May 11 13:31:10.061: INFO: Creating a pod which consumes cpu=16625m on Node macpro-3
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/11/23 13:31:12.079
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-33dbe3dd-88c1-454b-96b4-35f95f08492c.175e19f97f254c39], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7069/filler-pod-33dbe3dd-88c1-454b-96b4-35f95f08492c to macpro-1] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-33dbe3dd-88c1-454b-96b4-35f95f08492c.175e19f9a09eabd8], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-33dbe3dd-88c1-454b-96b4-35f95f08492c.175e19f9a1c60af1], Reason = [Created], Message = [Created container filler-pod-33dbe3dd-88c1-454b-96b4-35f95f08492c] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-33dbe3dd-88c1-454b-96b4-35f95f08492c.175e19f9a5cc1a6b], Reason = [Started], Message = [Started container filler-pod-33dbe3dd-88c1-454b-96b4-35f95f08492c] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e455090-d193-492f-a95e-18dc12953b14.175e19f9803bcf11], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7069/filler-pod-6e455090-d193-492f-a95e-18dc12953b14 to macpro-2] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e455090-d193-492f-a95e-18dc12953b14.175e19f9a60db072], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e455090-d193-492f-a95e-18dc12953b14.175e19f9a78c7721], Reason = [Created], Message = [Created container filler-pod-6e455090-d193-492f-a95e-18dc12953b14] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-6e455090-d193-492f-a95e-18dc12953b14.175e19f9ac5f6413], Reason = [Started], Message = [Started container filler-pod-6e455090-d193-492f-a95e-18dc12953b14] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f964bdd9-b1a1-47e9-bfdd-be7d93fc8b2b.175e19f98081e3c0], Reason = [Scheduled], Message = [Successfully assigned sched-pred-7069/filler-pod-f964bdd9-b1a1-47e9-bfdd-be7d93fc8b2b to macpro-3] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f964bdd9-b1a1-47e9-bfdd-be7d93fc8b2b.175e19f9a44f95ee], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f964bdd9-b1a1-47e9-bfdd-be7d93fc8b2b.175e19f9a5b5c8a2], Reason = [Created], Message = [Created container filler-pod-f964bdd9-b1a1-47e9-bfdd-be7d93fc8b2b] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-f964bdd9-b1a1-47e9-bfdd-be7d93fc8b2b.175e19f9aa5f156b], Reason = [Started], Message = [Started container filler-pod-f964bdd9-b1a1-47e9-bfdd-be7d93fc8b2b] @ 05/11/23 13:31:12.082
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175e19f9f87f898e], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 Insufficient cpu, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 3 No preemption victims found for incoming pod, 3 Preemption is not helpful for scheduling..] @ 05/11/23 13:31:12.1
  STEP: removing the label node off the node macpro-1 @ 05/11/23 13:31:13.096
  STEP: verifying the node doesn't have the label node @ 05/11/23 13:31:13.114
  STEP: removing the label node off the node macpro-2 @ 05/11/23 13:31:13.128
  STEP: verifying the node doesn't have the label node @ 05/11/23 13:31:13.145
  STEP: removing the label node off the node macpro-3 @ 05/11/23 13:31:13.153
  STEP: verifying the node doesn't have the label node @ 05/11/23 13:31:13.176
  May 11 13:31:13.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-7069" for this suite. @ 05/11/23 13:31:13.19
• [3.312 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/11/23 13:31:13.202
  May 11 13:31:13.203: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl-logs @ 05/11/23 13:31:13.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:13.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:13.256
  STEP: creating an pod @ 05/11/23 13:31:13.259
  May 11 13:31:13.259: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-logs-7996 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May 11 13:31:13.328: INFO: stderr: ""
  May 11 13:31:13.328: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/11/23 13:31:13.328
  May 11 13:31:13.328: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  May 11 13:31:15.335: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/11/23 13:31:15.335
  May 11 13:31:15.335: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-logs-7996 logs logs-generator logs-generator'
  May 11 13:31:15.410: INFO: stderr: ""
  May 11 13:31:15.410: INFO: stdout: "I0511 13:31:14.004452       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/bfjw 348\nI0511 13:31:14.204585       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/w8kt 520\nI0511 13:31:14.405140       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/49xm 496\nI0511 13:31:14.605513       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/hchd 542\nI0511 13:31:14.804890       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/v866 591\nI0511 13:31:15.005265       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/khf 462\nI0511 13:31:15.204563       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9gp 499\nI0511 13:31:15.404898       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4qsq 355\n"
  STEP: limiting log lines @ 05/11/23 13:31:15.41
  May 11 13:31:15.410: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-logs-7996 logs logs-generator logs-generator --tail=1'
  May 11 13:31:15.484: INFO: stderr: ""
  May 11 13:31:15.484: INFO: stdout: "I0511 13:31:15.404898       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4qsq 355\n"
  May 11 13:31:15.484: INFO: got output "I0511 13:31:15.404898       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4qsq 355\n"
  STEP: limiting log bytes @ 05/11/23 13:31:15.484
  May 11 13:31:15.484: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-logs-7996 logs logs-generator logs-generator --limit-bytes=1'
  May 11 13:31:15.551: INFO: stderr: ""
  May 11 13:31:15.551: INFO: stdout: "I"
  May 11 13:31:15.551: INFO: got output "I"
  STEP: exposing timestamps @ 05/11/23 13:31:15.551
  May 11 13:31:15.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-logs-7996 logs logs-generator logs-generator --tail=1 --timestamps'
  May 11 13:31:15.617: INFO: stderr: ""
  May 11 13:31:15.617: INFO: stdout: "2023-05-11T13:31:15.605365264Z I0511 13:31:15.605266       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/n25 296\n"
  May 11 13:31:15.617: INFO: got output "2023-05-11T13:31:15.605365264Z I0511 13:31:15.605266       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/n25 296\n"
  STEP: restricting to a time range @ 05/11/23 13:31:15.617
  May 11 13:31:18.119: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-logs-7996 logs logs-generator logs-generator --since=1s'
  May 11 13:31:18.183: INFO: stderr: ""
  May 11 13:31:18.183: INFO: stdout: "I0511 13:31:17.204886       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/rc4 253\nI0511 13:31:17.405352       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/gwwb 200\nI0511 13:31:17.604656       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/d5x 412\nI0511 13:31:17.805012       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5vk 282\nI0511 13:31:18.005383       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/6cgt 472\n"
  May 11 13:31:18.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-logs-7996 logs logs-generator logs-generator --since=24h'
  May 11 13:31:18.249: INFO: stderr: ""
  May 11 13:31:18.249: INFO: stdout: "I0511 13:31:14.004452       1 logs_generator.go:76] 0 POST /api/v1/namespaces/ns/pods/bfjw 348\nI0511 13:31:14.204585       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/default/pods/w8kt 520\nI0511 13:31:14.405140       1 logs_generator.go:76] 2 POST /api/v1/namespaces/ns/pods/49xm 496\nI0511 13:31:14.605513       1 logs_generator.go:76] 3 GET /api/v1/namespaces/default/pods/hchd 542\nI0511 13:31:14.804890       1 logs_generator.go:76] 4 POST /api/v1/namespaces/kube-system/pods/v866 591\nI0511 13:31:15.005265       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/khf 462\nI0511 13:31:15.204563       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/kube-system/pods/9gp 499\nI0511 13:31:15.404898       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/4qsq 355\nI0511 13:31:15.605266       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/n25 296\nI0511 13:31:15.804692       1 logs_generator.go:76] 9 PUT /api/v1/namespaces/ns/pods/45z 396\nI0511 13:31:16.005060       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/default/pods/ckpz 300\nI0511 13:31:16.205312       1 logs_generator.go:76] 11 POST /api/v1/namespaces/default/pods/sxr 210\nI0511 13:31:16.404570       1 logs_generator.go:76] 12 GET /api/v1/namespaces/default/pods/tlx 435\nI0511 13:31:16.604974       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/ns/pods/4529 284\nI0511 13:31:16.805318       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/ns/pods/b8n4 283\nI0511 13:31:17.004578       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/default/pods/szg 502\nI0511 13:31:17.204886       1 logs_generator.go:76] 16 GET /api/v1/namespaces/default/pods/rc4 253\nI0511 13:31:17.405352       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/gwwb 200\nI0511 13:31:17.604656       1 logs_generator.go:76] 18 POST /api/v1/namespaces/kube-system/pods/d5x 412\nI0511 13:31:17.805012       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/5vk 282\nI0511 13:31:18.005383       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/kube-system/pods/6cgt 472\nI0511 13:31:18.204675       1 logs_generator.go:76] 21 GET /api/v1/namespaces/kube-system/pods/hvk 245\n"
  May 11 13:31:18.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-logs-7996 delete pod logs-generator'
  May 11 13:31:18.776: INFO: stderr: ""
  May 11 13:31:18.776: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May 11 13:31:18.776: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-7996" for this suite. @ 05/11/23 13:31:18.78
• [5.583 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/11/23 13:31:18.788
  May 11 13:31:18.788: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename endpointslice @ 05/11/23 13:31:18.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:18.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:18.821
  STEP: getting /apis @ 05/11/23 13:31:18.825
  STEP: getting /apis/discovery.k8s.io @ 05/11/23 13:31:18.831
  STEP: getting /apis/discovery.k8s.iov1 @ 05/11/23 13:31:18.832
  STEP: creating @ 05/11/23 13:31:18.833
  STEP: getting @ 05/11/23 13:31:18.849
  STEP: listing @ 05/11/23 13:31:18.853
  STEP: watching @ 05/11/23 13:31:18.857
  May 11 13:31:18.857: INFO: starting watch
  STEP: cluster-wide listing @ 05/11/23 13:31:18.859
  STEP: cluster-wide watching @ 05/11/23 13:31:18.864
  May 11 13:31:18.864: INFO: starting watch
  STEP: patching @ 05/11/23 13:31:18.865
  STEP: updating @ 05/11/23 13:31:18.873
  May 11 13:31:18.881: INFO: waiting for watch events with expected annotations
  May 11 13:31:18.881: INFO: saw patched and updated annotations
  STEP: deleting @ 05/11/23 13:31:18.882
  STEP: deleting a collection @ 05/11/23 13:31:18.895
  May 11 13:31:18.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-1527" for this suite. @ 05/11/23 13:31:18.92
• [0.138 seconds]
------------------------------
SSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/11/23 13:31:18.926
  May 11 13:31:18.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 13:31:18.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:18.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:18.946
  STEP: Creating projection with secret that has name secret-emptykey-test-d30a7d70-9c64-4939-acd2-153fdb9fd692 @ 05/11/23 13:31:18.948
  May 11 13:31:18.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8657" for this suite. @ 05/11/23 13:31:18.955
• [0.038 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/11/23 13:31:18.964
  May 11 13:31:18.964: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:31:18.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:18.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:18.994
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:31:18.997
  STEP: Saw pod success @ 05/11/23 13:31:21.024
  May 11 13:31:21.027: INFO: Trying to get logs from node macpro-3 pod downwardapi-volume-d5eeb578-fa7e-4959-a0ba-f8552de3eab0 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:31:21.039
  May 11 13:31:21.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8112" for this suite. @ 05/11/23 13:31:21.081
• [2.166 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/11/23 13:31:21.131
  May 11 13:31:21.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 13:31:21.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:21.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:21.178
  STEP: Setting up server cert @ 05/11/23 13:31:21.245
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 13:31:21.544
  STEP: Deploying the webhook pod @ 05/11/23 13:31:21.55
  STEP: Wait for the deployment to be ready @ 05/11/23 13:31:21.561
  May 11 13:31:21.566: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 13:31:23.577
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 13:31:23.597
  May 11 13:31:24.597: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 11 13:31:24.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/11/23 13:31:25.111
  STEP: Creating a custom resource that should be denied by the webhook @ 05/11/23 13:31:25.128
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/11/23 13:31:27.149
  STEP: Updating the custom resource with disallowed data should be denied @ 05/11/23 13:31:27.155
  STEP: Deleting the custom resource should be denied @ 05/11/23 13:31:27.161
  STEP: Remove the offending key and value from the custom resource data @ 05/11/23 13:31:27.166
  STEP: Deleting the updated custom resource should be successful @ 05/11/23 13:31:27.174
  May 11 13:31:27.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3308" for this suite. @ 05/11/23 13:31:27.743
  STEP: Destroying namespace "webhook-markers-1695" for this suite. @ 05/11/23 13:31:27.749
• [6.627 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/11/23 13:31:27.758
  May 11 13:31:27.758: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename namespaces @ 05/11/23 13:31:27.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:27.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:27.793
  STEP: creating a Namespace @ 05/11/23 13:31:27.795
  STEP: patching the Namespace @ 05/11/23 13:31:27.811
  STEP: get the Namespace and ensuring it has the label @ 05/11/23 13:31:27.815
  May 11 13:31:27.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-397" for this suite. @ 05/11/23 13:31:27.822
  STEP: Destroying namespace "nspatchtest-2fb0e759-4e59-4168-96d9-c7c60cd23c0d-6788" for this suite. @ 05/11/23 13:31:27.827
• [0.073 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/11/23 13:31:27.832
  May 11 13:31:27.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 13:31:27.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:27.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:27.851
  STEP: Creating Pod @ 05/11/23 13:31:27.853
  STEP: Reading file content from the nginx-container @ 05/11/23 13:31:29.869
  May 11 13:31:29.869: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-454 PodName:pod-sharedvolume-568fa3e5-34d3-4f76-9b55-6e8b7314f79d ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 13:31:29.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:31:29.870: INFO: ExecWithOptions: Clientset creation
  May 11 13:31:29.870: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-454/pods/pod-sharedvolume-568fa3e5-34d3-4f76-9b55-6e8b7314f79d/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May 11 13:31:29.930: INFO: Exec stderr: ""
  May 11 13:31:29.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-454" for this suite. @ 05/11/23 13:31:29.934
• [2.110 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/11/23 13:31:29.942
  May 11 13:31:29.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 13:31:29.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:29.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:29.963
  May 11 13:31:29.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3083" for this suite. @ 05/11/23 13:31:29.972
• [0.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/11/23 13:31:29.977
  May 11 13:31:29.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:31:29.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:29.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:29.997
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:31:29.999
  STEP: Saw pod success @ 05/11/23 13:31:32.014
  May 11 13:31:32.016: INFO: Trying to get logs from node macpro-3 pod downwardapi-volume-5f427903-295e-4f7c-99df-2b12646d48c5 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:31:32.021
  May 11 13:31:32.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-198" for this suite. @ 05/11/23 13:31:32.034
• [2.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/11/23 13:31:32.041
  May 11 13:31:32.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replicaset @ 05/11/23 13:31:32.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:32.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:32.057
  May 11 13:31:32.059: INFO: Creating ReplicaSet my-hostname-basic-7bbb4320-897e-4a1d-a942-783465119ba8
  May 11 13:31:32.066: INFO: Pod name my-hostname-basic-7bbb4320-897e-4a1d-a942-783465119ba8: Found 0 pods out of 1
  May 11 13:31:37.071: INFO: Pod name my-hostname-basic-7bbb4320-897e-4a1d-a942-783465119ba8: Found 1 pods out of 1
  May 11 13:31:37.071: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-7bbb4320-897e-4a1d-a942-783465119ba8" is running
  May 11 13:31:37.074: INFO: Pod "my-hostname-basic-7bbb4320-897e-4a1d-a942-783465119ba8-zwlx9" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-11 13:31:32 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-11 13:31:32 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-11 13:31:32 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-11 13:31:32 +0000 UTC Reason: Message:}])
  May 11 13:31:37.074: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/11/23 13:31:37.074
  May 11 13:31:37.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4348" for this suite. @ 05/11/23 13:31:37.086
• [5.051 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/11/23 13:31:37.092
  May 11 13:31:37.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replicaset @ 05/11/23 13:31:37.093
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:37.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:37.112
  STEP: Create a ReplicaSet @ 05/11/23 13:31:37.115
  STEP: Verify that the required pods have come up @ 05/11/23 13:31:37.119
  May 11 13:31:37.121: INFO: Pod name sample-pod: Found 0 pods out of 3
  May 11 13:31:42.127: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/11/23 13:31:42.127
  May 11 13:31:42.129: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/11/23 13:31:42.129
  STEP: DeleteCollection of the ReplicaSets @ 05/11/23 13:31:42.132
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/11/23 13:31:42.141
  May 11 13:31:42.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3108" for this suite. @ 05/11/23 13:31:42.149
• [5.075 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/11/23 13:31:42.167
  May 11 13:31:42.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename dns @ 05/11/23 13:31:42.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:31:42.186
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:31:42.188
  STEP: Creating a test headless service @ 05/11/23 13:31:42.192
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-7771.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-7771.svc.cluster.local;sleep 1; done
   @ 05/11/23 13:31:42.196
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-7771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-7771.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-7771.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-7771.svc.cluster.local;sleep 1; done
   @ 05/11/23 13:31:42.196
  STEP: creating a pod to probe DNS @ 05/11/23 13:31:42.196
  STEP: submitting the pod to kubernetes @ 05/11/23 13:31:42.196
  STEP: retrieving the pod @ 05/11/23 13:31:44.223
  STEP: looking for the results for each expected name from probers @ 05/11/23 13:31:44.229
  May 11 13:31:44.234: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:44.237: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:44.243: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:44.246: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:44.249: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:44.252: INFO: Unable to read jessie_udp@dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:44.255: INFO: Lookups using dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-7771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_udp@dns-test-service-2.dns-7771.svc.cluster.local]

  May 11 13:31:49.259: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:49.262: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:49.269: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:49.272: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:49.277: INFO: Lookups using dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local]

  May 11 13:31:54.260: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:54.263: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:54.274: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:54.277: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:54.283: INFO: Lookups using dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local]

  May 11 13:31:59.261: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:59.264: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:59.272: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:59.275: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:31:59.280: INFO: Lookups using dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local]

  May 11 13:32:04.262: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:32:04.265: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:32:04.280: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:32:04.283: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:32:04.292: INFO: Lookups using dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local]

  May 11 13:32:09.259: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:32:09.263: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:32:09.271: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:32:09.274: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local from pod dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905: the server could not find the requested resource (get pods dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905)
  May 11 13:32:09.279: INFO: Lookups using dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-7771.svc.cluster.local]

  May 11 13:32:14.279: INFO: DNS probes using dns-7771/dns-test-9939456f-e33f-4c8b-b3f1-db7b49e60905 succeeded

  May 11 13:32:14.279: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 13:32:14.282
  STEP: deleting the test headless service @ 05/11/23 13:32:14.294
  STEP: Destroying namespace "dns-7771" for this suite. @ 05/11/23 13:32:14.309
• [32.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/11/23 13:32:14.316
  May 11 13:32:14.316: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename endpointslice @ 05/11/23 13:32:14.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:32:14.349
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:32:14.352
  STEP: referencing a single matching pod @ 05/11/23 13:32:19.452
  STEP: referencing matching pods with named port @ 05/11/23 13:32:24.459
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/11/23 13:32:29.466
  STEP: recreating EndpointSlices after they've been deleted @ 05/11/23 13:32:34.474
  May 11 13:32:34.492: INFO: EndpointSlice for Service endpointslice-3993/example-named-port not found
  May 11 13:32:44.500: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-3993" for this suite. @ 05/11/23 13:32:44.505
• [30.194 seconds]
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/11/23 13:32:44.51
  May 11 13:32:44.510: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename var-expansion @ 05/11/23 13:32:44.511
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:32:44.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:32:44.526
  STEP: creating the pod @ 05/11/23 13:32:44.528
  STEP: waiting for pod running @ 05/11/23 13:32:44.537
  STEP: creating a file in subpath @ 05/11/23 13:32:46.547
  May 11 13:32:46.550: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-6934 PodName:var-expansion-d212d064-dc77-4974-b7df-ecf864d53f7a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 13:32:46.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:32:46.550: INFO: ExecWithOptions: Clientset creation
  May 11 13:32:46.550: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-6934/pods/var-expansion-d212d064-dc77-4974-b7df-ecf864d53f7a/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/11/23 13:32:46.596
  May 11 13:32:46.598: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-6934 PodName:var-expansion-d212d064-dc77-4974-b7df-ecf864d53f7a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 13:32:46.598: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:32:46.599: INFO: ExecWithOptions: Clientset creation
  May 11 13:32:46.599: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-6934/pods/var-expansion-d212d064-dc77-4974-b7df-ecf864d53f7a/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/11/23 13:32:46.64
  May 11 13:32:47.153: INFO: Successfully updated pod "var-expansion-d212d064-dc77-4974-b7df-ecf864d53f7a"
  STEP: waiting for annotated pod running @ 05/11/23 13:32:47.153
  STEP: deleting the pod gracefully @ 05/11/23 13:32:47.157
  May 11 13:32:47.157: INFO: Deleting pod "var-expansion-d212d064-dc77-4974-b7df-ecf864d53f7a" in namespace "var-expansion-6934"
  May 11 13:32:47.166: INFO: Wait up to 5m0s for pod "var-expansion-d212d064-dc77-4974-b7df-ecf864d53f7a" to be fully deleted
  May 11 13:33:21.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6934" for this suite. @ 05/11/23 13:33:21.282
• [36.778 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/11/23 13:33:21.289
  May 11 13:33:21.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replicaset @ 05/11/23 13:33:21.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:33:21.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:33:21.308
  May 11 13:33:21.321: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 11 13:33:26.326: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/11/23 13:33:26.326
  STEP: Scaling up "test-rs" replicaset  @ 05/11/23 13:33:26.327
  May 11 13:33:26.336: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/11/23 13:33:26.336
  W0511 13:33:26.365986      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 11 13:33:26.368: INFO: observed ReplicaSet test-rs in namespace replicaset-7975 with ReadyReplicas 1, AvailableReplicas 1
  May 11 13:33:26.383: INFO: observed ReplicaSet test-rs in namespace replicaset-7975 with ReadyReplicas 1, AvailableReplicas 1
  May 11 13:33:26.401: INFO: observed ReplicaSet test-rs in namespace replicaset-7975 with ReadyReplicas 1, AvailableReplicas 1
  May 11 13:33:27.553: INFO: observed ReplicaSet test-rs in namespace replicaset-7975 with ReadyReplicas 2, AvailableReplicas 2
  May 11 13:33:27.973: INFO: observed Replicaset test-rs in namespace replicaset-7975 with ReadyReplicas 3 found true
  May 11 13:33:27.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7975" for this suite. @ 05/11/23 13:33:27.989
• [6.707 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/11/23 13:33:27.998
  May 11 13:33:27.998: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 13:33:27.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:33:28.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:33:28.023
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:33:28.026
  STEP: Saw pod success @ 05/11/23 13:33:32.053
  May 11 13:33:32.056: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-2a979590-e1fc-447e-924c-7cb142180da5 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:33:32.069
  May 11 13:33:32.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4756" for this suite. @ 05/11/23 13:33:32.09
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/11/23 13:33:32.098
  May 11 13:33:32.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename containers @ 05/11/23 13:33:32.099
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:33:32.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:33:32.117
  STEP: Creating a pod to test override command @ 05/11/23 13:33:32.119
  STEP: Saw pod success @ 05/11/23 13:33:36.14
  May 11 13:33:36.143: INFO: Trying to get logs from node macpro-2 pod client-containers-8c58b951-0ea7-4be3-bb97-9fdc7aa8c1c0 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 13:33:36.149
  May 11 13:33:36.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9509" for this suite. @ 05/11/23 13:33:36.166
• [4.073 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/11/23 13:33:36.171
  May 11 13:33:36.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:33:36.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:33:36.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:33:36.19
  STEP: Creating secret with name s-test-opt-del-6c9beced-e04a-41bc-87c9-0c129fdac725 @ 05/11/23 13:33:36.196
  STEP: Creating secret with name s-test-opt-upd-fe986708-90f7-4150-8d2e-18a6d3bcb605 @ 05/11/23 13:33:36.202
  STEP: Creating the pod @ 05/11/23 13:33:36.207
  STEP: Deleting secret s-test-opt-del-6c9beced-e04a-41bc-87c9-0c129fdac725 @ 05/11/23 13:33:38.267
  STEP: Updating secret s-test-opt-upd-fe986708-90f7-4150-8d2e-18a6d3bcb605 @ 05/11/23 13:33:38.276
  STEP: Creating secret with name s-test-opt-create-32ba3106-1c9c-406f-b0b1-a0a96e36a341 @ 05/11/23 13:33:38.28
  STEP: waiting to observe update in volume @ 05/11/23 13:33:38.286
  May 11 13:33:42.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4674" for this suite. @ 05/11/23 13:33:42.323
• [6.157 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/11/23 13:33:42.329
  May 11 13:33:42.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-runtime @ 05/11/23 13:33:42.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:33:42.345
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:33:42.348
  STEP: create the container @ 05/11/23 13:33:42.35
  W0511 13:33:42.357291      24 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/11/23 13:33:42.357
  STEP: get the container status @ 05/11/23 13:33:45.374
  STEP: the container should be terminated @ 05/11/23 13:33:45.377
  STEP: the termination message should be set @ 05/11/23 13:33:45.377
  May 11 13:33:45.377: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/11/23 13:33:45.377
  May 11 13:33:45.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9693" for this suite. @ 05/11/23 13:33:45.393
• [3.070 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/11/23 13:33:45.399
  May 11 13:33:45.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename statefulset @ 05/11/23 13:33:45.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:33:45.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:33:45.423
  STEP: Creating service test in namespace statefulset-4920 @ 05/11/23 13:33:45.425
  STEP: Creating stateful set ss in namespace statefulset-4920 @ 05/11/23 13:33:45.431
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4920 @ 05/11/23 13:33:45.442
  May 11 13:33:45.445: INFO: Found 0 stateful pods, waiting for 1
  May 11 13:33:55.451: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/11/23 13:33:55.451
  May 11 13:33:55.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-4920 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 13:33:55.584: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 13:33:55.585: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 13:33:55.585: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 13:33:55.588: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May 11 13:34:05.592: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 11 13:34:05.592: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 13:34:05.605: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
  May 11 13:34:05.605: INFO: ss-0  macpro-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:33:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:33:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:33:55 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:33:45 +0000 UTC  }]
  May 11 13:34:05.605: INFO: 
  May 11 13:34:05.605: INFO: StatefulSet ss has not reached scale 3, at 1
  May 11 13:34:06.612: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997234121s
  May 11 13:34:07.617: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.989429496s
  May 11 13:34:08.621: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.985248127s
  May 11 13:34:09.625: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.981309434s
  May 11 13:34:10.629: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.97708184s
  May 11 13:34:11.633: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.973146082s
  May 11 13:34:12.637: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.969157853s
  May 11 13:34:13.645: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.964441984s
  May 11 13:34:14.650: INFO: Verifying statefulset ss doesn't scale past 3 for another 957.568853ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4920 @ 05/11/23 13:34:15.65
  May 11 13:34:15.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-4920 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 13:34:15.765: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 11 13:34:15.765: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 13:34:15.765: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 11 13:34:15.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-4920 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 13:34:15.886: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 11 13:34:15.886: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 13:34:15.886: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 11 13:34:15.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-4920 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 13:34:16.013: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 11 13:34:16.013: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 13:34:16.013: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 11 13:34:16.017: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 11 13:34:16.017: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 11 13:34:16.017: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/11/23 13:34:16.017
  May 11 13:34:16.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-4920 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 13:34:16.126: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 13:34:16.126: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 13:34:16.126: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 13:34:16.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-4920 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 13:34:16.246: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 13:34:16.246: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 13:34:16.246: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 13:34:16.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-4920 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 13:34:16.356: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 13:34:16.356: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 13:34:16.356: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 13:34:16.356: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 13:34:16.360: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May 11 13:34:26.368: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 11 13:34:26.368: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 11 13:34:26.368: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 11 13:34:26.380: INFO: POD   NODE      PHASE    GRACE  CONDITIONS
  May 11 13:34:26.380: INFO: ss-0  macpro-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:33:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:33:45 +0000 UTC  }]
  May 11 13:34:26.380: INFO: ss-1  macpro-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:05 +0000 UTC  }]
  May 11 13:34:26.380: INFO: ss-2  macpro-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:05 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:16 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:05 +0000 UTC  }]
  May 11 13:34:26.380: INFO: 
  May 11 13:34:26.380: INFO: StatefulSet ss has not reached scale 0, at 3
  May 11 13:34:27.384: INFO: POD   NODE      PHASE      GRACE  CONDITIONS
  May 11 13:34:27.384: INFO: ss-2  macpro-1  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:05 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:16 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:16 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:34:05 +0000 UTC  }]
  May 11 13:34:27.384: INFO: 
  May 11 13:34:27.384: INFO: StatefulSet ss has not reached scale 0, at 1
  May 11 13:34:28.396: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.99188516s
  May 11 13:34:29.399: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.980034149s
  May 11 13:34:30.404: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.976508704s
  May 11 13:34:31.409: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.972002161s
  May 11 13:34:32.413: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.966653225s
  May 11 13:34:33.418: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.962577948s
  May 11 13:34:34.422: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.958107161s
  May 11 13:34:35.425: INFO: Verifying statefulset ss doesn't scale past 0 for another 954.29073ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4920 @ 05/11/23 13:34:36.425
  May 11 13:34:36.429: INFO: Scaling statefulset ss to 0
  May 11 13:34:36.450: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 13:34:36.452: INFO: Deleting all statefulset in ns statefulset-4920
  May 11 13:34:36.455: INFO: Scaling statefulset ss to 0
  May 11 13:34:36.465: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 13:34:36.469: INFO: Deleting statefulset ss
  May 11 13:34:36.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4920" for this suite. @ 05/11/23 13:34:36.5
• [51.107 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/11/23 13:34:36.506
  May 11 13:34:36.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 13:34:36.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:34:36.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:34:36.527
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/11/23 13:34:36.53
  May 11 13:34:36.530: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8010 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 11 13:34:36.595: INFO: stderr: ""
  May 11 13:34:36.595: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/11/23 13:34:36.596
  May 11 13:34:36.596: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8010 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May 11 13:34:36.660: INFO: stderr: ""
  May 11 13:34:36.660: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/11/23 13:34:36.66
  May 11 13:34:36.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8010 delete pods e2e-test-httpd-pod'
  May 11 13:34:39.136: INFO: stderr: ""
  May 11 13:34:39.136: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 11 13:34:39.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8010" for this suite. @ 05/11/23 13:34:39.142
• [2.641 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/11/23 13:34:39.148
  May 11 13:34:39.148: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename var-expansion @ 05/11/23 13:34:39.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:34:39.166
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:34:39.169
  STEP: Creating a pod to test substitution in volume subpath @ 05/11/23 13:34:39.171
  STEP: Saw pod success @ 05/11/23 13:34:43.192
  May 11 13:34:43.196: INFO: Trying to get logs from node macpro-2 pod var-expansion-c23c9e8a-6ad9-48f4-a0ad-68c12137348a container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 13:34:43.204
  May 11 13:34:43.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5457" for this suite. @ 05/11/23 13:34:43.227
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/11/23 13:34:43.237
  May 11 13:34:43.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 13:34:43.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:34:43.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:34:43.256
  STEP: Discovering how many secrets are in namespace by default @ 05/11/23 13:34:43.258
  STEP: Counting existing ResourceQuota @ 05/11/23 13:34:48.261
  STEP: Creating a ResourceQuota @ 05/11/23 13:34:53.265
  STEP: Ensuring resource quota status is calculated @ 05/11/23 13:34:53.27
  STEP: Creating a Secret @ 05/11/23 13:34:55.274
  STEP: Ensuring resource quota status captures secret creation @ 05/11/23 13:34:55.285
  STEP: Deleting a secret @ 05/11/23 13:34:57.29
  STEP: Ensuring resource quota status released usage @ 05/11/23 13:34:57.297
  May 11 13:34:59.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3767" for this suite. @ 05/11/23 13:34:59.306
• [16.075 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/11/23 13:34:59.312
  May 11 13:34:59.312: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename endpointslice @ 05/11/23 13:34:59.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:34:59.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:34:59.334
  May 11 13:34:59.344: INFO: Endpoints addresses: [10.221.188.5 10.221.188.6 10.221.188.7] , ports: [6443]
  May 11 13:34:59.344: INFO: EndpointSlices addresses: [10.221.188.5 10.221.188.6 10.221.188.7] , ports: [6443]
  May 11 13:34:59.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-2786" for this suite. @ 05/11/23 13:34:59.348
• [0.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/11/23 13:34:59.355
  May 11 13:34:59.355: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:34:59.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:34:59.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:34:59.373
  STEP: Creating projection with secret that has name projected-secret-test-eb363f7c-e588-4881-af3b-181d513ab4c0 @ 05/11/23 13:34:59.375
  STEP: Creating a pod to test consume secrets @ 05/11/23 13:34:59.38
  STEP: Saw pod success @ 05/11/23 13:35:01.392
  May 11 13:35:01.395: INFO: Trying to get logs from node macpro-2 pod pod-projected-secrets-9f870f97-2ae2-48bd-bfa7-0d91a5436a12 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 13:35:01.4
  May 11 13:35:01.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3750" for this suite. @ 05/11/23 13:35:01.415
• [2.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/11/23 13:35:01.421
  May 11 13:35:01.421: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 13:35:01.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:01.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:01.448
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-9773 @ 05/11/23 13:35:01.45
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/11/23 13:35:01.465
  STEP: creating service externalsvc in namespace services-9773 @ 05/11/23 13:35:01.465
  STEP: creating replication controller externalsvc in namespace services-9773 @ 05/11/23 13:35:01.502
  I0511 13:35:01.510465      24 runners.go:194] Created replication controller with name: externalsvc, namespace: services-9773, replica count: 2
  I0511 13:35:04.560773      24 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/11/23 13:35:04.565
  May 11 13:35:04.586: INFO: Creating new exec pod
  May 11 13:35:06.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-9773 exec execpodqv75p -- /bin/sh -x -c nslookup clusterip-service.services-9773.svc.cluster.local'
  May 11 13:35:06.755: INFO: stderr: "+ nslookup clusterip-service.services-9773.svc.cluster.local\n"
  May 11 13:35:06.755: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nName:\tclusterip-service.services-9773.svc.cluster.local\nAddress: 10.98.70.130\n\n"
  May 11 13:35:06.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-9773, will wait for the garbage collector to delete the pods @ 05/11/23 13:35:06.759
  May 11 13:35:06.818: INFO: Deleting ReplicationController externalsvc took: 5.318605ms
  May 11 13:35:06.918: INFO: Terminating ReplicationController externalsvc pods took: 100.128948ms
  May 11 13:35:09.258: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-9773" for this suite. @ 05/11/23 13:35:09.273
• [7.867 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/11/23 13:35:09.288
  May 11 13:35:09.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-webhook @ 05/11/23 13:35:09.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:09.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:09.307
  STEP: Setting up server cert @ 05/11/23 13:35:09.309
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/11/23 13:35:09.847
  STEP: Deploying the custom resource conversion webhook pod @ 05/11/23 13:35:09.855
  STEP: Wait for the deployment to be ready @ 05/11/23 13:35:09.866
  May 11 13:35:09.873: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/11/23 13:35:11.883
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 13:35:11.895
  May 11 13:35:12.895: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 11 13:35:12.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Creating a v1 custom resource @ 05/11/23 13:35:15.46
  STEP: Create a v2 custom resource @ 05/11/23 13:35:15.476
  STEP: List CRs in v1 @ 05/11/23 13:35:15.49
  STEP: List CRs in v2 @ 05/11/23 13:35:15.517
  May 11 13:35:15.523: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6477" for this suite. @ 05/11/23 13:35:16.095
• [6.812 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/11/23 13:35:16.1
  May 11 13:35:16.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename dns @ 05/11/23 13:35:16.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:16.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:16.129
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/11/23 13:35:16.131
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/11/23 13:35:16.131
  STEP: creating a pod to probe DNS @ 05/11/23 13:35:16.131
  STEP: submitting the pod to kubernetes @ 05/11/23 13:35:16.131
  STEP: retrieving the pod @ 05/11/23 13:35:18.15
  STEP: looking for the results for each expected name from probers @ 05/11/23 13:35:18.153
  May 11 13:35:18.165: INFO: DNS probes using dns-6991/dns-test-3f18a5f9-3913-4edb-bb25-66f2f2804151 succeeded

  May 11 13:35:18.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 13:35:18.169
  STEP: Destroying namespace "dns-6991" for this suite. @ 05/11/23 13:35:18.183
• [2.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/11/23 13:35:18.193
  May 11 13:35:18.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename daemonsets @ 05/11/23 13:35:18.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:18.209
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:18.211
  STEP: Creating simple DaemonSet "daemon-set" @ 05/11/23 13:35:18.234
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/11/23 13:35:18.246
  May 11 13:35:18.250: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:18.250: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:18.250: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:18.252: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 13:35:18.252: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 13:35:19.258: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:19.258: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:19.258: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:19.262: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 13:35:19.262: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 13:35:20.260: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:20.260: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:20.260: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:35:20.263: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 13:35:20.263: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/11/23 13:35:20.266
  STEP: DeleteCollection of the DaemonSets @ 05/11/23 13:35:20.27
  STEP: Verify that ReplicaSets have been deleted @ 05/11/23 13:35:20.281
  May 11 13:35:20.288: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"10282"},"items":null}

  May 11 13:35:20.291: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"10282"},"items":[{"metadata":{"name":"daemon-set-296p4","generateName":"daemon-set-","namespace":"daemonsets-4656","uid":"f1456d0c-4781-43f5-9169-3c9c0fcf5931","resourceVersion":"10274","creationTimestamp":"2023-05-11T13:35:18Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"d18388f334140d09e73bcb962412cb054cb5b4938f4217e2166becf062e7789d","cni.projectcalico.org/podIP":"192.168.153.52/32","cni.projectcalico.org/podIPs":"192.168.153.52/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"24b38572-dff0-4c89-91d3-8c4252b76ac5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24b38572-dff0-4c89-91d3-8c4252b76ac5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-5kcm9","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-5kcm9","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"macpro-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["macpro-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:18Z"}],"hostIP":"10.221.188.13","podIP":"192.168.153.52","podIPs":[{"ip":"192.168.153.52"}],"startTime":"2023-05-11T13:35:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-11T13:35:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://0a57e6b45720d2e221bf5dc0b38a5d39ec6517f7ade2e1cfb60f64837800e763","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-8b9dm","generateName":"daemon-set-","namespace":"daemonsets-4656","uid":"c7643be1-0967-40a6-b393-b84b07adba04","resourceVersion":"10276","creationTimestamp":"2023-05-11T13:35:18Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"3156e25c655c9c38ade82390a9e6232754fcabd8cefee6a0468987da5a5abe22","cni.projectcalico.org/podIP":"192.168.150.132/32","cni.projectcalico.org/podIPs":"192.168.150.132/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"24b38572-dff0-4c89-91d3-8c4252b76ac5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24b38572-dff0-4c89-91d3-8c4252b76ac5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.132\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-l2rb6","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-l2rb6","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"macpro-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["macpro-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:18Z"}],"hostIP":"10.221.188.12","podIP":"192.168.150.132","podIPs":[{"ip":"192.168.150.132"}],"startTime":"2023-05-11T13:35:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-11T13:35:19Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://88dfed4210a95e511950624c5cfd27dae55c7575ae32d2c307a745c931d5112d","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-gcxhj","generateName":"daemon-set-","namespace":"daemonsets-4656","uid":"5035087d-20e1-49cc-b2b3-feb1592c4eaa","resourceVersion":"10278","creationTimestamp":"2023-05-11T13:35:18Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"921fb3d16f7c3e4bac720ed2eaa83d6e2f2dfdcefabab01b4109bcada1415fb9","cni.projectcalico.org/podIP":"192.168.151.229/32","cni.projectcalico.org/podIPs":"192.168.151.229/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"24b38572-dff0-4c89-91d3-8c4252b76ac5","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:18Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"24b38572-dff0-4c89-91d3-8c4252b76ac5\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-11T13:35:19Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.151.229\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-d5wh5","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-d5wh5","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"macpro-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["macpro-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:18Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:19Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:19Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-11T13:35:18Z"}],"hostIP":"10.221.188.11","podIP":"192.168.151.229","podIPs":[{"ip":"192.168.151.229"}],"startTime":"2023-05-11T13:35:18Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-11T13:35:18Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://01c2ba3f3cc7d9f6ab1632fb75396abf0df4fe08461e4fe7e92784f8f701c8a0","started":true}],"qosClass":"BestEffort"}}]}

  May 11 13:35:20.311: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4656" for this suite. @ 05/11/23 13:35:20.317
• [2.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/11/23 13:35:20.324
  May 11 13:35:20.324: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename server-version @ 05/11/23 13:35:20.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:20.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:20.352
  STEP: Request ServerVersion @ 05/11/23 13:35:20.354
  STEP: Confirm major version @ 05/11/23 13:35:20.355
  May 11 13:35:20.355: INFO: Major version: 1
  STEP: Confirm minor version @ 05/11/23 13:35:20.355
  May 11 13:35:20.355: INFO: cleanMinorVersion: 27
  May 11 13:35:20.355: INFO: Minor version: 27
  May 11 13:35:20.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-6167" for this suite. @ 05/11/23 13:35:20.359
• [0.041 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/11/23 13:35:20.365
  May 11 13:35:20.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubelet-test @ 05/11/23 13:35:20.366
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:20.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:20.396
  May 11 13:35:24.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6441" for this suite. @ 05/11/23 13:35:24.42
• [4.061 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/11/23 13:35:24.426
  May 11 13:35:24.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 13:35:24.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:24.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:24.448
  STEP: creating service multi-endpoint-test in namespace services-148 @ 05/11/23 13:35:24.451
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-148 to expose endpoints map[] @ 05/11/23 13:35:24.462
  May 11 13:35:24.467: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  May 11 13:35:25.475: INFO: successfully validated that service multi-endpoint-test in namespace services-148 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-148 @ 05/11/23 13:35:25.475
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-148 to expose endpoints map[pod1:[100]] @ 05/11/23 13:35:27.491
  May 11 13:35:27.498: INFO: successfully validated that service multi-endpoint-test in namespace services-148 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-148 @ 05/11/23 13:35:27.499
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-148 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/11/23 13:35:29.513
  May 11 13:35:29.524: INFO: successfully validated that service multi-endpoint-test in namespace services-148 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/11/23 13:35:29.524
  May 11 13:35:29.524: INFO: Creating new exec pod
  May 11 13:35:32.542: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-148 exec execpodf9v8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May 11 13:35:32.669: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May 11 13:35:32.669: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 13:35:32.669: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-148 exec execpodf9v8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.61.131 80'
  May 11 13:35:32.777: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.61.131 80\nConnection to 10.111.61.131 80 port [tcp/http] succeeded!\n"
  May 11 13:35:32.777: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 13:35:32.777: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-148 exec execpodf9v8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May 11 13:35:32.883: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May 11 13:35:32.883: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 13:35:32.883: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-148 exec execpodf9v8b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.61.131 81'
  May 11 13:35:32.997: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.61.131 81\nConnection to 10.111.61.131 81 port [tcp/*] succeeded!\n"
  May 11 13:35:32.997: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-148 @ 05/11/23 13:35:32.997
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-148 to expose endpoints map[pod2:[101]] @ 05/11/23 13:35:33.019
  May 11 13:35:33.033: INFO: successfully validated that service multi-endpoint-test in namespace services-148 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-148 @ 05/11/23 13:35:33.033
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-148 to expose endpoints map[] @ 05/11/23 13:35:33.05
  May 11 13:35:34.080: INFO: successfully validated that service multi-endpoint-test in namespace services-148 exposes endpoints map[]
  May 11 13:35:34.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-148" for this suite. @ 05/11/23 13:35:34.109
• [9.691 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/11/23 13:35:34.118
  May 11 13:35:34.118: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 13:35:34.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:34.136
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:34.138
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:35:34.14
  STEP: Saw pod success @ 05/11/23 13:35:38.159
  May 11 13:35:38.162: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-ddf0c0d2-0714-4f2e-9f22-1d6d1df6d279 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:35:38.167
  May 11 13:35:38.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9532" for this suite. @ 05/11/23 13:35:38.184
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/11/23 13:35:38.191
  May 11 13:35:38.191: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename svcaccounts @ 05/11/23 13:35:38.192
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:38.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:38.215
  May 11 13:35:38.221: INFO: Got root ca configmap in namespace "svcaccounts-2371"
  May 11 13:35:38.226: INFO: Deleted root ca configmap in namespace "svcaccounts-2371"
  STEP: waiting for a new root ca configmap created @ 05/11/23 13:35:38.727
  May 11 13:35:38.730: INFO: Recreated root ca configmap in namespace "svcaccounts-2371"
  May 11 13:35:38.735: INFO: Updated root ca configmap in namespace "svcaccounts-2371"
  STEP: waiting for the root ca configmap reconciled @ 05/11/23 13:35:39.236
  May 11 13:35:39.243: INFO: Reconciled root ca configmap in namespace "svcaccounts-2371"
  May 11 13:35:39.243: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2371" for this suite. @ 05/11/23 13:35:39.247
• [1.062 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/11/23 13:35:39.254
  May 11 13:35:39.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 13:35:39.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:39.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:39.276
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:35:39.279
  STEP: Saw pod success @ 05/11/23 13:35:43.304
  May 11 13:35:43.307: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-5a9afa18-55dd-42b0-bbd1-cc147ee8c2fc container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:35:43.314
  May 11 13:35:43.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8521" for this suite. @ 05/11/23 13:35:43.331
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/11/23 13:35:43.338
  May 11 13:35:43.338: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:35:43.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:35:43.357
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:35:43.359
  STEP: Creating configMap with name cm-test-opt-del-67f6067d-d475-4da5-b066-97df5f70d225 @ 05/11/23 13:35:43.366
  STEP: Creating configMap with name cm-test-opt-upd-b3f27489-e896-4d42-a5a7-1465240bdd8e @ 05/11/23 13:35:43.371
  STEP: Creating the pod @ 05/11/23 13:35:43.376
  STEP: Deleting configmap cm-test-opt-del-67f6067d-d475-4da5-b066-97df5f70d225 @ 05/11/23 13:35:45.416
  STEP: Updating configmap cm-test-opt-upd-b3f27489-e896-4d42-a5a7-1465240bdd8e @ 05/11/23 13:35:45.421
  STEP: Creating configMap with name cm-test-opt-create-2cae1495-1141-458b-92e2-3e56080e4781 @ 05/11/23 13:35:45.425
  STEP: waiting to observe update in volume @ 05/11/23 13:35:45.428
  May 11 13:36:55.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8782" for this suite. @ 05/11/23 13:36:55.754
• [72.421 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/11/23 13:36:55.76
  May 11 13:36:55.760: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 13:36:55.761
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:36:55.777
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:36:55.78
  STEP: Starting the proxy @ 05/11/23 13:36:55.782
  May 11 13:36:55.783: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-2486 proxy --unix-socket=/tmp/kubectl-proxy-unix1327662790/test'
  STEP: retrieving proxy /api/ output @ 05/11/23 13:36:55.827
  May 11 13:36:55.827: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2486" for this suite. @ 05/11/23 13:36:55.832
• [0.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/11/23 13:36:55.838
  May 11 13:36:55.838: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 13:36:55.839
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:36:55.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:36:55.855
  STEP: Creating pod busybox-6d686841-728a-4b26-a08e-87cdbde2d597 in namespace container-probe-4080 @ 05/11/23 13:36:55.857
  May 11 13:36:57.870: INFO: Started pod busybox-6d686841-728a-4b26-a08e-87cdbde2d597 in namespace container-probe-4080
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/11/23 13:36:57.871
  May 11 13:36:57.873: INFO: Initial restart count of pod busybox-6d686841-728a-4b26-a08e-87cdbde2d597 is 0
  May 11 13:37:47.995: INFO: Restart count of pod container-probe-4080/busybox-6d686841-728a-4b26-a08e-87cdbde2d597 is now 1 (50.121582333s elapsed)
  May 11 13:37:47.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 13:37:48
  STEP: Destroying namespace "container-probe-4080" for this suite. @ 05/11/23 13:37:48.01
• [52.178 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/11/23 13:37:48.016
  May 11 13:37:48.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename runtimeclass @ 05/11/23 13:37:48.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:37:48.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:37:48.037
  STEP: getting /apis @ 05/11/23 13:37:48.04
  STEP: getting /apis/node.k8s.io @ 05/11/23 13:37:48.043
  STEP: getting /apis/node.k8s.io/v1 @ 05/11/23 13:37:48.044
  STEP: creating @ 05/11/23 13:37:48.045
  STEP: watching @ 05/11/23 13:37:48.058
  May 11 13:37:48.058: INFO: starting watch
  STEP: getting @ 05/11/23 13:37:48.063
  STEP: listing @ 05/11/23 13:37:48.065
  STEP: patching @ 05/11/23 13:37:48.068
  STEP: updating @ 05/11/23 13:37:48.073
  May 11 13:37:48.077: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/11/23 13:37:48.077
  STEP: deleting a collection @ 05/11/23 13:37:48.085
  May 11 13:37:48.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5323" for this suite. @ 05/11/23 13:37:48.101
• [0.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/11/23 13:37:48.11
  May 11 13:37:48.110: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 13:37:48.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:37:48.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:37:48.128
  STEP: creating a Pod with a static label @ 05/11/23 13:37:48.135
  STEP: watching for Pod to be ready @ 05/11/23 13:37:48.141
  May 11 13:37:48.143: INFO: observed Pod pod-test in namespace pods-9888 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May 11 13:37:48.145: INFO: observed Pod pod-test in namespace pods-9888 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC  }]
  May 11 13:37:48.158: INFO: observed Pod pod-test in namespace pods-9888 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC  }]
  May 11 13:37:48.716: INFO: observed Pod pod-test in namespace pods-9888 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC  }]
  May 11 13:37:49.447: INFO: Found Pod pod-test in namespace pods-9888 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:49 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:49 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-11 13:37:48 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/11/23 13:37:49.451
  STEP: getting the Pod and ensuring that it's patched @ 05/11/23 13:37:49.461
  STEP: replacing the Pod's status Ready condition to False @ 05/11/23 13:37:49.464
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/11/23 13:37:49.477
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/11/23 13:37:49.477
  STEP: watching for the Pod to be deleted @ 05/11/23 13:37:49.487
  May 11 13:37:49.489: INFO: observed event type MODIFIED
  May 11 13:37:51.446: INFO: observed event type MODIFIED
  May 11 13:37:51.593: INFO: observed event type MODIFIED
  May 11 13:37:51.729: INFO: observed event type MODIFIED
  May 11 13:37:52.450: INFO: observed event type MODIFIED
  May 11 13:37:52.460: INFO: observed event type MODIFIED
  May 11 13:37:52.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9888" for this suite. @ 05/11/23 13:37:52.474
• [4.370 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/11/23 13:37:52.481
  May 11 13:37:52.481: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename security-context-test @ 05/11/23 13:37:52.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:37:52.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:37:52.5
  May 11 13:37:56.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8770" for this suite. @ 05/11/23 13:37:56.534
• [4.060 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/11/23 13:37:56.541
  May 11 13:37:56.541: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pod-network-test @ 05/11/23 13:37:56.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:37:56.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:37:56.559
  STEP: Performing setup for networking test in namespace pod-network-test-1597 @ 05/11/23 13:37:56.562
  STEP: creating a selector @ 05/11/23 13:37:56.562
  STEP: Creating the service pods in kubernetes @ 05/11/23 13:37:56.562
  May 11 13:37:56.562: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/11/23 13:38:18.664
  May 11 13:38:20.682: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 11 13:38:20.682: INFO: Breadth first check of 192.168.151.230 on host 10.221.188.11...
  May 11 13:38:20.686: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.152:9080/dial?request=hostname&protocol=udp&host=192.168.151.230&port=8081&tries=1'] Namespace:pod-network-test-1597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 13:38:20.686: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:38:20.686: INFO: ExecWithOptions: Clientset creation
  May 11 13:38:20.686: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.151.230%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 11 13:38:20.752: INFO: Waiting for responses: map[]
  May 11 13:38:20.752: INFO: reached 192.168.151.230 after 0/1 tries
  May 11 13:38:20.752: INFO: Breadth first check of 192.168.150.160 on host 10.221.188.12...
  May 11 13:38:20.757: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.152:9080/dial?request=hostname&protocol=udp&host=192.168.150.160&port=8081&tries=1'] Namespace:pod-network-test-1597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 13:38:20.757: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:38:20.757: INFO: ExecWithOptions: Clientset creation
  May 11 13:38:20.757: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.150.160%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 11 13:38:20.810: INFO: Waiting for responses: map[]
  May 11 13:38:20.810: INFO: reached 192.168.150.160 after 0/1 tries
  May 11 13:38:20.810: INFO: Breadth first check of 192.168.153.55 on host 10.221.188.13...
  May 11 13:38:20.814: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.152:9080/dial?request=hostname&protocol=udp&host=192.168.153.55&port=8081&tries=1'] Namespace:pod-network-test-1597 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 13:38:20.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:38:20.814: INFO: ExecWithOptions: Clientset creation
  May 11 13:38:20.814: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1597/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.152%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D192.168.153.55%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 11 13:38:20.870: INFO: Waiting for responses: map[]
  May 11 13:38:20.870: INFO: reached 192.168.153.55 after 0/1 tries
  May 11 13:38:20.870: INFO: Going to retry 0 out of 3 pods....
  May 11 13:38:20.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1597" for this suite. @ 05/11/23 13:38:20.874
• [24.342 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/11/23 13:38:20.883
  May 11 13:38:20.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 13:38:20.884
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:38:20.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:38:20.905
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:38:20.907
  STEP: Saw pod success @ 05/11/23 13:38:24.93
  May 11 13:38:24.932: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-79a10124-a6dd-4639-bb32-65720f74f8c8 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:38:24.937
  May 11 13:38:24.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4708" for this suite. @ 05/11/23 13:38:24.961
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/11/23 13:38:24.968
  May 11 13:38:24.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 13:38:24.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:38:24.984
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:38:24.987
  STEP: creating the pod @ 05/11/23 13:38:24.989
  STEP: submitting the pod to kubernetes @ 05/11/23 13:38:24.989
  STEP: verifying the pod is in kubernetes @ 05/11/23 13:38:27.006
  STEP: updating the pod @ 05/11/23 13:38:27.008
  May 11 13:38:27.521: INFO: Successfully updated pod "pod-update-b3753dc0-eb41-4171-bb89-65c5344c21e2"
  STEP: verifying the updated pod is in kubernetes @ 05/11/23 13:38:27.523
  May 11 13:38:27.527: INFO: Pod update OK
  May 11 13:38:27.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-161" for this suite. @ 05/11/23 13:38:27.531
• [2.568 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/11/23 13:38:27.538
  May 11 13:38:27.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 13:38:27.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:38:27.555
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:38:27.557
  STEP: Counting existing ResourceQuota @ 05/11/23 13:38:27.561
  STEP: Creating a ResourceQuota @ 05/11/23 13:38:32.565
  STEP: Ensuring resource quota status is calculated @ 05/11/23 13:38:32.571
  STEP: Creating a Pod that fits quota @ 05/11/23 13:38:34.574
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/11/23 13:38:34.587
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/11/23 13:38:36.591
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/11/23 13:38:36.594
  STEP: Ensuring a pod cannot update its resource requirements @ 05/11/23 13:38:36.596
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/11/23 13:38:36.599
  STEP: Deleting the pod @ 05/11/23 13:38:38.603
  STEP: Ensuring resource quota status released the pod usage @ 05/11/23 13:38:38.611
  May 11 13:38:40.616: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2851" for this suite. @ 05/11/23 13:38:40.621
• [13.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/11/23 13:38:40.629
  May 11 13:38:40.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename gc @ 05/11/23 13:38:40.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:38:40.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:38:40.647
  STEP: create the deployment @ 05/11/23 13:38:40.649
  W0511 13:38:40.654952      24 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/11/23 13:38:40.655
  STEP: delete the deployment @ 05/11/23 13:38:41.16
  STEP: wait for all rs to be garbage collected @ 05/11/23 13:38:41.165
  STEP: expected 0 rs, got 1 rs @ 05/11/23 13:38:41.17
  STEP: expected 0 pods, got 2 pods @ 05/11/23 13:38:41.173
  STEP: Gathering metrics @ 05/11/23 13:38:41.68
  May 11 13:38:41.779: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 11 13:38:41.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1164" for this suite. @ 05/11/23 13:38:41.785
• [1.162 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/11/23 13:38:41.791
  May 11 13:38:41.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename dns @ 05/11/23 13:38:41.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:38:41.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:38:41.811
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/11/23 13:38:41.815
  May 11 13:38:41.821: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3765  c444a7fc-c631-40e0-800e-8cbbaf229dee 11428 0 2023-05-11 13:38:41 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-11 13:38:41 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cxpj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cxpj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/11/23 13:38:43.834
  May 11 13:38:43.834: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3765 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 13:38:43.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:38:43.834: INFO: ExecWithOptions: Clientset creation
  May 11 13:38:43.835: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3765/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/11/23 13:38:43.898
  May 11 13:38:43.898: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3765 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 13:38:43.898: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:38:43.898: INFO: ExecWithOptions: Clientset creation
  May 11 13:38:43.898: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-3765/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 11 13:38:43.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 13:38:43.961: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-3765" for this suite. @ 05/11/23 13:38:43.971
• [2.189 seconds]
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/11/23 13:38:43.98
  May 11 13:38:43.980: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:38:43.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:38:44.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:38:44.007
  STEP: Creating projection with secret that has name projected-secret-test-map-66939174-fcea-4de1-b1cc-fd8d1f5e8576 @ 05/11/23 13:38:44.009
  STEP: Creating a pod to test consume secrets @ 05/11/23 13:38:44.018
  STEP: Saw pod success @ 05/11/23 13:38:48.041
  May 11 13:38:48.043: INFO: Trying to get logs from node macpro-2 pod pod-projected-secrets-dd745053-9f7e-4f96-afff-d4697a53f911 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 13:38:48.048
  May 11 13:38:48.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-722" for this suite. @ 05/11/23 13:38:48.068
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/11/23 13:38:48.075
  May 11 13:38:48.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:38:48.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:38:48.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:38:48.097
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:38:48.1
  STEP: Saw pod success @ 05/11/23 13:38:52.118
  May 11 13:38:52.121: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-694d028c-6765-42f9-a143-5ed62ad9d1d6 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:38:52.126
  May 11 13:38:52.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6818" for this suite. @ 05/11/23 13:38:52.141
• [4.071 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/11/23 13:38:52.146
  May 11 13:38:52.146: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename deployment @ 05/11/23 13:38:52.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:38:52.162
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:38:52.165
  May 11 13:38:52.178: INFO: Pod name rollover-pod: Found 0 pods out of 1
  May 11 13:38:57.182: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/11/23 13:38:57.182
  May 11 13:38:57.182: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  May 11 13:38:59.186: INFO: Creating deployment "test-rollover-deployment"
  May 11 13:38:59.194: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  May 11 13:39:01.201: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May 11 13:39:01.206: INFO: Ensure that both replica sets have 1 created replica
  May 11 13:39:01.210: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May 11 13:39:01.216: INFO: Updating deployment test-rollover-deployment
  May 11 13:39:01.216: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  May 11 13:39:03.227: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May 11 13:39:03.233: INFO: Make sure deployment "test-rollover-deployment" is complete
  May 11 13:39:03.238: INFO: all replica sets need to contain the pod-template-hash label
  May 11 13:39:03.239: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 11 13:39:05.245: INFO: all replica sets need to contain the pod-template-hash label
  May 11 13:39:05.245: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 11 13:39:07.246: INFO: all replica sets need to contain the pod-template-hash label
  May 11 13:39:07.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 11 13:39:09.246: INFO: all replica sets need to contain the pod-template-hash label
  May 11 13:39:09.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 11 13:39:11.246: INFO: all replica sets need to contain the pod-template-hash label
  May 11 13:39:11.246: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 13, 39, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 13, 38, 59, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 11 13:39:13.246: INFO: 
  May 11 13:39:13.246: INFO: Ensure that both old replica sets have no replicas
  May 11 13:39:13.255: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-1890  3b30f8a0-68cb-4cd3-9e93-bc4a339cad09 11704 2 2023-05-11 13:38:59 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-11 13:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005050718 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-11 13:38:59 +0000 UTC,LastTransitionTime:2023-05-11 13:38:59 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-11 13:39:12 +0000 UTC,LastTransitionTime:2023-05-11 13:38:59 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 11 13:39:13.258: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-1890  7d860c52-8f40-4b6c-a741-96563a530a69 11694 2 2023-05-11 13:39:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 3b30f8a0-68cb-4cd3-9e93-bc4a339cad09 0xc005050be7 0xc005050be8}] [] [{kube-controller-manager Update apps/v1 2023-05-11 13:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b30f8a0-68cb-4cd3-9e93-bc4a339cad09\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:39:12 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005050c98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 11 13:39:13.258: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May 11 13:39:13.258: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-1890  d11b53bf-4811-4869-84e2-67f8ac32027a 11703 2 2023-05-11 13:38:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 3b30f8a0-68cb-4cd3-9e93-bc4a339cad09 0xc005050ab7 0xc005050ab8}] [] [{e2e.test Update apps/v1 2023-05-11 13:38:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:39:12 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b30f8a0-68cb-4cd3-9e93-bc4a339cad09\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:39:12 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005050b78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 11 13:39:13.259: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-1890  15cd871f-fbdb-47a0-84ef-8df6efaa65c8 11649 2 2023-05-11 13:38:59 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 3b30f8a0-68cb-4cd3-9e93-bc4a339cad09 0xc005050d07 0xc005050d08}] [] [{kube-controller-manager Update apps/v1 2023-05-11 13:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b30f8a0-68cb-4cd3-9e93-bc4a339cad09\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 13:39:01 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005050db8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 11 13:39:13.263: INFO: Pod "test-rollover-deployment-57777854c9-8s2p8" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-8s2p8 test-rollover-deployment-57777854c9- deployment-1890  9d600c19-8cab-4f15-81d5-66158cce4f9e 11666 0 2023-05-11 13:39:01 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:71cb3b3932692c26afd16a78c7dd942ca5f9ceb4dbaa26ec31c5ce797f8e805e cni.projectcalico.org/podIP:192.168.150.161/32 cni.projectcalico.org/podIPs:192.168.150.161/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 7d860c52-8f40-4b6c-a741-96563a530a69 0xc005051317 0xc005051318}] [] [{calico Update v1 2023-05-11 13:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 13:39:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7d860c52-8f40-4b6c-a741-96563a530a69\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 13:39:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.161\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jnjcn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jnjcn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 13:39:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 13:39:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 13:39:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 13:39:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.161,StartTime:2023-05-11 13:39:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 13:39:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://1c1a2772c7ce46f8b3a9c02150d4d1f9a757d4fee362263996c713f4ceb061f6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.161,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 13:39:13.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1890" for this suite. @ 05/11/23 13:39:13.268
• [21.127 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/11/23 13:39:13.273
  May 11 13:39:13.273: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:39:13.274
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:13.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:13.294
  STEP: Creating configMap with name projected-configmap-test-volume-map-b6275d51-389e-4cf7-a297-cff79b79bc98 @ 05/11/23 13:39:13.296
  STEP: Creating a pod to test consume configMaps @ 05/11/23 13:39:13.3
  STEP: Saw pod success @ 05/11/23 13:39:17.326
  May 11 13:39:17.328: INFO: Trying to get logs from node macpro-2 pod pod-projected-configmaps-3446103f-5799-490b-a77c-d8bd072deecc container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 13:39:17.333
  May 11 13:39:17.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8731" for this suite. @ 05/11/23 13:39:17.347
• [4.079 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/11/23 13:39:17.352
  May 11 13:39:17.352: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 13:39:17.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:17.381
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:17.383
  STEP: Setting up server cert @ 05/11/23 13:39:17.407
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 13:39:18.072
  STEP: Deploying the webhook pod @ 05/11/23 13:39:18.078
  STEP: Wait for the deployment to be ready @ 05/11/23 13:39:18.107
  May 11 13:39:18.111: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 13:39:20.12
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 13:39:20.141
  May 11 13:39:21.141: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/11/23 13:39:21.21
  STEP: Creating a configMap that should be mutated @ 05/11/23 13:39:21.226
  STEP: Deleting the collection of validation webhooks @ 05/11/23 13:39:21.293
  STEP: Creating a configMap that should not be mutated @ 05/11/23 13:39:21.436
  May 11 13:39:21.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8996" for this suite. @ 05/11/23 13:39:21.634
  STEP: Destroying namespace "webhook-markers-2686" for this suite. @ 05/11/23 13:39:21.647
• [4.305 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/11/23 13:39:21.658
  May 11 13:39:21.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/11/23 13:39:21.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:21.681
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:21.684
  STEP: create the container to handle the HTTPGet hook request. @ 05/11/23 13:39:21.691
  STEP: create the pod with lifecycle hook @ 05/11/23 13:39:23.729
  STEP: delete the pod with lifecycle hook @ 05/11/23 13:39:25.77
  STEP: check prestop hook @ 05/11/23 13:39:27.788
  May 11 13:39:27.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1654" for this suite. @ 05/11/23 13:39:27.797
• [6.145 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/11/23 13:39:27.804
  May 11 13:39:27.804: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replicaset @ 05/11/23 13:39:27.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:27.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:27.824
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/11/23 13:39:27.826
  May 11 13:39:27.832: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 11 13:39:32.836: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/11/23 13:39:32.836
  STEP: getting scale subresource @ 05/11/23 13:39:32.836
  STEP: updating a scale subresource @ 05/11/23 13:39:32.841
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/11/23 13:39:32.846
  STEP: Patch a scale subresource @ 05/11/23 13:39:32.851
  May 11 13:39:32.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8274" for this suite. @ 05/11/23 13:39:32.871
• [5.073 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/11/23 13:39:32.877
  May 11 13:39:32.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename gc @ 05/11/23 13:39:32.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:32.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:32.904
  STEP: create the rc @ 05/11/23 13:39:32.906
  W0511 13:39:32.916562      24 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/11/23 13:39:37.921
  STEP: wait for all pods to be garbage collected @ 05/11/23 13:39:37.927
  STEP: Gathering metrics @ 05/11/23 13:39:42.932
  May 11 13:39:43.013: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 11 13:39:43.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6085" for this suite. @ 05/11/23 13:39:43.018
• [10.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/11/23 13:39:43.026
  May 11 13:39:43.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:39:43.026
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:43.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:43.045
  STEP: Creating configMap with name projected-configmap-test-volume-map-d5c87421-1c58-4800-bb25-6de36289bf63 @ 05/11/23 13:39:43.047
  STEP: Creating a pod to test consume configMaps @ 05/11/23 13:39:43.051
  STEP: Saw pod success @ 05/11/23 13:39:47.073
  May 11 13:39:47.075: INFO: Trying to get logs from node macpro-2 pod pod-projected-configmaps-9617dbee-c06c-480f-8537-2d250d9970e4 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 13:39:47.083
  May 11 13:39:47.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9683" for this suite. @ 05/11/23 13:39:47.102
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/11/23 13:39:47.108
  May 11 13:39:47.108: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 13:39:47.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:47.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:47.129
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:39:47.132
  STEP: Saw pod success @ 05/11/23 13:39:51.156
  May 11 13:39:51.159: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-f34190ed-214d-43fd-9a9d-8c199954c067 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:39:51.164
  May 11 13:39:51.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1459" for this suite. @ 05/11/23 13:39:51.183
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/11/23 13:39:51.189
  May 11 13:39:51.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replication-controller @ 05/11/23 13:39:51.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:51.21
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:51.212
  STEP: Creating ReplicationController "e2e-rc-qss75" @ 05/11/23 13:39:51.215
  May 11 13:39:51.223: INFO: Get Replication Controller "e2e-rc-qss75" to confirm replicas
  May 11 13:39:52.227: INFO: Get Replication Controller "e2e-rc-qss75" to confirm replicas
  May 11 13:39:52.230: INFO: Found 1 replicas for "e2e-rc-qss75" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-qss75" @ 05/11/23 13:39:52.23
  STEP: Updating a scale subresource @ 05/11/23 13:39:52.233
  STEP: Verifying replicas where modified for replication controller "e2e-rc-qss75" @ 05/11/23 13:39:52.239
  May 11 13:39:52.239: INFO: Get Replication Controller "e2e-rc-qss75" to confirm replicas
  May 11 13:39:53.242: INFO: Get Replication Controller "e2e-rc-qss75" to confirm replicas
  May 11 13:39:53.246: INFO: Found 2 replicas for "e2e-rc-qss75" replication controller
  May 11 13:39:53.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-4895" for this suite. @ 05/11/23 13:39:53.251
• [2.066 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/11/23 13:39:53.256
  May 11 13:39:53.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:39:53.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:53.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:53.281
  STEP: Creating secret with name projected-secret-test-b9b016d6-ceee-4ff7-bd6a-8b26b1a25a16 @ 05/11/23 13:39:53.283
  STEP: Creating a pod to test consume secrets @ 05/11/23 13:39:53.287
  STEP: Saw pod success @ 05/11/23 13:39:57.306
  May 11 13:39:57.310: INFO: Trying to get logs from node macpro-2 pod pod-projected-secrets-fa6a049d-9b30-442d-8760-4ab3ef176948 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 13:39:57.316
  May 11 13:39:57.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6831" for this suite. @ 05/11/23 13:39:57.334
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/11/23 13:39:57.34
  May 11 13:39:57.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 13:39:57.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:39:57.355
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:39:57.357
  STEP: creating the pod @ 05/11/23 13:39:57.359
  May 11 13:39:57.359: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8221 create -f -'
  May 11 13:39:57.963: INFO: stderr: ""
  May 11 13:39:57.963: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/11/23 13:39:59.971
  May 11 13:39:59.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8221 label pods pause testing-label=testing-label-value'
  May 11 13:40:00.040: INFO: stderr: ""
  May 11 13:40:00.040: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/11/23 13:40:00.04
  May 11 13:40:00.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8221 get pod pause -L testing-label'
  May 11 13:40:00.099: INFO: stderr: ""
  May 11 13:40:00.099: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/11/23 13:40:00.099
  May 11 13:40:00.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8221 label pods pause testing-label-'
  May 11 13:40:00.166: INFO: stderr: ""
  May 11 13:40:00.166: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/11/23 13:40:00.166
  May 11 13:40:00.166: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8221 get pod pause -L testing-label'
  May 11 13:40:00.225: INFO: stderr: ""
  May 11 13:40:00.225: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 05/11/23 13:40:00.225
  May 11 13:40:00.225: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8221 delete --grace-period=0 --force -f -'
  May 11 13:40:00.300: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 13:40:00.300: INFO: stdout: "pod \"pause\" force deleted\n"
  May 11 13:40:00.300: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8221 get rc,svc -l name=pause --no-headers'
  May 11 13:40:00.364: INFO: stderr: "No resources found in kubectl-8221 namespace.\n"
  May 11 13:40:00.364: INFO: stdout: ""
  May 11 13:40:00.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8221 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 11 13:40:00.428: INFO: stderr: ""
  May 11 13:40:00.428: INFO: stdout: ""
  May 11 13:40:00.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8221" for this suite. @ 05/11/23 13:40:00.433
• [3.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/11/23 13:40:00.441
  May 11 13:40:00.441: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 13:40:00.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:40:00.46
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:40:00.465
  May 11 13:40:00.470: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-1207 version'
  May 11 13:40:00.522: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May 11 13:40:00.522: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:14:42Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May 11 13:40:00.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1207" for this suite. @ 05/11/23 13:40:00.529
• [0.093 seconds]
------------------------------
SSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/11/23 13:40:00.535
  May 11 13:40:00.535: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename var-expansion @ 05/11/23 13:40:00.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:40:00.556
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:40:00.559
  STEP: creating the pod with failed condition @ 05/11/23 13:40:00.562
  STEP: updating the pod @ 05/11/23 13:42:00.571
  May 11 13:42:01.084: INFO: Successfully updated pod "var-expansion-1c6e3cb3-bd54-4eb6-821d-16965c679303"
  STEP: waiting for pod running @ 05/11/23 13:42:01.084
  STEP: deleting the pod gracefully @ 05/11/23 13:42:03.092
  May 11 13:42:03.092: INFO: Deleting pod "var-expansion-1c6e3cb3-bd54-4eb6-821d-16965c679303" in namespace "var-expansion-6121"
  May 11 13:42:03.098: INFO: Wait up to 5m0s for pod "var-expansion-1c6e3cb3-bd54-4eb6-821d-16965c679303" to be fully deleted
  May 11 13:42:35.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-6121" for this suite. @ 05/11/23 13:42:35.182
• [154.655 seconds]
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/11/23 13:42:35.19
  May 11 13:42:35.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename job @ 05/11/23 13:42:35.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:42:35.218
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:42:35.221
  STEP: Creating a job @ 05/11/23 13:42:35.223
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/11/23 13:42:35.232
  STEP: patching /status @ 05/11/23 13:42:37.236
  STEP: updating /status @ 05/11/23 13:42:37.244
  STEP: get /status @ 05/11/23 13:42:37.272
  May 11 13:42:37.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-987" for this suite. @ 05/11/23 13:42:37.285
• [2.101 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/11/23 13:42:37.291
  May 11 13:42:37.291: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 13:42:37.292
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:42:37.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:42:37.397
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/11/23 13:42:37.401
  STEP: Saw pod success @ 05/11/23 13:42:41.427
  May 11 13:42:41.429: INFO: Trying to get logs from node macpro-2 pod pod-4704992b-0334-4ebd-bd36-1ee1072a3f67 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 13:42:41.442
  May 11 13:42:41.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8800" for this suite. @ 05/11/23 13:42:41.46
• [4.173 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/11/23 13:42:41.464
  May 11 13:42:41.464: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 13:42:41.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:42:41.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:42:41.487
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/11/23 13:42:41.49
  STEP: Saw pod success @ 05/11/23 13:42:45.549
  May 11 13:42:45.558: INFO: Trying to get logs from node macpro-2 pod pod-b3834b28-8286-468b-8c5f-993b3d9d7cc8 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 13:42:45.598
  May 11 13:42:45.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-686" for this suite. @ 05/11/23 13:42:45.777
• [4.339 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/11/23 13:42:45.803
  May 11 13:42:45.803: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/11/23 13:42:45.804
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:42:45.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:42:45.858
  STEP: creating @ 05/11/23 13:42:45.865
  STEP: getting @ 05/11/23 13:42:45.967
  STEP: listing @ 05/11/23 13:42:45.991
  STEP: deleting @ 05/11/23 13:42:46.003
  May 11 13:42:46.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-6927" for this suite. @ 05/11/23 13:42:46.15
• [0.372 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/11/23 13:42:46.175
  May 11 13:42:46.175: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 13:42:46.176
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:42:46.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:42:46.263
  STEP: Creating pod test-grpc-cf5b5099-33d5-47ef-a7d6-03c46a54b1ae in namespace container-probe-8913 @ 05/11/23 13:42:46.272
  May 11 13:42:48.396: INFO: Started pod test-grpc-cf5b5099-33d5-47ef-a7d6-03c46a54b1ae in namespace container-probe-8913
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/11/23 13:42:48.396
  May 11 13:42:48.401: INFO: Initial restart count of pod test-grpc-cf5b5099-33d5-47ef-a7d6-03c46a54b1ae is 0
  May 11 13:46:48.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 13:46:48.937
  STEP: Destroying namespace "container-probe-8913" for this suite. @ 05/11/23 13:46:48.956
• [242.795 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/11/23 13:46:48.97
  May 11 13:46:48.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:46:48.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:46:49.002
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:46:49.005
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 13:46:49.008
  STEP: Saw pod success @ 05/11/23 13:46:53.03
  May 11 13:46:53.034: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-118d1988-836c-440b-9c15-bbd1405d8a0f container client-container: <nil>
  STEP: delete the pod @ 05/11/23 13:46:53.052
  May 11 13:46:53.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2559" for this suite. @ 05/11/23 13:46:53.082
• [4.146 seconds]
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/11/23 13:46:53.117
  May 11 13:46:53.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename init-container @ 05/11/23 13:46:53.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:46:53.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:46:53.15
  STEP: creating the pod @ 05/11/23 13:46:53.153
  May 11 13:46:53.153: INFO: PodSpec: initContainers in spec.initContainers
  May 11 13:46:57.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-7691" for this suite. @ 05/11/23 13:46:57.576
• [4.467 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/11/23 13:46:57.584
  May 11 13:46:57.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 13:46:57.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:46:57.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:46:57.605
  STEP: creating a replication controller @ 05/11/23 13:46:57.608
  May 11 13:46:57.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 create -f -'
  May 11 13:46:58.788: INFO: stderr: ""
  May 11 13:46:58.788: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/11/23 13:46:58.788
  May 11 13:46:58.788: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 11 13:46:58.855: INFO: stderr: ""
  May 11 13:46:58.855: INFO: stdout: "update-demo-nautilus-6jct7 update-demo-nautilus-ht99p "
  May 11 13:46:58.855: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get pods update-demo-nautilus-6jct7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:46:58.914: INFO: stderr: ""
  May 11 13:46:58.914: INFO: stdout: ""
  May 11 13:46:58.914: INFO: update-demo-nautilus-6jct7 is created but not running
  May 11 13:47:03.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 11 13:47:03.978: INFO: stderr: ""
  May 11 13:47:03.979: INFO: stdout: "update-demo-nautilus-6jct7 update-demo-nautilus-ht99p "
  May 11 13:47:03.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get pods update-demo-nautilus-6jct7 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:47:04.036: INFO: stderr: ""
  May 11 13:47:04.036: INFO: stdout: "true"
  May 11 13:47:04.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get pods update-demo-nautilus-6jct7 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 11 13:47:04.093: INFO: stderr: ""
  May 11 13:47:04.093: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 11 13:47:04.093: INFO: validating pod update-demo-nautilus-6jct7
  May 11 13:47:04.099: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 11 13:47:04.099: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 11 13:47:04.100: INFO: update-demo-nautilus-6jct7 is verified up and running
  May 11 13:47:04.100: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get pods update-demo-nautilus-ht99p -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 11 13:47:04.156: INFO: stderr: ""
  May 11 13:47:04.156: INFO: stdout: "true"
  May 11 13:47:04.156: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get pods update-demo-nautilus-ht99p -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 11 13:47:04.213: INFO: stderr: ""
  May 11 13:47:04.213: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 11 13:47:04.213: INFO: validating pod update-demo-nautilus-ht99p
  May 11 13:47:04.218: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 11 13:47:04.218: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 11 13:47:04.218: INFO: update-demo-nautilus-ht99p is verified up and running
  STEP: using delete to clean up resources @ 05/11/23 13:47:04.218
  May 11 13:47:04.218: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 delete --grace-period=0 --force -f -'
  May 11 13:47:04.286: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 13:47:04.286: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 11 13:47:04.286: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get rc,svc -l name=update-demo --no-headers'
  May 11 13:47:04.482: INFO: stderr: "No resources found in kubectl-8881 namespace.\n"
  May 11 13:47:04.482: INFO: stdout: ""
  May 11 13:47:04.482: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8881 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 11 13:47:04.560: INFO: stderr: ""
  May 11 13:47:04.560: INFO: stdout: ""
  May 11 13:47:04.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8881" for this suite. @ 05/11/23 13:47:04.571
• [7.015 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/11/23 13:47:04.6
  May 11 13:47:04.600: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 13:47:04.601
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:04.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:04.782
  STEP: Counting existing ResourceQuota @ 05/11/23 13:47:04.785
  STEP: Creating a ResourceQuota @ 05/11/23 13:47:09.788
  STEP: Ensuring resource quota status is calculated @ 05/11/23 13:47:09.793
  May 11 13:47:11.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4893" for this suite. @ 05/11/23 13:47:11.808
• [7.223 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/11/23 13:47:11.824
  May 11 13:47:11.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 13:47:11.824
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:11.849
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:11.852
  STEP: Setting up server cert @ 05/11/23 13:47:11.879
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 13:47:12.483
  STEP: Deploying the webhook pod @ 05/11/23 13:47:12.49
  STEP: Wait for the deployment to be ready @ 05/11/23 13:47:12.5
  May 11 13:47:12.508: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/11/23 13:47:14.518
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 13:47:14.536
  May 11 13:47:15.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/11/23 13:47:15.539
  STEP: create a pod that should be updated by the webhook @ 05/11/23 13:47:15.553
  May 11 13:47:15.569: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6271" for this suite. @ 05/11/23 13:47:15.655
  STEP: Destroying namespace "webhook-markers-8747" for this suite. @ 05/11/23 13:47:15.661
• [3.842 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/11/23 13:47:15.666
  May 11 13:47:15.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 13:47:15.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:15.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:15.698
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/11/23 13:47:15.701
  STEP: Saw pod success @ 05/11/23 13:47:19.722
  May 11 13:47:19.725: INFO: Trying to get logs from node macpro-2 pod pod-cb52e341-5ef2-46ff-91b8-20477d086a68 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 13:47:19.73
  May 11 13:47:19.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5205" for this suite. @ 05/11/23 13:47:19.747
• [4.089 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/11/23 13:47:19.755
  May 11 13:47:19.755: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename runtimeclass @ 05/11/23 13:47:19.756
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:19.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:19.774
  May 11 13:47:21.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-4497" for this suite. @ 05/11/23 13:47:21.818
• [2.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/11/23 13:47:21.828
  May 11 13:47:21.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename disruption @ 05/11/23 13:47:21.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:21.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:21.847
  STEP: creating the pdb @ 05/11/23 13:47:21.849
  STEP: Waiting for the pdb to be processed @ 05/11/23 13:47:21.853
  STEP: updating the pdb @ 05/11/23 13:47:23.862
  STEP: Waiting for the pdb to be processed @ 05/11/23 13:47:23.872
  STEP: patching the pdb @ 05/11/23 13:47:23.881
  STEP: Waiting for the pdb to be processed @ 05/11/23 13:47:23.89
  STEP: Waiting for the pdb to be deleted @ 05/11/23 13:47:25.902
  May 11 13:47:25.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-472" for this suite. @ 05/11/23 13:47:25.925
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/11/23 13:47:25.933
  May 11 13:47:25.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 13:47:25.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:25.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:25.961
  STEP: Creating configMap with name cm-test-opt-del-27b8a536-d262-473e-9b55-a226cc01cb95 @ 05/11/23 13:47:26.204
  STEP: Creating configMap with name cm-test-opt-upd-04763fe1-2cec-4bb4-99b7-88781b8e7da9 @ 05/11/23 13:47:26.23
  STEP: Creating the pod @ 05/11/23 13:47:26.246
  STEP: Deleting configmap cm-test-opt-del-27b8a536-d262-473e-9b55-a226cc01cb95 @ 05/11/23 13:47:28.388
  STEP: Updating configmap cm-test-opt-upd-04763fe1-2cec-4bb4-99b7-88781b8e7da9 @ 05/11/23 13:47:28.396
  STEP: Creating configMap with name cm-test-opt-create-f55d7956-d962-4379-8914-6b2632ef43d7 @ 05/11/23 13:47:28.404
  STEP: waiting to observe update in volume @ 05/11/23 13:47:28.409
  May 11 13:47:30.458: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8959" for this suite. @ 05/11/23 13:47:30.468
• [4.546 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/11/23 13:47:30.479
  May 11 13:47:30.479: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 13:47:30.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:30.499
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:30.502
  STEP: Creating a ResourceQuota with terminating scope @ 05/11/23 13:47:30.506
  STEP: Ensuring ResourceQuota status is calculated @ 05/11/23 13:47:30.511
  STEP: Creating a ResourceQuota with not terminating scope @ 05/11/23 13:47:32.515
  STEP: Ensuring ResourceQuota status is calculated @ 05/11/23 13:47:32.522
  STEP: Creating a long running pod @ 05/11/23 13:47:34.529
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/11/23 13:47:34.567
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/11/23 13:47:36.571
  STEP: Deleting the pod @ 05/11/23 13:47:38.574
  STEP: Ensuring resource quota status released the pod usage @ 05/11/23 13:47:38.587
  STEP: Creating a terminating pod @ 05/11/23 13:47:40.596
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/11/23 13:47:40.65
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/11/23 13:47:42.656
  STEP: Deleting the pod @ 05/11/23 13:47:44.659
  STEP: Ensuring resource quota status released the pod usage @ 05/11/23 13:47:44.676
  May 11 13:47:46.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2920" for this suite. @ 05/11/23 13:47:46.687
• [16.214 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/11/23 13:47:46.693
  May 11 13:47:46.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 13:47:46.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:46.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:46.714
  STEP: Creating secret with name secret-test-3966b53d-d644-440d-a5f6-f964552c69f5 @ 05/11/23 13:47:46.717
  STEP: Creating a pod to test consume secrets @ 05/11/23 13:47:46.739
  STEP: Saw pod success @ 05/11/23 13:47:50.766
  May 11 13:47:50.769: INFO: Trying to get logs from node macpro-2 pod pod-secrets-7a87318f-8a47-4cee-87b0-a90eaa1e41cc container secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 13:47:50.774
  May 11 13:47:50.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5405" for this suite. @ 05/11/23 13:47:50.79
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/11/23 13:47:50.796
  May 11 13:47:50.796: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename field-validation @ 05/11/23 13:47:50.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:50.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:50.823
  STEP: apply creating a deployment @ 05/11/23 13:47:50.825
  May 11 13:47:50.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5115" for this suite. @ 05/11/23 13:47:50.838
• [0.048 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/11/23 13:47:50.845
  May 11 13:47:50.845: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 13:47:50.845
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:50.86
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:50.863
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/11/23 13:47:50.908
  STEP: Saw pod success @ 05/11/23 13:47:54.927
  May 11 13:47:54.929: INFO: Trying to get logs from node macpro-2 pod pod-1b613109-2308-45dc-aa94-c378162c238f container test-container: <nil>
  STEP: delete the pod @ 05/11/23 13:47:54.936
  May 11 13:47:54.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7378" for this suite. @ 05/11/23 13:47:54.954
• [4.117 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/11/23 13:47:54.961
  May 11 13:47:54.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 13:47:54.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:47:54.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:47:54.98
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3849 @ 05/11/23 13:47:54.982
  STEP: changing the ExternalName service to type=ClusterIP @ 05/11/23 13:47:54.987
  STEP: creating replication controller externalname-service in namespace services-3849 @ 05/11/23 13:47:55.007
  I0511 13:47:55.017675      24 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3849, replica count: 2
  I0511 13:47:58.070280      24 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 13:47:58.070: INFO: Creating new exec pod
  May 11 13:48:01.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-3849 exec execpodlblld -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 11 13:48:01.216: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 11 13:48:01.216: INFO: stdout: "externalname-service-j5qvb"
  May 11 13:48:01.216: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-3849 exec execpodlblld -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.104.106.150 80'
  May 11 13:48:01.329: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.104.106.150 80\nConnection to 10.104.106.150 80 port [tcp/http] succeeded!\n"
  May 11 13:48:01.329: INFO: stdout: "externalname-service-j5qvb"
  May 11 13:48:01.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 13:48:01.334: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-3849" for this suite. @ 05/11/23 13:48:01.362
• [6.407 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/11/23 13:48:01.369
  May 11 13:48:01.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 13:48:01.369
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:48:01.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:48:01.4
  May 11 13:48:01.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/11/23 13:48:03.176
  May 11 13:48:03.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-5264 --namespace=crd-publish-openapi-5264 create -f -'
  May 11 13:48:04.124: INFO: stderr: ""
  May 11 13:48:04.124: INFO: stdout: "e2e-test-crd-publish-openapi-6154-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 11 13:48:04.124: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-5264 --namespace=crd-publish-openapi-5264 delete e2e-test-crd-publish-openapi-6154-crds test-cr'
  May 11 13:48:04.198: INFO: stderr: ""
  May 11 13:48:04.198: INFO: stdout: "e2e-test-crd-publish-openapi-6154-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May 11 13:48:04.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-5264 --namespace=crd-publish-openapi-5264 apply -f -'
  May 11 13:48:04.411: INFO: stderr: ""
  May 11 13:48:04.411: INFO: stdout: "e2e-test-crd-publish-openapi-6154-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 11 13:48:04.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-5264 --namespace=crd-publish-openapi-5264 delete e2e-test-crd-publish-openapi-6154-crds test-cr'
  May 11 13:48:04.479: INFO: stderr: ""
  May 11 13:48:04.479: INFO: stdout: "e2e-test-crd-publish-openapi-6154-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/11/23 13:48:04.479
  May 11 13:48:04.480: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-5264 explain e2e-test-crd-publish-openapi-6154-crds'
  May 11 13:48:05.327: INFO: stderr: ""
  May 11 13:48:05.327: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-6154-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May 11 13:48:06.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5264" for this suite. @ 05/11/23 13:48:06.968
• [5.608 seconds]
------------------------------
S
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/11/23 13:48:06.976
  May 11 13:48:06.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 13:48:06.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:48:07
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:48:07.002
  STEP: creating a ConfigMap @ 05/11/23 13:48:07.006
  STEP: fetching the ConfigMap @ 05/11/23 13:48:07.01
  STEP: patching the ConfigMap @ 05/11/23 13:48:07.018
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/11/23 13:48:07.029
  STEP: deleting the ConfigMap by collection with a label selector @ 05/11/23 13:48:07.038
  STEP: listing all ConfigMaps in test namespace @ 05/11/23 13:48:07.048
  May 11 13:48:07.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2705" for this suite. @ 05/11/23 13:48:07.058
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/11/23 13:48:07.066
  May 11 13:48:07.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 13:48:07.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:48:07.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:48:07.084
  STEP: creating the pod @ 05/11/23 13:48:07.086
  STEP: setting up watch @ 05/11/23 13:48:07.086
  STEP: submitting the pod to kubernetes @ 05/11/23 13:48:07.19
  STEP: verifying the pod is in kubernetes @ 05/11/23 13:48:07.203
  STEP: verifying pod creation was observed @ 05/11/23 13:48:07.21
  STEP: deleting the pod gracefully @ 05/11/23 13:48:09.226
  STEP: verifying pod deletion was observed @ 05/11/23 13:48:09.232
  May 11 13:48:11.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6433" for this suite. @ 05/11/23 13:48:11.552
• [4.495 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/11/23 13:48:11.56
  May 11 13:48:11.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-watch @ 05/11/23 13:48:11.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:48:11.611
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:48:11.619
  May 11 13:48:11.623: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Creating first CR  @ 05/11/23 13:48:14.195
  May 11 13:48:14.200: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-11T13:48:14Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-11T13:48:14Z]] name:name1 resourceVersion:14299 uid:5ba25dea-b92d-4ab3-9659-6070b46737a7] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 05/11/23 13:48:24.204
  May 11 13:48:24.210: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-11T13:48:24Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-11T13:48:24Z]] name:name2 resourceVersion:14331 uid:f662ea10-9426-4a3f-a4e0-080f6dd6a943] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 05/11/23 13:48:34.215
  May 11 13:48:34.221: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-11T13:48:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-11T13:48:34Z]] name:name1 resourceVersion:14354 uid:5ba25dea-b92d-4ab3-9659-6070b46737a7] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 05/11/23 13:48:44.222
  May 11 13:48:44.228: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-11T13:48:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-11T13:48:44Z]] name:name2 resourceVersion:14377 uid:f662ea10-9426-4a3f-a4e0-080f6dd6a943] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 05/11/23 13:48:54.228
  May 11 13:48:54.235: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-11T13:48:14Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-11T13:48:34Z]] name:name1 resourceVersion:14402 uid:5ba25dea-b92d-4ab3-9659-6070b46737a7] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 05/11/23 13:49:04.236
  May 11 13:49:04.242: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-11T13:48:24Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-11T13:48:44Z]] name:name2 resourceVersion:14428 uid:f662ea10-9426-4a3f-a4e0-080f6dd6a943] num:map[num1:9223372036854775807 num2:1000000]]}
  May 11 13:49:14.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-6692" for this suite. @ 05/11/23 13:49:14.76
• [63.205 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/11/23 13:49:14.766
  May 11 13:49:14.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replication-controller @ 05/11/23 13:49:14.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:49:14.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:49:14.783
  May 11 13:49:14.785: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/11/23 13:49:15.806
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/11/23 13:49:15.811
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/11/23 13:49:16.819
  May 11 13:49:16.826: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/11/23 13:49:16.826
  May 11 13:49:17.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-5430" for this suite. @ 05/11/23 13:49:17.84
• [3.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/11/23 13:49:17.848
  May 11 13:49:17.848: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 13:49:17.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:49:17.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:49:17.87
  May 11 13:50:17.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3281" for this suite. @ 05/11/23 13:50:17.89
• [60.049 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/11/23 13:50:17.897
  May 11 13:50:17.897: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename dns @ 05/11/23 13:50:17.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:50:17.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:50:17.915
  STEP: Creating a test headless service @ 05/11/23 13:50:17.917
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7666.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-7666.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/11/23 13:50:17.923
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-7666.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-7666.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/11/23 13:50:17.923
  STEP: creating a pod to probe DNS @ 05/11/23 13:50:17.923
  STEP: submitting the pod to kubernetes @ 05/11/23 13:50:17.923
  STEP: retrieving the pod @ 05/11/23 13:50:19.952
  STEP: looking for the results for each expected name from probers @ 05/11/23 13:50:19.954
  May 11 13:50:19.965: INFO: DNS probes using dns-7666/dns-test-6cb41bff-6180-4a0b-bd87-2091621eea6e succeeded

  May 11 13:50:19.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 13:50:19.968
  STEP: deleting the test headless service @ 05/11/23 13:50:19.98
  STEP: Destroying namespace "dns-7666" for this suite. @ 05/11/23 13:50:20.001
• [2.111 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/11/23 13:50:20.008
  May 11 13:50:20.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 13:50:20.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:50:20.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:50:20.023
  May 11 13:50:20.026: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-2582 create -f -'
  May 11 13:50:20.523: INFO: stderr: ""
  May 11 13:50:20.523: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May 11 13:50:20.523: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-2582 create -f -'
  May 11 13:50:21.443: INFO: stderr: ""
  May 11 13:50:21.443: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/11/23 13:50:21.443
  May 11 13:50:22.447: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 13:50:22.447: INFO: Found 1 / 1
  May 11 13:50:22.447: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 11 13:50:22.450: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 13:50:22.450: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 11 13:50:22.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-2582 describe pod agnhost-primary-gx5kt'
  May 11 13:50:22.524: INFO: stderr: ""
  May 11 13:50:22.524: INFO: stdout: "Name:             agnhost-primary-gx5kt\nNamespace:        kubectl-2582\nPriority:         0\nService Account:  default\nNode:             macpro-3/10.221.188.13\nStart Time:       Thu, 11 May 2023 13:50:20 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 031df9db7dad3c2f69590acf72d47a78ac2495bed29a03af2d3dff48ee10c6da\n                  cni.projectcalico.org/podIP: 192.168.153.11/32\n                  cni.projectcalico.org/podIPs: 192.168.153.11/32\nStatus:           Running\nIP:               192.168.153.11\nIPs:\n  IP:           192.168.153.11\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://486fe53afe298e41ffdb6273c152b5abef3d7afaf279698f61647e061c9dae44\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Thu, 11 May 2023 13:50:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-rj9xc (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-rj9xc:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-2582/agnhost-primary-gx5kt to macpro-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  May 11 13:50:22.524: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-2582 describe rc agnhost-primary'
  May 11 13:50:22.592: INFO: stderr: ""
  May 11 13:50:22.592: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-2582\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-gx5kt\n"
  May 11 13:50:22.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-2582 describe service agnhost-primary'
  May 11 13:50:22.657: INFO: stderr: ""
  May 11 13:50:22.657: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-2582\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.103.83.145\nIPs:               10.103.83.145\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         192.168.153.11:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May 11 13:50:22.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-2582 describe node macpro-1'
  May 11 13:50:22.776: INFO: stderr: ""
  May 11 13:50:22.776: INFO: stdout: "Name:               macpro-1\nRoles:              <none>\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=macpro-1\n                    kubernetes.io/os=linux\n                    node-role.orka/worker-node=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 10.221.188.11/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.151.192\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Thu, 11 May 2023 13:11:34 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  macpro-1\n  AcquireTime:     <unset>\n  RenewTime:       Thu, 11 May 2023 13:50:12 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Thu, 11 May 2023 13:11:37 +0000   Thu, 11 May 2023 13:11:37 +0000   CalicoIsUp                   Calico is running on this node\n  MemoryPressure       False   Thu, 11 May 2023 13:49:32 +0000   Thu, 11 May 2023 13:11:34 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Thu, 11 May 2023 13:49:32 +0000   Thu, 11 May 2023 13:11:34 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Thu, 11 May 2023 13:49:32 +0000   Thu, 11 May 2023 13:11:34 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Thu, 11 May 2023 13:49:32 +0000   Thu, 11 May 2023 13:11:34 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  10.221.188.11\n  Hostname:    macpro-1\nCapacity:\n  apple.com/gpu:      1\n  cpu:                12\n  ephemeral-storage:  976227012Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32912408Ki\n  pods:               110\nAllocatable:\n  apple.com/gpu:      1\n  cpu:                12\n  ephemeral-storage:  899690812770\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             32810008Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 356aaa8b81424c7f8a8061f030f46f0f\n  System UUID:                1d4c4fec-70e6-965c-8258-84e7ade85d8e\n  Boot ID:                    f08ad932-e70a-4238-af74-1e7a9836c32a\n  Kernel Version:             5.6.14-300.fc32.x86_64\n  OS Image:                   Fedora CoreOS 32.20200601.3.0\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.8\n  Kubelet Version:            v1.27.1\n  Kube-Proxy Version:         v1.27.1\nPodCIDR:                      192.168.4.0/24\nPodCIDRs:                     192.168.4.0/24\nNon-terminated Pods:          (4 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 calico-node-p8f6p                                          250m (2%)     0 (0%)      0 (0%)           0 (0%)         38m\n  kube-system                 kube-proxy-t8w6l                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\n  metallb-system              speaker-pq5r2                                              0 (0%)        0 (0%)      0 (0%)           0 (0%)         38m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-9g8bt    0 (0%)        0 (0%)      0 (0%)           0 (0%)         22m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests   Limits\n  --------           --------   ------\n  cpu                250m (2%)  0 (0%)\n  memory             0 (0%)     0 (0%)\n  ephemeral-storage  0 (0%)     0 (0%)\n  hugepages-1Gi      0 (0%)     0 (0%)\n  hugepages-2Mi      0 (0%)     0 (0%)\n  apple.com/gpu      0          0\nEvents:\n  Type     Reason                   Age                From             Message\n  ----     ------                   ----               ----             -------\n  Normal   Starting                 38m                kube-proxy       \n  Normal   NodeHasSufficientMemory  38m (x5 over 38m)  kubelet          Node macpro-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    38m (x5 over 38m)  kubelet          Node macpro-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     38m (x5 over 38m)  kubelet          Node macpro-1 status is now: NodeHasSufficientPID\n  Normal   NodeReady                38m                kubelet          Node macpro-1 status is now: NodeReady\n  Normal   RegisteredNode           38m                node-controller  Node macpro-1 event: Registered Node macpro-1 in Controller\n  Normal   Starting                 37m                kubelet          Starting kubelet.\n  Warning  InvalidDiskCapacity      37m                kubelet          invalid capacity 0 on image filesystem\n  Normal   NodeHasSufficientMemory  37m                kubelet          Node macpro-1 status is now: NodeHasSufficientMemory\n  Normal   NodeHasNoDiskPressure    37m                kubelet          Node macpro-1 status is now: NodeHasNoDiskPressure\n  Normal   NodeHasSufficientPID     37m                kubelet          Node macpro-1 status is now: NodeHasSufficientPID\n  Normal   NodeAllocatableEnforced  37m                kubelet          Updated Node Allocatable limit across pods\n"
  May 11 13:50:22.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-2582 describe namespace kubectl-2582'
  May 11 13:50:22.851: INFO: stderr: ""
  May 11 13:50:22.851: INFO: stdout: "Name:         kubectl-2582\nLabels:       e2e-framework=kubectl\n              e2e-run=3f89617c-aa68-4c2a-8d4a-02ca06753a9f\n              kubernetes.io/metadata.name=kubectl-2582\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May 11 13:50:22.852: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2582" for this suite. @ 05/11/23 13:50:22.856
• [2.855 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/11/23 13:50:22.864
  May 11 13:50:22.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replication-controller @ 05/11/23 13:50:22.864
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:50:22.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:50:22.88
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/11/23 13:50:22.882
  STEP: When a replication controller with a matching selector is created @ 05/11/23 13:50:24.897
  STEP: Then the orphan pod is adopted @ 05/11/23 13:50:24.901
  May 11 13:50:25.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8945" for this suite. @ 05/11/23 13:50:25.916
• [3.059 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/11/23 13:50:25.923
  May 11 13:50:25.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename dns @ 05/11/23 13:50:25.924
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:50:25.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:50:25.945
  STEP: Creating a test externalName service @ 05/11/23 13:50:25.947
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6084.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local; sleep 1; done
   @ 05/11/23 13:50:25.954
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6084.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local; sleep 1; done
   @ 05/11/23 13:50:25.954
  STEP: creating a pod to probe DNS @ 05/11/23 13:50:25.954
  STEP: submitting the pod to kubernetes @ 05/11/23 13:50:25.954
  STEP: retrieving the pod @ 05/11/23 13:50:27.974
  STEP: looking for the results for each expected name from probers @ 05/11/23 13:50:27.978
  May 11 13:50:27.987: INFO: DNS probes using dns-test-0e2d2ca1-b026-4ccd-b43a-952bcf5b71d2 succeeded

  STEP: changing the externalName to bar.example.com @ 05/11/23 13:50:27.987
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6084.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local; sleep 1; done
   @ 05/11/23 13:50:27.995
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6084.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local; sleep 1; done
   @ 05/11/23 13:50:27.996
  STEP: creating a second pod to probe DNS @ 05/11/23 13:50:27.996
  STEP: submitting the pod to kubernetes @ 05/11/23 13:50:27.996
  STEP: retrieving the pod @ 05/11/23 13:50:30.02
  STEP: looking for the results for each expected name from probers @ 05/11/23 13:50:30.023
  May 11 13:50:30.026: INFO: File wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:30.029: INFO: File jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:30.029: INFO: Lookups using dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb failed for: [wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local]

  May 11 13:50:35.035: INFO: File wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:35.038: INFO: File jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:35.038: INFO: Lookups using dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb failed for: [wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local]

  May 11 13:50:40.036: INFO: File wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:40.039: INFO: File jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:40.039: INFO: Lookups using dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb failed for: [wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local]

  May 11 13:50:45.034: INFO: File wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:45.037: INFO: File jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:45.037: INFO: Lookups using dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb failed for: [wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local]

  May 11 13:50:50.035: INFO: File wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:50.039: INFO: File jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:50.039: INFO: Lookups using dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb failed for: [wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local]

  May 11 13:50:55.034: INFO: File wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:55.037: INFO: File jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local from pod  dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 11 13:50:55.037: INFO: Lookups using dns-6084/dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb failed for: [wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local]

  May 11 13:51:00.038: INFO: DNS probes using dns-test-25b40d58-2e2f-4c26-aed0-9e756e1a44eb succeeded

  STEP: changing the service to type=ClusterIP @ 05/11/23 13:51:00.038
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6084.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-6084.svc.cluster.local; sleep 1; done
   @ 05/11/23 13:51:00.062
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-6084.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-6084.svc.cluster.local; sleep 1; done
   @ 05/11/23 13:51:00.062
  STEP: creating a third pod to probe DNS @ 05/11/23 13:51:00.062
  STEP: submitting the pod to kubernetes @ 05/11/23 13:51:00.067
  STEP: retrieving the pod @ 05/11/23 13:51:02.091
  STEP: looking for the results for each expected name from probers @ 05/11/23 13:51:02.094
  May 11 13:51:02.101: INFO: DNS probes using dns-test-eb4eed6f-d91e-4922-8a8d-cb15cb856e3e succeeded

  May 11 13:51:02.101: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 13:51:02.105
  STEP: deleting the pod @ 05/11/23 13:51:02.12
  STEP: deleting the pod @ 05/11/23 13:51:02.132
  STEP: deleting the test externalName service @ 05/11/23 13:51:02.153
  STEP: Destroying namespace "dns-6084" for this suite. @ 05/11/23 13:51:02.19
• [36.277 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/11/23 13:51:02.2
  May 11 13:51:02.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 13:51:02.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:02.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:02.224
  STEP: Creating secret with name secret-test-map-8b230fa1-f1f2-44d4-9541-542c5842cf5d @ 05/11/23 13:51:02.228
  STEP: Creating a pod to test consume secrets @ 05/11/23 13:51:02.233
  STEP: Saw pod success @ 05/11/23 13:51:06.255
  May 11 13:51:06.258: INFO: Trying to get logs from node macpro-2 pod pod-secrets-e65c8572-5c86-4a93-8d51-68721fd53d9a container secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 13:51:06.271
  May 11 13:51:06.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5174" for this suite. @ 05/11/23 13:51:06.288
• [4.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/11/23 13:51:06.295
  May 11 13:51:06.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename field-validation @ 05/11/23 13:51:06.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:06.31
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:06.313
  May 11 13:51:06.315: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:51:08.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4165" for this suite. @ 05/11/23 13:51:08.887
• [2.597 seconds]
------------------------------
SS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/11/23 13:51:08.893
  May 11 13:51:08.893: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename controllerrevisions @ 05/11/23 13:51:08.894
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:08.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:08.911
  STEP: Creating DaemonSet "e2e-wn7x2-daemon-set" @ 05/11/23 13:51:08.94
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/11/23 13:51:08.945
  May 11 13:51:08.952: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:08.952: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:08.952: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:08.958: INFO: Number of nodes with available pods controlled by daemonset e2e-wn7x2-daemon-set: 0
  May 11 13:51:08.958: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 13:51:09.964: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:09.964: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:09.964: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:09.968: INFO: Number of nodes with available pods controlled by daemonset e2e-wn7x2-daemon-set: 2
  May 11 13:51:09.968: INFO: Node macpro-3 is running 0 daemon pod, expected 1
  May 11 13:51:11.021: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:11.021: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:11.022: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 13:51:11.033: INFO: Number of nodes with available pods controlled by daemonset e2e-wn7x2-daemon-set: 3
  May 11 13:51:11.033: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-wn7x2-daemon-set
  STEP: Confirm DaemonSet "e2e-wn7x2-daemon-set" successfully created with "daemonset-name=e2e-wn7x2-daemon-set" label @ 05/11/23 13:51:11.041
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-wn7x2-daemon-set" @ 05/11/23 13:51:11.083
  May 11 13:51:11.112: INFO: Located ControllerRevision: "e2e-wn7x2-daemon-set-856f9f456f"
  STEP: Patching ControllerRevision "e2e-wn7x2-daemon-set-856f9f456f" @ 05/11/23 13:51:11.119
  May 11 13:51:11.192: INFO: e2e-wn7x2-daemon-set-856f9f456f has been patched
  STEP: Create a new ControllerRevision @ 05/11/23 13:51:11.192
  May 11 13:51:11.220: INFO: Created ControllerRevision: e2e-wn7x2-daemon-set-56d6b869f7
  STEP: Confirm that there are two ControllerRevisions @ 05/11/23 13:51:11.22
  May 11 13:51:11.220: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 11 13:51:11.229: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-wn7x2-daemon-set-856f9f456f" @ 05/11/23 13:51:11.229
  STEP: Confirm that there is only one ControllerRevision @ 05/11/23 13:51:11.242
  May 11 13:51:11.242: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 11 13:51:11.251: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-wn7x2-daemon-set-56d6b869f7" @ 05/11/23 13:51:11.257
  May 11 13:51:11.278: INFO: e2e-wn7x2-daemon-set-56d6b869f7 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/11/23 13:51:11.278
  W0511 13:51:11.718726      24 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/11/23 13:51:11.718
  May 11 13:51:11.718: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 11 13:51:12.761: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 11 13:51:12.764: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-wn7x2-daemon-set-56d6b869f7=updated" @ 05/11/23 13:51:12.764
  STEP: Confirm that there is only one ControllerRevision @ 05/11/23 13:51:12.77
  May 11 13:51:12.770: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 11 13:51:12.778: INFO: Found 1 ControllerRevisions
  May 11 13:51:12.781: INFO: ControllerRevision "e2e-wn7x2-daemon-set-5d6cb64956" has revision 3
  STEP: Deleting DaemonSet "e2e-wn7x2-daemon-set" @ 05/11/23 13:51:12.785
  STEP: deleting DaemonSet.extensions e2e-wn7x2-daemon-set in namespace controllerrevisions-3985, will wait for the garbage collector to delete the pods @ 05/11/23 13:51:12.785
  May 11 13:51:12.843: INFO: Deleting DaemonSet.extensions e2e-wn7x2-daemon-set took: 5.028598ms
  May 11 13:51:12.944: INFO: Terminating DaemonSet.extensions e2e-wn7x2-daemon-set pods took: 100.856188ms
  May 11 13:51:13.948: INFO: Number of nodes with available pods controlled by daemonset e2e-wn7x2-daemon-set: 0
  May 11 13:51:13.948: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-wn7x2-daemon-set
  May 11 13:51:13.950: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"15272"},"items":null}

  May 11 13:51:13.953: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"15272"},"items":null}

  May 11 13:51:13.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-3985" for this suite. @ 05/11/23 13:51:13.972
• [5.084 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/11/23 13:51:13.977
  May 11 13:51:13.977: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename proxy @ 05/11/23 13:51:13.978
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:13.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:13.995
  May 11 13:51:13.997: INFO: Creating pod...
  May 11 13:51:16.011: INFO: Creating service...
  May 11 13:51:16.025: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/pods/agnhost/proxy/some/path/with/DELETE
  May 11 13:51:16.038: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 11 13:51:16.038: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/pods/agnhost/proxy/some/path/with/GET
  May 11 13:51:16.044: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 11 13:51:16.044: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/pods/agnhost/proxy/some/path/with/HEAD
  May 11 13:51:16.046: INFO: http.Client request:HEAD | StatusCode:200
  May 11 13:51:16.046: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/pods/agnhost/proxy/some/path/with/OPTIONS
  May 11 13:51:16.053: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 11 13:51:16.053: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/pods/agnhost/proxy/some/path/with/PATCH
  May 11 13:51:16.057: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 11 13:51:16.057: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/pods/agnhost/proxy/some/path/with/POST
  May 11 13:51:16.063: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 11 13:51:16.063: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/pods/agnhost/proxy/some/path/with/PUT
  May 11 13:51:16.066: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 11 13:51:16.066: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/services/test-service/proxy/some/path/with/DELETE
  May 11 13:51:16.071: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 11 13:51:16.071: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/services/test-service/proxy/some/path/with/GET
  May 11 13:51:16.076: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 11 13:51:16.076: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/services/test-service/proxy/some/path/with/HEAD
  May 11 13:51:16.079: INFO: http.Client request:HEAD | StatusCode:200
  May 11 13:51:16.079: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/services/test-service/proxy/some/path/with/OPTIONS
  May 11 13:51:16.084: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 11 13:51:16.084: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/services/test-service/proxy/some/path/with/PATCH
  May 11 13:51:16.088: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 11 13:51:16.088: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/services/test-service/proxy/some/path/with/POST
  May 11 13:51:16.091: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 11 13:51:16.091: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-2182/services/test-service/proxy/some/path/with/PUT
  May 11 13:51:16.094: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 11 13:51:16.094: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2182" for this suite. @ 05/11/23 13:51:16.098
• [2.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/11/23 13:51:16.105
  May 11 13:51:16.105: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 13:51:16.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:16.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:16.127
  STEP: Creating configMap with name configmap-test-upd-f04af496-e3a8-416a-bc9f-019e28b38039 @ 05/11/23 13:51:16.134
  STEP: Creating the pod @ 05/11/23 13:51:16.138
  STEP: Updating configmap configmap-test-upd-f04af496-e3a8-416a-bc9f-019e28b38039 @ 05/11/23 13:51:18.17
  STEP: waiting to observe update in volume @ 05/11/23 13:51:18.176
  May 11 13:51:22.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9673" for this suite. @ 05/11/23 13:51:22.2
• [6.100 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/11/23 13:51:22.206
  May 11 13:51:22.206: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename watch @ 05/11/23 13:51:22.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:22.224
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:22.227
  STEP: getting a starting resourceVersion @ 05/11/23 13:51:22.228
  STEP: starting a background goroutine to produce watch events @ 05/11/23 13:51:22.232
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/11/23 13:51:22.232
  May 11 13:51:24.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6594" for this suite. @ 05/11/23 13:51:25.012
• [2.860 seconds]
------------------------------
SS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/11/23 13:51:25.066
  May 11 13:51:25.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename disruption @ 05/11/23 13:51:25.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:25.083
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:25.085
  STEP: Waiting for the pdb to be processed @ 05/11/23 13:51:25.099
  STEP: Waiting for all pods to be running @ 05/11/23 13:51:27.135
  May 11 13:51:27.147: INFO: running pods: 0 < 3
  May 11 13:51:29.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9490" for this suite. @ 05/11/23 13:51:29.161
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/11/23 13:51:29.171
  May 11 13:51:29.171: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 13:51:29.172
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:29.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:29.197
  May 11 13:51:29.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/11/23 13:51:31.918
  May 11 13:51:31.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-9011 --namespace=crd-publish-openapi-9011 create -f -'
  May 11 13:51:32.842: INFO: stderr: ""
  May 11 13:51:32.842: INFO: stdout: "e2e-test-crd-publish-openapi-7188-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 11 13:51:32.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-9011 --namespace=crd-publish-openapi-9011 delete e2e-test-crd-publish-openapi-7188-crds test-cr'
  May 11 13:51:33.298: INFO: stderr: ""
  May 11 13:51:33.298: INFO: stdout: "e2e-test-crd-publish-openapi-7188-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May 11 13:51:33.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-9011 --namespace=crd-publish-openapi-9011 apply -f -'
  May 11 13:51:33.509: INFO: stderr: ""
  May 11 13:51:33.509: INFO: stdout: "e2e-test-crd-publish-openapi-7188-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 11 13:51:33.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-9011 --namespace=crd-publish-openapi-9011 delete e2e-test-crd-publish-openapi-7188-crds test-cr'
  May 11 13:51:33.575: INFO: stderr: ""
  May 11 13:51:33.575: INFO: stdout: "e2e-test-crd-publish-openapi-7188-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/11/23 13:51:33.575
  May 11 13:51:33.575: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-9011 explain e2e-test-crd-publish-openapi-7188-crds'
  May 11 13:51:33.780: INFO: stderr: ""
  May 11 13:51:33.780: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-7188-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  May 11 13:51:35.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9011" for this suite. @ 05/11/23 13:51:35.402
• [6.262 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/11/23 13:51:35.434
  May 11 13:51:35.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/11/23 13:51:35.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:35.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:35.487
  STEP: Creating 50 configmaps @ 05/11/23 13:51:35.497
  STEP: Creating RC which spawns configmap-volume pods @ 05/11/23 13:51:35.758
  May 11 13:51:35.794: INFO: Pod name wrapped-volume-race-22b5158f-bdab-4f6b-8ba6-d978c26b4675: Found 1 pods out of 5
  May 11 13:51:40.801: INFO: Pod name wrapped-volume-race-22b5158f-bdab-4f6b-8ba6-d978c26b4675: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/11/23 13:51:40.801
  STEP: Creating RC which spawns configmap-volume pods @ 05/11/23 13:51:40.818
  May 11 13:51:40.831: INFO: Pod name wrapped-volume-race-97548fe5-81fe-40c3-a947-8999a112dca7: Found 0 pods out of 5
  May 11 13:51:45.840: INFO: Pod name wrapped-volume-race-97548fe5-81fe-40c3-a947-8999a112dca7: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/11/23 13:51:45.84
  STEP: Creating RC which spawns configmap-volume pods @ 05/11/23 13:51:45.887
  May 11 13:51:45.950: INFO: Pod name wrapped-volume-race-16242cbe-baf7-4b67-afbd-592714f1ca8d: Found 1 pods out of 5
  May 11 13:51:50.959: INFO: Pod name wrapped-volume-race-16242cbe-baf7-4b67-afbd-592714f1ca8d: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/11/23 13:51:50.959
  May 11 13:51:50.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-16242cbe-baf7-4b67-afbd-592714f1ca8d in namespace emptydir-wrapper-1325, will wait for the garbage collector to delete the pods @ 05/11/23 13:51:50.977
  May 11 13:51:51.036: INFO: Deleting ReplicationController wrapped-volume-race-16242cbe-baf7-4b67-afbd-592714f1ca8d took: 6.083766ms
  May 11 13:51:51.137: INFO: Terminating ReplicationController wrapped-volume-race-16242cbe-baf7-4b67-afbd-592714f1ca8d pods took: 101.038511ms
  STEP: deleting ReplicationController wrapped-volume-race-97548fe5-81fe-40c3-a947-8999a112dca7 in namespace emptydir-wrapper-1325, will wait for the garbage collector to delete the pods @ 05/11/23 13:51:52.138
  May 11 13:51:52.198: INFO: Deleting ReplicationController wrapped-volume-race-97548fe5-81fe-40c3-a947-8999a112dca7 took: 6.032484ms
  May 11 13:51:52.299: INFO: Terminating ReplicationController wrapped-volume-race-97548fe5-81fe-40c3-a947-8999a112dca7 pods took: 100.962216ms
  STEP: deleting ReplicationController wrapped-volume-race-22b5158f-bdab-4f6b-8ba6-d978c26b4675 in namespace emptydir-wrapper-1325, will wait for the garbage collector to delete the pods @ 05/11/23 13:51:53.3
  May 11 13:51:53.361: INFO: Deleting ReplicationController wrapped-volume-race-22b5158f-bdab-4f6b-8ba6-d978c26b4675 took: 5.777611ms
  May 11 13:51:53.462: INFO: Terminating ReplicationController wrapped-volume-race-22b5158f-bdab-4f6b-8ba6-d978c26b4675 pods took: 101.09928ms
  STEP: Cleaning up the configMaps @ 05/11/23 13:51:54.363
  STEP: Destroying namespace "emptydir-wrapper-1325" for this suite. @ 05/11/23 13:51:54.569
• [19.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/11/23 13:51:54.577
  May 11 13:51:54.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename var-expansion @ 05/11/23 13:51:54.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:51:54.6
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:51:54.602
  May 11 13:51:56.632: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 13:51:56.635: INFO: Deleting pod "var-expansion-784452ee-b4de-4563-9b4c-97672a739361" in namespace "var-expansion-8204"
  May 11 13:51:56.641: INFO: Wait up to 5m0s for pod "var-expansion-784452ee-b4de-4563-9b4c-97672a739361" to be fully deleted
  STEP: Destroying namespace "var-expansion-8204" for this suite. @ 05/11/23 13:52:00.651
• [6.079 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/11/23 13:52:00.656
  May 11 13:52:00.656: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 13:52:00.656
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:52:00.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:52:00.674
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/11/23 13:52:00.676
  May 11 13:52:00.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:52:02.268: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 13:52:08.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9225" for this suite. @ 05/11/23 13:52:08.512
• [7.862 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/11/23 13:52:08.519
  May 11 13:52:08.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 13:52:08.519
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:52:08.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:52:08.539
  STEP: Creating configMap with name configmap-test-volume-map-475bbd27-5a8f-46c6-95d5-3859ce3825d9 @ 05/11/23 13:52:08.541
  STEP: Creating a pod to test consume configMaps @ 05/11/23 13:52:08.544
  STEP: Saw pod success @ 05/11/23 13:52:12.612
  May 11 13:52:12.615: INFO: Trying to get logs from node macpro-2 pod pod-configmaps-59ec56b1-8e76-4c1c-8b62-ea30542d1af9 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 13:52:12.638
  May 11 13:52:12.651: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6168" for this suite. @ 05/11/23 13:52:12.654
• [4.140 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/11/23 13:52:12.659
  May 11 13:52:12.659: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-runtime @ 05/11/23 13:52:12.66
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:52:12.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:52:12.698
  STEP: create the container @ 05/11/23 13:52:12.701
  W0511 13:52:12.709126      24 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/11/23 13:52:12.709
  STEP: get the container status @ 05/11/23 13:52:15.724
  STEP: the container should be terminated @ 05/11/23 13:52:15.727
  STEP: the termination message should be set @ 05/11/23 13:52:15.727
  May 11 13:52:15.727: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/11/23 13:52:15.727
  May 11 13:52:15.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-6179" for this suite. @ 05/11/23 13:52:15.744
• [3.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/11/23 13:52:15.752
  May 11 13:52:15.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:52:15.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:52:15.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:52:15.768
  STEP: Creating projection with secret that has name projected-secret-test-map-0c3b7382-731c-4012-9c1c-64df6299fff9 @ 05/11/23 13:52:15.77
  STEP: Creating a pod to test consume secrets @ 05/11/23 13:52:15.774
  STEP: Saw pod success @ 05/11/23 13:52:19.794
  May 11 13:52:19.796: INFO: Trying to get logs from node macpro-2 pod pod-projected-secrets-7eb365ab-2c88-46ad-88b8-5714b98c6ec7 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 13:52:19.801
  May 11 13:52:19.811: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2467" for this suite. @ 05/11/23 13:52:19.814
• [4.067 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/11/23 13:52:19.819
  May 11 13:52:19.819: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename taint-single-pod @ 05/11/23 13:52:19.82
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:52:19.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:52:19.838
  May 11 13:52:19.840: INFO: Waiting up to 1m0s for all nodes to be ready
  May 11 13:53:19.873: INFO: Waiting for terminating namespaces to be deleted...
  May 11 13:53:19.876: INFO: Starting informer...
  STEP: Starting pod... @ 05/11/23 13:53:19.876
  May 11 13:53:20.089: INFO: Pod is running on macpro-2. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/11/23 13:53:20.089
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/11/23 13:53:20.101
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/11/23 13:53:20.107
  May 11 13:53:20.107: INFO: Pod wasn't evicted. Proceeding
  May 11 13:53:20.107: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/11/23 13:53:20.121
  STEP: Waiting some time to make sure that toleration time passed. @ 05/11/23 13:53:20.127
  May 11 13:54:35.128: INFO: Pod wasn't evicted. Test successful
  May 11 13:54:35.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-8978" for this suite. @ 05/11/23 13:54:35.133
• [135.321 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/11/23 13:54:35.141
  May 11 13:54:35.141: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 13:54:35.141
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:54:35.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:54:35.166
  STEP: Creating a pod to test downward api env vars @ 05/11/23 13:54:35.168
  STEP: Saw pod success @ 05/11/23 13:54:39.192
  May 11 13:54:39.194: INFO: Trying to get logs from node macpro-2 pod downward-api-ee99395b-a6bf-4e12-9017-2b32c8c65154 container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 13:54:39.209
  May 11 13:54:39.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-449" for this suite. @ 05/11/23 13:54:39.229
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/11/23 13:54:39.234
  May 11 13:54:39.234: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename field-validation @ 05/11/23 13:54:39.235
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:54:39.25
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:54:39.254
  May 11 13:54:39.256: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  W0511 13:54:41.799824      24 warnings.go:70] unknown field "alpha"
  W0511 13:54:41.799863      24 warnings.go:70] unknown field "beta"
  W0511 13:54:41.799869      24 warnings.go:70] unknown field "delta"
  W0511 13:54:41.799874      24 warnings.go:70] unknown field "epsilon"
  W0511 13:54:41.799879      24 warnings.go:70] unknown field "gamma"
  May 11 13:54:41.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-1400" for this suite. @ 05/11/23 13:54:41.826
• [2.596 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/11/23 13:54:41.831
  May 11 13:54:41.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 13:54:41.832
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:54:41.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:54:41.852
  STEP: Creating secret with name secret-test-d93d52b7-a553-4a20-bcec-bb33290b818b @ 05/11/23 13:54:41.855
  STEP: Creating a pod to test consume secrets @ 05/11/23 13:54:41.859
  STEP: Saw pod success @ 05/11/23 13:54:45.88
  May 11 13:54:45.883: INFO: Trying to get logs from node macpro-2 pod pod-secrets-e5f9aca9-faa0-437a-8e98-12499674bd24 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 13:54:45.888
  May 11 13:54:45.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6314" for this suite. @ 05/11/23 13:54:45.905
• [4.079 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/11/23 13:54:45.911
  May 11 13:54:45.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename job @ 05/11/23 13:54:45.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:54:45.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:54:45.93
  STEP: Creating Indexed job @ 05/11/23 13:54:45.932
  STEP: Ensuring job reaches completions @ 05/11/23 13:54:45.936
  STEP: Ensuring pods with index for job exist @ 05/11/23 13:54:51.942
  May 11 13:54:51.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9990" for this suite. @ 05/11/23 13:54:51.949
• [6.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/11/23 13:54:51.956
  May 11 13:54:51.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 13:54:51.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:54:51.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:54:51.972
  STEP: creating the pod @ 05/11/23 13:54:51.975
  STEP: submitting the pod to kubernetes @ 05/11/23 13:54:51.975
  STEP: verifying QOS class is set on the pod @ 05/11/23 13:54:51.981
  May 11 13:54:51.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8904" for this suite. @ 05/11/23 13:54:51.993
• [0.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/11/23 13:54:52.002
  May 11 13:54:52.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/11/23 13:54:52.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:54:52.018
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:54:52.02
  May 11 13:54:52.022: INFO: Waiting up to 1m0s for all nodes to be ready
  May 11 13:55:52.051: INFO: Waiting for terminating namespaces to be deleted...
  May 11 13:55:52.053: INFO: Starting informer...
  STEP: Starting pods... @ 05/11/23 13:55:52.054
  May 11 13:55:52.271: INFO: Pod1 is running on macpro-3. Tainting Node
  May 11 13:55:54.490: INFO: Pod2 is running on macpro-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/11/23 13:55:54.49
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/11/23 13:55:54.502
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/11/23 13:55:54.509
  May 11 13:55:59.978: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  May 11 13:56:19.999: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May 11 13:56:19.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/11/23 13:56:20.019
  STEP: Destroying namespace "taint-multiple-pods-3053" for this suite. @ 05/11/23 13:56:20.024
• [88.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/11/23 13:56:20.031
  May 11 13:56:20.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 13:56:20.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:56:20.118
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:56:20.122
  STEP: Creating the pod @ 05/11/23 13:56:20.124
  May 11 13:56:22.697: INFO: Successfully updated pod "labelsupdated3b9fa51-6964-4266-9214-3e2c80af683b"
  May 11 13:56:26.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8369" for this suite. @ 05/11/23 13:56:26.719
• [6.694 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/11/23 13:56:26.726
  May 11 13:56:26.726: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 13:56:26.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:56:26.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:56:26.743
  May 11 13:56:48.804: INFO: Container started at 2023-05-11 13:56:27 +0000 UTC, pod became ready at 2023-05-11 13:56:47 +0000 UTC
  May 11 13:56:48.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3418" for this suite. @ 05/11/23 13:56:48.807
• [22.085 seconds]
------------------------------
S
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/11/23 13:56:48.812
  May 11 13:56:48.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 13:56:48.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:56:48.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:56:48.83
  STEP: Creating a pod to test downward api env vars @ 05/11/23 13:56:48.832
  STEP: Saw pod success @ 05/11/23 13:56:52.852
  May 11 13:56:52.854: INFO: Trying to get logs from node macpro-3 pod downward-api-62d29b69-1260-4242-b12b-c5e440b65b12 container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 13:56:52.859
  May 11 13:56:52.871: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2525" for this suite. @ 05/11/23 13:56:52.875
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/11/23 13:56:52.88
  May 11 13:56:52.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 13:56:52.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:56:52.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:56:52.899
  May 11 13:56:52.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7254" for this suite. @ 05/11/23 13:56:52.933
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/11/23 13:56:52.937
  May 11 13:56:52.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename job @ 05/11/23 13:56:52.938
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:56:52.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:56:52.953
  STEP: Creating a job @ 05/11/23 13:56:52.955
  STEP: Ensuring active pods == parallelism @ 05/11/23 13:56:52.959
  STEP: delete a job @ 05/11/23 13:56:54.963
  STEP: deleting Job.batch foo in namespace job-6285, will wait for the garbage collector to delete the pods @ 05/11/23 13:56:54.963
  May 11 13:56:55.021: INFO: Deleting Job.batch foo took: 5.017093ms
  May 11 13:56:55.122: INFO: Terminating Job.batch foo pods took: 100.411469ms
  STEP: Ensuring job was deleted @ 05/11/23 13:57:27.422
  May 11 13:57:27.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6285" for this suite. @ 05/11/23 13:57:27.429
• [34.496 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/11/23 13:57:27.434
  May 11 13:57:27.434: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 13:57:27.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:57:27.451
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:57:27.454
  STEP: Creating configMap with name configmap-test-volume-map-58d72573-c3b3-48b3-9dc2-0cac4280b522 @ 05/11/23 13:57:27.456
  STEP: Creating a pod to test consume configMaps @ 05/11/23 13:57:27.459
  STEP: Saw pod success @ 05/11/23 13:57:31.477
  May 11 13:57:31.479: INFO: Trying to get logs from node macpro-2 pod pod-configmaps-a15c6cbe-c71a-418b-8bd9-5a4d7da922b9 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 13:57:31.494
  May 11 13:57:31.505: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4063" for this suite. @ 05/11/23 13:57:31.51
• [4.081 seconds]
------------------------------
S
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/11/23 13:57:31.515
  May 11 13:57:31.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 13:57:31.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:57:31.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:57:31.532
  STEP: fetching services @ 05/11/23 13:57:31.534
  May 11 13:57:31.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1202" for this suite. @ 05/11/23 13:57:31.541
• [0.030 seconds]
------------------------------
SSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/11/23 13:57:31.545
  May 11 13:57:31.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename podtemplate @ 05/11/23 13:57:31.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:57:31.562
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:57:31.565
  STEP: Create a pod template @ 05/11/23 13:57:31.567
  STEP: Replace a pod template @ 05/11/23 13:57:31.571
  May 11 13:57:31.576: INFO: Found updated podtemplate annotation: "true"

  May 11 13:57:31.577: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-9756" for this suite. @ 05/11/23 13:57:31.58
• [0.039 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/11/23 13:57:31.584
  May 11 13:57:31.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename runtimeclass @ 05/11/23 13:57:31.584
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:57:31.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:57:31.605
  May 11 13:57:31.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-7303" for this suite. @ 05/11/23 13:57:31.615
• [0.035 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/11/23 13:57:31.619
  May 11 13:57:31.619: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 13:57:31.62
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:57:31.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:57:31.64
  May 11 13:57:31.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5332" for this suite. @ 05/11/23 13:57:31.671
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/11/23 13:57:31.677
  May 11 13:57:31.677: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 13:57:31.678
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:57:31.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:57:31.697
  STEP: creating a collection of services @ 05/11/23 13:57:31.699
  May 11 13:57:31.699: INFO: Creating e2e-svc-a-lvm2c
  May 11 13:57:31.709: INFO: Creating e2e-svc-b-hxlwc
  May 11 13:57:31.729: INFO: Creating e2e-svc-c-g7nts
  STEP: deleting service collection @ 05/11/23 13:57:31.747
  May 11 13:57:31.783: INFO: Collection of services has been deleted
  May 11 13:57:31.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4881" for this suite. @ 05/11/23 13:57:31.786
• [0.114 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/11/23 13:57:31.791
  May 11 13:57:31.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 13:57:31.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:57:31.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:57:31.811
  STEP: Creating pod liveness-39df6460-fc5a-4636-aa22-bdf9e7d8357d in namespace container-probe-3233 @ 05/11/23 13:57:31.813
  May 11 13:57:33.828: INFO: Started pod liveness-39df6460-fc5a-4636-aa22-bdf9e7d8357d in namespace container-probe-3233
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/11/23 13:57:33.828
  May 11 13:57:33.830: INFO: Initial restart count of pod liveness-39df6460-fc5a-4636-aa22-bdf9e7d8357d is 0
  May 11 13:57:53.872: INFO: Restart count of pod container-probe-3233/liveness-39df6460-fc5a-4636-aa22-bdf9e7d8357d is now 1 (20.041903488s elapsed)
  May 11 13:57:53.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 13:57:53.876
  STEP: Destroying namespace "container-probe-3233" for this suite. @ 05/11/23 13:57:53.886
• [22.101 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/11/23 13:57:53.892
  May 11 13:57:53.892: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-pred @ 05/11/23 13:57:53.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 13:57:53.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 13:57:53.917
  May 11 13:57:53.919: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 11 13:57:53.925: INFO: Waiting for terminating namespaces to be deleted...
  May 11 13:57:53.927: INFO: 
  Logging pods the apiserver thinks is on node macpro-1 before test
  May 11 13:57:53.934: INFO: calico-node-p8f6p from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.934: INFO: 	Container calico-node ready: true, restart count 0
  May 11 13:57:53.934: INFO: kube-proxy-t8w6l from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.934: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 13:57:53.934: INFO: speaker-pq5r2 from metallb-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.934: INFO: 	Container speaker ready: true, restart count 1
  May 11 13:57:53.934: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-9g8bt from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 13:57:53.934: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 13:57:53.934: INFO: 	Container systemd-logs ready: true, restart count 0
  May 11 13:57:53.934: INFO: 
  Logging pods the apiserver thinks is on node macpro-2 before test
  May 11 13:57:53.949: INFO: calico-node-92dft from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.949: INFO: 	Container calico-node ready: true, restart count 0
  May 11 13:57:53.949: INFO: kube-proxy-gl6v8 from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.949: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 13:57:53.949: INFO: speaker-vthm7 from metallb-system started at 2023-05-11 13:53:21 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.949: INFO: 	Container speaker ready: true, restart count 0
  May 11 13:57:53.949: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-6lx9v from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 13:57:53.949: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 13:57:53.949: INFO: 	Container systemd-logs ready: true, restart count 0
  May 11 13:57:53.949: INFO: 
  Logging pods the apiserver thinks is on node macpro-3 before test
  May 11 13:57:53.959: INFO: calico-node-gql49 from kube-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.959: INFO: 	Container calico-node ready: true, restart count 0
  May 11 13:57:53.959: INFO: kube-proxy-w7svn from kube-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.959: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 13:57:53.959: INFO: speaker-6dx5t from metallb-system started at 2023-05-11 13:56:20 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.959: INFO: 	Container speaker ready: true, restart count 0
  May 11 13:57:53.959: INFO: sonobuoy from sonobuoy started at 2023-05-11 13:28:12 +0000 UTC (1 container statuses recorded)
  May 11 13:57:53.959: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 11 13:57:53.959: INFO: sonobuoy-e2e-job-ae6f9d200d3b496f from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 13:57:53.959: INFO: 	Container e2e ready: true, restart count 0
  May 11 13:57:53.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 13:57:53.959: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-kc8bx from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 13:57:53.959: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 13:57:53.959: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/11/23 13:57:53.959
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/11/23 13:57:55.975
  STEP: Trying to apply a random label on the found node. @ 05/11/23 13:57:55.988
  STEP: verifying the node has the label kubernetes.io/e2e-25b02dd4-b0c2-4897-bb53-c61e8965b895 95 @ 05/11/23 13:57:55.999
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/11/23 13:57:56.008
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 10.221.188.12 on the node which pod4 resides and expect not scheduled @ 05/11/23 13:57:58.041
  STEP: removing the label kubernetes.io/e2e-25b02dd4-b0c2-4897-bb53-c61e8965b895 off the node macpro-2 @ 05/11/23 14:02:58.047
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-25b02dd4-b0c2-4897-bb53-c61e8965b895 @ 05/11/23 14:02:58.062
  May 11 14:02:58.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5039" for this suite. @ 05/11/23 14:02:58.08
• [304.195 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/11/23 14:02:58.088
  May 11 14:02:58.088: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 14:02:58.089
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:02:58.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:02:58.118
  STEP: creating service endpoint-test2 in namespace services-6634 @ 05/11/23 14:02:58.12
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6634 to expose endpoints map[] @ 05/11/23 14:02:58.137
  May 11 14:02:58.140: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  May 11 14:02:59.147: INFO: successfully validated that service endpoint-test2 in namespace services-6634 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6634 @ 05/11/23 14:02:59.147
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6634 to expose endpoints map[pod1:[80]] @ 05/11/23 14:03:01.166
  May 11 14:03:01.177: INFO: successfully validated that service endpoint-test2 in namespace services-6634 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/11/23 14:03:01.177
  May 11 14:03:01.177: INFO: Creating new exec pod
  May 11 14:03:04.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6634 exec execpods2knc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 11 14:03:04.304: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 11 14:03:04.304: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:03:04.304: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6634 exec execpods2knc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.248.119 80'
  May 11 14:03:04.409: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.248.119 80\nConnection to 10.103.248.119 80 port [tcp/http] succeeded!\n"
  May 11 14:03:04.409: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-6634 @ 05/11/23 14:03:04.409
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6634 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/11/23 14:03:06.428
  May 11 14:03:06.440: INFO: successfully validated that service endpoint-test2 in namespace services-6634 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/11/23 14:03:06.44
  May 11 14:03:07.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6634 exec execpods2knc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 11 14:03:07.556: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 11 14:03:07.556: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:03:07.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6634 exec execpods2knc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.248.119 80'
  May 11 14:03:07.666: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.248.119 80\nConnection to 10.103.248.119 80 port [tcp/http] succeeded!\n"
  May 11 14:03:07.666: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6634 @ 05/11/23 14:03:07.666
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6634 to expose endpoints map[pod2:[80]] @ 05/11/23 14:03:07.686
  May 11 14:03:07.703: INFO: successfully validated that service endpoint-test2 in namespace services-6634 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/11/23 14:03:07.703
  May 11 14:03:08.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6634 exec execpods2knc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 11 14:03:08.818: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 11 14:03:08.818: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:03:08.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6634 exec execpods2knc -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.248.119 80'
  May 11 14:03:08.932: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.248.119 80\nConnection to 10.103.248.119 80 port [tcp/http] succeeded!\n"
  May 11 14:03:08.932: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-6634 @ 05/11/23 14:03:08.932
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-6634 to expose endpoints map[] @ 05/11/23 14:03:08.944
  May 11 14:03:09.954: INFO: successfully validated that service endpoint-test2 in namespace services-6634 exposes endpoints map[]
  May 11 14:03:09.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6634" for this suite. @ 05/11/23 14:03:09.98
• [11.901 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/11/23 14:03:09.99
  May 11 14:03:09.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sysctl @ 05/11/23 14:03:09.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:03:10.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:03:10.028
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/11/23 14:03:10.031
  STEP: Watching for error events or started pod @ 05/11/23 14:03:10.038
  STEP: Waiting for pod completion @ 05/11/23 14:03:12.043
  STEP: Checking that the pod succeeded @ 05/11/23 14:03:14.055
  STEP: Getting logs from the pod @ 05/11/23 14:03:14.055
  STEP: Checking that the sysctl is actually updated @ 05/11/23 14:03:14.07
  May 11 14:03:14.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-5178" for this suite. @ 05/11/23 14:03:14.075
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/11/23 14:03:14.082
  May 11 14:03:14.082: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:03:14.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:03:14.1
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:03:14.103
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 14:03:14.105
  STEP: Saw pod success @ 05/11/23 14:03:16.122
  May 11 14:03:16.124: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-6a69ee73-40d2-47e4-b271-10b038f4b0e5 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 14:03:16.13
  May 11 14:03:16.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-577" for this suite. @ 05/11/23 14:03:16.152
• [2.084 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/11/23 14:03:16.167
  May 11 14:03:16.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 14:03:16.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:03:16.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:03:16.186
  STEP: Saw pod success @ 05/11/23 14:03:22.243
  May 11 14:03:22.246: INFO: Trying to get logs from node macpro-2 pod client-envvars-bde78d18-57c2-4481-92ee-2c5ec5d3586b container env3cont: <nil>
  STEP: delete the pod @ 05/11/23 14:03:22.251
  May 11 14:03:22.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2408" for this suite. @ 05/11/23 14:03:22.266
• [6.104 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/11/23 14:03:22.271
  May 11 14:03:22.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:03:22.271
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:03:22.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:03:22.289
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/11/23 14:03:22.291
  STEP: Saw pod success @ 05/11/23 14:03:26.309
  May 11 14:03:26.311: INFO: Trying to get logs from node macpro-3 pod pod-05ad521d-512d-40a8-92b2-97979fee50f6 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:03:26.325
  May 11 14:03:26.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5340" for this suite. @ 05/11/23 14:03:26.343
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/11/23 14:03:26.35
  May 11 14:03:26.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename dns @ 05/11/23 14:03:26.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:03:26.367
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:03:26.369
  STEP: Creating a test headless service @ 05/11/23 14:03:26.371
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6820 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6820;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6820 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6820;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6820.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6820.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6820.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6820.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6820.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6820.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6820.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6820.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6820.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6820.svc;check="$$(dig +notcp +noall +answer +search 70.238.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.238.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.238.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.238.70_tcp@PTR;sleep 1; done
   @ 05/11/23 14:03:26.388
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6820 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6820;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6820 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6820;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6820.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6820.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6820.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6820.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6820.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6820.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6820.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6820.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6820.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6820.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6820.svc;check="$$(dig +notcp +noall +answer +search 70.238.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.238.70_udp@PTR;check="$$(dig +tcp +noall +answer +search 70.238.101.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.101.238.70_tcp@PTR;sleep 1; done
   @ 05/11/23 14:03:26.388
  STEP: creating a pod to probe DNS @ 05/11/23 14:03:26.388
  STEP: submitting the pod to kubernetes @ 05/11/23 14:03:26.388
  STEP: retrieving the pod @ 05/11/23 14:03:28.409
  STEP: looking for the results for each expected name from probers @ 05/11/23 14:03:28.411
  May 11 14:03:28.414: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.417: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.420: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.423: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.425: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.427: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.430: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.432: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.443: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.446: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.448: INFO: Unable to read jessie_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.453: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.456: INFO: Unable to read jessie_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.459: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.467: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.470: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:28.480: INFO: Lookups using dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6820 wheezy_tcp@dns-test-service.dns-6820 wheezy_udp@dns-test-service.dns-6820.svc wheezy_tcp@dns-test-service.dns-6820.svc wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6820 jessie_tcp@dns-test-service.dns-6820 jessie_udp@dns-test-service.dns-6820.svc jessie_tcp@dns-test-service.dns-6820.svc jessie_udp@_http._tcp.dns-test-service.dns-6820.svc jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc]

  May 11 14:03:33.489: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.492: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.494: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.497: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.500: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.502: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.505: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.507: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.519: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.522: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.524: INFO: Unable to read jessie_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.526: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.528: INFO: Unable to read jessie_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.533: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.535: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:33.544: INFO: Lookups using dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6820 wheezy_tcp@dns-test-service.dns-6820 wheezy_udp@dns-test-service.dns-6820.svc wheezy_tcp@dns-test-service.dns-6820.svc wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6820 jessie_tcp@dns-test-service.dns-6820 jessie_udp@dns-test-service.dns-6820.svc jessie_tcp@dns-test-service.dns-6820.svc jessie_udp@_http._tcp.dns-test-service.dns-6820.svc jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc]

  May 11 14:03:38.488: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.491: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.493: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.496: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.498: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.500: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.503: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.505: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.516: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.518: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.520: INFO: Unable to read jessie_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.522: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.524: INFO: Unable to read jessie_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.526: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.528: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.530: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:38.539: INFO: Lookups using dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6820 wheezy_tcp@dns-test-service.dns-6820 wheezy_udp@dns-test-service.dns-6820.svc wheezy_tcp@dns-test-service.dns-6820.svc wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6820 jessie_tcp@dns-test-service.dns-6820 jessie_udp@dns-test-service.dns-6820.svc jessie_tcp@dns-test-service.dns-6820.svc jessie_udp@_http._tcp.dns-test-service.dns-6820.svc jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc]

  May 11 14:03:43.491: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.502: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.507: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.514: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.519: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.526: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.529: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.535: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.563: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.567: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.572: INFO: Unable to read jessie_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.580: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.585: INFO: Unable to read jessie_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.590: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.596: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.599: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:43.618: INFO: Lookups using dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6820 wheezy_tcp@dns-test-service.dns-6820 wheezy_udp@dns-test-service.dns-6820.svc wheezy_tcp@dns-test-service.dns-6820.svc wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6820 jessie_tcp@dns-test-service.dns-6820 jessie_udp@dns-test-service.dns-6820.svc jessie_tcp@dns-test-service.dns-6820.svc jessie_udp@_http._tcp.dns-test-service.dns-6820.svc jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc]

  May 11 14:03:48.489: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.492: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.495: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.497: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.500: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.502: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.504: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.506: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.517: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.520: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.522: INFO: Unable to read jessie_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.524: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.526: INFO: Unable to read jessie_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.528: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.530: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.538: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:48.547: INFO: Lookups using dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6820 wheezy_tcp@dns-test-service.dns-6820 wheezy_udp@dns-test-service.dns-6820.svc wheezy_tcp@dns-test-service.dns-6820.svc wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6820 jessie_tcp@dns-test-service.dns-6820 jessie_udp@dns-test-service.dns-6820.svc jessie_tcp@dns-test-service.dns-6820.svc jessie_udp@_http._tcp.dns-test-service.dns-6820.svc jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc]

  May 11 14:03:53.485: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.488: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.491: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.494: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.496: INFO: Unable to read wheezy_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.499: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.502: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.504: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.519: INFO: Unable to read jessie_udp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.521: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.524: INFO: Unable to read jessie_udp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.526: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820 from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.529: INFO: Unable to read jessie_udp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.531: INFO: Unable to read jessie_tcp@dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.534: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.536: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc from pod dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860: the server could not find the requested resource (get pods dns-test-3616c562-a69a-438b-a90b-4efd88117860)
  May 11 14:03:53.546: INFO: Lookups using dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-6820 wheezy_tcp@dns-test-service.dns-6820 wheezy_udp@dns-test-service.dns-6820.svc wheezy_tcp@dns-test-service.dns-6820.svc wheezy_udp@_http._tcp.dns-test-service.dns-6820.svc wheezy_tcp@_http._tcp.dns-test-service.dns-6820.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-6820 jessie_tcp@dns-test-service.dns-6820 jessie_udp@dns-test-service.dns-6820.svc jessie_tcp@dns-test-service.dns-6820.svc jessie_udp@_http._tcp.dns-test-service.dns-6820.svc jessie_tcp@_http._tcp.dns-test-service.dns-6820.svc]

  May 11 14:03:58.544: INFO: DNS probes using dns-6820/dns-test-3616c562-a69a-438b-a90b-4efd88117860 succeeded

  May 11 14:03:58.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 14:03:58.549
  STEP: deleting the test service @ 05/11/23 14:03:58.56
  STEP: deleting the test headless service @ 05/11/23 14:03:58.598
  STEP: Destroying namespace "dns-6820" for this suite. @ 05/11/23 14:03:58.614
• [32.271 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/11/23 14:03:58.621
  May 11 14:03:58.621: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 14:03:58.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:03:58.648
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:03:58.651
  May 11 14:03:58.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/11/23 14:04:00.621
  May 11 14:04:00.621: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 create -f -'
  May 11 14:04:01.413: INFO: stderr: ""
  May 11 14:04:01.413: INFO: stdout: "e2e-test-crd-publish-openapi-8123-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 11 14:04:01.413: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 delete e2e-test-crd-publish-openapi-8123-crds test-foo'
  May 11 14:04:01.505: INFO: stderr: ""
  May 11 14:04:01.505: INFO: stdout: "e2e-test-crd-publish-openapi-8123-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May 11 14:04:01.505: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 apply -f -'
  May 11 14:04:01.719: INFO: stderr: ""
  May 11 14:04:01.719: INFO: stdout: "e2e-test-crd-publish-openapi-8123-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 11 14:04:01.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 delete e2e-test-crd-publish-openapi-8123-crds test-foo'
  May 11 14:04:01.805: INFO: stderr: ""
  May 11 14:04:01.805: INFO: stdout: "e2e-test-crd-publish-openapi-8123-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/11/23 14:04:01.805
  May 11 14:04:01.805: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 create -f -'
  May 11 14:04:02.540: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/11/23 14:04:02.54
  May 11 14:04:02.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 create -f -'
  May 11 14:04:02.745: INFO: rc: 1
  May 11 14:04:02.745: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 apply -f -'
  May 11 14:04:04.176: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/11/23 14:04:04.176
  May 11 14:04:04.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 create -f -'
  May 11 14:04:04.385: INFO: rc: 1
  May 11 14:04:04.385: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 --namespace=crd-publish-openapi-7350 apply -f -'
  May 11 14:04:04.598: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/11/23 14:04:04.598
  May 11 14:04:04.598: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 explain e2e-test-crd-publish-openapi-8123-crds'
  May 11 14:04:04.800: INFO: stderr: ""
  May 11 14:04:04.800: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8123-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/11/23 14:04:04.8
  May 11 14:04:04.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 explain e2e-test-crd-publish-openapi-8123-crds.metadata'
  May 11 14:04:05.006: INFO: stderr: ""
  May 11 14:04:05.006: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8123-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May 11 14:04:05.007: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 explain e2e-test-crd-publish-openapi-8123-crds.spec'
  May 11 14:04:05.208: INFO: stderr: ""
  May 11 14:04:05.208: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8123-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May 11 14:04:05.208: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 explain e2e-test-crd-publish-openapi-8123-crds.spec.bars'
  May 11 14:04:05.417: INFO: stderr: ""
  May 11 14:04:05.417: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8123-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/11/23 14:04:05.417
  May 11 14:04:05.417: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-7350 explain e2e-test-crd-publish-openapi-8123-crds.spec.bars2'
  May 11 14:04:05.618: INFO: rc: 1
  May 11 14:04:07.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7350" for this suite. @ 05/11/23 14:04:07.18
• [8.564 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/11/23 14:04:07.185
  May 11 14:04:07.185: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 14:04:07.186
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:07.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:07.202
  STEP: creating service in namespace services-5509 @ 05/11/23 14:04:07.204
  STEP: creating service affinity-clusterip in namespace services-5509 @ 05/11/23 14:04:07.204
  STEP: creating replication controller affinity-clusterip in namespace services-5509 @ 05/11/23 14:04:07.215
  I0511 14:04:07.222195      24 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-5509, replica count: 3
  I0511 14:04:10.272775      24 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 14:04:10.277: INFO: Creating new exec pod
  May 11 14:04:13.290: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-5509 exec execpod-affinityndmhj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May 11 14:04:13.411: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May 11 14:04:13.411: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:04:13.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-5509 exec execpod-affinityndmhj -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.100.123.159 80'
  May 11 14:04:13.521: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.100.123.159 80\nConnection to 10.100.123.159 80 port [tcp/http] succeeded!\n"
  May 11 14:04:13.521: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:04:13.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-5509 exec execpod-affinityndmhj -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.100.123.159:80/ ; done'
  May 11 14:04:13.676: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.100.123.159:80/\n"
  May 11 14:04:13.676: INFO: stdout: "\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh\naffinity-clusterip-8hqzh"
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Received response from host: affinity-clusterip-8hqzh
  May 11 14:04:13.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 14:04:13.680: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-5509, will wait for the garbage collector to delete the pods @ 05/11/23 14:04:13.69
  May 11 14:04:13.749: INFO: Deleting ReplicationController affinity-clusterip took: 5.047155ms
  May 11 14:04:13.849: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.548868ms
  STEP: Destroying namespace "services-5509" for this suite. @ 05/11/23 14:04:16.068
• [8.890 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/11/23 14:04:16.076
  May 11 14:04:16.076: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename daemonsets @ 05/11/23 14:04:16.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:16.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:16.103
  STEP: Creating simple DaemonSet "daemon-set" @ 05/11/23 14:04:16.121
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/11/23 14:04:16.125
  May 11 14:04:16.128: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:16.128: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:16.128: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:16.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:04:16.131: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:04:17.136: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:17.136: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:17.136: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:17.139: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 14:04:17.139: INFO: Node macpro-3 is running 0 daemon pod, expected 1
  May 11 14:04:18.136: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:18.136: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:18.136: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:18.138: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 14:04:18.138: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/11/23 14:04:18.14
  May 11 14:04:18.152: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:18.152: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:18.152: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:18.155: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 14:04:18.155: INFO: Node macpro-3 is running 0 daemon pod, expected 1
  May 11 14:04:19.159: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:19.160: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:19.160: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:19.162: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 14:04:19.162: INFO: Node macpro-3 is running 0 daemon pod, expected 1
  May 11 14:04:20.160: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:20.160: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:20.160: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:20.163: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 14:04:20.163: INFO: Node macpro-3 is running 0 daemon pod, expected 1
  May 11 14:04:21.159: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:21.159: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:21.159: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:21.166: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 14:04:21.166: INFO: Node macpro-3 is running 0 daemon pod, expected 1
  May 11 14:04:22.159: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:22.159: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:22.159: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:22.162: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 14:04:22.162: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/11/23 14:04:22.164
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-7224, will wait for the garbage collector to delete the pods @ 05/11/23 14:04:22.164
  May 11 14:04:22.221: INFO: Deleting DaemonSet.extensions daemon-set took: 4.714126ms
  May 11 14:04:22.321: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.374678ms
  May 11 14:04:24.725: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:04:24.725: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 11 14:04:24.727: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19425"},"items":null}

  May 11 14:04:24.729: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19425"},"items":null}

  May 11 14:04:24.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-7224" for this suite. @ 05/11/23 14:04:24.741
• [8.671 seconds]
------------------------------
S
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/11/23 14:04:24.747
  May 11 14:04:24.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/11/23 14:04:24.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:24.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:24.764
  STEP: creating a target pod @ 05/11/23 14:04:24.766
  STEP: adding an ephemeral container @ 05/11/23 14:04:26.781
  STEP: checking pod container endpoints @ 05/11/23 14:04:30.801
  May 11 14:04:30.801: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5372 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:04:30.801: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:04:30.801: INFO: ExecWithOptions: Clientset creation
  May 11 14:04:30.801: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-5372/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May 11 14:04:30.862: INFO: Exec stderr: ""
  May 11 14:04:30.867: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-5372" for this suite. @ 05/11/23 14:04:30.871
• [6.129 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/11/23 14:04:30.876
  May 11 14:04:30.876: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 14:04:30.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:30.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:30.898
  STEP: Creating a ResourceQuota @ 05/11/23 14:04:30.9
  STEP: Getting a ResourceQuota @ 05/11/23 14:04:30.904
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/11/23 14:04:30.906
  STEP: Patching the ResourceQuota @ 05/11/23 14:04:30.909
  STEP: Deleting a Collection of ResourceQuotas @ 05/11/23 14:04:30.915
  STEP: Verifying the deleted ResourceQuota @ 05/11/23 14:04:30.921
  May 11 14:04:30.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4344" for this suite. @ 05/11/23 14:04:30.927
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/11/23 14:04:30.932
  May 11 14:04:30.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename daemonsets @ 05/11/23 14:04:30.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:30.949
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:30.951
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/11/23 14:04:30.97
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/11/23 14:04:30.976
  May 11 14:04:30.980: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:30.980: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:30.980: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:30.982: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:04:30.982: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:04:31.986: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:31.986: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:31.986: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:31.989: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 14:04:31.989: INFO: Node macpro-2 is running 0 daemon pod, expected 1
  May 11 14:04:32.987: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:32.987: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:32.987: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:32.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 14:04:32.990: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/11/23 14:04:32.992
  May 11 14:04:33.005: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:33.005: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:33.005: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:04:33.008: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 14:04:33.008: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/11/23 14:04:33.008
  STEP: Deleting DaemonSet "daemon-set" @ 05/11/23 14:04:34.018
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2900, will wait for the garbage collector to delete the pods @ 05/11/23 14:04:34.018
  May 11 14:04:34.079: INFO: Deleting DaemonSet.extensions daemon-set took: 7.952054ms
  May 11 14:04:34.179: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.487775ms
  May 11 14:04:36.984: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:04:36.984: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 11 14:04:36.987: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19626"},"items":null}

  May 11 14:04:37.007: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19626"},"items":null}

  May 11 14:04:37.069: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-2900" for this suite. @ 05/11/23 14:04:37.079
• [6.167 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/11/23 14:04:37.1
  May 11 14:04:37.100: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:04:37.1
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:37.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:37.128
  STEP: creating all guestbook components @ 05/11/23 14:04:37.132
  May 11 14:04:37.132: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May 11 14:04:37.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 create -f -'
  May 11 14:04:37.914: INFO: stderr: ""
  May 11 14:04:37.914: INFO: stdout: "service/agnhost-replica created\n"
  May 11 14:04:37.914: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May 11 14:04:37.915: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 create -f -'
  May 11 14:04:38.606: INFO: stderr: ""
  May 11 14:04:38.607: INFO: stdout: "service/agnhost-primary created\n"
  May 11 14:04:38.607: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May 11 14:04:38.607: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 create -f -'
  May 11 14:04:38.878: INFO: stderr: ""
  May 11 14:04:38.878: INFO: stdout: "service/frontend created\n"
  May 11 14:04:38.879: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May 11 14:04:38.879: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 create -f -'
  May 11 14:04:39.120: INFO: stderr: ""
  May 11 14:04:39.120: INFO: stdout: "deployment.apps/frontend created\n"
  May 11 14:04:39.120: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 11 14:04:39.120: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 create -f -'
  May 11 14:04:39.363: INFO: stderr: ""
  May 11 14:04:39.363: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May 11 14:04:39.364: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 11 14:04:39.364: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 create -f -'
  May 11 14:04:39.606: INFO: stderr: ""
  May 11 14:04:39.606: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/11/23 14:04:39.606
  May 11 14:04:39.606: INFO: Waiting for all frontend pods to be Running.
  May 11 14:04:44.657: INFO: Waiting for frontend to serve content.
  May 11 14:04:44.666: INFO: Trying to add a new entry to the guestbook.
  May 11 14:04:44.673: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/11/23 14:04:44.681
  May 11 14:04:44.681: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 delete --grace-period=0 --force -f -'
  May 11 14:04:44.773: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 14:04:44.773: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/11/23 14:04:44.773
  May 11 14:04:44.773: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 delete --grace-period=0 --force -f -'
  May 11 14:04:44.848: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 14:04:44.848: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/11/23 14:04:44.848
  May 11 14:04:44.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 delete --grace-period=0 --force -f -'
  May 11 14:04:44.918: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 14:04:44.918: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/11/23 14:04:44.918
  May 11 14:04:44.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 delete --grace-period=0 --force -f -'
  May 11 14:04:44.983: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 14:04:44.983: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/11/23 14:04:44.983
  May 11 14:04:44.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 delete --grace-period=0 --force -f -'
  May 11 14:04:45.048: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 14:04:45.048: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/11/23 14:04:45.048
  May 11 14:04:45.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-6587 delete --grace-period=0 --force -f -'
  May 11 14:04:45.114: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 11 14:04:45.114: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May 11 14:04:45.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6587" for this suite. @ 05/11/23 14:04:45.118
• [8.028 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/11/23 14:04:45.129
  May 11 14:04:45.129: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 14:04:45.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:45.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:45.173
  STEP: creating the pod @ 05/11/23 14:04:45.178
  STEP: submitting the pod to kubernetes @ 05/11/23 14:04:45.178
  W0511 14:04:45.189010      24 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 05/11/23 14:04:49.209
  STEP: updating the pod @ 05/11/23 14:04:49.212
  May 11 14:04:49.721: INFO: Successfully updated pod "pod-update-activedeadlineseconds-26bccb75-4f37-412f-bf1d-f9e9b36ca7db"
  May 11 14:04:51.728: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5159" for this suite. @ 05/11/23 14:04:51.733
• [6.610 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/11/23 14:04:51.739
  May 11 14:04:51.739: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename init-container @ 05/11/23 14:04:51.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:51.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:51.757
  STEP: creating the pod @ 05/11/23 14:04:51.759
  May 11 14:04:51.759: INFO: PodSpec: initContainers in spec.initContainers
  May 11 14:04:55.252: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5743" for this suite. @ 05/11/23 14:04:55.257
• [3.522 seconds]
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/11/23 14:04:55.261
  May 11 14:04:55.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename podtemplate @ 05/11/23 14:04:55.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:55.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:55.279
  STEP: Create set of pod templates @ 05/11/23 14:04:55.281
  May 11 14:04:55.284: INFO: created test-podtemplate-1
  May 11 14:04:55.288: INFO: created test-podtemplate-2
  May 11 14:04:55.291: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/11/23 14:04:55.291
  STEP: delete collection of pod templates @ 05/11/23 14:04:55.293
  May 11 14:04:55.293: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/11/23 14:04:55.303
  May 11 14:04:55.303: INFO: requesting list of pod templates to confirm quantity
  May 11 14:04:55.305: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-2868" for this suite. @ 05/11/23 14:04:55.308
• [0.052 seconds]
------------------------------
SSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/11/23 14:04:55.313
  May 11 14:04:55.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:04:55.313
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:04:55.327
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:04:55.33
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/11/23 14:04:55.331
  May 11 14:04:55.331: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-3419 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 11 14:04:55.398: INFO: stderr: ""
  May 11 14:04:55.398: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/11/23 14:04:55.398
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/11/23 14:05:00.45
  May 11 14:05:00.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-3419 get pod e2e-test-httpd-pod -o json'
  May 11 14:05:00.510: INFO: stderr: ""
  May 11 14:05:00.510: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"92145303c34d0b5b357e8026c836bdc628c151fc7c1e8c676d4a0eee986e36eb\",\n            \"cni.projectcalico.org/podIP\": \"192.168.153.28/32\",\n            \"cni.projectcalico.org/podIPs\": \"192.168.153.28/32\"\n        },\n        \"creationTimestamp\": \"2023-05-11T14:04:55Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3419\",\n        \"resourceVersion\": \"20036\",\n        \"uid\": \"978d4138-267c-49da-835f-cabad596fca2\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-j4xdj\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"macpro-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-j4xdj\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-11T14:04:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-11T14:04:56Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-11T14:04:56Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-11T14:04:55Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://9d4a6d70d68816fe9c1f828de4bee6b7c7a8307611bcd8d1176803653e49b9d5\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-11T14:04:56Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"10.221.188.13\",\n        \"phase\": \"Running\",\n        \"podIP\": \"192.168.153.28\",\n        \"podIPs\": [\n            {\n                \"ip\": \"192.168.153.28\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-11T14:04:55Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/11/23 14:05:00.51
  May 11 14:05:00.510: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-3419 replace -f -'
  May 11 14:05:00.774: INFO: stderr: ""
  May 11 14:05:00.774: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/11/23 14:05:00.774
  May 11 14:05:00.776: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-3419 delete pods e2e-test-httpd-pod'
  May 11 14:05:02.779: INFO: stderr: ""
  May 11 14:05:02.779: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 11 14:05:02.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3419" for this suite. @ 05/11/23 14:05:02.786
• [7.478 seconds]
------------------------------
S
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/11/23 14:05:02.791
  May 11 14:05:02.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-preemption @ 05/11/23 14:05:02.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:05:02.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:05:02.808
  May 11 14:05:02.819: INFO: Waiting up to 1m0s for all nodes to be ready
  May 11 14:06:02.850: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/11/23 14:06:02.852
  May 11 14:06:02.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/11/23 14:06:02.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:06:02.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:06:02.874
  May 11 14:06:02.886: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May 11 14:06:02.888: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May 11 14:06:02.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 14:06:02.913: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-59" for this suite. @ 05/11/23 14:06:02.946
  STEP: Destroying namespace "sched-preemption-1077" for this suite. @ 05/11/23 14:06:02.951
• [60.164 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/11/23 14:06:02.956
  May 11 14:06:02.956: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename svcaccounts @ 05/11/23 14:06:02.956
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:06:02.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:06:02.97
  STEP: Creating a pod to test service account token:  @ 05/11/23 14:06:02.972
  STEP: Saw pod success @ 05/11/23 14:06:06.99
  May 11 14:06:06.993: INFO: Trying to get logs from node macpro-2 pod test-pod-6e30b47b-4493-4447-9057-e1ecfe3c9507 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:06:07.007
  May 11 14:06:07.020: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7333" for this suite. @ 05/11/23 14:06:07.024
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/11/23 14:06:07.03
  May 11 14:06:07.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:06:07.031
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:06:07.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:06:07.047
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 14:06:07.049
  STEP: Saw pod success @ 05/11/23 14:06:11.072
  May 11 14:06:11.075: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-0bb8b047-4579-4e70-b3f9-dad819265947 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 14:06:11.08
  May 11 14:06:11.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2428" for this suite. @ 05/11/23 14:06:11.096
• [4.071 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/11/23 14:06:11.101
  May 11 14:06:11.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename endpointslice @ 05/11/23 14:06:11.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:06:11.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:06:11.126
  May 11 14:06:11.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4059" for this suite. @ 05/11/23 14:06:11.193
• [0.098 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/11/23 14:06:11.199
  May 11 14:06:11.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename var-expansion @ 05/11/23 14:06:11.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:06:11.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:06:11.228
  STEP: Creating a pod to test substitution in container's command @ 05/11/23 14:06:11.23
  STEP: Saw pod success @ 05/11/23 14:06:15.25
  May 11 14:06:15.253: INFO: Trying to get logs from node macpro-2 pod var-expansion-3af8232e-5b59-443a-aebe-8aabbb0e0863 container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 14:06:15.258
  May 11 14:06:15.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5706" for this suite. @ 05/11/23 14:06:15.277
• [4.083 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/11/23 14:06:15.283
  May 11 14:06:15.284: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename proxy @ 05/11/23 14:06:15.284
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:06:15.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:06:15.305
  STEP: starting an echo server on multiple ports @ 05/11/23 14:06:15.32
  STEP: creating replication controller proxy-service-9xsvh in namespace proxy-3871 @ 05/11/23 14:06:15.32
  I0511 14:06:15.328838      24 runners.go:194] Created replication controller with name: proxy-service-9xsvh, namespace: proxy-3871, replica count: 1
  I0511 14:06:16.380287      24 runners.go:194] proxy-service-9xsvh Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0511 14:06:17.381120      24 runners.go:194] proxy-service-9xsvh Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 14:06:17.384: INFO: setup took 2.076559687s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/11/23 14:06:17.384
  May 11 14:06:17.399: INFO: (0) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 14.136341ms)
  May 11 14:06:17.399: INFO: (0) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 14.154458ms)
  May 11 14:06:17.399: INFO: (0) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 14.204368ms)
  May 11 14:06:17.399: INFO: (0) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 14.43452ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 15.45631ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 15.565301ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 15.530333ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 15.526664ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 15.570389ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 15.650652ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 15.660244ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 15.652063ms)
  May 11 14:06:17.400: INFO: (0) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 15.73562ms)
  May 11 14:06:17.419: INFO: (0) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 34.337112ms)
  May 11 14:06:17.420: INFO: (0) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 35.112065ms)
  May 11 14:06:17.420: INFO: (0) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 35.1086ms)
  May 11 14:06:17.432: INFO: (1) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 12.496067ms)
  May 11 14:06:17.432: INFO: (1) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 12.444507ms)
  May 11 14:06:17.432: INFO: (1) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 12.469423ms)
  May 11 14:06:17.433: INFO: (1) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 12.792664ms)
  May 11 14:06:17.433: INFO: (1) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 12.787098ms)
  May 11 14:06:17.433: INFO: (1) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 12.913743ms)
  May 11 14:06:17.433: INFO: (1) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 12.9263ms)
  May 11 14:06:17.433: INFO: (1) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 13.060825ms)
  May 11 14:06:17.433: INFO: (1) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 12.963263ms)
  May 11 14:06:17.433: INFO: (1) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 12.962295ms)
  May 11 14:06:17.435: INFO: (1) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 14.826693ms)
  May 11 14:06:17.436: INFO: (1) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 15.811498ms)
  May 11 14:06:17.437: INFO: (1) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 16.848001ms)
  May 11 14:06:17.437: INFO: (1) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 17.080444ms)
  May 11 14:06:17.437: INFO: (1) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 17.033181ms)
  May 11 14:06:17.437: INFO: (1) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 17.241856ms)
  May 11 14:06:17.444: INFO: (2) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 7.333571ms)
  May 11 14:06:17.445: INFO: (2) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 7.960409ms)
  May 11 14:06:17.447: INFO: (2) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.236847ms)
  May 11 14:06:17.447: INFO: (2) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 10.233892ms)
  May 11 14:06:17.449: INFO: (2) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 12.013681ms)
  May 11 14:06:17.449: INFO: (2) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 12.023463ms)
  May 11 14:06:17.449: INFO: (2) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 12.253902ms)
  May 11 14:06:17.449: INFO: (2) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 12.448091ms)
  May 11 14:06:17.450: INFO: (2) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 12.4929ms)
  May 11 14:06:17.450: INFO: (2) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 12.955099ms)
  May 11 14:06:17.450: INFO: (2) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 12.968891ms)
  May 11 14:06:17.450: INFO: (2) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 13.145773ms)
  May 11 14:06:17.451: INFO: (2) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 13.504856ms)
  May 11 14:06:17.453: INFO: (2) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 15.855831ms)
  May 11 14:06:17.454: INFO: (2) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 16.682808ms)
  May 11 14:06:17.454: INFO: (2) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 16.725561ms)
  May 11 14:06:17.462: INFO: (3) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 8.295182ms)
  May 11 14:06:17.467: INFO: (3) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 12.851493ms)
  May 11 14:06:17.469: INFO: (3) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 14.832865ms)
  May 11 14:06:17.469: INFO: (3) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 15.000737ms)
  May 11 14:06:17.469: INFO: (3) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 14.987742ms)
  May 11 14:06:17.469: INFO: (3) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 15.039494ms)
  May 11 14:06:17.469: INFO: (3) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 15.058758ms)
  May 11 14:06:17.469: INFO: (3) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 15.07143ms)
  May 11 14:06:17.470: INFO: (3) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 15.585839ms)
  May 11 14:06:17.470: INFO: (3) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 15.727589ms)
  May 11 14:06:17.471: INFO: (3) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 17.216234ms)
  May 11 14:06:17.471: INFO: (3) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 17.243471ms)
  May 11 14:06:17.471: INFO: (3) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 17.436117ms)
  May 11 14:06:17.471: INFO: (3) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 17.337612ms)
  May 11 14:06:17.472: INFO: (3) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 17.576706ms)
  May 11 14:06:17.472: INFO: (3) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 17.651605ms)
  May 11 14:06:17.476: INFO: (4) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 4.635284ms)
  May 11 14:06:17.482: INFO: (4) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 10.742132ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.818864ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 11.182587ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 11.197581ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 11.2646ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 11.265965ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 11.297651ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 11.333401ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 11.455201ms)
  May 11 14:06:17.483: INFO: (4) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 11.488731ms)
  May 11 14:06:17.484: INFO: (4) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 12.10864ms)
  May 11 14:06:17.485: INFO: (4) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 13.272585ms)
  May 11 14:06:17.486: INFO: (4) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 14.071476ms)
  May 11 14:06:17.486: INFO: (4) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 14.204484ms)
  May 11 14:06:17.486: INFO: (4) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 14.256631ms)
  May 11 14:06:17.492: INFO: (5) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 6.489601ms)
  May 11 14:06:17.493: INFO: (5) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 6.516417ms)
  May 11 14:06:17.493: INFO: (5) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 6.56124ms)
  May 11 14:06:17.493: INFO: (5) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 6.561455ms)
  May 11 14:06:17.493: INFO: (5) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 6.55039ms)
  May 11 14:06:17.496: INFO: (5) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 9.703606ms)
  May 11 14:06:17.496: INFO: (5) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 9.797385ms)
  May 11 14:06:17.496: INFO: (5) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 9.837589ms)
  May 11 14:06:17.496: INFO: (5) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 9.938348ms)
  May 11 14:06:17.496: INFO: (5) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 9.950948ms)
  May 11 14:06:17.496: INFO: (5) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 10.091577ms)
  May 11 14:06:17.498: INFO: (5) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 11.800853ms)
  May 11 14:06:17.499: INFO: (5) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 12.597938ms)
  May 11 14:06:17.499: INFO: (5) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 12.708605ms)
  May 11 14:06:17.499: INFO: (5) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 12.771593ms)
  May 11 14:06:17.499: INFO: (5) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 12.796824ms)
  May 11 14:06:17.504: INFO: (6) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 5.172076ms)
  May 11 14:06:17.508: INFO: (6) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 8.748609ms)
  May 11 14:06:17.509: INFO: (6) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 9.857846ms)
  May 11 14:06:17.510: INFO: (6) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 10.653616ms)
  May 11 14:06:17.510: INFO: (6) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.603487ms)
  May 11 14:06:17.510: INFO: (6) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 10.905706ms)
  May 11 14:06:17.510: INFO: (6) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 10.936599ms)
  May 11 14:06:17.510: INFO: (6) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 10.977855ms)
  May 11 14:06:17.510: INFO: (6) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 11.051421ms)
  May 11 14:06:17.510: INFO: (6) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 11.024959ms)
  May 11 14:06:17.510: INFO: (6) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 11.199449ms)
  May 11 14:06:17.511: INFO: (6) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 11.732785ms)
  May 11 14:06:17.511: INFO: (6) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 11.797941ms)
  May 11 14:06:17.511: INFO: (6) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 11.930431ms)
  May 11 14:06:17.511: INFO: (6) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 12.493509ms)
  May 11 14:06:17.512: INFO: (6) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 12.59243ms)
  May 11 14:06:17.516: INFO: (7) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 4.052165ms)
  May 11 14:06:17.518: INFO: (7) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 6.16119ms)
  May 11 14:06:17.519: INFO: (7) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 6.968094ms)
  May 11 14:06:17.519: INFO: (7) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 7.068536ms)
  May 11 14:06:17.520: INFO: (7) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 8.124307ms)
  May 11 14:06:17.520: INFO: (7) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 8.137054ms)
  May 11 14:06:17.520: INFO: (7) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 8.150751ms)
  May 11 14:06:17.520: INFO: (7) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 8.263563ms)
  May 11 14:06:17.521: INFO: (7) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 9.335679ms)
  May 11 14:06:17.522: INFO: (7) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 9.960229ms)
  May 11 14:06:17.523: INFO: (7) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 11.363191ms)
  May 11 14:06:17.524: INFO: (7) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 12.290826ms)
  May 11 14:06:17.524: INFO: (7) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 12.278798ms)
  May 11 14:06:17.524: INFO: (7) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 12.49748ms)
  May 11 14:06:17.524: INFO: (7) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 12.470572ms)
  May 11 14:06:17.524: INFO: (7) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 12.59739ms)
  May 11 14:06:17.533: INFO: (8) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 8.252023ms)
  May 11 14:06:17.533: INFO: (8) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 8.352438ms)
  May 11 14:06:17.535: INFO: (8) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 10.491158ms)
  May 11 14:06:17.535: INFO: (8) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 10.597675ms)
  May 11 14:06:17.535: INFO: (8) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.536748ms)
  May 11 14:06:17.535: INFO: (8) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.722217ms)
  May 11 14:06:17.535: INFO: (8) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 10.739376ms)
  May 11 14:06:17.535: INFO: (8) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 10.814079ms)
  May 11 14:06:17.535: INFO: (8) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 10.964045ms)
  May 11 14:06:17.536: INFO: (8) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 11.502338ms)
  May 11 14:06:17.536: INFO: (8) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 11.521144ms)
  May 11 14:06:17.536: INFO: (8) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 11.583007ms)
  May 11 14:06:17.538: INFO: (8) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 13.841476ms)
  May 11 14:06:17.538: INFO: (8) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 13.843672ms)
  May 11 14:06:17.538: INFO: (8) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 13.943042ms)
  May 11 14:06:17.538: INFO: (8) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 13.930683ms)
  May 11 14:06:17.549: INFO: (9) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.212235ms)
  May 11 14:06:17.549: INFO: (9) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 10.27174ms)
  May 11 14:06:17.551: INFO: (9) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 12.623155ms)
  May 11 14:06:17.551: INFO: (9) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 12.634974ms)
  May 11 14:06:17.551: INFO: (9) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 12.690874ms)
  May 11 14:06:17.551: INFO: (9) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 12.691084ms)
  May 11 14:06:17.552: INFO: (9) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 13.357541ms)
  May 11 14:06:17.552: INFO: (9) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 13.417845ms)
  May 11 14:06:17.552: INFO: (9) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 13.958528ms)
  May 11 14:06:17.552: INFO: (9) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 13.989535ms)
  May 11 14:06:17.552: INFO: (9) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 13.949083ms)
  May 11 14:06:17.553: INFO: (9) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 14.149248ms)
  May 11 14:06:17.553: INFO: (9) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 14.226098ms)
  May 11 14:06:17.553: INFO: (9) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 14.296157ms)
  May 11 14:06:17.553: INFO: (9) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 14.245787ms)
  May 11 14:06:17.553: INFO: (9) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 14.258715ms)
  May 11 14:06:17.558: INFO: (10) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 5.04197ms)
  May 11 14:06:17.564: INFO: (10) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 11.683514ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 11.825312ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 12.025307ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 11.964627ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 11.987462ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 12.176359ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 12.142848ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 12.195077ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 12.213155ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 12.181626ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 12.38102ms)
  May 11 14:06:17.565: INFO: (10) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 12.447915ms)
  May 11 14:06:17.568: INFO: (10) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 14.741554ms)
  May 11 14:06:17.568: INFO: (10) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 14.832229ms)
  May 11 14:06:17.568: INFO: (10) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 14.853283ms)
  May 11 14:06:17.573: INFO: (11) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 5.18003ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 15.259515ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 15.289136ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 15.278897ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 15.43749ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 15.428101ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 15.474716ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 15.42588ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 15.671697ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 15.661728ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 15.749028ms)
  May 11 14:06:17.583: INFO: (11) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 15.665269ms)
  May 11 14:06:17.584: INFO: (11) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 16.240863ms)
  May 11 14:06:17.584: INFO: (11) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 16.316496ms)
  May 11 14:06:17.584: INFO: (11) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 16.33917ms)
  May 11 14:06:17.584: INFO: (11) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 16.425745ms)
  May 11 14:06:17.590: INFO: (12) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 5.502142ms)
  May 11 14:06:17.595: INFO: (12) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.736618ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 12.36305ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 12.382901ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 12.432811ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 12.412309ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 12.736782ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 12.876494ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 12.810039ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 12.885095ms)
  May 11 14:06:17.597: INFO: (12) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 13.169104ms)
  May 11 14:06:17.598: INFO: (12) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 13.360944ms)
  May 11 14:06:17.598: INFO: (12) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 13.371966ms)
  May 11 14:06:17.598: INFO: (12) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 13.560091ms)
  May 11 14:06:17.598: INFO: (12) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 13.951121ms)
  May 11 14:06:17.599: INFO: (12) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 14.759376ms)
  May 11 14:06:17.607: INFO: (13) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 7.702298ms)
  May 11 14:06:17.613: INFO: (13) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 14.251071ms)
  May 11 14:06:17.613: INFO: (13) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 14.223655ms)
  May 11 14:06:17.613: INFO: (13) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 14.266895ms)
  May 11 14:06:17.613: INFO: (13) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 14.347168ms)
  May 11 14:06:17.614: INFO: (13) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 14.662936ms)
  May 11 14:06:17.614: INFO: (13) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 14.751606ms)
  May 11 14:06:17.614: INFO: (13) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 14.756566ms)
  May 11 14:06:17.614: INFO: (13) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 14.781977ms)
  May 11 14:06:17.614: INFO: (13) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 14.772594ms)
  May 11 14:06:17.614: INFO: (13) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 14.909636ms)
  May 11 14:06:17.617: INFO: (13) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 17.365185ms)
  May 11 14:06:17.618: INFO: (13) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 18.728737ms)
  May 11 14:06:17.618: INFO: (13) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 18.847543ms)
  May 11 14:06:17.618: INFO: (13) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 19.271897ms)
  May 11 14:06:17.619: INFO: (13) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 19.358047ms)
  May 11 14:06:17.625: INFO: (14) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 6.208431ms)
  May 11 14:06:17.628: INFO: (14) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 9.462451ms)
  May 11 14:06:17.628: INFO: (14) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 9.536104ms)
  May 11 14:06:17.629: INFO: (14) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 10.596378ms)
  May 11 14:06:17.629: INFO: (14) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 10.716375ms)
  May 11 14:06:17.629: INFO: (14) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 10.680893ms)
  May 11 14:06:17.630: INFO: (14) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 10.854219ms)
  May 11 14:06:17.630: INFO: (14) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 11.005346ms)
  May 11 14:06:17.630: INFO: (14) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 10.989998ms)
  May 11 14:06:17.630: INFO: (14) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 11.492021ms)
  May 11 14:06:17.631: INFO: (14) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 12.577636ms)
  May 11 14:06:17.631: INFO: (14) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 12.684871ms)
  May 11 14:06:17.632: INFO: (14) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 13.185064ms)
  May 11 14:06:17.632: INFO: (14) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 13.561482ms)
  May 11 14:06:17.632: INFO: (14) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 13.612954ms)
  May 11 14:06:17.632: INFO: (14) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 13.751557ms)
  May 11 14:06:17.638: INFO: (15) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 5.565849ms)
  May 11 14:06:17.638: INFO: (15) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 5.599557ms)
  May 11 14:06:17.639: INFO: (15) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 6.448346ms)
  May 11 14:06:17.639: INFO: (15) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 6.625751ms)
  May 11 14:06:17.641: INFO: (15) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 8.193645ms)
  May 11 14:06:17.641: INFO: (15) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 8.548854ms)
  May 11 14:06:17.641: INFO: (15) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 8.548905ms)
  May 11 14:06:17.641: INFO: (15) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 8.727239ms)
  May 11 14:06:17.642: INFO: (15) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 9.345418ms)
  May 11 14:06:17.642: INFO: (15) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 9.448476ms)
  May 11 14:06:17.643: INFO: (15) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 10.657915ms)
  May 11 14:06:17.644: INFO: (15) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 11.665954ms)
  May 11 14:06:17.644: INFO: (15) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 11.941268ms)
  May 11 14:06:17.644: INFO: (15) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 11.960793ms)
  May 11 14:06:17.644: INFO: (15) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 11.980387ms)
  May 11 14:06:17.644: INFO: (15) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 11.929508ms)
  May 11 14:06:17.651: INFO: (16) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 6.104554ms)
  May 11 14:06:17.654: INFO: (16) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 9.937588ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 9.982707ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 9.938103ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 9.992375ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 10.131415ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 10.032965ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.095415ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 10.201238ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 10.233492ms)
  May 11 14:06:17.655: INFO: (16) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.296331ms)
  May 11 14:06:17.662: INFO: (16) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 17.065877ms)
  May 11 14:06:17.662: INFO: (16) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 17.562944ms)
  May 11 14:06:17.662: INFO: (16) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 17.658482ms)
  May 11 14:06:17.662: INFO: (16) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 17.60459ms)
  May 11 14:06:17.662: INFO: (16) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 17.622735ms)
  May 11 14:06:17.677: INFO: (17) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 14.784743ms)
  May 11 14:06:17.677: INFO: (17) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 14.810551ms)
  May 11 14:06:17.677: INFO: (17) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 14.846948ms)
  May 11 14:06:17.677: INFO: (17) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 14.834908ms)
  May 11 14:06:17.677: INFO: (17) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 14.896124ms)
  May 11 14:06:17.677: INFO: (17) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 14.93184ms)
  May 11 14:06:17.677: INFO: (17) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 15.024726ms)
  May 11 14:06:17.678: INFO: (17) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 15.379408ms)
  May 11 14:06:17.678: INFO: (17) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 15.425897ms)
  May 11 14:06:17.678: INFO: (17) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 15.417984ms)
  May 11 14:06:17.680: INFO: (17) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 17.842185ms)
  May 11 14:06:17.681: INFO: (17) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 19.14406ms)
  May 11 14:06:17.682: INFO: (17) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 19.326627ms)
  May 11 14:06:17.682: INFO: (17) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 19.496961ms)
  May 11 14:06:17.682: INFO: (17) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 19.553948ms)
  May 11 14:06:17.682: INFO: (17) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 19.60397ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 17.511656ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 17.553577ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 17.76363ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 17.870415ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 17.81334ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 17.841309ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 17.836624ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 18.034862ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 18.119973ms)
  May 11 14:06:17.700: INFO: (18) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 18.247654ms)
  May 11 14:06:17.702: INFO: (18) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 20.59867ms)
  May 11 14:06:17.705: INFO: (18) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 23.27401ms)
  May 11 14:06:17.705: INFO: (18) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 23.485599ms)
  May 11 14:06:17.705: INFO: (18) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 23.468578ms)
  May 11 14:06:17.705: INFO: (18) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 23.484257ms)
  May 11 14:06:17.705: INFO: (18) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 23.52257ms)
  May 11 14:06:17.716: INFO: (19) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 10.638962ms)
  May 11 14:06:17.716: INFO: (19) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 10.673301ms)
  May 11 14:06:17.716: INFO: (19) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:162/proxy/: bar (200; 10.923627ms)
  May 11 14:06:17.717: INFO: (19) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">test<... (200; 10.980263ms)
  May 11 14:06:17.717: INFO: (19) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:160/proxy/: foo (200; 11.069307ms)
  May 11 14:06:17.717: INFO: (19) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:462/proxy/: tls qux (200; 11.04533ms)
  May 11 14:06:17.717: INFO: (19) /api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/http:proxy-service-9xsvh-mrx47:1080/proxy/rewriteme">... (200; 11.060903ms)
  May 11 14:06:17.717: INFO: (19) /api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/proxy-service-9xsvh-mrx47/proxy/rewriteme">test</a> (200; 11.197223ms)
  May 11 14:06:17.717: INFO: (19) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/: <a href="/api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:443/proxy/tlsrewritem... (200; 11.069085ms)
  May 11 14:06:17.717: INFO: (19) /api/v1/namespaces/proxy-3871/pods/https:proxy-service-9xsvh-mrx47:460/proxy/: tls baz (200; 11.148222ms)
  May 11 14:06:17.722: INFO: (19) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname1/proxy/: tls baz (200; 16.672912ms)
  May 11 14:06:17.722: INFO: (19) /api/v1/namespaces/proxy-3871/services/https:proxy-service-9xsvh:tlsportname2/proxy/: tls qux (200; 16.6444ms)
  May 11 14:06:17.722: INFO: (19) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname2/proxy/: bar (200; 16.717574ms)
  May 11 14:06:17.722: INFO: (19) /api/v1/namespaces/proxy-3871/services/http:proxy-service-9xsvh:portname1/proxy/: foo (200; 16.67644ms)
  May 11 14:06:17.722: INFO: (19) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname2/proxy/: bar (200; 16.724784ms)
  May 11 14:06:17.722: INFO: (19) /api/v1/namespaces/proxy-3871/services/proxy-service-9xsvh:portname1/proxy/: foo (200; 16.732811ms)
  May 11 14:06:17.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-9xsvh in namespace proxy-3871, will wait for the garbage collector to delete the pods @ 05/11/23 14:06:17.734
  May 11 14:06:17.803: INFO: Deleting ReplicationController proxy-service-9xsvh took: 6.856871ms
  May 11 14:06:17.903: INFO: Terminating ReplicationController proxy-service-9xsvh pods took: 100.076459ms
  STEP: Destroying namespace "proxy-3871" for this suite. @ 05/11/23 14:06:20.303
• [5.026 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/11/23 14:06:20.31
  May 11 14:06:20.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename runtimeclass @ 05/11/23 14:06:20.311
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:06:20.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:06:20.333
  May 11 14:06:22.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9860" for this suite. @ 05/11/23 14:06:22.364
• [2.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/11/23 14:06:22.37
  May 11 14:06:22.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 14:06:22.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:06:22.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:06:22.392
  STEP: Creating pod test-grpc-ea850405-b6d8-471b-8913-3d364d4e0501 in namespace container-probe-5418 @ 05/11/23 14:06:22.394
  May 11 14:06:24.430: INFO: Started pod test-grpc-ea850405-b6d8-471b-8913-3d364d4e0501 in namespace container-probe-5418
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/11/23 14:06:24.43
  May 11 14:06:24.434: INFO: Initial restart count of pod test-grpc-ea850405-b6d8-471b-8913-3d364d4e0501 is 0
  May 11 14:07:38.644: INFO: Restart count of pod container-probe-5418/test-grpc-ea850405-b6d8-471b-8913-3d364d4e0501 is now 1 (1m14.210087638s elapsed)
  May 11 14:07:38.644: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 14:07:38.649
  STEP: Destroying namespace "container-probe-5418" for this suite. @ 05/11/23 14:07:38.659
• [76.297 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/11/23 14:07:38.668
  May 11 14:07:38.668: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename gc @ 05/11/23 14:07:38.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:07:38.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:07:38.695
  STEP: create the rc @ 05/11/23 14:07:38.703
  W0511 14:07:38.710274      24 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/11/23 14:07:44.79
  STEP: wait for the rc to be deleted @ 05/11/23 14:07:44.812
  May 11 14:07:45.832: INFO: 80 pods remaining
  May 11 14:07:45.832: INFO: 80 pods has nil DeletionTimestamp
  May 11 14:07:45.832: INFO: 
  May 11 14:07:48.455: INFO: 58 pods remaining
  May 11 14:07:48.455: INFO: 52 pods has nil DeletionTimestamp
  May 11 14:07:48.455: INFO: 
  May 11 14:07:48.901: INFO: 45 pods remaining
  May 11 14:07:48.901: INFO: 43 pods has nil DeletionTimestamp
  May 11 14:07:48.901: INFO: 
  May 11 14:07:49.926: INFO: 38 pods remaining
  May 11 14:07:49.926: INFO: 36 pods has nil DeletionTimestamp
  May 11 14:07:49.926: INFO: 
  May 11 14:07:50.833: INFO: 17 pods remaining
  May 11 14:07:50.833: INFO: 17 pods has nil DeletionTimestamp
  May 11 14:07:50.833: INFO: 
  May 11 14:07:51.824: INFO: 15 pods remaining
  May 11 14:07:51.824: INFO: 6 pods has nil DeletionTimestamp
  May 11 14:07:51.824: INFO: 
  STEP: Gathering metrics @ 05/11/23 14:07:52.881
  May 11 14:07:53.330: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 11 14:07:53.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5232" for this suite. @ 05/11/23 14:07:53.334
• [14.672 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/11/23 14:07:53.34
  May 11 14:07:53.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename var-expansion @ 05/11/23 14:07:53.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:07:53.365
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:07:53.373
  STEP: Creating a pod to test env composition @ 05/11/23 14:07:53.375
  STEP: Saw pod success @ 05/11/23 14:08:05.669
  May 11 14:08:05.680: INFO: Trying to get logs from node macpro-2 pod var-expansion-229a6157-f3cb-4579-852c-20de365a3ff3 container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 14:08:05.693
  May 11 14:08:05.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7056" for this suite. @ 05/11/23 14:08:05.738
• [12.404 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/11/23 14:08:05.744
  May 11 14:08:05.744: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename statefulset @ 05/11/23 14:08:05.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:08:05.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:08:05.769
  STEP: Creating service test in namespace statefulset-8311 @ 05/11/23 14:08:05.772
  STEP: Looking for a node to schedule stateful set and pod @ 05/11/23 14:08:05.777
  STEP: Creating pod with conflicting port in namespace statefulset-8311 @ 05/11/23 14:08:05.782
  STEP: Waiting until pod test-pod will start running in namespace statefulset-8311 @ 05/11/23 14:08:05.795
  STEP: Creating statefulset with conflicting port in namespace statefulset-8311 @ 05/11/23 14:08:07.804
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-8311 @ 05/11/23 14:08:07.809
  May 11 14:08:07.834: INFO: Observed stateful pod in namespace: statefulset-8311, name: ss-0, uid: c1122220-8b32-4121-8d1e-5a36c0fcf413, status phase: Pending. Waiting for statefulset controller to delete.
  May 11 14:08:07.851: INFO: Observed stateful pod in namespace: statefulset-8311, name: ss-0, uid: c1122220-8b32-4121-8d1e-5a36c0fcf413, status phase: Failed. Waiting for statefulset controller to delete.
  May 11 14:08:07.881: INFO: Observed stateful pod in namespace: statefulset-8311, name: ss-0, uid: c1122220-8b32-4121-8d1e-5a36c0fcf413, status phase: Failed. Waiting for statefulset controller to delete.
  May 11 14:08:07.884: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-8311
  STEP: Removing pod with conflicting port in namespace statefulset-8311 @ 05/11/23 14:08:07.884
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-8311 and will be in running state @ 05/11/23 14:08:07.894
  May 11 14:08:09.901: INFO: Deleting all statefulset in ns statefulset-8311
  May 11 14:08:09.903: INFO: Scaling statefulset ss to 0
  May 11 14:08:19.919: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 14:08:19.922: INFO: Deleting statefulset ss
  May 11 14:08:19.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8311" for this suite. @ 05/11/23 14:08:19.94
• [14.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/11/23 14:08:19.947
  May 11 14:08:19.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename statefulset @ 05/11/23 14:08:19.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:08:19.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:08:19.968
  STEP: Creating service test in namespace statefulset-6473 @ 05/11/23 14:08:19.97
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/11/23 14:08:19.975
  STEP: Creating stateful set ss in namespace statefulset-6473 @ 05/11/23 14:08:19.978
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6473 @ 05/11/23 14:08:19.99
  May 11 14:08:19.993: INFO: Found 0 stateful pods, waiting for 1
  May 11 14:08:29.996: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/11/23 14:08:29.996
  May 11 14:08:29.999: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-6473 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 14:08:30.138: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 14:08:30.138: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 14:08:30.138: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 14:08:30.141: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May 11 14:08:40.146: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 11 14:08:40.146: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 14:08:40.158: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999808s
  May 11 14:08:41.161: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.996752305s
  May 11 14:08:42.164: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.99377273s
  May 11 14:08:43.168: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.99099302s
  May 11 14:08:44.171: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.986907997s
  May 11 14:08:45.174: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.98372448s
  May 11 14:08:46.178: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.980559108s
  May 11 14:08:47.181: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.977251218s
  May 11 14:08:48.184: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.974153918s
  May 11 14:08:49.188: INFO: Verifying statefulset ss doesn't scale past 1 for another 970.314054ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6473 @ 05/11/23 14:08:50.188
  May 11 14:08:50.192: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-6473 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 14:08:50.308: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 11 14:08:50.308: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 14:08:50.308: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 11 14:08:50.312: INFO: Found 1 stateful pods, waiting for 3
  May 11 14:09:00.316: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:09:00.316: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:09:00.316: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/11/23 14:09:00.316
  STEP: Scale down will halt with unhealthy stateful pod @ 05/11/23 14:09:00.316
  May 11 14:09:00.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-6473 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 14:09:00.446: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 14:09:00.446: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 14:09:00.446: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 14:09:00.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-6473 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 14:09:00.581: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 14:09:00.581: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 14:09:00.581: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 14:09:00.581: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-6473 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 14:09:00.711: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 14:09:00.711: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 14:09:00.711: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 14:09:00.711: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 14:09:00.713: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May 11 14:09:10.720: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 11 14:09:10.720: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 11 14:09:10.720: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 11 14:09:10.730: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999977s
  May 11 14:09:11.736: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996515354s
  May 11 14:09:12.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.99130649s
  May 11 14:09:13.744: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.988002878s
  May 11 14:09:14.747: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.984127997s
  May 11 14:09:15.751: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.980002741s
  May 11 14:09:16.754: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.976584513s
  May 11 14:09:17.758: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.973700134s
  May 11 14:09:18.761: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.969342134s
  May 11 14:09:19.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 965.646469ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6473 @ 05/11/23 14:09:20.765
  May 11 14:09:20.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-6473 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 14:09:20.888: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 11 14:09:20.888: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 14:09:20.888: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 11 14:09:20.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-6473 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 14:09:21.008: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 11 14:09:21.008: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 14:09:21.008: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 11 14:09:21.008: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-6473 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 14:09:21.126: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 11 14:09:21.126: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 14:09:21.126: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 11 14:09:21.126: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/11/23 14:09:31.14
  May 11 14:09:31.140: INFO: Deleting all statefulset in ns statefulset-6473
  May 11 14:09:31.143: INFO: Scaling statefulset ss to 0
  May 11 14:09:31.151: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 14:09:31.153: INFO: Deleting statefulset ss
  May 11 14:09:31.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6473" for this suite. @ 05/11/23 14:09:31.164
• [71.222 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/11/23 14:09:31.169
  May 11 14:09:31.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:09:31.17
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:09:31.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:09:31.191
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/11/23 14:09:31.193
  STEP: Saw pod success @ 05/11/23 14:09:35.211
  May 11 14:09:35.214: INFO: Trying to get logs from node macpro-2 pod pod-395a93d6-fb87-41f1-a3a6-7f6ffdc91f03 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:09:35.221
  May 11 14:09:35.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6554" for this suite. @ 05/11/23 14:09:35.241
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/11/23 14:09:35.246
  May 11 14:09:35.246: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename job @ 05/11/23 14:09:35.247
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:09:35.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:09:35.263
  STEP: Creating a job @ 05/11/23 14:09:35.265
  STEP: Ensuring job reaches completions @ 05/11/23 14:09:35.269
  May 11 14:09:45.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9384" for this suite. @ 05/11/23 14:09:45.276
• [10.034 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/11/23 14:09:45.28
  May 11 14:09:45.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 14:09:45.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:09:45.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:09:45.297
  May 11 14:09:45.299: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: creating the pod @ 05/11/23 14:09:45.299
  STEP: submitting the pod to kubernetes @ 05/11/23 14:09:45.299
  May 11 14:09:47.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6616" for this suite. @ 05/11/23 14:09:47.391
• [2.117 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/11/23 14:09:47.397
  May 11 14:09:47.397: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 14:09:47.397
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:09:47.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:09:47.418
  STEP: Creating pod busybox-c7f72d13-3e66-4a1e-afce-7d2652a8ea2b in namespace container-probe-2261 @ 05/11/23 14:09:47.421
  May 11 14:09:49.434: INFO: Started pod busybox-c7f72d13-3e66-4a1e-afce-7d2652a8ea2b in namespace container-probe-2261
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/11/23 14:09:49.434
  May 11 14:09:49.437: INFO: Initial restart count of pod busybox-c7f72d13-3e66-4a1e-afce-7d2652a8ea2b is 0
  May 11 14:13:49.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 14:13:50
  STEP: Destroying namespace "container-probe-2261" for this suite. @ 05/11/23 14:13:50.009
• [242.618 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/11/23 14:13:50.016
  May 11 14:13:50.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename containers @ 05/11/23 14:13:50.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:13:50.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:13:50.034
  May 11 14:13:52.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1256" for this suite. @ 05/11/23 14:13:52.066
• [2.055 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/11/23 14:13:52.071
  May 11 14:13:52.071: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 14:13:52.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:13:52.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:13:52.088
  STEP: Creating pod liveness-247e9d8b-f568-4e9d-8188-e0ddbe82fa9f in namespace container-probe-3937 @ 05/11/23 14:13:52.09
  May 11 14:13:54.102: INFO: Started pod liveness-247e9d8b-f568-4e9d-8188-e0ddbe82fa9f in namespace container-probe-3937
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/11/23 14:13:54.102
  May 11 14:13:54.104: INFO: Initial restart count of pod liveness-247e9d8b-f568-4e9d-8188-e0ddbe82fa9f is 0
  May 11 14:14:14.149: INFO: Restart count of pod container-probe-3937/liveness-247e9d8b-f568-4e9d-8188-e0ddbe82fa9f is now 1 (20.045552039s elapsed)
  May 11 14:14:34.195: INFO: Restart count of pod container-probe-3937/liveness-247e9d8b-f568-4e9d-8188-e0ddbe82fa9f is now 2 (40.091449838s elapsed)
  May 11 14:14:54.238: INFO: Restart count of pod container-probe-3937/liveness-247e9d8b-f568-4e9d-8188-e0ddbe82fa9f is now 3 (1m0.133850312s elapsed)
  May 11 14:15:14.284: INFO: Restart count of pod container-probe-3937/liveness-247e9d8b-f568-4e9d-8188-e0ddbe82fa9f is now 4 (1m20.180544837s elapsed)
  May 11 14:16:22.451: INFO: Restart count of pod container-probe-3937/liveness-247e9d8b-f568-4e9d-8188-e0ddbe82fa9f is now 5 (2m28.347270604s elapsed)
  May 11 14:16:22.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 14:16:22.455
  STEP: Destroying namespace "container-probe-3937" for this suite. @ 05/11/23 14:16:22.464
• [150.401 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/11/23 14:16:22.475
  May 11 14:16:22.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 14:16:22.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:16:22.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:16:22.51
  STEP: Creating a pod to test downward api env vars @ 05/11/23 14:16:22.512
  STEP: Saw pod success @ 05/11/23 14:16:26.533
  May 11 14:16:26.535: INFO: Trying to get logs from node macpro-2 pod downward-api-cf85d2d6-5eff-4085-b1c7-5ad514b8bf77 container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 14:16:26.551
  May 11 14:16:26.562: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3762" for this suite. @ 05/11/23 14:16:26.566
• [4.096 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/11/23 14:16:26.572
  May 11 14:16:26.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-webhook @ 05/11/23 14:16:26.572
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:16:26.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:16:26.593
  STEP: Setting up server cert @ 05/11/23 14:16:26.595
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/11/23 14:16:27.292
  STEP: Deploying the custom resource conversion webhook pod @ 05/11/23 14:16:27.298
  STEP: Wait for the deployment to be ready @ 05/11/23 14:16:27.307
  May 11 14:16:27.315: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 14:16:29.323
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:16:29.334
  May 11 14:16:30.334: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 11 14:16:30.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Creating a v1 custom resource @ 05/11/23 14:16:32.9
  STEP: v2 custom resource should be converted @ 05/11/23 14:16:32.904
  May 11 14:16:32.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-4245" for this suite. @ 05/11/23 14:16:33.486
• [6.921 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/11/23 14:16:33.493
  May 11 14:16:33.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename cronjob @ 05/11/23 14:16:33.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:16:33.523
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:16:33.533
  STEP: Creating a cronjob @ 05/11/23 14:16:33.535
  STEP: Ensuring more than one job is running at a time @ 05/11/23 14:16:33.54
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/11/23 14:18:01.544
  STEP: Removing cronjob @ 05/11/23 14:18:01.547
  May 11 14:18:01.550: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2996" for this suite. @ 05/11/23 14:18:01.554
• [88.066 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/11/23 14:18:01.56
  May 11 14:18:01.560: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:18:01.56
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:01.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:01.589
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/11/23 14:18:01.592
  STEP: Saw pod success @ 05/11/23 14:18:05.615
  May 11 14:18:05.618: INFO: Trying to get logs from node macpro-2 pod pod-1d0648b9-d772-415f-9fa1-f76dee8685b1 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:18:05.63
  May 11 14:18:05.643: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3794" for this suite. @ 05/11/23 14:18:05.647
• [4.092 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/11/23 14:18:05.652
  May 11 14:18:05.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 14:18:05.652
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:05.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:05.669
  STEP: Creating secret with name secret-test-cd86e3e1-d156-40fe-80fd-33506d9206c7 @ 05/11/23 14:18:05.672
  STEP: Creating a pod to test consume secrets @ 05/11/23 14:18:05.676
  STEP: Saw pod success @ 05/11/23 14:18:09.697
  May 11 14:18:09.699: INFO: Trying to get logs from node macpro-2 pod pod-secrets-34371346-fb18-4ebd-b172-2c7b9cee4d5d container secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:18:09.704
  May 11 14:18:09.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8132" for this suite. @ 05/11/23 14:18:09.719
• [4.072 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/11/23 14:18:09.724
  May 11 14:18:09.724: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubelet-test @ 05/11/23 14:18:09.725
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:09.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:09.743
  May 11 14:18:11.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-6018" for this suite. @ 05/11/23 14:18:11.778
• [2.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/11/23 14:18:11.784
  May 11 14:18:11.784: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:18:11.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:11.8
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:11.803
  STEP: Creating configMap with name projected-configmap-test-volume-2bf2c684-cf95-40f9-b85d-a6f5fd78bdd1 @ 05/11/23 14:18:11.807
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:18:11.812
  STEP: Saw pod success @ 05/11/23 14:18:15.832
  May 11 14:18:15.835: INFO: Trying to get logs from node macpro-2 pod pod-projected-configmaps-8a66c04c-36da-4201-9dd0-6341db79113d container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:18:15.841
  May 11 14:18:15.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5128" for this suite. @ 05/11/23 14:18:15.863
• [4.086 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/11/23 14:18:15.87
  May 11 14:18:15.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 14:18:15.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:15.888
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:15.891
  STEP: Creating secret with name secret-test-b708a5ae-7cf6-47aa-b34d-183d4cc99082 @ 05/11/23 14:18:15.894
  STEP: Creating a pod to test consume secrets @ 05/11/23 14:18:15.898
  STEP: Saw pod success @ 05/11/23 14:18:19.923
  May 11 14:18:19.926: INFO: Trying to get logs from node macpro-2 pod pod-secrets-e3aab9ad-a6a1-4fe3-9fce-9905f99b7226 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:18:19.931
  May 11 14:18:19.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6184" for this suite. @ 05/11/23 14:18:19.95
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/11/23 14:18:19.958
  May 11 14:18:19.958: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 14:18:19.959
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:19.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:19.978
  STEP: Creating a pod to test downward api env vars @ 05/11/23 14:18:19.981
  STEP: Saw pod success @ 05/11/23 14:18:24.005
  May 11 14:18:24.008: INFO: Trying to get logs from node macpro-2 pod downward-api-6d6a5dea-bd07-49d0-b8a7-aaca46589939 container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 14:18:24.014
  May 11 14:18:24.028: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1925" for this suite. @ 05/11/23 14:18:24.033
• [4.080 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/11/23 14:18:24.038
  May 11 14:18:24.038: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename namespaces @ 05/11/23 14:18:24.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:24.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:24.065
  STEP: Creating a test namespace @ 05/11/23 14:18:24.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:24.089
  STEP: Creating a pod in the namespace @ 05/11/23 14:18:24.092
  STEP: Waiting for the pod to have running status @ 05/11/23 14:18:24.101
  STEP: Deleting the namespace @ 05/11/23 14:18:26.121
  STEP: Waiting for the namespace to be removed. @ 05/11/23 14:18:26.133
  STEP: Recreating the namespace @ 05/11/23 14:18:37.137
  STEP: Verifying there are no pods in the namespace @ 05/11/23 14:18:37.156
  May 11 14:18:37.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7049" for this suite. @ 05/11/23 14:18:37.165
  STEP: Destroying namespace "nsdeletetest-9147" for this suite. @ 05/11/23 14:18:37.172
  May 11 14:18:37.175: INFO: Namespace nsdeletetest-9147 was already deleted
  STEP: Destroying namespace "nsdeletetest-3355" for this suite. @ 05/11/23 14:18:37.175
• [13.142 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/11/23 14:18:37.181
  May 11 14:18:37.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:18:37.182
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:37.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:37.204
  STEP: creating Agnhost RC @ 05/11/23 14:18:37.206
  May 11 14:18:37.206: INFO: namespace kubectl-9824
  May 11 14:18:37.206: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-9824 create -f -'
  May 11 14:18:37.810: INFO: stderr: ""
  May 11 14:18:37.810: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/11/23 14:18:37.81
  May 11 14:18:38.815: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 14:18:38.815: INFO: Found 0 / 1
  May 11 14:18:39.814: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 14:18:39.814: INFO: Found 1 / 1
  May 11 14:18:39.814: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 11 14:18:39.817: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 14:18:39.817: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 11 14:18:39.817: INFO: wait on agnhost-primary startup in kubectl-9824 
  May 11 14:18:39.817: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-9824 logs agnhost-primary-5x4sz agnhost-primary'
  May 11 14:18:39.891: INFO: stderr: ""
  May 11 14:18:39.891: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/11/23 14:18:39.891
  May 11 14:18:39.891: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-9824 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May 11 14:18:39.969: INFO: stderr: ""
  May 11 14:18:39.969: INFO: stdout: "service/rm2 exposed\n"
  May 11 14:18:39.972: INFO: Service rm2 in namespace kubectl-9824 found.
  STEP: exposing service @ 05/11/23 14:18:41.98
  May 11 14:18:41.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-9824 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May 11 14:18:42.073: INFO: stderr: ""
  May 11 14:18:42.073: INFO: stdout: "service/rm3 exposed\n"
  May 11 14:18:42.085: INFO: Service rm3 in namespace kubectl-9824 found.
  May 11 14:18:44.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9824" for this suite. @ 05/11/23 14:18:44.099
• [6.924 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/11/23 14:18:44.106
  May 11 14:18:44.106: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:18:44.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:44.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:44.128
  STEP: Creating projection with secret that has name projected-secret-test-025410ce-1a5d-4924-9b44-dad682b858ff @ 05/11/23 14:18:44.132
  STEP: Creating a pod to test consume secrets @ 05/11/23 14:18:44.138
  STEP: Saw pod success @ 05/11/23 14:18:48.166
  May 11 14:18:48.168: INFO: Trying to get logs from node macpro-2 pod pod-projected-secrets-c5e41d09-23d0-48f5-abfe-c26568f861bd container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:18:48.174
  May 11 14:18:48.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6634" for this suite. @ 05/11/23 14:18:48.193
• [4.093 seconds]
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/11/23 14:18:48.199
  May 11 14:18:48.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replication-controller @ 05/11/23 14:18:48.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:48.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:48.219
  STEP: Creating replication controller my-hostname-basic-f57f3bb0-08f2-4328-925a-0b2777136764 @ 05/11/23 14:18:48.222
  May 11 14:18:48.228: INFO: Pod name my-hostname-basic-f57f3bb0-08f2-4328-925a-0b2777136764: Found 0 pods out of 1
  May 11 14:18:53.233: INFO: Pod name my-hostname-basic-f57f3bb0-08f2-4328-925a-0b2777136764: Found 1 pods out of 1
  May 11 14:18:53.233: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f57f3bb0-08f2-4328-925a-0b2777136764" are running
  May 11 14:18:53.236: INFO: Pod "my-hostname-basic-f57f3bb0-08f2-4328-925a-0b2777136764-cpxfn" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-11 14:18:48 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-11 14:18:49 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-11 14:18:49 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-11 14:18:48 +0000 UTC Reason: Message:}])
  May 11 14:18:53.236: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/11/23 14:18:53.236
  May 11 14:18:53.245: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2550" for this suite. @ 05/11/23 14:18:53.251
• [5.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/11/23 14:18:53.259
  May 11 14:18:53.259: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:18:53.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:53.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:53.282
  STEP: Creating configMap with name configmap-projected-all-test-volume-b7ee6f3f-615d-41ce-ad1a-6fbbb1e10b59 @ 05/11/23 14:18:53.284
  STEP: Creating secret with name secret-projected-all-test-volume-466177a4-17b4-4c60-b187-e8dc8244c5b6 @ 05/11/23 14:18:53.288
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/11/23 14:18:53.293
  STEP: Saw pod success @ 05/11/23 14:18:57.324
  May 11 14:18:57.327: INFO: Trying to get logs from node macpro-2 pod projected-volume-9771f0b7-fc63-4db7-91d8-2f345836f224 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:18:57.332
  May 11 14:18:57.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8867" for this suite. @ 05/11/23 14:18:57.348
• [4.095 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/11/23 14:18:57.354
  May 11 14:18:57.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 14:18:57.355
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:18:57.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:18:57.372
  STEP: Creating a ResourceQuota with best effort scope @ 05/11/23 14:18:57.374
  STEP: Ensuring ResourceQuota status is calculated @ 05/11/23 14:18:57.378
  STEP: Creating a ResourceQuota with not best effort scope @ 05/11/23 14:18:59.383
  STEP: Ensuring ResourceQuota status is calculated @ 05/11/23 14:18:59.387
  STEP: Creating a best-effort pod @ 05/11/23 14:19:01.391
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/11/23 14:19:01.402
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/11/23 14:19:03.406
  STEP: Deleting the pod @ 05/11/23 14:19:05.41
  STEP: Ensuring resource quota status released the pod usage @ 05/11/23 14:19:05.422
  STEP: Creating a not best-effort pod @ 05/11/23 14:19:07.425
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/11/23 14:19:07.435
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/11/23 14:19:09.439
  STEP: Deleting the pod @ 05/11/23 14:19:11.443
  STEP: Ensuring resource quota status released the pod usage @ 05/11/23 14:19:11.456
  May 11 14:19:13.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4618" for this suite. @ 05/11/23 14:19:13.468
• [16.122 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/11/23 14:19:13.477
  May 11 14:19:13.477: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename dns @ 05/11/23 14:19:13.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:19:13.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:19:13.515
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8976.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8976.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/11/23 14:19:13.518
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8976.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8976.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/11/23 14:19:13.518
  STEP: creating a pod to probe /etc/hosts @ 05/11/23 14:19:13.518
  STEP: submitting the pod to kubernetes @ 05/11/23 14:19:13.518
  STEP: retrieving the pod @ 05/11/23 14:19:15.551
  STEP: looking for the results for each expected name from probers @ 05/11/23 14:19:15.554
  May 11 14:19:15.573: INFO: DNS probes using dns-8976/dns-test-f52a8f15-205e-4b44-a9b0-36748dac2207 succeeded

  May 11 14:19:15.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 14:19:15.577
  STEP: Destroying namespace "dns-8976" for this suite. @ 05/11/23 14:19:15.587
• [2.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/11/23 14:19:15.595
  May 11 14:19:15.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename svcaccounts @ 05/11/23 14:19:15.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:19:15.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:19:15.616
  STEP: reading a file in the container @ 05/11/23 14:19:17.64
  May 11 14:19:17.640: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8281 pod-service-account-4c28ae27-95e6-43f9-b52a-be61f2072c86 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/11/23 14:19:17.758
  May 11 14:19:17.758: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8281 pod-service-account-4c28ae27-95e6-43f9-b52a-be61f2072c86 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/11/23 14:19:17.868
  May 11 14:19:17.868: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-8281 pod-service-account-4c28ae27-95e6-43f9-b52a-be61f2072c86 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May 11 14:19:17.983: INFO: Got root ca configmap in namespace "svcaccounts-8281"
  May 11 14:19:17.986: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8281" for this suite. @ 05/11/23 14:19:17.992
• [2.403 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/11/23 14:19:17.999
  May 11 14:19:17.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/11/23 14:19:17.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:19:18.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:19:18.02
  May 11 14:19:18.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:19:18.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9913" for this suite. @ 05/11/23 14:19:18.579
• [0.592 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/11/23 14:19:18.591
  May 11 14:19:18.591: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/11/23 14:19:18.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:19:18.623
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:19:18.626
  STEP: getting /apis @ 05/11/23 14:19:18.628
  STEP: getting /apis/storage.k8s.io @ 05/11/23 14:19:18.633
  STEP: getting /apis/storage.k8s.io/v1 @ 05/11/23 14:19:18.634
  STEP: creating @ 05/11/23 14:19:18.635
  STEP: watching @ 05/11/23 14:19:18.65
  May 11 14:19:18.650: INFO: starting watch
  STEP: getting @ 05/11/23 14:19:18.656
  STEP: listing in namespace @ 05/11/23 14:19:18.659
  STEP: listing across namespaces @ 05/11/23 14:19:18.661
  STEP: patching @ 05/11/23 14:19:18.663
  STEP: updating @ 05/11/23 14:19:18.669
  May 11 14:19:18.679: INFO: waiting for watch events with expected annotations in namespace
  May 11 14:19:18.679: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/11/23 14:19:18.679
  STEP: deleting a collection @ 05/11/23 14:19:18.69
  May 11 14:19:18.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-3154" for this suite. @ 05/11/23 14:19:18.704
• [0.120 seconds]
------------------------------
SS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/11/23 14:19:18.711
  May 11 14:19:18.711: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename svcaccounts @ 05/11/23 14:19:18.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:19:18.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:19:18.735
  May 11 14:19:18.749: INFO: created pod
  STEP: Saw pod success @ 05/11/23 14:19:22.769
  May 11 14:19:52.772: INFO: polling logs
  May 11 14:19:52.804: INFO: Pod logs: 
  I0511 14:19:19.442385       1 log.go:198] OK: Got token
  I0511 14:19:19.442420       1 log.go:198] validating with in-cluster discovery
  I0511 14:19:19.442711       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0511 14:19:19.442737       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1562:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683815359, NotBefore:1683814759, IssuedAt:1683814759, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1562", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ae6d5e97-bfc4-449f-800a-0da4af0ff2c4"}}}
  I0511 14:19:19.455520       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0511 14:19:19.461799       1 log.go:198] OK: Validated signature on JWT
  I0511 14:19:19.461889       1 log.go:198] OK: Got valid claims from token!
  I0511 14:19:19.461915       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1562:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683815359, NotBefore:1683814759, IssuedAt:1683814759, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1562", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"ae6d5e97-bfc4-449f-800a-0da4af0ff2c4"}}}

  May 11 14:19:52.804: INFO: completed pod
  May 11 14:19:52.828: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1562" for this suite. @ 05/11/23 14:19:52.849
• [34.178 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/11/23 14:19:52.89
  May 11 14:19:52.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename statefulset @ 05/11/23 14:19:52.89
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:19:52.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:19:52.94
  STEP: Creating service test in namespace statefulset-2726 @ 05/11/23 14:19:52.943
  STEP: Creating a new StatefulSet @ 05/11/23 14:19:52.961
  May 11 14:19:52.981: INFO: Found 0 stateful pods, waiting for 3
  May 11 14:20:02.989: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:20:02.989: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:20:02.989: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:20:02.997: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-2726 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 14:20:03.131: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 14:20:03.131: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 14:20:03.131: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/11/23 14:20:13.207
  May 11 14:20:13.332: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/11/23 14:20:13.332
  STEP: Updating Pods in reverse ordinal order @ 05/11/23 14:20:23.409
  May 11 14:20:23.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-2726 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 14:20:23.531: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 11 14:20:23.531: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 14:20:23.531: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 05/11/23 14:20:33.549
  May 11 14:20:33.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-2726 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 11 14:20:33.682: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 11 14:20:33.682: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 11 14:20:33.682: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 11 14:20:43.711: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 05/11/23 14:20:53.723
  May 11 14:20:53.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=statefulset-2726 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 11 14:20:53.848: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 11 14:20:53.848: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 11 14:20:53.848: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 11 14:21:03.872: INFO: Deleting all statefulset in ns statefulset-2726
  May 11 14:21:03.875: INFO: Scaling statefulset ss2 to 0
  May 11 14:21:13.893: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 14:21:13.895: INFO: Deleting statefulset ss2
  May 11 14:21:13.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2726" for this suite. @ 05/11/23 14:21:13.91
• [81.027 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/11/23 14:21:13.917
  May 11 14:21:13.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename svcaccounts @ 05/11/23 14:21:13.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:21:13.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:21:13.94
  STEP: Creating ServiceAccount "e2e-sa-x6pq5"  @ 05/11/23 14:21:13.942
  May 11 14:21:13.945: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-x6pq5"  @ 05/11/23 14:21:13.945
  May 11 14:21:13.952: INFO: AutomountServiceAccountToken: true
  May 11 14:21:13.952: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5879" for this suite. @ 05/11/23 14:21:13.956
• [0.045 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/11/23 14:21:13.962
  May 11 14:21:13.962: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/11/23 14:21:13.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:21:13.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:21:13.98
  STEP: Setting up the test @ 05/11/23 14:21:13.985
  STEP: Creating hostNetwork=false pod @ 05/11/23 14:21:13.985
  STEP: Creating hostNetwork=true pod @ 05/11/23 14:21:16.008
  STEP: Running the test @ 05/11/23 14:21:18.023
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/11/23 14:21:18.023
  May 11 14:21:18.023: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.023: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.023: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 11 14:21:18.079: INFO: Exec stderr: ""
  May 11 14:21:18.079: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.079: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.079: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.079: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 11 14:21:18.126: INFO: Exec stderr: ""
  May 11 14:21:18.126: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.127: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.127: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 11 14:21:18.172: INFO: Exec stderr: ""
  May 11 14:21:18.172: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.173: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.173: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 11 14:21:18.215: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/11/23 14:21:18.215
  May 11 14:21:18.215: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.215: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.215: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.215: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 11 14:21:18.261: INFO: Exec stderr: ""
  May 11 14:21:18.261: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.261: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.261: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 11 14:21:18.307: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/11/23 14:21:18.307
  May 11 14:21:18.307: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.307: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.308: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.308: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 11 14:21:18.365: INFO: Exec stderr: ""
  May 11 14:21:18.365: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.365: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.365: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.365: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 11 14:21:18.407: INFO: Exec stderr: ""
  May 11 14:21:18.407: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.408: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.408: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 11 14:21:18.453: INFO: Exec stderr: ""
  May 11 14:21:18.453: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-660 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:21:18.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:21:18.454: INFO: ExecWithOptions: Clientset creation
  May 11 14:21:18.454: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-660/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 11 14:21:18.502: INFO: Exec stderr: ""
  May 11 14:21:18.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-660" for this suite. @ 05/11/23 14:21:18.507
• [4.549 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/11/23 14:21:18.515
  May 11 14:21:18.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 14:21:18.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:21:18.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:21:18.531
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-8891 @ 05/11/23 14:21:18.536
  STEP: changing the ExternalName service to type=NodePort @ 05/11/23 14:21:18.541
  STEP: creating replication controller externalname-service in namespace services-8891 @ 05/11/23 14:21:18.561
  I0511 14:21:18.569036      24 runners.go:194] Created replication controller with name: externalname-service, namespace: services-8891, replica count: 2
  I0511 14:21:21.620700      24 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 14:21:21.620: INFO: Creating new exec pod
  May 11 14:21:24.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-8891 exec execpodn9sx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 11 14:21:24.755: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 11 14:21:24.755: INFO: stdout: "externalname-service-79ksd"
  May 11 14:21:24.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-8891 exec execpodn9sx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.163.138 80'
  May 11 14:21:24.863: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.163.138 80\nConnection to 10.101.163.138 80 port [tcp/http] succeeded!\n"
  May 11 14:21:24.863: INFO: stdout: "externalname-service-p2k5c"
  May 11 14:21:24.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-8891 exec execpodn9sx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.12 31340'
  May 11 14:21:24.966: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.12 31340\nConnection to 10.221.188.12 31340 port [tcp/*] succeeded!\n"
  May 11 14:21:24.966: INFO: stdout: "externalname-service-p2k5c"
  May 11 14:21:24.966: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-8891 exec execpodn9sx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.13 31340'
  May 11 14:21:25.071: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.13 31340\nConnection to 10.221.188.13 31340 port [tcp/*] succeeded!\n"
  May 11 14:21:25.071: INFO: stdout: ""
  May 11 14:21:26.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-8891 exec execpodn9sx5 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.13 31340'
  May 11 14:21:26.184: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.13 31340\nConnection to 10.221.188.13 31340 port [tcp/*] succeeded!\n"
  May 11 14:21:26.185: INFO: stdout: "externalname-service-p2k5c"
  May 11 14:21:26.185: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 14:21:26.189: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-8891" for this suite. @ 05/11/23 14:21:26.232
• [7.725 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/11/23 14:21:26.24
  May 11 14:21:26.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename statefulset @ 05/11/23 14:21:26.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:21:26.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:21:26.269
  STEP: Creating service test in namespace statefulset-9890 @ 05/11/23 14:21:26.271
  May 11 14:21:26.294: INFO: Found 0 stateful pods, waiting for 1
  May 11 14:21:36.299: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/11/23 14:21:36.304
  W0511 14:21:36.314293      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 11 14:21:36.319: INFO: Found 1 stateful pods, waiting for 2
  May 11 14:21:46.323: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:21:46.323: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/11/23 14:21:46.327
  STEP: Delete all of the StatefulSets @ 05/11/23 14:21:46.33
  STEP: Verify that StatefulSets have been deleted @ 05/11/23 14:21:46.334
  May 11 14:21:46.338: INFO: Deleting all statefulset in ns statefulset-9890
  May 11 14:21:46.351: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9890" for this suite. @ 05/11/23 14:21:46.36
• [20.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/11/23 14:21:46.366
  May 11 14:21:46.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pod-network-test @ 05/11/23 14:21:46.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:21:46.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:21:46.383
  STEP: Performing setup for networking test in namespace pod-network-test-813 @ 05/11/23 14:21:46.385
  STEP: creating a selector @ 05/11/23 14:21:46.385
  STEP: Creating the service pods in kubernetes @ 05/11/23 14:21:46.385
  May 11 14:21:46.385: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/11/23 14:22:08.487
  May 11 14:22:10.517: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 11 14:22:10.517: INFO: Going to poll 192.168.151.225 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 11 14:22:10.519: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.151.225:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-813 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:22:10.519: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:22:10.519: INFO: ExecWithOptions: Clientset creation
  May 11 14:22:10.519: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-813/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.151.225%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 11 14:22:10.575: INFO: Found all 1 expected endpoints: [netserver-0]
  May 11 14:22:10.575: INFO: Going to poll 192.168.150.187 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 11 14:22:10.578: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.150.187:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-813 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:22:10.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:22:10.578: INFO: ExecWithOptions: Clientset creation
  May 11 14:22:10.578: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-813/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.150.187%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 11 14:22:10.631: INFO: Found all 1 expected endpoints: [netserver-1]
  May 11 14:22:10.631: INFO: Going to poll 192.168.153.18 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  May 11 14:22:10.633: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://192.168.153.18:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-813 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:22:10.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:22:10.634: INFO: ExecWithOptions: Clientset creation
  May 11 14:22:10.634: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-813/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F192.168.153.18%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 11 14:22:10.683: INFO: Found all 1 expected endpoints: [netserver-2]
  May 11 14:22:10.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-813" for this suite. @ 05/11/23 14:22:10.687
• [24.326 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/11/23 14:22:10.692
  May 11 14:22:10.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:22:10.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:22:10.714
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:22:10.716
  STEP: Creating configMap configmap-9864/configmap-test-60664a8d-c420-46e4-b2cf-f794f371e36a @ 05/11/23 14:22:10.719
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:22:10.722
  STEP: Saw pod success @ 05/11/23 14:22:14.74
  May 11 14:22:14.742: INFO: Trying to get logs from node macpro-2 pod pod-configmaps-eebefd91-d863-4309-8e22-75f8680b293f container env-test: <nil>
  STEP: delete the pod @ 05/11/23 14:22:14.757
  May 11 14:22:14.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9864" for this suite. @ 05/11/23 14:22:14.772
• [4.086 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/11/23 14:22:14.779
  May 11 14:22:14.779: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-pred @ 05/11/23 14:22:14.78
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:22:14.793
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:22:14.795
  May 11 14:22:14.797: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 11 14:22:14.804: INFO: Waiting for terminating namespaces to be deleted...
  May 11 14:22:14.806: INFO: 
  Logging pods the apiserver thinks is on node macpro-1 before test
  May 11 14:22:14.812: INFO: calico-node-p8f6p from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.812: INFO: 	Container calico-node ready: true, restart count 0
  May 11 14:22:14.812: INFO: kube-proxy-t8w6l from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.812: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 14:22:14.812: INFO: speaker-pq5r2 from metallb-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.812: INFO: 	Container speaker ready: true, restart count 1
  May 11 14:22:14.812: INFO: netserver-0 from pod-network-test-813 started at 2023-05-11 14:21:46 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.812: INFO: 	Container webserver ready: true, restart count 0
  May 11 14:22:14.812: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-9g8bt from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 14:22:14.812: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 14:22:14.812: INFO: 	Container systemd-logs ready: true, restart count 0
  May 11 14:22:14.812: INFO: 
  Logging pods the apiserver thinks is on node macpro-2 before test
  May 11 14:22:14.819: INFO: calico-node-92dft from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.819: INFO: 	Container calico-node ready: true, restart count 0
  May 11 14:22:14.819: INFO: kube-proxy-gl6v8 from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.819: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 14:22:14.819: INFO: speaker-vthm7 from metallb-system started at 2023-05-11 13:53:21 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.819: INFO: 	Container speaker ready: true, restart count 0
  May 11 14:22:14.819: INFO: host-test-container-pod from pod-network-test-813 started at 2023-05-11 14:22:08 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.819: INFO: 	Container agnhost-container ready: true, restart count 0
  May 11 14:22:14.819: INFO: netserver-1 from pod-network-test-813 started at 2023-05-11 14:21:46 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.819: INFO: 	Container webserver ready: true, restart count 0
  May 11 14:22:14.819: INFO: test-container-pod from pod-network-test-813 started at 2023-05-11 14:22:08 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.819: INFO: 	Container webserver ready: true, restart count 0
  May 11 14:22:14.819: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-6lx9v from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 14:22:14.819: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 14:22:14.819: INFO: 	Container systemd-logs ready: true, restart count 0
  May 11 14:22:14.819: INFO: 
  Logging pods the apiserver thinks is on node macpro-3 before test
  May 11 14:22:14.828: INFO: calico-node-gql49 from kube-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.828: INFO: 	Container calico-node ready: true, restart count 0
  May 11 14:22:14.828: INFO: kube-proxy-w7svn from kube-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.828: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 14:22:14.828: INFO: speaker-6dx5t from metallb-system started at 2023-05-11 13:56:20 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.828: INFO: 	Container speaker ready: true, restart count 0
  May 11 14:22:14.828: INFO: netserver-2 from pod-network-test-813 started at 2023-05-11 14:21:46 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.828: INFO: 	Container webserver ready: true, restart count 0
  May 11 14:22:14.828: INFO: sonobuoy from sonobuoy started at 2023-05-11 13:28:12 +0000 UTC (1 container statuses recorded)
  May 11 14:22:14.828: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 11 14:22:14.828: INFO: sonobuoy-e2e-job-ae6f9d200d3b496f from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 14:22:14.828: INFO: 	Container e2e ready: true, restart count 0
  May 11 14:22:14.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 14:22:14.828: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-kc8bx from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 14:22:14.828: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 14:22:14.828: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/11/23 14:22:14.828
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/11/23 14:22:16.848
  STEP: Trying to apply a random label on the found node. @ 05/11/23 14:22:16.858
  STEP: verifying the node has the label kubernetes.io/e2e-4a287280-b63e-468e-b21f-6c3f29d4f51c 42 @ 05/11/23 14:22:16.872
  STEP: Trying to relaunch the pod, now with labels. @ 05/11/23 14:22:16.875
  STEP: removing the label kubernetes.io/e2e-4a287280-b63e-468e-b21f-6c3f29d4f51c off the node macpro-2 @ 05/11/23 14:22:18.895
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-4a287280-b63e-468e-b21f-6c3f29d4f51c @ 05/11/23 14:22:18.909
  May 11 14:22:18.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1627" for this suite. @ 05/11/23 14:22:18.918
• [4.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/11/23 14:22:18.924
  May 11 14:22:18.924: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 14:22:18.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:22:18.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:22:18.968
  STEP: Creating the pod @ 05/11/23 14:22:18.97
  May 11 14:22:21.504: INFO: Successfully updated pod "annotationupdatec078317c-4d0f-4da1-a36b-5a3d74ea066d"
  May 11 14:22:25.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7424" for this suite. @ 05/11/23 14:22:25.525
• [6.606 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/11/23 14:22:25.53
  May 11 14:22:25.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename var-expansion @ 05/11/23 14:22:25.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:22:25.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:22:25.55
  May 11 14:22:27.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 14:22:27.571: INFO: Deleting pod "var-expansion-4008b688-4519-4db2-b842-e29e23ac0136" in namespace "var-expansion-6734"
  May 11 14:22:27.577: INFO: Wait up to 5m0s for pod "var-expansion-4008b688-4519-4db2-b842-e29e23ac0136" to be fully deleted
  STEP: Destroying namespace "var-expansion-6734" for this suite. @ 05/11/23 14:22:31.586
• [6.061 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/11/23 14:22:31.592
  May 11 14:22:31.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:22:31.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:22:31.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:22:31.618
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 14:22:31.62
  STEP: Saw pod success @ 05/11/23 14:22:35.64
  May 11 14:22:35.642: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-fd55b434-c1c9-4e6f-abf2-8df15fc7ac07 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 14:22:35.647
  May 11 14:22:35.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2045" for this suite. @ 05/11/23 14:22:35.661
• [4.074 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/11/23 14:22:35.666
  May 11 14:22:35.666: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename cronjob @ 05/11/23 14:22:35.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:22:35.683
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:22:35.686
  STEP: Creating a ForbidConcurrent cronjob @ 05/11/23 14:22:35.688
  STEP: Ensuring a job is scheduled @ 05/11/23 14:22:35.691
  STEP: Ensuring exactly one is scheduled @ 05/11/23 14:23:01.695
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/11/23 14:23:01.7
  STEP: Ensuring no more jobs are scheduled @ 05/11/23 14:23:01.703
  STEP: Removing cronjob @ 05/11/23 14:28:01.711
  May 11 14:28:01.717: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2668" for this suite. @ 05/11/23 14:28:01.722
• [326.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/11/23 14:28:01.731
  May 11 14:28:01.731: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename watch @ 05/11/23 14:28:01.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:01.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:01.765
  STEP: creating a watch on configmaps @ 05/11/23 14:28:01.767
  STEP: creating a new configmap @ 05/11/23 14:28:01.768
  STEP: modifying the configmap once @ 05/11/23 14:28:01.774
  STEP: closing the watch once it receives two notifications @ 05/11/23 14:28:01.784
  May 11 14:28:01.784: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6351  905125b3-2f6a-45a1-818f-0b0d3bbc1a66 27879 0 2023-05-11 14:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-11 14:28:01 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:28:01.784: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6351  905125b3-2f6a-45a1-818f-0b0d3bbc1a66 27880 0 2023-05-11 14:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-11 14:28:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/11/23 14:28:01.784
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/11/23 14:28:01.792
  STEP: deleting the configmap @ 05/11/23 14:28:01.794
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/11/23 14:28:01.8
  May 11 14:28:01.800: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6351  905125b3-2f6a-45a1-818f-0b0d3bbc1a66 27882 0 2023-05-11 14:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-11 14:28:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:28:01.800: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-6351  905125b3-2f6a-45a1-818f-0b0d3bbc1a66 27883 0 2023-05-11 14:28:01 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-11 14:28:01 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:28:01.800: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6351" for this suite. @ 05/11/23 14:28:01.806
• [0.081 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/11/23 14:28:01.812
  May 11 14:28:01.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:28:01.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:01.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:01.832
  STEP: Creating the pod @ 05/11/23 14:28:01.835
  May 11 14:28:04.379: INFO: Successfully updated pod "annotationupdate88cfbc0e-f781-4b6d-910a-57a7aa6c1c75"
  May 11 14:28:08.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4485" for this suite. @ 05/11/23 14:28:08.403
• [6.596 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/11/23 14:28:08.408
  May 11 14:28:08.408: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:28:08.409
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:08.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:08.426
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/11/23 14:28:08.428
  STEP: Saw pod success @ 05/11/23 14:28:12.461
  May 11 14:28:12.474: INFO: Trying to get logs from node macpro-3 pod pod-32b8b650-fd61-40b4-b9f4-e07879b0d022 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:28:12.521
  May 11 14:28:12.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4933" for this suite. @ 05/11/23 14:28:12.64
• [4.244 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/11/23 14:28:12.653
  May 11 14:28:12.653: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 14:28:12.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:12.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:12.762
  STEP: Counting existing ResourceQuota @ 05/11/23 14:28:12.781
  STEP: Creating a ResourceQuota @ 05/11/23 14:28:17.784
  STEP: Ensuring resource quota status is calculated @ 05/11/23 14:28:17.789
  STEP: Creating a Service @ 05/11/23 14:28:19.792
  STEP: Creating a NodePort Service @ 05/11/23 14:28:19.814
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/11/23 14:28:19.856
  STEP: Ensuring resource quota status captures service creation @ 05/11/23 14:28:19.887
  STEP: Deleting Services @ 05/11/23 14:28:21.893
  STEP: Ensuring resource quota status released usage @ 05/11/23 14:28:21.961
  May 11 14:28:23.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5469" for this suite. @ 05/11/23 14:28:23.97
• [11.323 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/11/23 14:28:23.976
  May 11 14:28:23.976: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:28:23.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:23.992
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:23.995
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/11/23 14:28:23.998
  May 11 14:28:23.998: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-5168 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May 11 14:28:24.067: INFO: stderr: ""
  May 11 14:28:24.068: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/11/23 14:28:24.068
  May 11 14:28:24.074: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-5168 delete pods e2e-test-httpd-pod'
  May 11 14:28:26.286: INFO: stderr: ""
  May 11 14:28:26.286: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 11 14:28:26.286: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5168" for this suite. @ 05/11/23 14:28:26.29
• [2.318 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/11/23 14:28:26.295
  May 11 14:28:26.295: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename namespaces @ 05/11/23 14:28:26.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:26.307
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:26.309
  STEP: Creating namespace "e2e-ns-6hq2x" @ 05/11/23 14:28:26.311
  May 11 14:28:26.323: INFO: Namespace "e2e-ns-6hq2x-6905" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-6hq2x-6905" @ 05/11/23 14:28:26.323
  May 11 14:28:26.329: INFO: Namespace "e2e-ns-6hq2x-6905" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-6hq2x-6905" @ 05/11/23 14:28:26.33
  May 11 14:28:26.335: INFO: Namespace "e2e-ns-6hq2x-6905" has []v1.FinalizerName{"kubernetes"}
  May 11 14:28:26.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1454" for this suite. @ 05/11/23 14:28:26.341
  STEP: Destroying namespace "e2e-ns-6hq2x-6905" for this suite. @ 05/11/23 14:28:26.345
• [0.055 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/11/23 14:28:26.35
  May 11 14:28:26.350: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename svcaccounts @ 05/11/23 14:28:26.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:26.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:26.367
  STEP: creating a ServiceAccount @ 05/11/23 14:28:26.369
  STEP: watching for the ServiceAccount to be added @ 05/11/23 14:28:26.375
  STEP: patching the ServiceAccount @ 05/11/23 14:28:26.377
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/11/23 14:28:26.381
  STEP: deleting the ServiceAccount @ 05/11/23 14:28:26.384
  May 11 14:28:26.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1828" for this suite. @ 05/11/23 14:28:26.396
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/11/23 14:28:26.402
  May 11 14:28:26.402: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename disruption @ 05/11/23 14:28:26.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:26.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:26.417
  STEP: Waiting for the pdb to be processed @ 05/11/23 14:28:26.422
  STEP: Updating PodDisruptionBudget status @ 05/11/23 14:28:28.445
  STEP: Waiting for all pods to be running @ 05/11/23 14:28:28.464
  May 11 14:28:28.473: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 05/11/23 14:28:30.476
  STEP: Waiting for the pdb to be processed @ 05/11/23 14:28:30.485
  STEP: Patching PodDisruptionBudget status @ 05/11/23 14:28:30.492
  STEP: Waiting for the pdb to be processed @ 05/11/23 14:28:30.5
  May 11 14:28:30.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9397" for this suite. @ 05/11/23 14:28:30.507
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/11/23 14:28:30.513
  May 11 14:28:30.513: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename gc @ 05/11/23 14:28:30.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:30.527
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:30.529
  STEP: create the deployment @ 05/11/23 14:28:30.531
  W0511 14:28:30.534906      24 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/11/23 14:28:30.534
  STEP: delete the deployment @ 05/11/23 14:28:31.045
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/11/23 14:28:31.051
  STEP: Gathering metrics @ 05/11/23 14:28:31.564
  May 11 14:28:31.669: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 11 14:28:31.669: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2152" for this suite. @ 05/11/23 14:28:31.674
• [1.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/11/23 14:28:31.68
  May 11 14:28:31.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/11/23 14:28:31.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:31.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:31.699
  STEP: fetching the /apis discovery document @ 05/11/23 14:28:31.701
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/11/23 14:28:31.702
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/11/23 14:28:31.702
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/11/23 14:28:31.702
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/11/23 14:28:31.703
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/11/23 14:28:31.703
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/11/23 14:28:31.704
  May 11 14:28:31.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3142" for this suite. @ 05/11/23 14:28:31.707
• [0.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/11/23 14:28:31.715
  May 11 14:28:31.715: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-preemption @ 05/11/23 14:28:31.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:28:31.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:28:31.749
  May 11 14:28:31.764: INFO: Waiting up to 1m0s for all nodes to be ready
  May 11 14:29:31.798: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/11/23 14:29:31.8
  May 11 14:29:31.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/11/23 14:29:31.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:29:31.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:29:31.819
  STEP: Finding an available node @ 05/11/23 14:29:31.821
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/11/23 14:29:31.821
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/11/23 14:29:33.839
  May 11 14:29:33.848: INFO: found a healthy node: macpro-2
  May 11 14:29:39.917: INFO: pods created so far: [1 1 1]
  May 11 14:29:39.917: INFO: length of pods created so far: 3
  May 11 14:29:41.929: INFO: pods created so far: [2 2 1]
  May 11 14:29:48.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 14:29:48.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-4224" for this suite. @ 05/11/23 14:29:48.999
  STEP: Destroying namespace "sched-preemption-4626" for this suite. @ 05/11/23 14:29:49.004
• [77.295 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/11/23 14:29:49.01
  May 11 14:29:49.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 14:29:49.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:29:49.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:29:49.035
  STEP: Creating a ResourceQuota @ 05/11/23 14:29:49.038
  STEP: Getting a ResourceQuota @ 05/11/23 14:29:49.042
  STEP: Updating a ResourceQuota @ 05/11/23 14:29:49.045
  STEP: Verifying a ResourceQuota was modified @ 05/11/23 14:29:49.053
  STEP: Deleting a ResourceQuota @ 05/11/23 14:29:49.055
  STEP: Verifying the deleted ResourceQuota @ 05/11/23 14:29:49.06
  May 11 14:29:49.062: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5694" for this suite. @ 05/11/23 14:29:49.065
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/11/23 14:29:49.072
  May 11 14:29:49.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:29:49.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:29:49.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:29:49.09
  STEP: Creating configMap with name configmap-test-volume-21acc4c8-e099-4ca9-80da-23d2d0c313ad @ 05/11/23 14:29:49.092
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:29:49.096
  STEP: Saw pod success @ 05/11/23 14:29:53.116
  May 11 14:29:53.119: INFO: Trying to get logs from node macpro-3 pod pod-configmaps-cd1d876b-f47a-4a63-913f-93871c17db0e container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:29:53.131
  May 11 14:29:53.141: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1969" for this suite. @ 05/11/23 14:29:53.145
• [4.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/11/23 14:29:53.151
  May 11 14:29:53.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:29:53.151
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:29:53.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:29:53.177
  STEP: validating cluster-info @ 05/11/23 14:29:53.179
  May 11 14:29:53.179: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-8632 cluster-info'
  May 11 14:29:53.238: INFO: stderr: ""
  May 11 14:29:53.238: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May 11 14:29:53.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8632" for this suite. @ 05/11/23 14:29:53.243
• [0.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/11/23 14:29:53.249
  May 11 14:29:53.249: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/11/23 14:29:53.249
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:29:53.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:29:53.266
  STEP: create the container to handle the HTTPGet hook request. @ 05/11/23 14:29:53.274
  STEP: create the pod with lifecycle hook @ 05/11/23 14:29:55.289
  STEP: delete the pod with lifecycle hook @ 05/11/23 14:29:57.3
  STEP: check prestop hook @ 05/11/23 14:30:01.315
  May 11 14:30:01.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-778" for this suite. @ 05/11/23 14:30:01.332
• [8.088 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/11/23 14:30:01.337
  May 11 14:30:01.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/11/23 14:30:01.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:30:01.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:30:01.355
  May 11 14:30:01.357: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:30:02.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9851" for this suite. @ 05/11/23 14:30:02.383
• [1.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/11/23 14:30:02.388
  May 11 14:30:02.388: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename certificates @ 05/11/23 14:30:02.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:30:02.402
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:30:02.404
  STEP: getting /apis @ 05/11/23 14:30:03.289
  STEP: getting /apis/certificates.k8s.io @ 05/11/23 14:30:03.293
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/11/23 14:30:03.294
  STEP: creating @ 05/11/23 14:30:03.295
  STEP: getting @ 05/11/23 14:30:03.307
  STEP: listing @ 05/11/23 14:30:03.31
  STEP: watching @ 05/11/23 14:30:03.313
  May 11 14:30:03.313: INFO: starting watch
  STEP: patching @ 05/11/23 14:30:03.314
  STEP: updating @ 05/11/23 14:30:03.318
  May 11 14:30:03.322: INFO: waiting for watch events with expected annotations
  May 11 14:30:03.322: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/11/23 14:30:03.322
  STEP: patching /approval @ 05/11/23 14:30:03.324
  STEP: updating /approval @ 05/11/23 14:30:03.329
  STEP: getting /status @ 05/11/23 14:30:03.334
  STEP: patching /status @ 05/11/23 14:30:03.337
  STEP: updating /status @ 05/11/23 14:30:03.342
  STEP: deleting @ 05/11/23 14:30:03.348
  STEP: deleting a collection @ 05/11/23 14:30:03.356
  May 11 14:30:03.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-7157" for this suite. @ 05/11/23 14:30:03.369
• [0.986 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/11/23 14:30:03.374
  May 11 14:30:03.374: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename limitrange @ 05/11/23 14:30:03.375
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:30:03.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:30:03.397
  STEP: Creating a LimitRange @ 05/11/23 14:30:03.399
  STEP: Setting up watch @ 05/11/23 14:30:03.399
  STEP: Submitting a LimitRange @ 05/11/23 14:30:03.502
  STEP: Verifying LimitRange creation was observed @ 05/11/23 14:30:03.508
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/11/23 14:30:03.508
  May 11 14:30:03.510: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 11 14:30:03.510: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/11/23 14:30:03.51
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/11/23 14:30:03.515
  May 11 14:30:03.524: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 11 14:30:03.524: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/11/23 14:30:03.524
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/11/23 14:30:03.534
  May 11 14:30:03.539: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May 11 14:30:03.539: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/11/23 14:30:03.539
  STEP: Failing to create a Pod with more than max resources @ 05/11/23 14:30:03.542
  STEP: Updating a LimitRange @ 05/11/23 14:30:03.546
  STEP: Verifying LimitRange updating is effective @ 05/11/23 14:30:03.551
  STEP: Creating a Pod with less than former min resources @ 05/11/23 14:30:05.554
  STEP: Failing to create a Pod with more than max resources @ 05/11/23 14:30:05.558
  STEP: Deleting a LimitRange @ 05/11/23 14:30:05.561
  STEP: Verifying the LimitRange was deleted @ 05/11/23 14:30:05.571
  May 11 14:30:10.575: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/11/23 14:30:10.575
  May 11 14:30:10.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7178" for this suite. @ 05/11/23 14:30:10.585
• [7.222 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/11/23 14:30:10.597
  May 11 14:30:10.597: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename statefulset @ 05/11/23 14:30:10.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:30:10.617
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:30:10.62
  STEP: Creating service test in namespace statefulset-5691 @ 05/11/23 14:30:10.622
  STEP: Creating a new StatefulSet @ 05/11/23 14:30:10.625
  May 11 14:30:10.636: INFO: Found 0 stateful pods, waiting for 3
  May 11 14:30:20.642: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:30:20.642: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:30:20.642: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/11/23 14:30:20.65
  May 11 14:30:20.670: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/11/23 14:30:20.67
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/11/23 14:30:30.685
  STEP: Performing a canary update @ 05/11/23 14:30:30.685
  May 11 14:30:30.704: INFO: Updating stateful set ss2
  May 11 14:30:30.710: INFO: Waiting for Pod statefulset-5691/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/11/23 14:30:40.718
  May 11 14:30:40.753: INFO: Found 1 stateful pods, waiting for 3
  May 11 14:30:50.759: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:30:50.759: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 11 14:30:50.759: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/11/23 14:30:50.765
  May 11 14:30:50.784: INFO: Updating stateful set ss2
  May 11 14:30:50.789: INFO: Waiting for Pod statefulset-5691/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May 11 14:31:00.816: INFO: Updating stateful set ss2
  May 11 14:31:00.822: INFO: Waiting for StatefulSet statefulset-5691/ss2 to complete update
  May 11 14:31:00.822: INFO: Waiting for Pod statefulset-5691/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  May 11 14:31:10.830: INFO: Deleting all statefulset in ns statefulset-5691
  May 11 14:31:10.832: INFO: Scaling statefulset ss2 to 0
  May 11 14:31:20.848: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 14:31:20.851: INFO: Deleting statefulset ss2
  May 11 14:31:20.859: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5691" for this suite. @ 05/11/23 14:31:20.864
• [70.273 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/11/23 14:31:20.87
  May 11 14:31:20.870: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:31:20.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:31:20.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:31:20.896
  STEP: Setting up server cert @ 05/11/23 14:31:20.918
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:31:21.395
  STEP: Deploying the webhook pod @ 05/11/23 14:31:21.403
  STEP: Wait for the deployment to be ready @ 05/11/23 14:31:21.413
  May 11 14:31:21.417: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 14:31:23.426
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:31:23.44
  May 11 14:31:24.440: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/11/23 14:31:24.444
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/11/23 14:31:24.444
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/11/23 14:31:24.459
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/11/23 14:31:25.469
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/11/23 14:31:25.469
  STEP: Having no error when timeout is longer than webhook latency @ 05/11/23 14:31:26.492
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/11/23 14:31:26.492
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/11/23 14:31:31.523
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/11/23 14:31:31.523
  May 11 14:31:36.548: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7283" for this suite. @ 05/11/23 14:31:36.616
  STEP: Destroying namespace "webhook-markers-2600" for this suite. @ 05/11/23 14:31:36.623
• [15.757 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/11/23 14:31:36.628
  May 11 14:31:36.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 14:31:36.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:31:36.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:31:36.672
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 14:31:36.674
  STEP: Saw pod success @ 05/11/23 14:31:40.699
  May 11 14:31:40.702: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-e0af3a78-6af4-40d7-a29e-8392d720dcb1 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 14:31:40.713
  May 11 14:31:40.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7161" for this suite. @ 05/11/23 14:31:40.73
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/11/23 14:31:40.738
  May 11 14:31:40.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename runtimeclass @ 05/11/23 14:31:40.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:31:40.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:31:40.763
  STEP: Deleting RuntimeClass runtimeclass-6700-delete-me @ 05/11/23 14:31:40.77
  STEP: Waiting for the RuntimeClass to disappear @ 05/11/23 14:31:40.775
  May 11 14:31:40.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6700" for this suite. @ 05/11/23 14:31:40.786
• [0.054 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/11/23 14:31:40.793
  May 11 14:31:40.793: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename var-expansion @ 05/11/23 14:31:40.793
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:31:40.808
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:31:40.812
  STEP: Creating a pod to test substitution in container's args @ 05/11/23 14:31:40.816
  STEP: Saw pod success @ 05/11/23 14:31:44.838
  May 11 14:31:44.841: INFO: Trying to get logs from node macpro-2 pod var-expansion-8d0fda95-a385-4322-819b-6a6c5570cb52 container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 14:31:44.847
  May 11 14:31:44.864: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5569" for this suite. @ 05/11/23 14:31:44.868
• [4.080 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/11/23 14:31:44.874
  May 11 14:31:44.874: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename hostport @ 05/11/23 14:31:44.874
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:31:44.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:31:44.893
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/11/23 14:31:44.899
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 10.221.188.11 on the node which pod1 resides and expect scheduled @ 05/11/23 14:31:48.919
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 10.221.188.11 but use UDP protocol on the node which pod2 resides @ 05/11/23 14:31:50.933
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/11/23 14:31:56.983
  May 11 14:31:56.983: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 10.221.188.11 http://127.0.0.1:54323/hostname] Namespace:hostport-2811 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:31:56.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:31:56.984: INFO: ExecWithOptions: Clientset creation
  May 11 14:31:56.984: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2811/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+10.221.188.11+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.221.188.11, port: 54323 @ 05/11/23 14:31:57.04
  May 11 14:31:57.040: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://10.221.188.11:54323/hostname] Namespace:hostport-2811 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:31:57.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:31:57.040: INFO: ExecWithOptions: Clientset creation
  May 11 14:31:57.040: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2811/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F10.221.188.11%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 10.221.188.11, port: 54323 UDP @ 05/11/23 14:31:57.091
  May 11 14:31:57.091: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 10.221.188.11 54323] Namespace:hostport-2811 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:31:57.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:31:57.092: INFO: ExecWithOptions: Clientset creation
  May 11 14:31:57.092: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-2811/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+10.221.188.11+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  May 11 14:32:02.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-2811" for this suite. @ 05/11/23 14:32:02.144
• [17.276 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/11/23 14:32:02.151
  May 11 14:32:02.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename ingress @ 05/11/23 14:32:02.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:32:02.172
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:32:02.182
  STEP: getting /apis @ 05/11/23 14:32:02.184
  STEP: getting /apis/networking.k8s.io @ 05/11/23 14:32:02.188
  STEP: getting /apis/networking.k8s.iov1 @ 05/11/23 14:32:02.189
  STEP: creating @ 05/11/23 14:32:02.19
  STEP: getting @ 05/11/23 14:32:02.201
  STEP: listing @ 05/11/23 14:32:02.203
  STEP: watching @ 05/11/23 14:32:02.206
  May 11 14:32:02.206: INFO: starting watch
  STEP: cluster-wide listing @ 05/11/23 14:32:02.207
  STEP: cluster-wide watching @ 05/11/23 14:32:02.208
  May 11 14:32:02.209: INFO: starting watch
  STEP: patching @ 05/11/23 14:32:02.209
  STEP: updating @ 05/11/23 14:32:02.214
  May 11 14:32:02.220: INFO: waiting for watch events with expected annotations
  May 11 14:32:02.220: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/11/23 14:32:02.22
  STEP: updating /status @ 05/11/23 14:32:02.223
  STEP: get /status @ 05/11/23 14:32:02.231
  STEP: deleting @ 05/11/23 14:32:02.233
  STEP: deleting a collection @ 05/11/23 14:32:02.239
  May 11 14:32:02.249: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-7069" for this suite. @ 05/11/23 14:32:02.252
• [0.107 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/11/23 14:32:02.258
  May 11 14:32:02.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pod-network-test @ 05/11/23 14:32:02.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:32:02.272
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:32:02.274
  STEP: Performing setup for networking test in namespace pod-network-test-5951 @ 05/11/23 14:32:02.277
  STEP: creating a selector @ 05/11/23 14:32:02.277
  STEP: Creating the service pods in kubernetes @ 05/11/23 14:32:02.277
  May 11 14:32:02.277: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/11/23 14:32:24.372
  May 11 14:32:26.391: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 11 14:32:26.391: INFO: Breadth first check of 192.168.151.246 on host 10.221.188.11...
  May 11 14:32:26.393: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.135:9080/dial?request=hostname&protocol=http&host=192.168.151.246&port=8083&tries=1'] Namespace:pod-network-test-5951 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:32:26.393: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:32:26.394: INFO: ExecWithOptions: Clientset creation
  May 11 14:32:26.394: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5951/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.151.246%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 11 14:32:26.450: INFO: Waiting for responses: map[]
  May 11 14:32:26.450: INFO: reached 192.168.151.246 after 0/1 tries
  May 11 14:32:26.450: INFO: Breadth first check of 192.168.150.162 on host 10.221.188.12...
  May 11 14:32:26.453: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.135:9080/dial?request=hostname&protocol=http&host=192.168.150.162&port=8083&tries=1'] Namespace:pod-network-test-5951 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:32:26.453: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:32:26.453: INFO: ExecWithOptions: Clientset creation
  May 11 14:32:26.453: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5951/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.150.162%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 11 14:32:26.501: INFO: Waiting for responses: map[]
  May 11 14:32:26.501: INFO: reached 192.168.150.162 after 0/1 tries
  May 11 14:32:26.501: INFO: Breadth first check of 192.168.153.32 on host 10.221.188.13...
  May 11 14:32:26.503: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://192.168.150.135:9080/dial?request=hostname&protocol=http&host=192.168.153.32&port=8083&tries=1'] Namespace:pod-network-test-5951 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:32:26.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:32:26.504: INFO: ExecWithOptions: Clientset creation
  May 11 14:32:26.504: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-5951/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F192.168.150.135%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D192.168.153.32%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 11 14:32:26.554: INFO: Waiting for responses: map[]
  May 11 14:32:26.554: INFO: reached 192.168.153.32 after 0/1 tries
  May 11 14:32:26.554: INFO: Going to retry 0 out of 3 pods....
  May 11 14:32:26.554: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5951" for this suite. @ 05/11/23 14:32:26.559
• [24.306 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/11/23 14:32:26.564
  May 11 14:32:26.564: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:32:26.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:32:26.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:32:26.581
  STEP: Setting up server cert @ 05/11/23 14:32:26.604
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:32:26.813
  STEP: Deploying the webhook pod @ 05/11/23 14:32:26.819
  STEP: Wait for the deployment to be ready @ 05/11/23 14:32:26.83
  May 11 14:32:26.834: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 14:32:28.844
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:32:28.861
  May 11 14:32:29.861: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 11 14:32:29.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-3023-crds.webhook.example.com via the AdmissionRegistration API @ 05/11/23 14:32:30.375
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/11/23 14:32:30.392
  May 11 14:32:32.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6755" for this suite. @ 05/11/23 14:32:32.994
  STEP: Destroying namespace "webhook-markers-3969" for this suite. @ 05/11/23 14:32:32.999
• [6.442 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/11/23 14:32:33.006
  May 11 14:32:33.006: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 14:32:33.007
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:32:33.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:32:33.023
  STEP: set up a multi version CRD @ 05/11/23 14:32:33.025
  May 11 14:32:33.026: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: rename a version @ 05/11/23 14:32:37.046
  STEP: check the new version name is served @ 05/11/23 14:32:37.064
  STEP: check the old version name is removed @ 05/11/23 14:32:38.921
  STEP: check the other version is not changed @ 05/11/23 14:32:39.692
  May 11 14:32:42.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6304" for this suite. @ 05/11/23 14:32:42.92
• [9.919 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/11/23 14:32:42.925
  May 11 14:32:42.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 14:32:42.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:32:42.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:32:42.945
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/11/23 14:32:42.947
  May 11 14:32:42.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/11/23 14:32:49.226
  May 11 14:32:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:32:50.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:32:58.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9697" for this suite. @ 05/11/23 14:32:58.024
• [15.105 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/11/23 14:32:58.031
  May 11 14:32:58.031: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename field-validation @ 05/11/23 14:32:58.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:32:58.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:32:58.049
  STEP: apply creating a deployment @ 05/11/23 14:32:58.051
  May 11 14:32:58.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9416" for this suite. @ 05/11/23 14:32:58.065
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/11/23 14:32:58.071
  May 11 14:32:58.072: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename gc @ 05/11/23 14:32:58.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:32:58.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:32:58.088
  STEP: create the rc @ 05/11/23 14:32:58.093
  W0511 14:32:58.097756      24 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/11/23 14:33:04.142
  STEP: wait for the rc to be deleted @ 05/11/23 14:33:04.174
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/11/23 14:33:09.177
  STEP: Gathering metrics @ 05/11/23 14:33:39.193
  May 11 14:33:39.272: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 11 14:33:39.272: INFO: Deleting pod "simpletest.rc-24d7n" in namespace "gc-9368"
  May 11 14:33:39.285: INFO: Deleting pod "simpletest.rc-2hnnx" in namespace "gc-9368"
  May 11 14:33:39.296: INFO: Deleting pod "simpletest.rc-4qmxh" in namespace "gc-9368"
  May 11 14:33:39.310: INFO: Deleting pod "simpletest.rc-4r46f" in namespace "gc-9368"
  May 11 14:33:39.327: INFO: Deleting pod "simpletest.rc-4wf5h" in namespace "gc-9368"
  May 11 14:33:39.337: INFO: Deleting pod "simpletest.rc-4wjzt" in namespace "gc-9368"
  May 11 14:33:39.355: INFO: Deleting pod "simpletest.rc-4zpv2" in namespace "gc-9368"
  May 11 14:33:39.368: INFO: Deleting pod "simpletest.rc-55rkx" in namespace "gc-9368"
  May 11 14:33:39.381: INFO: Deleting pod "simpletest.rc-57dqt" in namespace "gc-9368"
  May 11 14:33:39.395: INFO: Deleting pod "simpletest.rc-59dtr" in namespace "gc-9368"
  May 11 14:33:39.414: INFO: Deleting pod "simpletest.rc-5cfsd" in namespace "gc-9368"
  May 11 14:33:39.428: INFO: Deleting pod "simpletest.rc-5mcw2" in namespace "gc-9368"
  May 11 14:33:39.439: INFO: Deleting pod "simpletest.rc-5pt7q" in namespace "gc-9368"
  May 11 14:33:39.454: INFO: Deleting pod "simpletest.rc-5qbp5" in namespace "gc-9368"
  May 11 14:33:39.474: INFO: Deleting pod "simpletest.rc-5rmxq" in namespace "gc-9368"
  May 11 14:33:39.493: INFO: Deleting pod "simpletest.rc-5vpcn" in namespace "gc-9368"
  May 11 14:33:39.506: INFO: Deleting pod "simpletest.rc-62jn6" in namespace "gc-9368"
  May 11 14:33:39.533: INFO: Deleting pod "simpletest.rc-6rkss" in namespace "gc-9368"
  May 11 14:33:39.557: INFO: Deleting pod "simpletest.rc-6tm8v" in namespace "gc-9368"
  May 11 14:33:39.588: INFO: Deleting pod "simpletest.rc-6vz9r" in namespace "gc-9368"
  May 11 14:33:39.612: INFO: Deleting pod "simpletest.rc-7kkbc" in namespace "gc-9368"
  May 11 14:33:39.634: INFO: Deleting pod "simpletest.rc-7mlml" in namespace "gc-9368"
  May 11 14:33:39.652: INFO: Deleting pod "simpletest.rc-7mtbw" in namespace "gc-9368"
  May 11 14:33:39.677: INFO: Deleting pod "simpletest.rc-7q4ds" in namespace "gc-9368"
  May 11 14:33:39.708: INFO: Deleting pod "simpletest.rc-7sjqs" in namespace "gc-9368"
  May 11 14:33:39.736: INFO: Deleting pod "simpletest.rc-7v22k" in namespace "gc-9368"
  May 11 14:33:39.766: INFO: Deleting pod "simpletest.rc-8brnw" in namespace "gc-9368"
  May 11 14:33:39.797: INFO: Deleting pod "simpletest.rc-8fnnf" in namespace "gc-9368"
  May 11 14:33:39.826: INFO: Deleting pod "simpletest.rc-8rjlp" in namespace "gc-9368"
  May 11 14:33:39.886: INFO: Deleting pod "simpletest.rc-8zj2p" in namespace "gc-9368"
  May 11 14:33:39.904: INFO: Deleting pod "simpletest.rc-962dg" in namespace "gc-9368"
  May 11 14:33:39.917: INFO: Deleting pod "simpletest.rc-9w5r8" in namespace "gc-9368"
  May 11 14:33:39.947: INFO: Deleting pod "simpletest.rc-ccvfw" in namespace "gc-9368"
  May 11 14:33:39.972: INFO: Deleting pod "simpletest.rc-cd6qm" in namespace "gc-9368"
  May 11 14:33:39.999: INFO: Deleting pod "simpletest.rc-cn6r5" in namespace "gc-9368"
  May 11 14:33:40.025: INFO: Deleting pod "simpletest.rc-cnrlz" in namespace "gc-9368"
  May 11 14:33:40.049: INFO: Deleting pod "simpletest.rc-cp68p" in namespace "gc-9368"
  May 11 14:33:40.073: INFO: Deleting pod "simpletest.rc-crdr9" in namespace "gc-9368"
  May 11 14:33:40.101: INFO: Deleting pod "simpletest.rc-cwkhr" in namespace "gc-9368"
  May 11 14:33:40.116: INFO: Deleting pod "simpletest.rc-d588w" in namespace "gc-9368"
  May 11 14:33:40.141: INFO: Deleting pod "simpletest.rc-d66fv" in namespace "gc-9368"
  May 11 14:33:40.191: INFO: Deleting pod "simpletest.rc-dhq2s" in namespace "gc-9368"
  May 11 14:33:40.214: INFO: Deleting pod "simpletest.rc-drnbj" in namespace "gc-9368"
  May 11 14:33:40.244: INFO: Deleting pod "simpletest.rc-dx92c" in namespace "gc-9368"
  May 11 14:33:40.278: INFO: Deleting pod "simpletest.rc-dz55c" in namespace "gc-9368"
  May 11 14:33:40.304: INFO: Deleting pod "simpletest.rc-g5qlw" in namespace "gc-9368"
  May 11 14:33:40.324: INFO: Deleting pod "simpletest.rc-g69x2" in namespace "gc-9368"
  May 11 14:33:40.336: INFO: Deleting pod "simpletest.rc-gq75b" in namespace "gc-9368"
  May 11 14:33:40.353: INFO: Deleting pod "simpletest.rc-gvwbh" in namespace "gc-9368"
  May 11 14:33:40.383: INFO: Deleting pod "simpletest.rc-gvzbt" in namespace "gc-9368"
  May 11 14:33:40.428: INFO: Deleting pod "simpletest.rc-h96sk" in namespace "gc-9368"
  May 11 14:33:40.446: INFO: Deleting pod "simpletest.rc-hkbc9" in namespace "gc-9368"
  May 11 14:33:40.477: INFO: Deleting pod "simpletest.rc-hq56z" in namespace "gc-9368"
  May 11 14:33:40.545: INFO: Deleting pod "simpletest.rc-j5zjs" in namespace "gc-9368"
  May 11 14:33:40.619: INFO: Deleting pod "simpletest.rc-jmk8n" in namespace "gc-9368"
  May 11 14:33:40.674: INFO: Deleting pod "simpletest.rc-kkr9x" in namespace "gc-9368"
  May 11 14:33:40.739: INFO: Deleting pod "simpletest.rc-kml98" in namespace "gc-9368"
  May 11 14:33:40.764: INFO: Deleting pod "simpletest.rc-ks4vd" in namespace "gc-9368"
  May 11 14:33:40.784: INFO: Deleting pod "simpletest.rc-lf69m" in namespace "gc-9368"
  May 11 14:33:40.811: INFO: Deleting pod "simpletest.rc-m5qsm" in namespace "gc-9368"
  May 11 14:33:40.832: INFO: Deleting pod "simpletest.rc-m8fxn" in namespace "gc-9368"
  May 11 14:33:40.868: INFO: Deleting pod "simpletest.rc-mdzdb" in namespace "gc-9368"
  May 11 14:33:40.884: INFO: Deleting pod "simpletest.rc-n8wgg" in namespace "gc-9368"
  May 11 14:33:40.919: INFO: Deleting pod "simpletest.rc-nd447" in namespace "gc-9368"
  May 11 14:33:40.957: INFO: Deleting pod "simpletest.rc-nhrjs" in namespace "gc-9368"
  May 11 14:33:40.977: INFO: Deleting pod "simpletest.rc-nl9df" in namespace "gc-9368"
  May 11 14:33:40.996: INFO: Deleting pod "simpletest.rc-nnbfn" in namespace "gc-9368"
  May 11 14:33:41.028: INFO: Deleting pod "simpletest.rc-nr2p4" in namespace "gc-9368"
  May 11 14:33:41.056: INFO: Deleting pod "simpletest.rc-pm6c9" in namespace "gc-9368"
  May 11 14:33:41.074: INFO: Deleting pod "simpletest.rc-pmz48" in namespace "gc-9368"
  May 11 14:33:41.090: INFO: Deleting pod "simpletest.rc-q5jld" in namespace "gc-9368"
  May 11 14:33:41.107: INFO: Deleting pod "simpletest.rc-qf8gl" in namespace "gc-9368"
  May 11 14:33:41.129: INFO: Deleting pod "simpletest.rc-qgphq" in namespace "gc-9368"
  May 11 14:33:41.141: INFO: Deleting pod "simpletest.rc-qw5wv" in namespace "gc-9368"
  May 11 14:33:41.162: INFO: Deleting pod "simpletest.rc-rh48r" in namespace "gc-9368"
  May 11 14:33:41.183: INFO: Deleting pod "simpletest.rc-s6pbv" in namespace "gc-9368"
  May 11 14:33:41.195: INFO: Deleting pod "simpletest.rc-snvxm" in namespace "gc-9368"
  May 11 14:33:41.210: INFO: Deleting pod "simpletest.rc-sslc5" in namespace "gc-9368"
  May 11 14:33:41.234: INFO: Deleting pod "simpletest.rc-sz54p" in namespace "gc-9368"
  May 11 14:33:41.253: INFO: Deleting pod "simpletest.rc-thcrt" in namespace "gc-9368"
  May 11 14:33:41.275: INFO: Deleting pod "simpletest.rc-tsd98" in namespace "gc-9368"
  May 11 14:33:41.291: INFO: Deleting pod "simpletest.rc-twr7q" in namespace "gc-9368"
  May 11 14:33:41.305: INFO: Deleting pod "simpletest.rc-tzb7r" in namespace "gc-9368"
  May 11 14:33:41.323: INFO: Deleting pod "simpletest.rc-vmslq" in namespace "gc-9368"
  May 11 14:33:41.342: INFO: Deleting pod "simpletest.rc-vq42j" in namespace "gc-9368"
  May 11 14:33:41.362: INFO: Deleting pod "simpletest.rc-w7wzx" in namespace "gc-9368"
  May 11 14:33:41.385: INFO: Deleting pod "simpletest.rc-w88mr" in namespace "gc-9368"
  May 11 14:33:41.402: INFO: Deleting pod "simpletest.rc-wjg9m" in namespace "gc-9368"
  May 11 14:33:41.440: INFO: Deleting pod "simpletest.rc-wkggh" in namespace "gc-9368"
  May 11 14:33:41.457: INFO: Deleting pod "simpletest.rc-wn4ck" in namespace "gc-9368"
  May 11 14:33:41.476: INFO: Deleting pod "simpletest.rc-wqn46" in namespace "gc-9368"
  May 11 14:33:41.496: INFO: Deleting pod "simpletest.rc-wwspw" in namespace "gc-9368"
  May 11 14:33:41.515: INFO: Deleting pod "simpletest.rc-wx425" in namespace "gc-9368"
  May 11 14:33:41.546: INFO: Deleting pod "simpletest.rc-x4snm" in namespace "gc-9368"
  May 11 14:33:41.605: INFO: Deleting pod "simpletest.rc-x4znw" in namespace "gc-9368"
  May 11 14:33:41.645: INFO: Deleting pod "simpletest.rc-xq4qk" in namespace "gc-9368"
  May 11 14:33:41.704: INFO: Deleting pod "simpletest.rc-z4mjh" in namespace "gc-9368"
  May 11 14:33:41.773: INFO: Deleting pod "simpletest.rc-z8qw7" in namespace "gc-9368"
  May 11 14:33:41.797: INFO: Deleting pod "simpletest.rc-zq5gb" in namespace "gc-9368"
  May 11 14:33:41.870: INFO: Deleting pod "simpletest.rc-zxdjn" in namespace "gc-9368"
  May 11 14:33:41.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-9368" for this suite. @ 05/11/23 14:33:41.938
• [43.918 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/11/23 14:33:41.99
  May 11 14:33:41.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:33:41.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:33:42.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:33:42.014
  STEP: Setting up server cert @ 05/11/23 14:33:42.035
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:33:42.852
  STEP: Deploying the webhook pod @ 05/11/23 14:33:42.869
  STEP: Wait for the deployment to be ready @ 05/11/23 14:33:42.891
  May 11 14:33:42.921: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/11/23 14:33:44.932
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:33:44.949
  May 11 14:33:45.949: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/11/23 14:33:46.009
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/11/23 14:33:46.039
  STEP: Deleting the collection of validation webhooks @ 05/11/23 14:33:46.064
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/11/23 14:33:46.109
  May 11 14:33:46.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-62" for this suite. @ 05/11/23 14:33:46.179
  STEP: Destroying namespace "webhook-markers-8298" for this suite. @ 05/11/23 14:33:46.186
• [4.210 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/11/23 14:33:46.201
  May 11 14:33:46.201: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename statefulset @ 05/11/23 14:33:46.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:33:46.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:33:46.219
  STEP: Creating service test in namespace statefulset-8760 @ 05/11/23 14:33:46.221
  STEP: Creating statefulset ss in namespace statefulset-8760 @ 05/11/23 14:33:46.226
  May 11 14:33:46.238: INFO: Found 0 stateful pods, waiting for 1
  May 11 14:33:56.243: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/11/23 14:33:56.248
  STEP: updating a scale subresource @ 05/11/23 14:33:56.251
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/11/23 14:33:56.256
  STEP: Patch a scale subresource @ 05/11/23 14:33:56.262
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/11/23 14:33:56.28
  May 11 14:33:56.287: INFO: Deleting all statefulset in ns statefulset-8760
  May 11 14:33:56.293: INFO: Scaling statefulset ss to 0
  May 11 14:34:06.312: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 14:34:06.315: INFO: Deleting statefulset ss
  May 11 14:34:06.329: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-8760" for this suite. @ 05/11/23 14:34:06.334
• [20.138 seconds]
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/11/23 14:34:06.339
  May 11 14:34:06.339: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 14:34:06.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:06.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:06.356
  STEP: creating a Service @ 05/11/23 14:34:06.36
  STEP: watching for the Service to be added @ 05/11/23 14:34:06.372
  May 11 14:34:06.373: INFO: Found Service test-service-sj64l in namespace services-2931 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May 11 14:34:06.373: INFO: Service test-service-sj64l created
  STEP: Getting /status @ 05/11/23 14:34:06.373
  May 11 14:34:06.377: INFO: Service test-service-sj64l has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/11/23 14:34:06.377
  STEP: watching for the Service to be patched @ 05/11/23 14:34:06.385
  May 11 14:34:06.386: INFO: observed Service test-service-sj64l in namespace services-2931 with annotations: map[] & LoadBalancer: {[]}
  May 11 14:34:06.386: INFO: Found Service test-service-sj64l in namespace services-2931 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May 11 14:34:06.386: INFO: Service test-service-sj64l has service status patched
  STEP: updating the ServiceStatus @ 05/11/23 14:34:06.386
  May 11 14:34:06.423: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/11/23 14:34:06.423
  May 11 14:34:06.424: INFO: Observed Service test-service-sj64l in namespace services-2931 with annotations: map[] & Conditions: {[]}
  May 11 14:34:06.425: INFO: Observed event: &Service{ObjectMeta:{test-service-sj64l  services-2931  46d87a65-7777-4253-b537-4200c422e19f 32863 0 2023-05-11 14:34:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-11 14:34:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-11 14:34:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.113.201,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.113.201],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May 11 14:34:06.425: INFO: Observed event: &Service{ObjectMeta:{test-service-sj64l  services-2931  46d87a65-7777-4253-b537-4200c422e19f 32864 0 2023-05-11 14:34:06 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-11 14:34:06 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-11 14:34:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.96.113.201,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.96.113.201],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
  May 11 14:34:06.425: INFO: Found Service test-service-sj64l in namespace services-2931 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 11 14:34:06.425: INFO: Service test-service-sj64l has service status updated
  STEP: patching the service @ 05/11/23 14:34:06.425
  STEP: watching for the Service to be patched @ 05/11/23 14:34:06.437
  May 11 14:34:06.439: INFO: observed Service test-service-sj64l in namespace services-2931 with labels: map[test-service-static:true]
  May 11 14:34:06.439: INFO: observed Service test-service-sj64l in namespace services-2931 with labels: map[test-service-static:true]
  May 11 14:34:06.439: INFO: observed Service test-service-sj64l in namespace services-2931 with labels: map[test-service-static:true]
  May 11 14:34:06.439: INFO: observed Service test-service-sj64l in namespace services-2931 with labels: map[test-service-static:true]
  May 11 14:34:06.439: INFO: Found Service test-service-sj64l in namespace services-2931 with labels: map[test-service:patched test-service-static:true]
  May 11 14:34:06.439: INFO: Service test-service-sj64l patched
  STEP: deleting the service @ 05/11/23 14:34:06.439
  STEP: watching for the Service to be deleted @ 05/11/23 14:34:06.458
  May 11 14:34:06.459: INFO: Observed event: ADDED
  May 11 14:34:06.459: INFO: Observed event: MODIFIED
  May 11 14:34:06.459: INFO: Observed event: MODIFIED
  May 11 14:34:06.459: INFO: Observed event: MODIFIED
  May 11 14:34:06.459: INFO: Observed event: MODIFIED
  May 11 14:34:06.459: INFO: Found Service test-service-sj64l in namespace services-2931 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May 11 14:34:06.459: INFO: Service test-service-sj64l deleted
  May 11 14:34:06.459: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2931" for this suite. @ 05/11/23 14:34:06.463
• [0.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/11/23 14:34:06.469
  May 11 14:34:06.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename discovery @ 05/11/23 14:34:06.47
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:06.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:06.488
  STEP: Setting up server cert @ 05/11/23 14:34:06.49
  May 11 14:34:07.253: INFO: Checking APIGroup: apiregistration.k8s.io
  May 11 14:34:07.254: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May 11 14:34:07.254: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May 11 14:34:07.254: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May 11 14:34:07.254: INFO: Checking APIGroup: apps
  May 11 14:34:07.255: INFO: PreferredVersion.GroupVersion: apps/v1
  May 11 14:34:07.255: INFO: Versions found [{apps/v1 v1}]
  May 11 14:34:07.255: INFO: apps/v1 matches apps/v1
  May 11 14:34:07.255: INFO: Checking APIGroup: events.k8s.io
  May 11 14:34:07.256: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May 11 14:34:07.256: INFO: Versions found [{events.k8s.io/v1 v1}]
  May 11 14:34:07.256: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May 11 14:34:07.256: INFO: Checking APIGroup: authentication.k8s.io
  May 11 14:34:07.257: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May 11 14:34:07.257: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May 11 14:34:07.257: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May 11 14:34:07.257: INFO: Checking APIGroup: authorization.k8s.io
  May 11 14:34:07.257: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May 11 14:34:07.257: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May 11 14:34:07.257: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May 11 14:34:07.257: INFO: Checking APIGroup: autoscaling
  May 11 14:34:07.258: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May 11 14:34:07.258: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May 11 14:34:07.258: INFO: autoscaling/v2 matches autoscaling/v2
  May 11 14:34:07.258: INFO: Checking APIGroup: batch
  May 11 14:34:07.259: INFO: PreferredVersion.GroupVersion: batch/v1
  May 11 14:34:07.259: INFO: Versions found [{batch/v1 v1}]
  May 11 14:34:07.259: INFO: batch/v1 matches batch/v1
  May 11 14:34:07.259: INFO: Checking APIGroup: certificates.k8s.io
  May 11 14:34:07.259: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May 11 14:34:07.259: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May 11 14:34:07.259: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May 11 14:34:07.259: INFO: Checking APIGroup: networking.k8s.io
  May 11 14:34:07.260: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May 11 14:34:07.260: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May 11 14:34:07.260: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May 11 14:34:07.260: INFO: Checking APIGroup: policy
  May 11 14:34:07.261: INFO: PreferredVersion.GroupVersion: policy/v1
  May 11 14:34:07.261: INFO: Versions found [{policy/v1 v1}]
  May 11 14:34:07.261: INFO: policy/v1 matches policy/v1
  May 11 14:34:07.261: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May 11 14:34:07.261: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May 11 14:34:07.261: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May 11 14:34:07.261: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May 11 14:34:07.261: INFO: Checking APIGroup: storage.k8s.io
  May 11 14:34:07.262: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May 11 14:34:07.262: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May 11 14:34:07.262: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May 11 14:34:07.262: INFO: Checking APIGroup: admissionregistration.k8s.io
  May 11 14:34:07.263: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May 11 14:34:07.263: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May 11 14:34:07.263: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May 11 14:34:07.263: INFO: Checking APIGroup: apiextensions.k8s.io
  May 11 14:34:07.264: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May 11 14:34:07.264: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May 11 14:34:07.264: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May 11 14:34:07.264: INFO: Checking APIGroup: scheduling.k8s.io
  May 11 14:34:07.264: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May 11 14:34:07.264: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May 11 14:34:07.264: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May 11 14:34:07.264: INFO: Checking APIGroup: coordination.k8s.io
  May 11 14:34:07.265: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May 11 14:34:07.265: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May 11 14:34:07.265: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May 11 14:34:07.265: INFO: Checking APIGroup: node.k8s.io
  May 11 14:34:07.266: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May 11 14:34:07.266: INFO: Versions found [{node.k8s.io/v1 v1}]
  May 11 14:34:07.266: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May 11 14:34:07.266: INFO: Checking APIGroup: discovery.k8s.io
  May 11 14:34:07.266: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May 11 14:34:07.266: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May 11 14:34:07.266: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May 11 14:34:07.266: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May 11 14:34:07.267: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May 11 14:34:07.267: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May 11 14:34:07.267: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May 11 14:34:07.267: INFO: Checking APIGroup: crd.projectcalico.org
  May 11 14:34:07.268: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  May 11 14:34:07.268: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  May 11 14:34:07.268: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  May 11 14:34:07.268: INFO: Checking APIGroup: logicmonitor.com
  May 11 14:34:07.269: INFO: PreferredVersion.GroupVersion: logicmonitor.com/v1alpha2
  May 11 14:34:07.269: INFO: Versions found [{logicmonitor.com/v1alpha2 v1alpha2} {logicmonitor.com/v1alpha1 v1alpha1}]
  May 11 14:34:07.269: INFO: logicmonitor.com/v1alpha2 matches logicmonitor.com/v1alpha2
  May 11 14:34:07.269: INFO: Checking APIGroup: macstadium.orka.com
  May 11 14:34:07.269: INFO: PreferredVersion.GroupVersion: macstadium.orka.com/v1alpha1
  May 11 14:34:07.269: INFO: Versions found [{macstadium.orka.com/v1alpha1 v1alpha1}]
  May 11 14:34:07.269: INFO: macstadium.orka.com/v1alpha1 matches macstadium.orka.com/v1alpha1
  May 11 14:34:07.269: INFO: Checking APIGroup: metallb.io
  May 11 14:34:07.270: INFO: PreferredVersion.GroupVersion: metallb.io/v1beta2
  May 11 14:34:07.270: INFO: Versions found [{metallb.io/v1beta2 v1beta2} {metallb.io/v1beta1 v1beta1} {metallb.io/v1alpha1 v1alpha1}]
  May 11 14:34:07.270: INFO: metallb.io/v1beta2 matches metallb.io/v1beta2
  May 11 14:34:07.270: INFO: Checking APIGroup: traefik.containo.us
  May 11 14:34:07.270: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
  May 11 14:34:07.270: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
  May 11 14:34:07.270: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
  May 11 14:34:07.270: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-1403" for this suite. @ 05/11/23 14:34:07.275
• [0.811 seconds]
------------------------------
SS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/11/23 14:34:07.281
  May 11 14:34:07.281: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename security-context-test @ 05/11/23 14:34:07.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:07.296
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:07.298
  May 11 14:34:11.338: INFO: Got logs for pod "busybox-privileged-false-8dcc0116-4252-4cd6-b4ce-39c00524edf9": "ip: RTNETLINK answers: Operation not permitted\n"
  May 11 14:34:11.338: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-5836" for this suite. @ 05/11/23 14:34:11.344
• [4.071 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/11/23 14:34:11.353
  May 11 14:34:11.353: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename watch @ 05/11/23 14:34:11.354
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:11.374
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:11.376
  STEP: creating a watch on configmaps with label A @ 05/11/23 14:34:11.379
  STEP: creating a watch on configmaps with label B @ 05/11/23 14:34:11.38
  STEP: creating a watch on configmaps with label A or B @ 05/11/23 14:34:11.381
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/11/23 14:34:11.382
  May 11 14:34:11.387: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4440  a7ead7be-7c1c-4da3-96f3-fa88715c8fa0 32911 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:34:11.387: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4440  a7ead7be-7c1c-4da3-96f3-fa88715c8fa0 32911 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/11/23 14:34:11.387
  May 11 14:34:11.395: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4440  a7ead7be-7c1c-4da3-96f3-fa88715c8fa0 32913 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:34:11.395: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4440  a7ead7be-7c1c-4da3-96f3-fa88715c8fa0 32913 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/11/23 14:34:11.395
  May 11 14:34:11.401: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4440  a7ead7be-7c1c-4da3-96f3-fa88715c8fa0 32915 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:34:11.401: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4440  a7ead7be-7c1c-4da3-96f3-fa88715c8fa0 32915 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/11/23 14:34:11.401
  May 11 14:34:11.405: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4440  a7ead7be-7c1c-4da3-96f3-fa88715c8fa0 32917 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:34:11.406: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-4440  a7ead7be-7c1c-4da3-96f3-fa88715c8fa0 32917 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/11/23 14:34:11.406
  May 11 14:34:11.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4440  06a5adb4-006b-4880-8918-07723263ff68 32919 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:34:11.410: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4440  06a5adb4-006b-4880-8918-07723263ff68 32919 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/11/23 14:34:21.412
  May 11 14:34:21.419: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4440  06a5adb4-006b-4880-8918-07723263ff68 32977 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:34:21.419: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-4440  06a5adb4-006b-4880-8918-07723263ff68 32977 0 2023-05-11 14:34:11 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-11 14:34:11 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:34:31.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4440" for this suite. @ 05/11/23 14:34:31.426
• [20.079 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/11/23 14:34:31.432
  May 11 14:34:31.432: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 14:34:31.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:31.45
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:31.453
  STEP: creating a secret @ 05/11/23 14:34:31.455
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/11/23 14:34:31.46
  STEP: patching the secret @ 05/11/23 14:34:31.463
  STEP: deleting the secret using a LabelSelector @ 05/11/23 14:34:31.471
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/11/23 14:34:31.48
  May 11 14:34:31.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6620" for this suite. @ 05/11/23 14:34:31.488
• [0.063 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/11/23 14:34:31.495
  May 11 14:34:31.495: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:34:31.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:31.51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:31.513
  STEP: Creating configMap with name projected-configmap-test-volume-bae01d7d-d088-4fb7-a56c-573db2e7daaf @ 05/11/23 14:34:31.515
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:34:31.52
  STEP: Saw pod success @ 05/11/23 14:34:35.545
  May 11 14:34:35.547: INFO: Trying to get logs from node macpro-2 pod pod-projected-configmaps-8012d4de-b3ad-4bfc-8a25-6332afc90f31 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:34:35.552
  May 11 14:34:35.576: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4531" for this suite. @ 05/11/23 14:34:35.582
• [4.104 seconds]
------------------------------
SSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/11/23 14:34:35.599
  May 11 14:34:35.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:34:35.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:35.618
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:35.62
  STEP: starting the proxy server @ 05/11/23 14:34:35.622
  May 11 14:34:35.622: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-307 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/11/23 14:34:35.665
  May 11 14:34:35.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-307" for this suite. @ 05/11/23 14:34:35.677
• [0.083 seconds]
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/11/23 14:34:35.682
  May 11 14:34:35.682: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 14:34:35.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:35.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:35.698
  STEP: creating service in namespace services-4590 @ 05/11/23 14:34:35.7
  STEP: creating service affinity-clusterip-transition in namespace services-4590 @ 05/11/23 14:34:35.7
  STEP: creating replication controller affinity-clusterip-transition in namespace services-4590 @ 05/11/23 14:34:35.712
  I0511 14:34:35.726832      24 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-4590, replica count: 3
  I0511 14:34:38.778795      24 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 14:34:38.784: INFO: Creating new exec pod
  May 11 14:34:41.797: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-4590 exec execpod-affinityv9hnf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  May 11 14:34:41.922: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May 11 14:34:41.922: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:34:41.922: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-4590 exec execpod-affinityv9hnf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.111.121.246 80'
  May 11 14:34:42.035: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.111.121.246 80\nConnection to 10.111.121.246 80 port [tcp/http] succeeded!\n"
  May 11 14:34:42.035: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:34:42.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-4590 exec execpod-affinityv9hnf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.121.246:80/ ; done'
  May 11 14:34:42.210: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n"
  May 11 14:34:42.210: INFO: stdout: "\naffinity-clusterip-transition-xhkq9\naffinity-clusterip-transition-hbs9l\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-hbs9l\naffinity-clusterip-transition-xhkq9\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-xhkq9\naffinity-clusterip-transition-hbs9l\naffinity-clusterip-transition-hbs9l\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-xhkq9\naffinity-clusterip-transition-hbs9l\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-xhkq9"
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-xhkq9
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-hbs9l
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-hbs9l
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-xhkq9
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-xhkq9
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-hbs9l
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-hbs9l
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-xhkq9
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-hbs9l
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.211: INFO: Received response from host: affinity-clusterip-transition-xhkq9
  May 11 14:34:42.222: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-4590 exec execpod-affinityv9hnf -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.111.121.246:80/ ; done'
  May 11 14:34:42.376: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.111.121.246:80/\n"
  May 11 14:34:42.376: INFO: stdout: "\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x\naffinity-clusterip-transition-czw5x"
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Received response from host: affinity-clusterip-transition-czw5x
  May 11 14:34:42.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 14:34:42.382: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4590, will wait for the garbage collector to delete the pods @ 05/11/23 14:34:42.395
  May 11 14:34:42.458: INFO: Deleting ReplicationController affinity-clusterip-transition took: 7.081684ms
  May 11 14:34:42.559: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.854204ms
  STEP: Destroying namespace "services-4590" for this suite. @ 05/11/23 14:34:44.686
• [9.009 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/11/23 14:34:44.691
  May 11 14:34:44.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename events @ 05/11/23 14:34:44.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:44.707
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:44.709
  STEP: creating a test event @ 05/11/23 14:34:44.711
  STEP: listing all events in all namespaces @ 05/11/23 14:34:44.714
  STEP: patching the test event @ 05/11/23 14:34:44.717
  STEP: fetching the test event @ 05/11/23 14:34:44.722
  STEP: updating the test event @ 05/11/23 14:34:44.724
  STEP: getting the test event @ 05/11/23 14:34:44.731
  STEP: deleting the test event @ 05/11/23 14:34:44.733
  STEP: listing all events in all namespaces @ 05/11/23 14:34:44.738
  May 11 14:34:44.741: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-1723" for this suite. @ 05/11/23 14:34:44.745
• [0.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/11/23 14:34:44.753
  May 11 14:34:44.753: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replication-controller @ 05/11/23 14:34:44.754
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:44.772
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:44.775
  STEP: Given a ReplicationController is created @ 05/11/23 14:34:44.776
  STEP: When the matched label of one of its pods change @ 05/11/23 14:34:44.781
  May 11 14:34:44.785: INFO: Pod name pod-release: Found 0 pods out of 1
  May 11 14:34:49.788: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/11/23 14:34:49.797
  May 11 14:34:50.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3563" for this suite. @ 05/11/23 14:34:50.812
• [6.065 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/11/23 14:34:50.82
  May 11 14:34:50.820: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubelet-test @ 05/11/23 14:34:50.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:50.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:50.838
  May 11 14:34:52.875: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-5704" for this suite. @ 05/11/23 14:34:52.879
• [2.064 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/11/23 14:34:52.884
  May 11 14:34:52.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:34:52.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:52.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:52.903
  STEP: Creating configMap with name configmap-test-upd-0bf2c45a-468e-46c9-98c0-5251e8d49eb1 @ 05/11/23 14:34:52.91
  STEP: Creating the pod @ 05/11/23 14:34:52.914
  STEP: Waiting for pod with text data @ 05/11/23 14:34:54.932
  STEP: Waiting for pod with binary data @ 05/11/23 14:34:54.938
  May 11 14:34:54.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-844" for this suite. @ 05/11/23 14:34:54.946
• [2.068 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/11/23 14:34:54.953
  May 11 14:34:54.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 14:34:54.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:34:54.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:34:54.97
  STEP: Counting existing ResourceQuota @ 05/11/23 14:34:54.972
  STEP: Creating a ResourceQuota @ 05/11/23 14:34:59.975
  STEP: Ensuring resource quota status is calculated @ 05/11/23 14:34:59.98
  STEP: Creating a ReplicationController @ 05/11/23 14:35:01.985
  STEP: Ensuring resource quota status captures replication controller creation @ 05/11/23 14:35:01.995
  STEP: Deleting a ReplicationController @ 05/11/23 14:35:04
  STEP: Ensuring resource quota status released usage @ 05/11/23 14:35:04.008
  May 11 14:35:06.014: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-194" for this suite. @ 05/11/23 14:35:06.018
• [11.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/11/23 14:35:06.025
  May 11 14:35:06.025: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/11/23 14:35:06.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:06.041
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:06.043
  May 11 14:35:06.045: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:35:09.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-3595" for this suite. @ 05/11/23 14:35:09.396
• [3.378 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/11/23 14:35:09.403
  May 11 14:35:09.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename namespaces @ 05/11/23 14:35:09.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:09.431
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:09.434
  STEP: Updating Namespace "namespaces-9597" @ 05/11/23 14:35:09.437
  May 11 14:35:09.448: INFO: Namespace "namespaces-9597" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"3f89617c-aa68-4c2a-8d4a-02ca06753a9f", "kubernetes.io/metadata.name":"namespaces-9597", "namespaces-9597":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May 11 14:35:09.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9597" for this suite. @ 05/11/23 14:35:09.453
• [0.058 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/11/23 14:35:09.461
  May 11 14:35:09.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 14:35:09.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:09.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:09.481
  STEP: creating service nodeport-test with type=NodePort in namespace services-97 @ 05/11/23 14:35:09.483
  STEP: creating replication controller nodeport-test in namespace services-97 @ 05/11/23 14:35:09.523
  I0511 14:35:09.548253      24 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-97, replica count: 2
  I0511 14:35:12.600540      24 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 14:35:12.600: INFO: Creating new exec pod
  May 11 14:35:15.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-97 exec execpodp57dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 11 14:35:15.728: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 11 14:35:15.728: INFO: stdout: "nodeport-test-s4qrn"
  May 11 14:35:15.728: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-97 exec execpodp57dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.234.7 80'
  May 11 14:35:15.835: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.234.7 80\nConnection to 10.102.234.7 80 port [tcp/http] succeeded!\n"
  May 11 14:35:15.835: INFO: stdout: "nodeport-test-s4qrn"
  May 11 14:35:15.836: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-97 exec execpodp57dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.11 30478'
  May 11 14:35:15.944: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.11 30478\nConnection to 10.221.188.11 30478 port [tcp/*] succeeded!\n"
  May 11 14:35:15.944: INFO: stdout: "nodeport-test-s4qrn"
  May 11 14:35:15.944: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-97 exec execpodp57dq -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.12 30478'
  May 11 14:35:16.056: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.12 30478\nConnection to 10.221.188.12 30478 port [tcp/*] succeeded!\n"
  May 11 14:35:16.056: INFO: stdout: "nodeport-test-glzp4"
  May 11 14:35:16.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-97" for this suite. @ 05/11/23 14:35:16.061
• [6.606 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/11/23 14:35:16.069
  May 11 14:35:16.069: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:35:16.07
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:16.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:16.086
  STEP: Creating configMap configmap-7060/configmap-test-42d17562-80a6-48f5-83a4-c87dd012fc4a @ 05/11/23 14:35:16.09
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:35:16.094
  STEP: Saw pod success @ 05/11/23 14:35:20.112
  May 11 14:35:20.119: INFO: Trying to get logs from node macpro-2 pod pod-configmaps-09dee38e-8eb6-4d6e-afa6-2827db014d91 container env-test: <nil>
  STEP: delete the pod @ 05/11/23 14:35:20.124
  May 11 14:35:20.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7060" for this suite. @ 05/11/23 14:35:20.142
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/11/23 14:35:20.151
  May 11 14:35:20.151: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename daemonsets @ 05/11/23 14:35:20.152
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:20.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:20.17
  May 11 14:35:20.186: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/11/23 14:35:20.191
  May 11 14:35:20.199: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:20.199: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/11/23 14:35:20.199
  May 11 14:35:20.224: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:20.224: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:35:21.235: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:21.235: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:35:22.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 11 14:35:22.228: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/11/23 14:35:22.23
  May 11 14:35:22.254: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 11 14:35:22.254: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  May 11 14:35:23.258: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:23.258: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/11/23 14:35:23.258
  May 11 14:35:23.271: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:23.271: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:35:24.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:24.275: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:35:25.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:25.275: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:35:26.275: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 11 14:35:26.275: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/11/23 14:35:26.279
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9280, will wait for the garbage collector to delete the pods @ 05/11/23 14:35:26.279
  May 11 14:35:26.337: INFO: Deleting DaemonSet.extensions daemon-set took: 5.466437ms
  May 11 14:35:26.438: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.079693ms
  May 11 14:35:28.542: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:28.542: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 11 14:35:28.545: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33718"},"items":null}

  May 11 14:35:28.547: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33718"},"items":null}

  May 11 14:35:28.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9280" for this suite. @ 05/11/23 14:35:28.584
• [8.438 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/11/23 14:35:28.592
  May 11 14:35:28.592: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:35:28.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:28.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:28.61
  STEP: Setting up server cert @ 05/11/23 14:35:28.632
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:35:29.084
  STEP: Deploying the webhook pod @ 05/11/23 14:35:29.091
  STEP: Wait for the deployment to be ready @ 05/11/23 14:35:29.107
  May 11 14:35:29.127: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/11/23 14:35:31.136
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:35:31.15
  May 11 14:35:32.151: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/11/23 14:35:32.153
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/11/23 14:35:32.154
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/11/23 14:35:32.154
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/11/23 14:35:32.154
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/11/23 14:35:32.155
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/11/23 14:35:32.155
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/11/23 14:35:32.156
  May 11 14:35:32.156: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1829" for this suite. @ 05/11/23 14:35:32.212
  STEP: Destroying namespace "webhook-markers-5039" for this suite. @ 05/11/23 14:35:32.218
• [3.639 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/11/23 14:35:32.232
  May 11 14:35:32.232: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubelet-test @ 05/11/23 14:35:32.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:32.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:32.249
  STEP: Waiting for pod completion @ 05/11/23 14:35:32.258
  May 11 14:35:36.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8461" for this suite. @ 05/11/23 14:35:36.282
• [4.056 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/11/23 14:35:36.288
  May 11 14:35:36.288: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 14:35:36.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:36.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:36.303
  May 11 14:35:36.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/11/23 14:35:37.895
  May 11 14:35:37.895: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-2517 --namespace=crd-publish-openapi-2517 create -f -'
  May 11 14:35:38.775: INFO: stderr: ""
  May 11 14:35:38.775: INFO: stdout: "e2e-test-crd-publish-openapi-5480-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 11 14:35:38.775: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-2517 --namespace=crd-publish-openapi-2517 delete e2e-test-crd-publish-openapi-5480-crds test-cr'
  May 11 14:35:38.851: INFO: stderr: ""
  May 11 14:35:38.851: INFO: stdout: "e2e-test-crd-publish-openapi-5480-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May 11 14:35:38.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-2517 --namespace=crd-publish-openapi-2517 apply -f -'
  May 11 14:35:39.588: INFO: stderr: ""
  May 11 14:35:39.588: INFO: stdout: "e2e-test-crd-publish-openapi-5480-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 11 14:35:39.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-2517 --namespace=crd-publish-openapi-2517 delete e2e-test-crd-publish-openapi-5480-crds test-cr'
  May 11 14:35:39.651: INFO: stderr: ""
  May 11 14:35:39.651: INFO: stdout: "e2e-test-crd-publish-openapi-5480-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/11/23 14:35:39.651
  May 11 14:35:39.651: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=crd-publish-openapi-2517 explain e2e-test-crd-publish-openapi-5480-crds'
  May 11 14:35:39.861: INFO: stderr: ""
  May 11 14:35:39.861: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-5480-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  May 11 14:35:41.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2517" for this suite. @ 05/11/23 14:35:41.589
• [5.305 seconds]
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/11/23 14:35:41.593
  May 11 14:35:41.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename security-context @ 05/11/23 14:35:41.594
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:41.614
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:41.617
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/11/23 14:35:41.619
  STEP: Saw pod success @ 05/11/23 14:35:45.638
  May 11 14:35:45.641: INFO: Trying to get logs from node macpro-2 pod security-context-4c471340-950d-49d8-ab71-e784a9525360 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:35:45.653
  May 11 14:35:45.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3509" for this suite. @ 05/11/23 14:35:45.667
• [4.078 seconds]
------------------------------
SSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/11/23 14:35:45.672
  May 11 14:35:45.672: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename conformance-tests @ 05/11/23 14:35:45.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:45.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:45.688
  STEP: Getting node addresses @ 05/11/23 14:35:45.69
  May 11 14:35:45.690: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May 11 14:35:45.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-3027" for this suite. @ 05/11/23 14:35:45.701
• [0.034 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/11/23 14:35:45.706
  May 11 14:35:45.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:35:45.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:45.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:45.724
  STEP: Creating configMap with name projected-configmap-test-volume-dc2eec4d-f3a9-4838-b128-11fa9340eea9 @ 05/11/23 14:35:45.726
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:35:45.73
  STEP: Saw pod success @ 05/11/23 14:35:49.749
  May 11 14:35:49.751: INFO: Trying to get logs from node macpro-2 pod pod-projected-configmaps-116e58de-2b8a-491c-8311-001cada35dd3 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:35:49.761
  May 11 14:35:49.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6922" for this suite. @ 05/11/23 14:35:49.778
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/11/23 14:35:49.783
  May 11 14:35:49.783: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:35:49.784
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:49.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:49.801
  STEP: Setting up server cert @ 05/11/23 14:35:49.824
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:35:50.347
  STEP: Deploying the webhook pod @ 05/11/23 14:35:50.354
  STEP: Wait for the deployment to be ready @ 05/11/23 14:35:50.363
  May 11 14:35:50.367: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 14:35:52.376
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:35:52.387
  May 11 14:35:53.387: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/11/23 14:35:53.391
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/11/23 14:35:53.407
  May 11 14:35:53.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:35:53.416: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5158" for this suite. @ 05/11/23 14:35:53.461
  STEP: Destroying namespace "webhook-markers-9571" for this suite. @ 05/11/23 14:35:53.469
• [3.693 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/11/23 14:35:53.476
  May 11 14:35:53.476: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename field-validation @ 05/11/23 14:35:53.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:53.492
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:53.495
  May 11 14:35:53.497: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  W0511 14:35:56.049545      24 warnings.go:70] unknown field "alpha"
  W0511 14:35:56.049575      24 warnings.go:70] unknown field "beta"
  W0511 14:35:56.049582      24 warnings.go:70] unknown field "delta"
  W0511 14:35:56.049587      24 warnings.go:70] unknown field "epsilon"
  W0511 14:35:56.049592      24 warnings.go:70] unknown field "gamma"
  May 11 14:35:56.058: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8720" for this suite. @ 05/11/23 14:35:56.069
• [2.597 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/11/23 14:35:56.074
  May 11 14:35:56.074: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename daemonsets @ 05/11/23 14:35:56.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:35:56.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:35:56.09
  May 11 14:35:56.112: INFO: Create a RollingUpdate DaemonSet
  May 11 14:35:56.116: INFO: Check that daemon pods launch on every node of the cluster
  May 11 14:35:56.119: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:56.119: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:56.119: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:56.121: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:56.121: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:35:57.129: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:57.129: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:57.129: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:57.135: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:35:57.135: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:35:58.126: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:58.126: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:58.126: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:35:58.129: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 14:35:58.129: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  May 11 14:35:58.129: INFO: Update the DaemonSet to trigger a rollout
  May 11 14:35:58.137: INFO: Updating DaemonSet daemon-set
  May 11 14:36:01.151: INFO: Roll back the DaemonSet before rollout is complete
  May 11 14:36:01.158: INFO: Updating DaemonSet daemon-set
  May 11 14:36:01.158: INFO: Make sure DaemonSet rollback is complete
  May 11 14:36:01.161: INFO: Wrong image for pod: daemon-set-8s8l5. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May 11 14:36:01.161: INFO: Pod daemon-set-8s8l5 is not available
  May 11 14:36:01.165: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:36:01.165: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:36:01.165: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:36:02.172: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:36:02.172: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:36:02.172: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:36:03.169: INFO: Pod daemon-set-ljsvf is not available
  May 11 14:36:03.173: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:36:03.173: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:36:03.173: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 05/11/23 14:36:03.177
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8603, will wait for the garbage collector to delete the pods @ 05/11/23 14:36:03.177
  May 11 14:36:03.237: INFO: Deleting DaemonSet.extensions daemon-set took: 7.358232ms
  May 11 14:36:03.338: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.820974ms
  May 11 14:36:04.841: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:36:04.841: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 11 14:36:04.843: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"34229"},"items":null}

  May 11 14:36:04.845: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"34229"},"items":null}

  May 11 14:36:04.857: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8603" for this suite. @ 05/11/23 14:36:04.861
• [8.793 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/11/23 14:36:04.868
  May 11 14:36:04.868: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:36:04.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:04.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:04.886
  STEP: Creating configMap with name configmap-test-volume-map-fd309993-9ecc-4839-905e-c41068df513d @ 05/11/23 14:36:04.889
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:36:04.892
  STEP: Saw pod success @ 05/11/23 14:36:08.907
  May 11 14:36:08.918: INFO: Trying to get logs from node macpro-2 pod pod-configmaps-b52f9d38-08d9-42a8-9aa3-37b588a463bc container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:36:08.93
  May 11 14:36:08.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7601" for this suite. @ 05/11/23 14:36:08.948
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/11/23 14:36:08.954
  May 11 14:36:08.954: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename disruption @ 05/11/23 14:36:08.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:08.969
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:08.971
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/11/23 14:36:08.973
  STEP: Waiting for the pdb to be processed @ 05/11/23 14:36:08.977
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/11/23 14:36:10.988
  STEP: Waiting for all pods to be running @ 05/11/23 14:36:10.988
  May 11 14:36:10.991: INFO: pods: 0 < 3
  STEP: locating a running pod @ 05/11/23 14:36:12.999
  STEP: Updating the pdb to allow a pod to be evicted @ 05/11/23 14:36:13.045
  STEP: Waiting for the pdb to be processed @ 05/11/23 14:36:13.067
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/11/23 14:36:15.076
  STEP: Waiting for all pods to be running @ 05/11/23 14:36:15.076
  STEP: Waiting for the pdb to observed all healthy pods @ 05/11/23 14:36:15.079
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/11/23 14:36:15.102
  STEP: Waiting for the pdb to be processed @ 05/11/23 14:36:15.115
  STEP: Waiting for all pods to be running @ 05/11/23 14:36:17.166
  STEP: locating a running pod @ 05/11/23 14:36:17.169
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/11/23 14:36:17.177
  STEP: Waiting for the pdb to be deleted @ 05/11/23 14:36:17.183
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/11/23 14:36:17.187
  STEP: Waiting for all pods to be running @ 05/11/23 14:36:17.187
  May 11 14:36:17.209: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5751" for this suite. @ 05/11/23 14:36:17.214
• [8.268 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/11/23 14:36:17.222
  May 11 14:36:17.222: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 14:36:17.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:17.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:17.249
  STEP: Create set of pods @ 05/11/23 14:36:17.253
  May 11 14:36:17.263: INFO: created test-pod-1
  May 11 14:36:17.269: INFO: created test-pod-2
  May 11 14:36:17.279: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/11/23 14:36:17.28
  STEP: waiting for all pods to be deleted @ 05/11/23 14:36:19.32
  May 11 14:36:19.322: INFO: Pod quantity 3 is different from expected quantity 0
  May 11 14:36:20.326: INFO: Pod quantity 3 is different from expected quantity 0
  May 11 14:36:21.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9204" for this suite. @ 05/11/23 14:36:21.33
• [4.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/11/23 14:36:21.335
  May 11 14:36:21.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/11/23 14:36:21.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:21.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:21.358
  May 11 14:36:23.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/11/23 14:36:23.392
  STEP: Cleaning up the configmap @ 05/11/23 14:36:23.396
  STEP: Cleaning up the pod @ 05/11/23 14:36:23.4
  STEP: Destroying namespace "emptydir-wrapper-2775" for this suite. @ 05/11/23 14:36:23.408
• [2.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/11/23 14:36:23.414
  May 11 14:36:23.414: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 14:36:23.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:23.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:23.435
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 14:36:23.437
  STEP: Saw pod success @ 05/11/23 14:36:25.455
  May 11 14:36:25.457: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-fa5a6f76-a382-49f3-9557-c0357d85951a container client-container: <nil>
  STEP: delete the pod @ 05/11/23 14:36:25.462
  May 11 14:36:25.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8739" for this suite. @ 05/11/23 14:36:25.478
• [2.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/11/23 14:36:25.484
  May 11 14:36:25.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename lease-test @ 05/11/23 14:36:25.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:25.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:25.505
  May 11 14:36:25.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-5221" for this suite. @ 05/11/23 14:36:25.56
• [0.081 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/11/23 14:36:25.566
  May 11 14:36:25.566: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-runtime @ 05/11/23 14:36:25.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:25.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:25.585
  STEP: create the container @ 05/11/23 14:36:25.587
  W0511 14:36:25.592370      24 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/11/23 14:36:25.592
  STEP: get the container status @ 05/11/23 14:36:28.606
  STEP: the container should be terminated @ 05/11/23 14:36:28.608
  STEP: the termination message should be set @ 05/11/23 14:36:28.608
  May 11 14:36:28.608: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/11/23 14:36:28.608
  May 11 14:36:28.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1391" for this suite. @ 05/11/23 14:36:28.624
• [3.062 seconds]
------------------------------
S
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/11/23 14:36:28.628
  May 11 14:36:28.628: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename deployment @ 05/11/23 14:36:28.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:28.652
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:28.654
  STEP: creating a Deployment @ 05/11/23 14:36:28.661
  STEP: waiting for Deployment to be created @ 05/11/23 14:36:28.665
  STEP: waiting for all Replicas to be Ready @ 05/11/23 14:36:28.666
  May 11 14:36:28.667: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 11 14:36:28.667: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 11 14:36:28.677: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 11 14:36:28.677: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 11 14:36:28.688: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 11 14:36:28.688: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 11 14:36:28.730: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 11 14:36:28.730: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 11 14:36:29.837: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 11 14:36:29.837: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 11 14:36:30.243: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/11/23 14:36:30.243
  W0511 14:36:30.253559      24 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 11 14:36:30.255: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/11/23 14:36:30.255
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 0
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:30.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:30.266: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:30.266: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:30.310: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:30.310: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:30.357: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:30.357: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:30.412: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:30.412: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:31.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:31.256: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:31.280: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  STEP: listing Deployments @ 05/11/23 14:36:31.28
  May 11 14:36:31.284: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/11/23 14:36:31.284
  May 11 14:36:31.296: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/11/23 14:36:31.296
  May 11 14:36:31.301: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:31.307: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:31.327: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:31.351: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:31.367: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:31.380: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:32.264: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:32.288: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:32.304: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:32.320: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 11 14:36:33.858: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/11/23 14:36:33.902
  STEP: fetching the DeploymentStatus @ 05/11/23 14:36:33.907
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 1
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 2
  May 11 14:36:33.916: INFO: observed Deployment test-deployment in namespace deployment-8202 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/11/23 14:36:33.916
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.924: INFO: observed event type MODIFIED
  May 11 14:36:33.925: INFO: observed event type MODIFIED
  May 11 14:36:33.925: INFO: observed event type MODIFIED
  May 11 14:36:33.927: INFO: Log out all the ReplicaSets if there is no deployment created
  May 11 14:36:33.929: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-8202  855de8ec-2ef1-4e1f-b317-670ec3e1c3ac 34771 3 2023-05-11 14:36:28 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 7e06db27-9b69-4a6d-8f41-1e0b323b17c8 0xc004275f67 0xc004275f68}] [] [{kube-controller-manager Update apps/v1 2023-05-11 14:36:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e06db27-9b69-4a6d-8f41-1e0b323b17c8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 14:36:31 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fc8000 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 11 14:36:33.932: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-8202  67816a53-3691-4300-b372-71bb9304d086 34876 4 2023-05-11 14:36:30 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 7e06db27-9b69-4a6d-8f41-1e0b323b17c8 0xc004fc8067 0xc004fc8068}] [] [{kube-controller-manager Update apps/v1 2023-05-11 14:36:33 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e06db27-9b69-4a6d-8f41-1e0b323b17c8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 14:36:33 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fc80f0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 11 14:36:33.934: INFO: pod: "test-deployment-5b5dcbcd95-54j2j":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-54j2j test-deployment-5b5dcbcd95- deployment-8202  d763fe09-ade8-4b8c-850d-e520a21727d3 34872 0 2023-05-11 14:36:30 +0000 UTC 2023-05-11 14:36:34 +0000 UTC 0xc004fc8758 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:22581572f2fbda26482d3b96d11f9ae97af523f0b9fd30d431f1e295cc4aaba8 cni.projectcalico.org/podIP:192.168.150.162/32 cni.projectcalico.org/podIPs:192.168.150.162/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 67816a53-3691-4300-b372-71bb9304d086 0xc004fc8787 0xc004fc8788}] [] [{calico Update v1 2023-05-11 14:36:30 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:36:30 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67816a53-3691-4300-b372-71bb9304d086\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:36:31 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zcmbm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zcmbm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:30 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:30 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.162,StartTime:2023-05-11 14:36:30 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:36:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://86d52b98dec4c8c69ee96e1b0a7089007df2a70c87a73f908c8ef034e3e461ef,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.162,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 11 14:36:33.934: INFO: pod: "test-deployment-5b5dcbcd95-5hwjz":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-5hwjz test-deployment-5b5dcbcd95- deployment-8202  2cf10c99-3020-47da-be09-4b9b74a67654 34848 0 2023-05-11 14:36:31 +0000 UTC 2023-05-11 14:36:33 +0000 UTC 0xc004fc8970 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:cac54e1246d60a54889afdf0ceff25469cf7f266d3bbe29f0aed8f02eea5493f cni.projectcalico.org/podIP:192.168.153.62/32 cni.projectcalico.org/podIPs:192.168.153.62/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 67816a53-3691-4300-b372-71bb9304d086 0xc004fc89a7 0xc004fc89a8}] [] [{calico Update v1 2023-05-11 14:36:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:36:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"67816a53-3691-4300-b372-71bb9304d086\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:36:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c7659,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c7659,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:192.168.153.62,StartTime:2023-05-11 14:36:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:36:31 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://97a7124df62dcff121cb2be62eb77dc27db739ef54b398e109938c8c0bbda571,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.62,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 11 14:36:33.934: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-8202  39ae6f34-c65c-4a91-9db7-723eae490f97 34868 2 2023-05-11 14:36:31 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 7e06db27-9b69-4a6d-8f41-1e0b323b17c8 0xc004fc8157 0xc004fc8158}] [] [{kube-controller-manager Update apps/v1 2023-05-11 14:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e06db27-9b69-4a6d-8f41-1e0b323b17c8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 14:36:33 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004fc81e0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  May 11 14:36:33.938: INFO: pod: "test-deployment-6fc78d85c6-2zjdq":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-2zjdq test-deployment-6fc78d85c6- deployment-8202  5c819d70-71c1-4a8a-92f9-cf74156c11da 34867 0 2023-05-11 14:36:32 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:c1d883e8a2268115ff8184e6726c040c599fa9ea58802dd24321de5333f3c913 cni.projectcalico.org/podIP:192.168.153.63/32 cni.projectcalico.org/podIPs:192.168.153.63/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 39ae6f34-c65c-4a91-9db7-723eae490f97 0xc004fc9e27 0xc004fc9e28}] [] [{calico Update v1 2023-05-11 14:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:36:32 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39ae6f34-c65c-4a91-9db7-723eae490f97\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:36:33 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.63\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tfsk7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tfsk7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:33 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:32 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:192.168.153.63,StartTime:2023-05-11 14:36:32 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:36:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1f654fa5d5e02dce3c15f79ee7a41619a2178c289e63a0d8672d4feb8c229609,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.63,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 11 14:36:33.938: INFO: pod: "test-deployment-6fc78d85c6-l6fnx":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-l6fnx test-deployment-6fc78d85c6- deployment-8202  b4bf0e72-8789-40e3-bd7e-35fef8a5699b 34818 0 2023-05-11 14:36:31 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:21c03734973314504001d439e075208dd1578469f74890ba55bd0a123ab2b0a0 cni.projectcalico.org/podIP:192.168.150.135/32 cni.projectcalico.org/podIPs:192.168.150.135/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 39ae6f34-c65c-4a91-9db7-723eae490f97 0xc004e8a347 0xc004e8a348}] [] [{calico Update v1 2023-05-11 14:36:31 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:36:31 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"39ae6f34-c65c-4a91-9db7-723eae490f97\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:36:32 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-dt5j2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-dt5j2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:31 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:32 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:36:31 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.135,StartTime:2023-05-11 14:36:31 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:36:32 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://78537ead26659071189dab8a494007fabaea593821dc407a2f9095a0e09f2098,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.135,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 11 14:36:33.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8202" for this suite. @ 05/11/23 14:36:33.942
• [5.319 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/11/23 14:36:33.947
  May 11 14:36:33.947: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename subpath @ 05/11/23 14:36:33.948
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:33.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:33.969
  STEP: Setting up data @ 05/11/23 14:36:33.971
  STEP: Creating pod pod-subpath-test-configmap-b5pl @ 05/11/23 14:36:33.979
  STEP: Creating a pod to test atomic-volume-subpath @ 05/11/23 14:36:33.979
  STEP: Saw pod success @ 05/11/23 14:36:58.042
  May 11 14:36:58.048: INFO: Trying to get logs from node macpro-2 pod pod-subpath-test-configmap-b5pl container test-container-subpath-configmap-b5pl: <nil>
  STEP: delete the pod @ 05/11/23 14:36:58.054
  STEP: Deleting pod pod-subpath-test-configmap-b5pl @ 05/11/23 14:36:58.065
  May 11 14:36:58.065: INFO: Deleting pod "pod-subpath-test-configmap-b5pl" in namespace "subpath-5631"
  May 11 14:36:58.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5631" for this suite. @ 05/11/23 14:36:58.07
• [24.127 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/11/23 14:36:58.075
  May 11 14:36:58.075: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 14:36:58.075
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:36:58.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:36:58.094
  STEP: Creating secret with name secret-test-map-5416e735-2f82-4626-b80e-764b8e984eb6 @ 05/11/23 14:36:58.097
  STEP: Creating a pod to test consume secrets @ 05/11/23 14:36:58.1
  STEP: Saw pod success @ 05/11/23 14:37:02.12
  May 11 14:37:02.122: INFO: Trying to get logs from node macpro-2 pod pod-secrets-95eee012-a33c-4a27-aec2-4082bd34b7cc container secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:37:02.127
  May 11 14:37:02.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6912" for this suite. @ 05/11/23 14:37:02.14
• [4.069 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/11/23 14:37:02.143
  May 11 14:37:02.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename subpath @ 05/11/23 14:37:02.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:37:02.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:37:02.158
  STEP: Setting up data @ 05/11/23 14:37:02.16
  STEP: Creating pod pod-subpath-test-secret-nf6w @ 05/11/23 14:37:02.166
  STEP: Creating a pod to test atomic-volume-subpath @ 05/11/23 14:37:02.166
  STEP: Saw pod success @ 05/11/23 14:37:26.224
  May 11 14:37:26.227: INFO: Trying to get logs from node macpro-2 pod pod-subpath-test-secret-nf6w container test-container-subpath-secret-nf6w: <nil>
  STEP: delete the pod @ 05/11/23 14:37:26.232
  STEP: Deleting pod pod-subpath-test-secret-nf6w @ 05/11/23 14:37:26.241
  May 11 14:37:26.241: INFO: Deleting pod "pod-subpath-test-secret-nf6w" in namespace "subpath-2161"
  May 11 14:37:26.242: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2161" for this suite. @ 05/11/23 14:37:26.246
• [24.107 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/11/23 14:37:26.251
  May 11 14:37:26.251: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 14:37:26.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:37:26.27
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:37:26.272
  STEP: Counting existing ResourceQuota @ 05/11/23 14:37:26.274
  STEP: Creating a ResourceQuota @ 05/11/23 14:37:31.277
  STEP: Ensuring resource quota status is calculated @ 05/11/23 14:37:31.281
  STEP: Creating a ReplicaSet @ 05/11/23 14:37:33.285
  STEP: Ensuring resource quota status captures replicaset creation @ 05/11/23 14:37:33.311
  STEP: Deleting a ReplicaSet @ 05/11/23 14:37:35.314
  STEP: Ensuring resource quota status released usage @ 05/11/23 14:37:35.319
  May 11 14:37:37.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2585" for this suite. @ 05/11/23 14:37:37.326
• [11.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/11/23 14:37:37.332
  May 11 14:37:37.332: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-preemption @ 05/11/23 14:37:37.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:37:37.347
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:37:37.349
  May 11 14:37:37.361: INFO: Waiting up to 1m0s for all nodes to be ready
  May 11 14:38:37.401: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/11/23 14:38:37.403
  May 11 14:38:37.439: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 11 14:38:37.448: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 11 14:38:37.494: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 11 14:38:37.511: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May 11 14:38:37.558: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May 11 14:38:37.585: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/11/23 14:38:37.585
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/11/23 14:38:39.628
  May 11 14:38:43.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-4466" for this suite. @ 05/11/23 14:38:43.735
• [66.409 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/11/23 14:38:43.741
  May 11 14:38:43.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 14:38:43.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:38:43.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:38:43.771
  May 11 14:38:43.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: creating the pod @ 05/11/23 14:38:43.774
  STEP: submitting the pod to kubernetes @ 05/11/23 14:38:43.774
  May 11 14:38:45.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8146" for this suite. @ 05/11/23 14:38:45.832
• [2.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/11/23 14:38:45.839
  May 11 14:38:45.839: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename deployment @ 05/11/23 14:38:45.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:38:45.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:38:45.871
  STEP: creating a Deployment @ 05/11/23 14:38:45.88
  May 11 14:38:45.880: INFO: Creating simple deployment test-deployment-8d62g
  May 11 14:38:45.892: INFO: new replicaset for deployment "test-deployment-8d62g" is yet to be created
  STEP: Getting /status @ 05/11/23 14:38:47.905
  May 11 14:38:47.908: INFO: Deployment test-deployment-8d62g has Conditions: [{Available True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8d62g-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/11/23 14:38:47.908
  May 11 14:38:47.917: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 14, 38, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 14, 38, 47, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 14, 38, 47, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 14, 38, 45, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-8d62g-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/11/23 14:38:47.917
  May 11 14:38:47.919: INFO: Observed &Deployment event: ADDED
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8d62g-5994cf9475"}
  May 11 14:38:47.919: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8d62g-5994cf9475"}
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 11 14:38:47.919: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8d62g-5994cf9475" is progressing.}
  May 11 14:38:47.919: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8d62g-5994cf9475" has successfully progressed.}
  May 11 14:38:47.919: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 11 14:38:47.919: INFO: Observed Deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8d62g-5994cf9475" has successfully progressed.}
  May 11 14:38:47.919: INFO: Found Deployment test-deployment-8d62g in namespace deployment-1992 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 11 14:38:47.919: INFO: Deployment test-deployment-8d62g has an updated status
  STEP: patching the Statefulset Status @ 05/11/23 14:38:47.919
  May 11 14:38:47.919: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 11 14:38:47.927: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/11/23 14:38:47.927
  May 11 14:38:47.929: INFO: Observed &Deployment event: ADDED
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8d62g-5994cf9475"}
  May 11 14:38:47.929: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-8d62g-5994cf9475"}
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 11 14:38:47.929: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:45 +0000 UTC 2023-05-11 14:38:45 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-8d62g-5994cf9475" is progressing.}
  May 11 14:38:47.929: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8d62g-5994cf9475" has successfully progressed.}
  May 11 14:38:47.929: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:47 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-11 14:38:47 +0000 UTC 2023-05-11 14:38:45 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-8d62g-5994cf9475" has successfully progressed.}
  May 11 14:38:47.929: INFO: Observed deployment test-deployment-8d62g in namespace deployment-1992 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 11 14:38:47.930: INFO: Observed &Deployment event: MODIFIED
  May 11 14:38:47.930: INFO: Found deployment test-deployment-8d62g in namespace deployment-1992 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May 11 14:38:47.930: INFO: Deployment test-deployment-8d62g has a patched status
  May 11 14:38:47.932: INFO: Deployment "test-deployment-8d62g":
  &Deployment{ObjectMeta:{test-deployment-8d62g  deployment-1992  f971f09d-7430-452c-bed9-3a8dbc1fb44c 35535 1 2023-05-11 14:38:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-11 14:38:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-11 14:38:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-11 14:38:47 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005083dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 11 14:38:47.937: INFO: New ReplicaSet "test-deployment-8d62g-5994cf9475" of Deployment "test-deployment-8d62g":
  &ReplicaSet{ObjectMeta:{test-deployment-8d62g-5994cf9475  deployment-1992  1a31a325-6e43-4100-8d91-00c8178ffa58 35532 1 2023-05-11 14:38:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-8d62g f971f09d-7430-452c-bed9-3a8dbc1fb44c 0xc005432180 0xc005432181}] [] [{kube-controller-manager Update apps/v1 2023-05-11 14:38:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f971f09d-7430-452c-bed9-3a8dbc1fb44c\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 14:38:47 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005432228 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 11 14:38:47.940: INFO: Pod "test-deployment-8d62g-5994cf9475-qphtl" is available:
  &Pod{ObjectMeta:{test-deployment-8d62g-5994cf9475-qphtl test-deployment-8d62g-5994cf9475- deployment-1992  dff25903-819c-4646-8513-e8600c238a70 35531 0 2023-05-11 14:38:45 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:97816fceebb3eea3900cc251187462263a8efc0266fd660d36b47c3aefc3c7d8 cni.projectcalico.org/podIP:192.168.150.158/32 cni.projectcalico.org/podIPs:192.168.150.158/32] [{apps/v1 ReplicaSet test-deployment-8d62g-5994cf9475 1a31a325-6e43-4100-8d91-00c8178ffa58 0xc0054325e0 0xc0054325e1}] [] [{kube-controller-manager Update v1 2023-05-11 14:38:45 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1a31a325-6e43-4100-8d91-00c8178ffa58\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-11 14:38:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-11 14:38:47 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d7wdh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d7wdh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:38:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:38:47 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:38:45 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.158,StartTime:2023-05-11 14:38:45 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:38:46 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://fa2412ef1e13b610163fe651fb4c1d128ee13e9334174e46b74a60ca4d16d3e3,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.158,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:38:47.940: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1992" for this suite. @ 05/11/23 14:38:47.946
• [2.113 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/11/23 14:38:47.953
  May 11 14:38:47.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename namespaces @ 05/11/23 14:38:47.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:38:47.976
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:38:47.979
  STEP: Creating a test namespace @ 05/11/23 14:38:47.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:38:48.004
  STEP: Creating a service in the namespace @ 05/11/23 14:38:48.007
  STEP: Deleting the namespace @ 05/11/23 14:38:48.033
  STEP: Waiting for the namespace to be removed. @ 05/11/23 14:38:48.056
  STEP: Recreating the namespace @ 05/11/23 14:38:54.062
  STEP: Verifying there is no service in the namespace @ 05/11/23 14:38:54.092
  May 11 14:38:54.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6271" for this suite. @ 05/11/23 14:38:54.101
  STEP: Destroying namespace "nsdeletetest-4504" for this suite. @ 05/11/23 14:38:54.108
  May 11 14:38:54.112: INFO: Namespace nsdeletetest-4504 was already deleted
  STEP: Destroying namespace "nsdeletetest-7669" for this suite. @ 05/11/23 14:38:54.112
• [6.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/11/23 14:38:54.119
  May 11 14:38:54.119: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:38:54.12
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:38:54.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:38:54.146
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/11/23 14:38:54.149
  STEP: Saw pod success @ 05/11/23 14:38:58.179
  May 11 14:38:58.182: INFO: Trying to get logs from node macpro-3 pod pod-c81c6453-28bf-42a8-8700-cc760b5828f4 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:38:58.199
  May 11 14:38:58.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3942" for this suite. @ 05/11/23 14:38:58.22
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/11/23 14:38:58.226
  May 11 14:38:58.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename subpath @ 05/11/23 14:38:58.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:38:58.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:38:58.251
  STEP: Setting up data @ 05/11/23 14:38:58.253
  STEP: Creating pod pod-subpath-test-configmap-7xjz @ 05/11/23 14:38:58.263
  STEP: Creating a pod to test atomic-volume-subpath @ 05/11/23 14:38:58.263
  STEP: Saw pod success @ 05/11/23 14:39:20.331
  May 11 14:39:20.334: INFO: Trying to get logs from node macpro-3 pod pod-subpath-test-configmap-7xjz container test-container-subpath-configmap-7xjz: <nil>
  STEP: delete the pod @ 05/11/23 14:39:20.338
  STEP: Deleting pod pod-subpath-test-configmap-7xjz @ 05/11/23 14:39:20.352
  May 11 14:39:20.352: INFO: Deleting pod "pod-subpath-test-configmap-7xjz" in namespace "subpath-9365"
  May 11 14:39:20.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9365" for this suite. @ 05/11/23 14:39:20.359
• [22.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/11/23 14:39:20.373
  May 11 14:39:20.373: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename namespaces @ 05/11/23 14:39:20.374
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:39:20.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:39:20.395
  STEP: Read namespace status @ 05/11/23 14:39:20.397
  May 11 14:39:20.400: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/11/23 14:39:20.4
  May 11 14:39:20.423: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/11/23 14:39:20.423
  May 11 14:39:20.429: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May 11 14:39:20.429: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3644" for this suite. @ 05/11/23 14:39:20.438
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/11/23 14:39:20.445
  May 11 14:39:20.445: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:39:20.445
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:39:20.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:39:20.471
  STEP: Creating configMap with name configmap-test-volume-b408777a-5586-454c-a700-9e3cd9793ea6 @ 05/11/23 14:39:20.473
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:39:20.478
  STEP: Saw pod success @ 05/11/23 14:39:24.497
  May 11 14:39:24.500: INFO: Trying to get logs from node macpro-3 pod pod-configmaps-f0b03804-3ddf-415a-a86b-5b586b5601cb container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:39:24.505
  May 11 14:39:24.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8688" for this suite. @ 05/11/23 14:39:24.526
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/11/23 14:39:24.538
  May 11 14:39:24.538: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/11/23 14:39:24.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:39:24.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:39:24.568
  May 11 14:39:24.572: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:39:30.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4534" for this suite. @ 05/11/23 14:39:30.919
• [6.388 seconds]
------------------------------
S
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/11/23 14:39:30.927
  May 11 14:39:30.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-runtime @ 05/11/23 14:39:30.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:39:30.948
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:39:30.95
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/11/23 14:39:30.96
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/11/23 14:39:51.055
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/11/23 14:39:51.059
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/11/23 14:39:51.065
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/11/23 14:39:51.065
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/11/23 14:39:51.088
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/11/23 14:39:54.105
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/11/23 14:39:55.112
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/11/23 14:39:55.119
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/11/23 14:39:55.119
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/11/23 14:39:55.139
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/11/23 14:39:56.147
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/11/23 14:39:58.155
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/11/23 14:39:58.16
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/11/23 14:39:58.16
  May 11 14:39:58.172: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8634" for this suite. @ 05/11/23 14:39:58.182
• [27.260 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/11/23 14:39:58.188
  May 11 14:39:58.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename prestop @ 05/11/23 14:39:58.188
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:39:58.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:39:58.211
  STEP: Creating server pod server in namespace prestop-9589 @ 05/11/23 14:39:58.213
  STEP: Waiting for pods to come up. @ 05/11/23 14:39:58.22
  STEP: Creating tester pod tester in namespace prestop-9589 @ 05/11/23 14:40:00.23
  STEP: Deleting pre-stop pod @ 05/11/23 14:40:02.242
  May 11 14:40:07.259: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May 11 14:40:07.260: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/11/23 14:40:07.267
  STEP: Destroying namespace "prestop-9589" for this suite. @ 05/11/23 14:40:07.277
• [9.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/11/23 14:40:07.285
  May 11 14:40:07.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename statefulset @ 05/11/23 14:40:07.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:40:07.304
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:40:07.306
  STEP: Creating service test in namespace statefulset-1215 @ 05/11/23 14:40:07.308
  STEP: Creating statefulset ss in namespace statefulset-1215 @ 05/11/23 14:40:07.317
  May 11 14:40:07.326: INFO: Found 0 stateful pods, waiting for 1
  May 11 14:40:17.330: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/11/23 14:40:17.339
  STEP: Getting /status @ 05/11/23 14:40:17.346
  May 11 14:40:17.349: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/11/23 14:40:17.349
  May 11 14:40:17.360: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/11/23 14:40:17.36
  May 11 14:40:17.362: INFO: Observed &StatefulSet event: ADDED
  May 11 14:40:17.362: INFO: Found Statefulset ss in namespace statefulset-1215 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 11 14:40:17.362: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/11/23 14:40:17.362
  May 11 14:40:17.362: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 11 14:40:17.371: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/11/23 14:40:17.371
  May 11 14:40:17.372: INFO: Observed &StatefulSet event: ADDED
  May 11 14:40:17.372: INFO: Deleting all statefulset in ns statefulset-1215
  May 11 14:40:17.376: INFO: Scaling statefulset ss to 0
  May 11 14:40:27.395: INFO: Waiting for statefulset status.replicas updated to 0
  May 11 14:40:27.398: INFO: Deleting statefulset ss
  May 11 14:40:27.406: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1215" for this suite. @ 05/11/23 14:40:27.409
• [20.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/11/23 14:40:27.415
  May 11 14:40:27.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:40:27.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:40:27.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:40:27.436
  STEP: Setting up server cert @ 05/11/23 14:40:27.458
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:40:28.004
  STEP: Deploying the webhook pod @ 05/11/23 14:40:28.011
  STEP: Wait for the deployment to be ready @ 05/11/23 14:40:28.02
  May 11 14:40:28.024: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  May 11 14:40:30.032: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 14, 40, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 14, 40, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 14, 40, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 14, 40, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/11/23 14:40:32.036
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:40:32.049
  May 11 14:40:33.050: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/11/23 14:40:33.053
  STEP: create a pod @ 05/11/23 14:40:33.067
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/11/23 14:40:35.082
  May 11 14:40:35.082: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=webhook-3917 attach --namespace=webhook-3917 to-be-attached-pod -i -c=container1'
  May 11 14:40:35.164: INFO: rc: 1
  May 11 14:40:35.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3917" for this suite. @ 05/11/23 14:40:35.24
  STEP: Destroying namespace "webhook-markers-4000" for this suite. @ 05/11/23 14:40:35.253
• [7.845 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/11/23 14:40:35.26
  May 11 14:40:35.260: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 14:40:35.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:40:35.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:40:35.278
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 14:40:35.28
  STEP: Saw pod success @ 05/11/23 14:40:39.297
  May 11 14:40:39.300: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-8395daa0-775e-4ad1-bfbf-825be71b2043 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 14:40:39.314
  May 11 14:40:39.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7345" for this suite. @ 05/11/23 14:40:39.328
• [4.073 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/11/23 14:40:39.333
  May 11 14:40:39.333: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename security-context @ 05/11/23 14:40:39.333
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:40:39.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:40:39.35
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/11/23 14:40:39.352
  STEP: Saw pod success @ 05/11/23 14:40:43.37
  May 11 14:40:43.372: INFO: Trying to get logs from node macpro-3 pod security-context-7bf34bec-0ec3-4539-a00c-d829e5a058c6 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:40:43.377
  May 11 14:40:43.389: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-3771" for this suite. @ 05/11/23 14:40:43.393
• [4.065 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/11/23 14:40:43.398
  May 11 14:40:43.398: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename watch @ 05/11/23 14:40:43.399
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:40:43.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:40:43.415
  STEP: creating a watch on configmaps with a certain label @ 05/11/23 14:40:43.418
  STEP: creating a new configmap @ 05/11/23 14:40:43.418
  STEP: modifying the configmap once @ 05/11/23 14:40:43.422
  STEP: changing the label value of the configmap @ 05/11/23 14:40:43.427
  STEP: Expecting to observe a delete notification for the watched object @ 05/11/23 14:40:43.432
  May 11 14:40:43.432: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1110  6a1e7fb5-2ce4-4da4-b469-45feeaffba67 36479 0 2023-05-11 14:40:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-11 14:40:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:40:43.432: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1110  6a1e7fb5-2ce4-4da4-b469-45feeaffba67 36480 0 2023-05-11 14:40:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-11 14:40:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:40:43.432: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1110  6a1e7fb5-2ce4-4da4-b469-45feeaffba67 36481 0 2023-05-11 14:40:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-11 14:40:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/11/23 14:40:43.432
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/11/23 14:40:43.437
  STEP: changing the label value of the configmap back @ 05/11/23 14:40:53.437
  STEP: modifying the configmap a third time @ 05/11/23 14:40:53.444
  STEP: deleting the configmap @ 05/11/23 14:40:53.451
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/11/23 14:40:53.455
  May 11 14:40:53.455: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1110  6a1e7fb5-2ce4-4da4-b469-45feeaffba67 36525 0 2023-05-11 14:40:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-11 14:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:40:53.455: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1110  6a1e7fb5-2ce4-4da4-b469-45feeaffba67 36526 0 2023-05-11 14:40:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-11 14:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:40:53.455: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-1110  6a1e7fb5-2ce4-4da4-b469-45feeaffba67 36527 0 2023-05-11 14:40:43 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-11 14:40:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 11 14:40:53.455: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1110" for this suite. @ 05/11/23 14:40:53.459
• [10.066 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/11/23 14:40:53.465
  May 11 14:40:53.465: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:40:53.465
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:40:53.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:40:53.483
  STEP: Setting up server cert @ 05/11/23 14:40:53.503
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:40:54.856
  STEP: Deploying the webhook pod @ 05/11/23 14:40:54.86
  STEP: Wait for the deployment to be ready @ 05/11/23 14:40:54.875
  May 11 14:40:54.884: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/11/23 14:40:56.892
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:40:56.902
  May 11 14:40:57.902: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/11/23 14:40:57.906
  STEP: create a configmap that should be updated by the webhook @ 05/11/23 14:40:57.919
  May 11 14:40:57.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3276" for this suite. @ 05/11/23 14:40:57.979
  STEP: Destroying namespace "webhook-markers-6043" for this suite. @ 05/11/23 14:40:57.985
• [4.527 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/11/23 14:40:57.992
  May 11 14:40:57.992: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 14:40:57.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:40:58.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:40:58.02
  STEP: Creating secret with name s-test-opt-del-9133c482-b83f-41d1-bcf4-5db8c972adc0 @ 05/11/23 14:40:58.025
  STEP: Creating secret with name s-test-opt-upd-a8d1bed2-5b18-4364-afec-95e0c2a6b101 @ 05/11/23 14:40:58.03
  STEP: Creating the pod @ 05/11/23 14:40:58.035
  STEP: Deleting secret s-test-opt-del-9133c482-b83f-41d1-bcf4-5db8c972adc0 @ 05/11/23 14:41:00.071
  STEP: Updating secret s-test-opt-upd-a8d1bed2-5b18-4364-afec-95e0c2a6b101 @ 05/11/23 14:41:00.076
  STEP: Creating secret with name s-test-opt-create-5723d6a0-79da-4cb8-a656-e8bbc8894b6a @ 05/11/23 14:41:00.08
  STEP: waiting to observe update in volume @ 05/11/23 14:41:00.083
  May 11 14:41:04.111: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6089" for this suite. @ 05/11/23 14:41:04.115
• [6.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/11/23 14:41:04.123
  May 11 14:41:04.123: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename cronjob @ 05/11/23 14:41:04.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:41:04.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:41:04.144
  STEP: Creating a suspended cronjob @ 05/11/23 14:41:04.146
  STEP: Ensuring no jobs are scheduled @ 05/11/23 14:41:04.153
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/11/23 14:46:04.159
  STEP: Removing cronjob @ 05/11/23 14:46:04.161
  May 11 14:46:04.166: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-822" for this suite. @ 05/11/23 14:46:04.169
• [300.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/11/23 14:46:04.174
  May 11 14:46:04.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 14:46:04.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:04.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:04.197
  STEP: set up a multi version CRD @ 05/11/23 14:46:04.199
  May 11 14:46:04.199: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: mark a version not serverd @ 05/11/23 14:46:08.143
  STEP: check the unserved version gets removed @ 05/11/23 14:46:08.164
  STEP: check the other version is not changed @ 05/11/23 14:46:09.816
  May 11 14:46:12.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5122" for this suite. @ 05/11/23 14:46:12.963
• [8.795 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/11/23 14:46:12.97
  May 11 14:46:12.970: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename deployment @ 05/11/23 14:46:12.971
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:12.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:12.988
  May 11 14:46:12.990: INFO: Creating deployment "test-recreate-deployment"
  May 11 14:46:12.997: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May 11 14:46:13.003: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  May 11 14:46:15.009: INFO: Waiting deployment "test-recreate-deployment" to complete
  May 11 14:46:15.011: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May 11 14:46:15.020: INFO: Updating deployment test-recreate-deployment
  May 11 14:46:15.020: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May 11 14:46:15.115: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-1960  d96bbf5c-b401-4462-ac81-46281498d7fd 37446 2 2023-05-11 14:46:12 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-11 14:46:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 14:46:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005980aa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-11 14:46:15 +0000 UTC,LastTransitionTime:2023-05-11 14:46:15 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-11 14:46:15 +0000 UTC,LastTransitionTime:2023-05-11 14:46:13 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May 11 14:46:15.118: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-1960  974ffcf2-2a28-4db7-a646-3c2857c9ad64 37444 1 2023-05-11 14:46:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment d96bbf5c-b401-4462-ac81-46281498d7fd 0xc005980e77 0xc005980e78}] [] [{kube-controller-manager Update apps/v1 2023-05-11 14:46:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d96bbf5c-b401-4462-ac81-46281498d7fd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 14:46:15 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005980f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 11 14:46:15.118: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May 11 14:46:15.118: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-1960  a10c958b-8a5f-41e4-a972-b01168be9aea 37433 2 2023-05-11 14:46:13 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment d96bbf5c-b401-4462-ac81-46281498d7fd 0xc005980f87 0xc005980f88}] [] [{kube-controller-manager Update apps/v1 2023-05-11 14:46:15 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d96bbf5c-b401-4462-ac81-46281498d7fd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 14:46:15 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005981038 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 11 14:46:15.121: INFO: Pod "test-recreate-deployment-54757ffd6c-5bmxb" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-5bmxb test-recreate-deployment-54757ffd6c- deployment-1960  61b9a690-1185-49cb-b22b-188376be9cd3 37445 0 2023-05-11 14:46:15 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 974ffcf2-2a28-4db7-a646-3c2857c9ad64 0xc0059814c7 0xc0059814c8}] [] [{kube-controller-manager Update v1 2023-05-11 14:46:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"974ffcf2-2a28-4db7-a646-3c2857c9ad64\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:46:15 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m2dsq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m2dsq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:46:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:46:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:46:15 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:46:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:,StartTime:2023-05-11 14:46:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:46:15.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-1960" for this suite. @ 05/11/23 14:46:15.125
• [2.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/11/23 14:46:15.135
  May 11 14:46:15.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename daemonsets @ 05/11/23 14:46:15.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:15.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:15.169
  May 11 14:46:15.194: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/11/23 14:46:15.199
  May 11 14:46:15.205: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:15.205: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:15.205: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:15.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:46:15.208: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:46:16.215: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:16.215: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:16.215: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:16.219: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 11 14:46:16.219: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:46:17.221: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:17.221: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:17.221: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:17.227: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 14:46:17.227: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/11/23 14:46:17.262
  STEP: Check that daemon pods images are updated. @ 05/11/23 14:46:17.287
  May 11 14:46:17.295: INFO: Wrong image for pod: daemon-set-hw8f4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:17.295: INFO: Wrong image for pod: daemon-set-jlcf7. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:17.295: INFO: Wrong image for pod: daemon-set-zc8xq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:17.305: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:17.305: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:17.305: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:18.309: INFO: Wrong image for pod: daemon-set-hw8f4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:18.309: INFO: Wrong image for pod: daemon-set-zc8xq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:18.313: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:18.313: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:18.313: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:19.309: INFO: Wrong image for pod: daemon-set-hw8f4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:19.309: INFO: Pod daemon-set-nph2t is not available
  May 11 14:46:19.309: INFO: Wrong image for pod: daemon-set-zc8xq. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:19.313: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:19.313: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:19.313: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:20.309: INFO: Wrong image for pod: daemon-set-hw8f4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:20.313: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:20.313: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:20.313: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:21.310: INFO: Wrong image for pod: daemon-set-hw8f4. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 11 14:46:21.310: INFO: Pod daemon-set-wpmfr is not available
  May 11 14:46:21.320: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:21.320: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:21.320: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:22.313: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:22.313: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:22.313: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:23.309: INFO: Pod daemon-set-g87wz is not available
  May 11 14:46:23.312: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:23.312: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:23.313: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/11/23 14:46:23.313
  May 11 14:46:23.316: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:23.316: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:23.316: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:23.319: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 14:46:23.319: INFO: Node macpro-3 is running 0 daemon pod, expected 1
  May 11 14:46:24.325: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:24.325: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:24.325: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:46:24.328: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 14:46:24.328: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/11/23 14:46:24.339
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9149, will wait for the garbage collector to delete the pods @ 05/11/23 14:46:24.339
  May 11 14:46:24.397: INFO: Deleting DaemonSet.extensions daemon-set took: 5.804158ms
  May 11 14:46:24.498: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.54929ms
  May 11 14:46:26.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:46:26.802: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 11 14:46:26.805: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"37660"},"items":null}

  May 11 14:46:26.807: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"37660"},"items":null}

  May 11 14:46:26.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9149" for this suite. @ 05/11/23 14:46:26.822
• [11.693 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/11/23 14:46:26.828
  May 11 14:46:26.828: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 14:46:26.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:26.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:26.849
  STEP: creating service in namespace services-6977 @ 05/11/23 14:46:26.851
  STEP: creating service affinity-nodeport in namespace services-6977 @ 05/11/23 14:46:26.851
  STEP: creating replication controller affinity-nodeport in namespace services-6977 @ 05/11/23 14:46:26.871
  I0511 14:46:26.882795      24 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-6977, replica count: 3
  I0511 14:46:29.934410      24 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 14:46:29.952: INFO: Creating new exec pod
  May 11 14:46:32.979: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6977 exec execpod-affinityshbww -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May 11 14:46:33.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May 11 14:46:33.116: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:46:33.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6977 exec execpod-affinityshbww -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.115.152 80'
  May 11 14:46:33.230: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.115.152 80\nConnection to 10.105.115.152 80 port [tcp/http] succeeded!\n"
  May 11 14:46:33.230: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:46:33.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6977 exec execpod-affinityshbww -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.13 31826'
  May 11 14:46:33.341: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.13 31826\nConnection to 10.221.188.13 31826 port [tcp/*] succeeded!\n"
  May 11 14:46:33.341: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:46:33.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6977 exec execpod-affinityshbww -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.11 31826'
  May 11 14:46:33.446: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.11 31826\nConnection to 10.221.188.11 31826 port [tcp/*] succeeded!\n"
  May 11 14:46:33.446: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 14:46:33.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-6977 exec execpod-affinityshbww -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.221.188.11:31826/ ; done'
  May 11 14:46:33.606: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31826/\n"
  May 11 14:46:33.606: INFO: stdout: "\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx\naffinity-nodeport-dsmcx"
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Received response from host: affinity-nodeport-dsmcx
  May 11 14:46:33.606: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 14:46:33.611: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-6977, will wait for the garbage collector to delete the pods @ 05/11/23 14:46:33.641
  May 11 14:46:33.703: INFO: Deleting ReplicationController affinity-nodeport took: 7.424716ms
  May 11 14:46:33.803: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.874875ms
  STEP: Destroying namespace "services-6977" for this suite. @ 05/11/23 14:46:35.846
• [9.024 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/11/23 14:46:35.852
  May 11 14:46:35.852: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:46:35.853
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:35.873
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:35.876
  STEP: validating api versions @ 05/11/23 14:46:35.878
  May 11 14:46:35.878: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-3732 api-versions'
  May 11 14:46:35.936: INFO: stderr: ""
  May 11 14:46:35.936: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nlogicmonitor.com/v1alpha1\nlogicmonitor.com/v1alpha2\nmacstadium.orka.com/v1alpha1\nmetallb.io/v1alpha1\nmetallb.io/v1beta1\nmetallb.io/v1beta2\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\ntraefik.containo.us/v1alpha1\nv1\n"
  May 11 14:46:35.936: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3732" for this suite. @ 05/11/23 14:46:35.942
• [0.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/11/23 14:46:35.95
  May 11 14:46:35.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename events @ 05/11/23 14:46:35.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:35.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:35.99
  STEP: Create set of events @ 05/11/23 14:46:35.992
  STEP: get a list of Events with a label in the current namespace @ 05/11/23 14:46:36.005
  STEP: delete a list of events @ 05/11/23 14:46:36.008
  May 11 14:46:36.008: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/11/23 14:46:36.024
  May 11 14:46:36.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9179" for this suite. @ 05/11/23 14:46:36.031
• [0.086 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/11/23 14:46:36.036
  May 11 14:46:36.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-pred @ 05/11/23 14:46:36.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:36.053
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:36.055
  May 11 14:46:36.057: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 11 14:46:36.074: INFO: Waiting for terminating namespaces to be deleted...
  May 11 14:46:36.082: INFO: 
  Logging pods the apiserver thinks is on node macpro-1 before test
  May 11 14:46:36.089: INFO: calico-node-p8f6p from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.089: INFO: 	Container calico-node ready: true, restart count 0
  May 11 14:46:36.089: INFO: kube-proxy-t8w6l from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.089: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 14:46:36.089: INFO: speaker-pq5r2 from metallb-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.089: INFO: 	Container speaker ready: true, restart count 1
  May 11 14:46:36.089: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-9g8bt from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 14:46:36.089: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 14:46:36.089: INFO: 	Container systemd-logs ready: true, restart count 0
  May 11 14:46:36.089: INFO: 
  Logging pods the apiserver thinks is on node macpro-2 before test
  May 11 14:46:36.096: INFO: calico-node-92dft from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.096: INFO: 	Container calico-node ready: true, restart count 0
  May 11 14:46:36.096: INFO: kube-proxy-gl6v8 from kube-system started at 2023-05-11 13:11:34 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.096: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 14:46:36.096: INFO: speaker-vthm7 from metallb-system started at 2023-05-11 13:53:21 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.096: INFO: 	Container speaker ready: true, restart count 0
  May 11 14:46:36.096: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-6lx9v from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 14:46:36.096: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 14:46:36.096: INFO: 	Container systemd-logs ready: true, restart count 0
  May 11 14:46:36.096: INFO: 
  Logging pods the apiserver thinks is on node macpro-3 before test
  May 11 14:46:36.103: INFO: calico-node-gql49 from kube-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.103: INFO: 	Container calico-node ready: true, restart count 0
  May 11 14:46:36.103: INFO: kube-proxy-w7svn from kube-system started at 2023-05-11 13:11:35 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.103: INFO: 	Container kube-proxy ready: true, restart count 0
  May 11 14:46:36.103: INFO: speaker-6dx5t from metallb-system started at 2023-05-11 13:56:20 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.103: INFO: 	Container speaker ready: true, restart count 0
  May 11 14:46:36.103: INFO: sonobuoy from sonobuoy started at 2023-05-11 13:28:12 +0000 UTC (1 container statuses recorded)
  May 11 14:46:36.103: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 11 14:46:36.103: INFO: sonobuoy-e2e-job-ae6f9d200d3b496f from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 14:46:36.103: INFO: 	Container e2e ready: true, restart count 0
  May 11 14:46:36.103: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 14:46:36.103: INFO: sonobuoy-systemd-logs-daemon-set-bf4c8c9c18fa4f41-kc8bx from sonobuoy started at 2023-05-11 13:28:13 +0000 UTC (2 container statuses recorded)
  May 11 14:46:36.103: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 11 14:46:36.103: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/11/23 14:46:36.103
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175e1e174e1587ff], Reason = [FailedScheduling], Message = [0/6 nodes are available: 3 node(s) didn't match Pod's node affinity/selector, 3 node(s) had untolerated taint {node-role.kubernetes.io/control-plane: }. preemption: 0/6 nodes are available: 6 Preemption is not helpful for scheduling..] @ 05/11/23 14:46:36.138
  May 11 14:46:37.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-227" for this suite. @ 05/11/23 14:46:37.135
• [1.107 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/11/23 14:46:37.144
  May 11 14:46:37.144: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:46:37.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:37.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:37.167
  STEP: Setting up server cert @ 05/11/23 14:46:37.193
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:46:37.691
  STEP: Deploying the webhook pod @ 05/11/23 14:46:37.699
  STEP: Wait for the deployment to be ready @ 05/11/23 14:46:37.712
  May 11 14:46:37.723: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/11/23 14:46:39.733
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:46:39.745
  May 11 14:46:40.745: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/11/23 14:46:40.748
  STEP: create a namespace for the webhook @ 05/11/23 14:46:40.762
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/11/23 14:46:40.782
  May 11 14:46:40.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9965" for this suite. @ 05/11/23 14:46:40.85
  STEP: Destroying namespace "webhook-markers-9819" for this suite. @ 05/11/23 14:46:40.86
  STEP: Destroying namespace "fail-closed-namespace-3278" for this suite. @ 05/11/23 14:46:40.869
• [3.733 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/11/23 14:46:40.877
  May 11 14:46:40.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename field-validation @ 05/11/23 14:46:40.877
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:40.897
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:40.9
  May 11 14:46:40.904: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  W0511 14:46:40.905523      24 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc003ab1d00 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0511 14:46:43.446013      24 warnings.go:70] unknown field "alpha"
  W0511 14:46:43.446054      24 warnings.go:70] unknown field "beta"
  W0511 14:46:43.446061      24 warnings.go:70] unknown field "delta"
  W0511 14:46:43.446066      24 warnings.go:70] unknown field "epsilon"
  W0511 14:46:43.446080      24 warnings.go:70] unknown field "gamma"
  May 11 14:46:43.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2904" for this suite. @ 05/11/23 14:46:43.498
• [2.626 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/11/23 14:46:43.503
  May 11 14:46:43.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:46:43.504
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:43.517
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:43.52
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/11/23 14:46:43.522
  STEP: Saw pod success @ 05/11/23 14:46:47.543
  May 11 14:46:47.545: INFO: Trying to get logs from node macpro-2 pod pod-bf07c557-cda6-48ca-99ed-700606a413a7 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:46:47.559
  May 11 14:46:47.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2436" for this suite. @ 05/11/23 14:46:47.574
• [4.076 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/11/23 14:46:47.58
  May 11 14:46:47.580: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 14:46:47.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:47.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:47.6
  STEP: creating secret secrets-80/secret-test-4da8e11a-f207-4aca-9a9e-f9a047fcc9a5 @ 05/11/23 14:46:47.601
  STEP: Creating a pod to test consume secrets @ 05/11/23 14:46:47.606
  STEP: Saw pod success @ 05/11/23 14:46:51.625
  May 11 14:46:51.629: INFO: Trying to get logs from node macpro-2 pod pod-configmaps-51013570-d024-46f6-8b09-c0230e01f639 container env-test: <nil>
  STEP: delete the pod @ 05/11/23 14:46:51.636
  May 11 14:46:51.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-80" for this suite. @ 05/11/23 14:46:51.654
• [4.085 seconds]
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/11/23 14:46:51.665
  May 11 14:46:51.665: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/11/23 14:46:51.666
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:51.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:51.687
  STEP: mirroring a new custom Endpoint @ 05/11/23 14:46:51.704
  May 11 14:46:51.714: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 05/11/23 14:46:53.718
  STEP: mirroring deletion of a custom Endpoint @ 05/11/23 14:46:53.732
  May 11 14:46:53.745: INFO: Waiting for 0 EndpointSlices to exist, got 1
  May 11 14:46:55.750: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-51" for this suite. @ 05/11/23 14:46:55.757
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/11/23 14:46:55.766
  May 11 14:46:55.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename limitrange @ 05/11/23 14:46:55.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:55.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:55.783
  STEP: Creating LimitRange "e2e-limitrange-v2t9h" in namespace "limitrange-7508" @ 05/11/23 14:46:55.785
  STEP: Creating another limitRange in another namespace @ 05/11/23 14:46:55.79
  May 11 14:46:55.804: INFO: Namespace "e2e-limitrange-v2t9h-5715" created
  May 11 14:46:55.804: INFO: Creating LimitRange "e2e-limitrange-v2t9h" in namespace "e2e-limitrange-v2t9h-5715"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-v2t9h" @ 05/11/23 14:46:55.81
  May 11 14:46:55.813: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-v2t9h" in "limitrange-7508" namespace @ 05/11/23 14:46:55.813
  May 11 14:46:55.821: INFO: LimitRange "e2e-limitrange-v2t9h" has been patched
  STEP: Delete LimitRange "e2e-limitrange-v2t9h" by Collection with labelSelector: "e2e-limitrange-v2t9h=patched" @ 05/11/23 14:46:55.821
  STEP: Confirm that the limitRange "e2e-limitrange-v2t9h" has been deleted @ 05/11/23 14:46:55.828
  May 11 14:46:55.828: INFO: Requesting list of LimitRange to confirm quantity
  May 11 14:46:55.830: INFO: Found 0 LimitRange with label "e2e-limitrange-v2t9h=patched"
  May 11 14:46:55.830: INFO: LimitRange "e2e-limitrange-v2t9h" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-v2t9h" @ 05/11/23 14:46:55.83
  May 11 14:46:55.833: INFO: Found 1 limitRange
  May 11 14:46:55.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-7508" for this suite. @ 05/11/23 14:46:55.837
  STEP: Destroying namespace "e2e-limitrange-v2t9h-5715" for this suite. @ 05/11/23 14:46:55.843
• [0.083 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/11/23 14:46:55.85
  May 11 14:46:55.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 14:46:55.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:55.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:55.87
  STEP: creating an Endpoint @ 05/11/23 14:46:55.875
  STEP: waiting for available Endpoint @ 05/11/23 14:46:55.879
  STEP: listing all Endpoints @ 05/11/23 14:46:55.88
  STEP: updating the Endpoint @ 05/11/23 14:46:55.884
  STEP: fetching the Endpoint @ 05/11/23 14:46:55.889
  STEP: patching the Endpoint @ 05/11/23 14:46:55.892
  STEP: fetching the Endpoint @ 05/11/23 14:46:55.9
  STEP: deleting the Endpoint by Collection @ 05/11/23 14:46:55.903
  STEP: waiting for Endpoint deletion @ 05/11/23 14:46:55.912
  STEP: fetching the Endpoint @ 05/11/23 14:46:55.914
  May 11 14:46:55.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5091" for this suite. @ 05/11/23 14:46:55.922
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/11/23 14:46:55.935
  May 11 14:46:55.935: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename containers @ 05/11/23 14:46:55.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:46:55.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:46:55.957
  STEP: Creating a pod to test override arguments @ 05/11/23 14:46:55.959
  STEP: Saw pod success @ 05/11/23 14:46:59.979
  May 11 14:46:59.982: INFO: Trying to get logs from node macpro-2 pod client-containers-a29c0220-e231-4541-987f-b282dc3ef326 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:46:59.987
  May 11 14:46:59.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9227" for this suite. @ 05/11/23 14:47:00.001
• [4.073 seconds]
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/11/23 14:47:00.008
  May 11 14:47:00.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename cronjob @ 05/11/23 14:47:00.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:00.023
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:00.026
  STEP: Creating a cronjob @ 05/11/23 14:47:00.028
  STEP: creating @ 05/11/23 14:47:00.028
  STEP: getting @ 05/11/23 14:47:00.032
  STEP: listing @ 05/11/23 14:47:00.034
  STEP: watching @ 05/11/23 14:47:00.037
  May 11 14:47:00.037: INFO: starting watch
  STEP: cluster-wide listing @ 05/11/23 14:47:00.037
  STEP: cluster-wide watching @ 05/11/23 14:47:00.04
  May 11 14:47:00.040: INFO: starting watch
  STEP: patching @ 05/11/23 14:47:00.041
  STEP: updating @ 05/11/23 14:47:00.047
  May 11 14:47:00.055: INFO: waiting for watch events with expected annotations
  May 11 14:47:00.055: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/11/23 14:47:00.055
  STEP: updating /status @ 05/11/23 14:47:00.061
  STEP: get /status @ 05/11/23 14:47:00.07
  STEP: deleting @ 05/11/23 14:47:00.075
  STEP: deleting a collection @ 05/11/23 14:47:00.089
  May 11 14:47:00.100: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8504" for this suite. @ 05/11/23 14:47:00.105
• [0.104 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/11/23 14:47:00.112
  May 11 14:47:00.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename job @ 05/11/23 14:47:00.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:00.131
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:00.133
  STEP: Creating a job @ 05/11/23 14:47:00.135
  STEP: Ensuring active pods == parallelism @ 05/11/23 14:47:00.14
  STEP: Orphaning one of the Job's Pods @ 05/11/23 14:47:02.144
  May 11 14:47:02.661: INFO: Successfully updated pod "adopt-release-7rvpj"
  STEP: Checking that the Job readopts the Pod @ 05/11/23 14:47:02.661
  STEP: Removing the labels from the Job's Pod @ 05/11/23 14:47:04.67
  May 11 14:47:05.187: INFO: Successfully updated pod "adopt-release-7rvpj"
  STEP: Checking that the Job releases the Pod @ 05/11/23 14:47:05.187
  May 11 14:47:07.196: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-3682" for this suite. @ 05/11/23 14:47:07.203
• [7.099 seconds]
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/11/23 14:47:07.211
  May 11 14:47:07.211: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/11/23 14:47:07.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:07.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:07.251
  STEP: create the container to handle the HTTPGet hook request. @ 05/11/23 14:47:07.259
  STEP: create the pod with lifecycle hook @ 05/11/23 14:47:09.283
  STEP: check poststart hook @ 05/11/23 14:47:11.304
  STEP: delete the pod with lifecycle hook @ 05/11/23 14:47:11.311
  May 11 14:47:13.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3633" for this suite. @ 05/11/23 14:47:13.337
• [6.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/11/23 14:47:13.345
  May 11 14:47:13.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 14:47:13.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:13.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:13.552
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 14:47:13.555
  STEP: Saw pod success @ 05/11/23 14:47:17.585
  May 11 14:47:17.587: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-8f540477-f272-4804-a787-0a2c9d94113e container client-container: <nil>
  STEP: delete the pod @ 05/11/23 14:47:17.592
  May 11 14:47:17.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7837" for this suite. @ 05/11/23 14:47:17.609
• [4.270 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/11/23 14:47:17.617
  May 11 14:47:17.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename events @ 05/11/23 14:47:17.618
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:17.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:17.639
  STEP: creating a test event @ 05/11/23 14:47:17.641
  STEP: listing events in all namespaces @ 05/11/23 14:47:17.644
  STEP: listing events in test namespace @ 05/11/23 14:47:17.647
  STEP: listing events with field selection filtering on source @ 05/11/23 14:47:17.658
  STEP: listing events with field selection filtering on reportingController @ 05/11/23 14:47:17.663
  STEP: getting the test event @ 05/11/23 14:47:17.667
  STEP: patching the test event @ 05/11/23 14:47:17.669
  STEP: getting the test event @ 05/11/23 14:47:17.679
  STEP: updating the test event @ 05/11/23 14:47:17.682
  STEP: getting the test event @ 05/11/23 14:47:17.69
  STEP: deleting the test event @ 05/11/23 14:47:17.696
  STEP: listing events in all namespaces @ 05/11/23 14:47:17.705
  STEP: listing events in test namespace @ 05/11/23 14:47:17.708
  May 11 14:47:17.711: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7353" for this suite. @ 05/11/23 14:47:17.717
• [0.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/11/23 14:47:17.723
  May 11 14:47:17.723: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename tables @ 05/11/23 14:47:17.724
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:17.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:17.75
  May 11 14:47:17.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-4665" for this suite. @ 05/11/23 14:47:17.758
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/11/23 14:47:17.765
  May 11 14:47:17.765: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename ingressclass @ 05/11/23 14:47:17.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:17.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:17.782
  STEP: getting /apis @ 05/11/23 14:47:17.784
  STEP: getting /apis/networking.k8s.io @ 05/11/23 14:47:17.788
  STEP: getting /apis/networking.k8s.iov1 @ 05/11/23 14:47:17.789
  STEP: creating @ 05/11/23 14:47:17.79
  STEP: getting @ 05/11/23 14:47:17.813
  STEP: listing @ 05/11/23 14:47:17.816
  STEP: watching @ 05/11/23 14:47:17.818
  May 11 14:47:17.818: INFO: starting watch
  STEP: patching @ 05/11/23 14:47:17.819
  STEP: updating @ 05/11/23 14:47:17.823
  May 11 14:47:17.828: INFO: waiting for watch events with expected annotations
  May 11 14:47:17.828: INFO: saw patched and updated annotations
  STEP: deleting @ 05/11/23 14:47:17.828
  STEP: deleting a collection @ 05/11/23 14:47:17.838
  May 11 14:47:17.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-769" for this suite. @ 05/11/23 14:47:17.853
• [0.093 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/11/23 14:47:17.859
  May 11 14:47:17.859: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:47:17.859
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:17.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:17.878
  STEP: Creating projection with secret that has name projected-secret-test-14f41875-ac5a-4377-8360-2bc03283c384 @ 05/11/23 14:47:17.88
  STEP: Creating a pod to test consume secrets @ 05/11/23 14:47:17.885
  STEP: Saw pod success @ 05/11/23 14:47:21.909
  May 11 14:47:21.911: INFO: Trying to get logs from node macpro-2 pod pod-projected-secrets-3bb11075-309e-4944-bc52-3a65b75c7024 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:47:21.918
  May 11 14:47:21.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9481" for this suite. @ 05/11/23 14:47:21.934
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/11/23 14:47:21.94
  May 11 14:47:21.940: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 14:47:21.941
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:47:21.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:47:21.962
  STEP: Creating pod test-webserver-0b367bef-e030-45ce-b971-2b617af6d63b in namespace container-probe-3129 @ 05/11/23 14:47:21.964
  May 11 14:47:23.977: INFO: Started pod test-webserver-0b367bef-e030-45ce-b971-2b617af6d63b in namespace container-probe-3129
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/11/23 14:47:23.977
  May 11 14:47:23.981: INFO: Initial restart count of pod test-webserver-0b367bef-e030-45ce-b971-2b617af6d63b is 0
  May 11 14:51:24.592: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 14:51:24.599
  STEP: Destroying namespace "container-probe-3129" for this suite. @ 05/11/23 14:51:24.615
• [242.690 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/11/23 14:51:24.631
  May 11 14:51:24.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 14:51:24.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:51:25.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:51:25.259
  STEP: Counting existing ResourceQuota @ 05/11/23 14:51:42.266
  STEP: Creating a ResourceQuota @ 05/11/23 14:51:47.269
  STEP: Ensuring resource quota status is calculated @ 05/11/23 14:51:47.274
  STEP: Creating a ConfigMap @ 05/11/23 14:51:49.278
  STEP: Ensuring resource quota status captures configMap creation @ 05/11/23 14:51:49.289
  STEP: Deleting a ConfigMap @ 05/11/23 14:51:51.294
  STEP: Ensuring resource quota status released usage @ 05/11/23 14:51:51.304
  May 11 14:51:53.308: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7507" for this suite. @ 05/11/23 14:51:53.314
• [28.689 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/11/23 14:51:53.321
  May 11 14:51:53.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:51:53.322
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:51:53.338
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:51:53.341
  STEP: create deployment with httpd image @ 05/11/23 14:51:53.343
  May 11 14:51:53.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-9517 create -f -'
  May 11 14:51:54.481: INFO: stderr: ""
  May 11 14:51:54.481: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/11/23 14:51:54.481
  May 11 14:51:54.481: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-9517 diff -f -'
  May 11 14:51:54.702: INFO: rc: 1
  May 11 14:51:54.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-9517 delete -f -'
  May 11 14:51:54.766: INFO: stderr: ""
  May 11 14:51:54.766: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May 11 14:51:54.766: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9517" for this suite. @ 05/11/23 14:51:54.77
• [1.455 seconds]
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/11/23 14:51:54.776
  May 11 14:51:54.776: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 14:51:54.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:51:54.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:51:54.809
  STEP: Creating a pod to test downward api env vars @ 05/11/23 14:51:54.812
  STEP: Saw pod success @ 05/11/23 14:51:58.843
  May 11 14:51:58.847: INFO: Trying to get logs from node macpro-3 pod downward-api-743620c7-5f7b-4985-9526-b6c0f29ccad9 container dapi-container: <nil>
  STEP: delete the pod @ 05/11/23 14:51:58.866
  May 11 14:51:58.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9896" for this suite. @ 05/11/23 14:51:58.889
• [4.119 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/11/23 14:51:58.896
  May 11 14:51:58.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-runtime @ 05/11/23 14:51:58.896
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:51:58.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:51:58.916
  STEP: create the container @ 05/11/23 14:51:58.918
  W0511 14:51:58.926524      24 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/11/23 14:51:58.926
  STEP: get the container status @ 05/11/23 14:52:00.944
  STEP: the container should be terminated @ 05/11/23 14:52:00.948
  STEP: the termination message should be set @ 05/11/23 14:52:00.948
  May 11 14:52:00.948: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/11/23 14:52:00.948
  May 11 14:52:00.959: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5805" for this suite. @ 05/11/23 14:52:00.967
• [2.077 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/11/23 14:52:00.974
  May 11 14:52:00.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:52:00.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:52:00.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:52:00.993
  STEP: Creating configMap with name projected-configmap-test-volume-map-8a8c28da-fc68-45c0-a01f-ca54f527e977 @ 05/11/23 14:52:00.995
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:52:01
  STEP: Saw pod success @ 05/11/23 14:52:05.02
  May 11 14:52:05.023: INFO: Trying to get logs from node macpro-2 pod pod-projected-configmaps-8d857879-f144-4105-8e0a-2233a433ee26 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:52:05.038
  May 11 14:52:05.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4767" for this suite. @ 05/11/23 14:52:05.058
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/11/23 14:52:05.065
  May 11 14:52:05.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:52:05.066
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:52:05.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:52:05.084
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/11/23 14:52:05.087
  STEP: Saw pod success @ 05/11/23 14:52:09.111
  May 11 14:52:09.114: INFO: Trying to get logs from node macpro-2 pod pod-4d6767f3-c38b-4ed9-ae89-003a1cbaf701 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:52:09.124
  May 11 14:52:09.145: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-885" for this suite. @ 05/11/23 14:52:09.149
• [4.090 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/11/23 14:52:09.155
  May 11 14:52:09.155: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename container-probe @ 05/11/23 14:52:09.156
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:52:09.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:52:09.181
  STEP: Creating pod liveness-48aa89e5-6ed3-4847-9d7f-a7282e02e159 in namespace container-probe-2681 @ 05/11/23 14:52:09.183
  May 11 14:52:11.217: INFO: Started pod liveness-48aa89e5-6ed3-4847-9d7f-a7282e02e159 in namespace container-probe-2681
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/11/23 14:52:11.217
  May 11 14:52:11.223: INFO: Initial restart count of pod liveness-48aa89e5-6ed3-4847-9d7f-a7282e02e159 is 0
  May 11 14:56:11.833: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 14:56:11.839
  STEP: Destroying namespace "container-probe-2681" for this suite. @ 05/11/23 14:56:11.852
• [242.708 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/11/23 14:56:11.864
  May 11 14:56:11.864: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:56:11.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:12.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:12.675
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/11/23 14:56:12.677
  STEP: Saw pod success @ 05/11/23 14:56:16.76
  May 11 14:56:16.763: INFO: Trying to get logs from node macpro-2 pod pod-9ee57682-7b17-4b10-89f7-1c635590dabc container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:56:16.775
  May 11 14:56:16.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3669" for this suite. @ 05/11/23 14:56:16.793
• [4.935 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/11/23 14:56:16.8
  May 11 14:56:16.800: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:56:16.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:16.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:16.818
  STEP: Creating configMap that has name configmap-test-emptyKey-c0ba10c1-c713-43de-84f6-ddb3e70f858c @ 05/11/23 14:56:16.82
  May 11 14:56:16.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6522" for this suite. @ 05/11/23 14:56:16.825
• [0.030 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/11/23 14:56:16.831
  May 11 14:56:16.831: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:56:16.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:16.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:16.847
  STEP: Setting up server cert @ 05/11/23 14:56:16.872
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:56:17.313
  STEP: Deploying the webhook pod @ 05/11/23 14:56:17.319
  STEP: Wait for the deployment to be ready @ 05/11/23 14:56:17.331
  May 11 14:56:17.341: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 14:56:19.349
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:56:19.363
  May 11 14:56:20.363: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/11/23 14:56:20.366
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/11/23 14:56:20.383
  STEP: Creating a configMap that should not be mutated @ 05/11/23 14:56:20.389
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/11/23 14:56:20.397
  STEP: Creating a configMap that should be mutated @ 05/11/23 14:56:20.404
  May 11 14:56:20.423: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4684" for this suite. @ 05/11/23 14:56:20.474
  STEP: Destroying namespace "webhook-markers-8575" for this suite. @ 05/11/23 14:56:20.481
• [3.657 seconds]
------------------------------
SSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/11/23 14:56:20.487
  May 11 14:56:20.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename events @ 05/11/23 14:56:20.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:20.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:20.51
  STEP: Create set of events @ 05/11/23 14:56:20.512
  May 11 14:56:20.519: INFO: created test-event-1
  May 11 14:56:20.523: INFO: created test-event-2
  May 11 14:56:20.526: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/11/23 14:56:20.526
  STEP: delete collection of events @ 05/11/23 14:56:20.528
  May 11 14:56:20.528: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/11/23 14:56:20.545
  May 11 14:56:20.545: INFO: requesting list of events to confirm quantity
  May 11 14:56:20.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9307" for this suite. @ 05/11/23 14:56:20.551
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/11/23 14:56:20.557
  May 11 14:56:20.557: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:56:20.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:20.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:20.575
  STEP: Creating configMap with name configmap-test-volume-b6c4d63d-d281-41d2-808e-9ee0900e5f03 @ 05/11/23 14:56:20.577
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:56:20.581
  STEP: Saw pod success @ 05/11/23 14:56:24.598
  May 11 14:56:24.601: INFO: Trying to get logs from node macpro-2 pod pod-configmaps-886c0ba3-7ed5-4373-ba93-932e4ef95705 container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:56:24.609
  May 11 14:56:24.620: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-448" for this suite. @ 05/11/23 14:56:24.625
• [4.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/11/23 14:56:24.633
  May 11 14:56:24.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename svcaccounts @ 05/11/23 14:56:24.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:24.651
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:24.653
  May 11 14:56:24.670: INFO: created pod pod-service-account-defaultsa
  May 11 14:56:24.670: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May 11 14:56:24.681: INFO: created pod pod-service-account-mountsa
  May 11 14:56:24.681: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May 11 14:56:24.695: INFO: created pod pod-service-account-nomountsa
  May 11 14:56:24.695: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May 11 14:56:24.702: INFO: created pod pod-service-account-defaultsa-mountspec
  May 11 14:56:24.702: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May 11 14:56:24.724: INFO: created pod pod-service-account-mountsa-mountspec
  May 11 14:56:24.724: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May 11 14:56:24.730: INFO: created pod pod-service-account-nomountsa-mountspec
  May 11 14:56:24.730: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May 11 14:56:24.752: INFO: created pod pod-service-account-defaultsa-nomountspec
  May 11 14:56:24.752: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May 11 14:56:24.759: INFO: created pod pod-service-account-mountsa-nomountspec
  May 11 14:56:24.759: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May 11 14:56:24.769: INFO: created pod pod-service-account-nomountsa-nomountspec
  May 11 14:56:24.769: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May 11 14:56:24.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3501" for this suite. @ 05/11/23 14:56:24.775
• [0.152 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/11/23 14:56:24.785
  May 11 14:56:24.785: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:56:24.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:24.813
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:24.815
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-bba103a1-d5ad-456c-babc-990af1b594ef @ 05/11/23 14:56:24.824
  STEP: Creating the pod @ 05/11/23 14:56:24.829
  STEP: Updating configmap projected-configmap-test-upd-bba103a1-d5ad-456c-babc-990af1b594ef @ 05/11/23 14:56:26.859
  STEP: waiting to observe update in volume @ 05/11/23 14:56:26.87
  May 11 14:56:28.903: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6183" for this suite. @ 05/11/23 14:56:28.907
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/11/23 14:56:28.914
  May 11 14:56:28.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename subjectreview @ 05/11/23 14:56:28.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:28.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:28.932
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-6165" @ 05/11/23 14:56:28.934
  May 11 14:56:28.939: INFO: saUsername: "system:serviceaccount:subjectreview-6165:e2e"
  May 11 14:56:28.939: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-6165"}
  May 11 14:56:28.939: INFO: saUID: "351ac718-4e9a-407e-93af-f5fdfdbdc29e"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-6165:e2e" @ 05/11/23 14:56:28.939
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-6165:e2e" @ 05/11/23 14:56:28.939
  May 11 14:56:28.941: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-6165:e2e" api 'list' configmaps in "subjectreview-6165" namespace @ 05/11/23 14:56:28.941
  May 11 14:56:28.942: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-6165:e2e" @ 05/11/23 14:56:28.942
  May 11 14:56:28.944: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May 11 14:56:28.944: INFO: LocalSubjectAccessReview has been verified
  May 11 14:56:28.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-6165" for this suite. @ 05/11/23 14:56:28.948
• [0.046 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/11/23 14:56:28.96
  May 11 14:56:28.960: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:56:28.961
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:28.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:28.982
  STEP: Setting up server cert @ 05/11/23 14:56:29.021
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:56:29.446
  STEP: Deploying the webhook pod @ 05/11/23 14:56:29.452
  STEP: Wait for the deployment to be ready @ 05/11/23 14:56:29.462
  May 11 14:56:29.471: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  May 11 14:56:31.481: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 14, 56, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 14, 56, 29, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 14, 56, 29, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 14, 56, 29, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/11/23 14:56:33.487
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:56:33.501
  May 11 14:56:34.501: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/11/23 14:56:34.505
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/11/23 14:56:34.518
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/11/23 14:56:34.527
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/11/23 14:56:34.541
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/11/23 14:56:34.557
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/11/23 14:56:34.564
  May 11 14:56:34.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3147" for this suite. @ 05/11/23 14:56:34.63
  STEP: Destroying namespace "webhook-markers-4021" for this suite. @ 05/11/23 14:56:34.637
• [5.688 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/11/23 14:56:34.649
  May 11 14:56:34.649: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replicaset @ 05/11/23 14:56:34.649
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:34.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:34.673
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/11/23 14:56:34.675
  STEP: When a replicaset with a matching selector is created @ 05/11/23 14:56:36.69
  STEP: Then the orphan pod is adopted @ 05/11/23 14:56:36.694
  STEP: When the matched label of one of its pods change @ 05/11/23 14:56:37.7
  May 11 14:56:37.703: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/11/23 14:56:37.715
  May 11 14:56:38.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-9216" for this suite. @ 05/11/23 14:56:38.728
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/11/23 14:56:38.737
  May 11 14:56:38.737: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubelet-test @ 05/11/23 14:56:38.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:38.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:38.753
  May 11 14:56:38.781: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7909" for this suite. @ 05/11/23 14:56:38.784
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/11/23 14:56:38.79
  May 11 14:56:38.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename dns @ 05/11/23 14:56:38.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:56:38.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:56:38.805
  STEP: Creating a test headless service @ 05/11/23 14:56:38.807
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6430.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6430.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6430.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6430.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6430.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 251.74.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.74.251_udp@PTR;check="$$(dig +tcp +noall +answer +search 251.74.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.74.251_tcp@PTR;sleep 1; done
   @ 05/11/23 14:56:38.83
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6430.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6430.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6430.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6430.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6430.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6430.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6430.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6430.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 251.74.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.74.251_udp@PTR;check="$$(dig +tcp +noall +answer +search 251.74.105.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.105.74.251_tcp@PTR;sleep 1; done
   @ 05/11/23 14:56:38.83
  STEP: creating a pod to probe DNS @ 05/11/23 14:56:38.83
  STEP: submitting the pod to kubernetes @ 05/11/23 14:56:38.831
  STEP: retrieving the pod @ 05/11/23 14:56:40.858
  STEP: looking for the results for each expected name from probers @ 05/11/23 14:56:40.862
  May 11 14:56:40.866: INFO: Unable to read wheezy_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:40.870: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:40.874: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:40.878: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:40.897: INFO: Unable to read jessie_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:40.900: INFO: Unable to read jessie_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:40.906: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:40.911: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:40.928: INFO: Lookups using dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a failed for: [wheezy_udp@dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_udp@dns-test-service.dns-6430.svc.cluster.local jessie_tcp@dns-test-service.dns-6430.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local]

  May 11 14:56:45.933: INFO: Unable to read wheezy_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:45.936: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:45.940: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:45.944: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:45.957: INFO: Unable to read jessie_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:45.959: INFO: Unable to read jessie_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:45.961: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:45.965: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:45.980: INFO: Lookups using dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a failed for: [wheezy_udp@dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_udp@dns-test-service.dns-6430.svc.cluster.local jessie_tcp@dns-test-service.dns-6430.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local]

  May 11 14:56:50.935: INFO: Unable to read wheezy_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:50.938: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:50.942: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:50.945: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:50.962: INFO: Unable to read jessie_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:50.965: INFO: Unable to read jessie_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:50.967: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:50.970: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:50.980: INFO: Lookups using dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a failed for: [wheezy_udp@dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_udp@dns-test-service.dns-6430.svc.cluster.local jessie_tcp@dns-test-service.dns-6430.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local]

  May 11 14:56:55.933: INFO: Unable to read wheezy_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:55.937: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:55.940: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:55.943: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:55.955: INFO: Unable to read jessie_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:55.957: INFO: Unable to read jessie_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:55.960: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:55.962: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:56:55.971: INFO: Lookups using dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a failed for: [wheezy_udp@dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_udp@dns-test-service.dns-6430.svc.cluster.local jessie_tcp@dns-test-service.dns-6430.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local]

  May 11 14:57:00.933: INFO: Unable to read wheezy_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:00.936: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:00.939: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:00.942: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:00.955: INFO: Unable to read jessie_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:00.958: INFO: Unable to read jessie_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:00.960: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:00.963: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:00.973: INFO: Lookups using dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a failed for: [wheezy_udp@dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_udp@dns-test-service.dns-6430.svc.cluster.local jessie_tcp@dns-test-service.dns-6430.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local]

  May 11 14:57:05.932: INFO: Unable to read wheezy_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:05.936: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:05.940: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:05.943: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:05.968: INFO: Unable to read jessie_udp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:05.970: INFO: Unable to read jessie_tcp@dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:05.972: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:05.975: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local from pod dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a: the server could not find the requested resource (get pods dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a)
  May 11 14:57:05.983: INFO: Lookups using dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a failed for: [wheezy_udp@dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@dns-test-service.dns-6430.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_udp@dns-test-service.dns-6430.svc.cluster.local jessie_tcp@dns-test-service.dns-6430.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6430.svc.cluster.local]

  May 11 14:57:10.977: INFO: DNS probes using dns-6430/dns-test-5c998786-0a01-4b98-ab37-6d88b525e33a succeeded

  May 11 14:57:10.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/11/23 14:57:10.981
  STEP: deleting the test service @ 05/11/23 14:57:10.992
  STEP: deleting the test headless service @ 05/11/23 14:57:11.024
  STEP: Destroying namespace "dns-6430" for this suite. @ 05/11/23 14:57:11.048
• [32.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/11/23 14:57:11.056
  May 11 14:57:11.056: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 14:57:11.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:57:11.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:57:11.082
  STEP: Setting up server cert @ 05/11/23 14:57:11.105
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 14:57:11.479
  STEP: Deploying the webhook pod @ 05/11/23 14:57:11.487
  STEP: Wait for the deployment to be ready @ 05/11/23 14:57:11.495
  May 11 14:57:11.502: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 05/11/23 14:57:13.512
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 14:57:13.524
  May 11 14:57:14.525: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 11 14:57:14.529: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2658-crds.webhook.example.com via the AdmissionRegistration API @ 05/11/23 14:57:15.05
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/11/23 14:57:15.074
  May 11 14:57:17.112: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6874" for this suite. @ 05/11/23 14:57:17.705
  STEP: Destroying namespace "webhook-markers-7807" for this suite. @ 05/11/23 14:57:17.713
• [6.664 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/11/23 14:57:17.721
  May 11 14:57:17.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename job @ 05/11/23 14:57:17.721
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:57:17.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:57:17.744
  STEP: Creating a suspended job @ 05/11/23 14:57:17.756
  STEP: Patching the Job @ 05/11/23 14:57:17.764
  STEP: Watching for Job to be patched @ 05/11/23 14:57:17.782
  May 11 14:57:17.783: INFO: Event ADDED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 11 14:57:17.783: INFO: Event MODIFIED found for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/11/23 14:57:17.783
  STEP: Watching for Job to be updated @ 05/11/23 14:57:17.799
  May 11 14:57:17.800: INFO: Event MODIFIED found for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:17.800: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/11/23 14:57:17.8
  May 11 14:57:17.804: INFO: Job: e2e-gsrj7 as labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7]
  STEP: Waiting for job to complete @ 05/11/23 14:57:17.804
  STEP: Delete a job collection with a labelselector @ 05/11/23 14:57:25.808
  STEP: Watching for Job to be deleted @ 05/11/23 14:57:25.817
  May 11 14:57:25.818: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event MODIFIED observed for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 11 14:57:25.819: INFO: Event DELETED found for Job e2e-gsrj7 in namespace job-1681 with labels: map[e2e-gsrj7:patched e2e-job-label:e2e-gsrj7] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/11/23 14:57:25.819
  May 11 14:57:25.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1681" for this suite. @ 05/11/23 14:57:25.826
• [8.113 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/11/23 14:57:25.834
  May 11 14:57:25.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/11/23 14:57:25.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:57:25.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:57:25.866
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/11/23 14:57:25.869
  May 11 14:57:25.869: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:57:27.354: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:57:33.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4229" for this suite. @ 05/11/23 14:57:33.784
• [7.956 seconds]
------------------------------
SSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/11/23 14:57:33.79
  May 11 14:57:33.790: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename security-context-test @ 05/11/23 14:57:33.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:57:33.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:57:33.808
  May 11 14:57:37.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7652" for this suite. @ 05/11/23 14:57:37.841
• [4.057 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/11/23 14:57:37.847
  May 11 14:57:37.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:57:37.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:57:37.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:57:37.867
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/11/23 14:57:37.869
  STEP: Saw pod success @ 05/11/23 14:57:41.893
  May 11 14:57:41.896: INFO: Trying to get logs from node macpro-2 pod pod-5efd2248-972e-4a1d-9765-1c8db2fcbfa1 container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:57:41.91
  May 11 14:57:41.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9059" for this suite. @ 05/11/23 14:57:41.926
• [4.084 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/11/23 14:57:41.932
  May 11 14:57:41.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename subpath @ 05/11/23 14:57:41.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:57:41.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:57:41.947
  STEP: Setting up data @ 05/11/23 14:57:41.948
  STEP: Creating pod pod-subpath-test-downwardapi-krcg @ 05/11/23 14:57:41.955
  STEP: Creating a pod to test atomic-volume-subpath @ 05/11/23 14:57:41.955
  STEP: Saw pod success @ 05/11/23 14:58:06.021
  May 11 14:58:06.023: INFO: Trying to get logs from node macpro-2 pod pod-subpath-test-downwardapi-krcg container test-container-subpath-downwardapi-krcg: <nil>
  STEP: delete the pod @ 05/11/23 14:58:06.029
  STEP: Deleting pod pod-subpath-test-downwardapi-krcg @ 05/11/23 14:58:06.041
  May 11 14:58:06.041: INFO: Deleting pod "pod-subpath-test-downwardapi-krcg" in namespace "subpath-958"
  May 11 14:58:06.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-958" for this suite. @ 05/11/23 14:58:06.048
• [24.123 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/11/23 14:58:06.055
  May 11 14:58:06.055: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pod-network-test @ 05/11/23 14:58:06.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:06.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:06.076
  STEP: Performing setup for networking test in namespace pod-network-test-350 @ 05/11/23 14:58:06.078
  STEP: creating a selector @ 05/11/23 14:58:06.078
  STEP: Creating the service pods in kubernetes @ 05/11/23 14:58:06.078
  May 11 14:58:06.078: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/11/23 14:58:18.16
  May 11 14:58:20.187: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  May 11 14:58:20.187: INFO: Going to poll 192.168.151.213 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 11 14:58:20.189: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.151.213 8081 | grep -v '^\s*$'] Namespace:pod-network-test-350 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:58:20.189: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:58:20.189: INFO: ExecWithOptions: Clientset creation
  May 11 14:58:20.189: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-350/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.151.213+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 11 14:58:21.254: INFO: Found all 1 expected endpoints: [netserver-0]
  May 11 14:58:21.254: INFO: Going to poll 192.168.150.171 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 11 14:58:21.258: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.150.171 8081 | grep -v '^\s*$'] Namespace:pod-network-test-350 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:58:21.258: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:58:21.258: INFO: ExecWithOptions: Clientset creation
  May 11 14:58:21.258: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-350/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.150.171+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 11 14:58:22.326: INFO: Found all 1 expected endpoints: [netserver-1]
  May 11 14:58:22.326: INFO: Going to poll 192.168.153.36 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  May 11 14:58:22.330: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 192.168.153.36 8081 | grep -v '^\s*$'] Namespace:pod-network-test-350 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 11 14:58:22.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  May 11 14:58:22.330: INFO: ExecWithOptions: Clientset creation
  May 11 14:58:22.330: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-350/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+192.168.153.36+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 11 14:58:23.391: INFO: Found all 1 expected endpoints: [netserver-2]
  May 11 14:58:23.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-350" for this suite. @ 05/11/23 14:58:23.396
• [17.347 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/11/23 14:58:23.403
  May 11 14:58:23.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/11/23 14:58:23.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:23.422
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:23.424
  STEP: creating @ 05/11/23 14:58:23.426
  STEP: getting @ 05/11/23 14:58:23.44
  STEP: listing in namespace @ 05/11/23 14:58:23.445
  STEP: patching @ 05/11/23 14:58:23.451
  STEP: deleting @ 05/11/23 14:58:23.459
  May 11 14:58:23.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-8372" for this suite. @ 05/11/23 14:58:23.483
• [0.085 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/11/23 14:58:23.488
  May 11 14:58:23.488: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename field-validation @ 05/11/23 14:58:23.489
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:23.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:23.503
  May 11 14:58:23.505: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  W0511 14:58:26.046434      24 warnings.go:70] unknown field "alpha"
  W0511 14:58:26.046478      24 warnings.go:70] unknown field "beta"
  W0511 14:58:26.046485      24 warnings.go:70] unknown field "delta"
  W0511 14:58:26.046490      24 warnings.go:70] unknown field "epsilon"
  W0511 14:58:26.046495      24 warnings.go:70] unknown field "gamma"
  May 11 14:58:26.056: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8773" for this suite. @ 05/11/23 14:58:26.071
• [2.588 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/11/23 14:58:26.078
  May 11 14:58:26.078: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename init-container @ 05/11/23 14:58:26.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:26.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:26.095
  STEP: creating the pod @ 05/11/23 14:58:26.096
  May 11 14:58:26.097: INFO: PodSpec: initContainers in spec.initContainers
  May 11 14:58:29.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-180" for this suite. @ 05/11/23 14:58:29.42
• [3.348 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/11/23 14:58:29.427
  May 11 14:58:29.427: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename secrets @ 05/11/23 14:58:29.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:29.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:29.446
  STEP: Creating secret with name secret-test-c2bc70c8-fa1f-4449-879e-bbb2ebaf0035 @ 05/11/23 14:58:29.465
  STEP: Creating a pod to test consume secrets @ 05/11/23 14:58:29.47
  STEP: Saw pod success @ 05/11/23 14:58:33.491
  May 11 14:58:33.494: INFO: Trying to get logs from node macpro-2 pod pod-secrets-9c9e6211-7ae6-48f2-8f0d-4977a36fa640 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/11/23 14:58:33.499
  May 11 14:58:33.513: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4662" for this suite. @ 05/11/23 14:58:33.517
  STEP: Destroying namespace "secret-namespace-6481" for this suite. @ 05/11/23 14:58:33.522
• [4.107 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/11/23 14:58:33.534
  May 11 14:58:33.534: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename svc-latency @ 05/11/23 14:58:33.534
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:33.552
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:33.556
  May 11 14:58:33.558: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-5955 @ 05/11/23 14:58:33.558
  I0511 14:58:33.574154      24 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-5955, replica count: 1
  I0511 14:58:34.625701      24 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0511 14:58:35.625862      24 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 14:58:35.747: INFO: Created: latency-svc-mlx85
  May 11 14:58:35.756: INFO: Got endpoints: latency-svc-mlx85 [29.756736ms]
  May 11 14:58:35.790: INFO: Created: latency-svc-8grv9
  May 11 14:58:35.803: INFO: Got endpoints: latency-svc-8grv9 [46.893807ms]
  May 11 14:58:35.810: INFO: Created: latency-svc-njzqf
  May 11 14:58:35.821: INFO: Got endpoints: latency-svc-njzqf [64.60427ms]
  May 11 14:58:35.826: INFO: Created: latency-svc-scn6w
  May 11 14:58:35.836: INFO: Got endpoints: latency-svc-scn6w [79.446977ms]
  May 11 14:58:35.843: INFO: Created: latency-svc-2wwl2
  May 11 14:58:35.854: INFO: Got endpoints: latency-svc-2wwl2 [96.838823ms]
  May 11 14:58:35.865: INFO: Created: latency-svc-qqwc4
  May 11 14:58:35.875: INFO: Got endpoints: latency-svc-qqwc4 [118.772527ms]
  May 11 14:58:35.880: INFO: Created: latency-svc-45cgx
  May 11 14:58:35.891: INFO: Got endpoints: latency-svc-45cgx [135.1476ms]
  May 11 14:58:35.903: INFO: Created: latency-svc-qw9jn
  May 11 14:58:35.911: INFO: Got endpoints: latency-svc-qw9jn [154.674018ms]
  May 11 14:58:35.918: INFO: Created: latency-svc-cffnl
  May 11 14:58:35.930: INFO: Got endpoints: latency-svc-cffnl [174.084863ms]
  May 11 14:58:35.935: INFO: Created: latency-svc-dmmjk
  May 11 14:58:35.944: INFO: Got endpoints: latency-svc-dmmjk [187.687755ms]
  May 11 14:58:35.952: INFO: Created: latency-svc-ljkzh
  May 11 14:58:35.963: INFO: Got endpoints: latency-svc-ljkzh [206.604137ms]
  May 11 14:58:36.173: INFO: Created: latency-svc-ttv62
  May 11 14:58:36.173: INFO: Created: latency-svc-8phsd
  May 11 14:58:36.173: INFO: Created: latency-svc-6rlpm
  May 11 14:58:36.173: INFO: Created: latency-svc-4tx2s
  May 11 14:58:36.184: INFO: Created: latency-svc-rcr2f
  May 11 14:58:36.184: INFO: Created: latency-svc-bjrqs
  May 11 14:58:36.190: INFO: Created: latency-svc-9cwnm
  May 11 14:58:36.190: INFO: Created: latency-svc-7lsd4
  May 11 14:58:36.190: INFO: Created: latency-svc-zvw4j
  May 11 14:58:36.190: INFO: Created: latency-svc-79chj
  May 11 14:58:36.190: INFO: Created: latency-svc-6j7wh
  May 11 14:58:36.190: INFO: Created: latency-svc-l5bxz
  May 11 14:58:36.190: INFO: Created: latency-svc-b9zkc
  May 11 14:58:36.190: INFO: Created: latency-svc-rhvhw
  May 11 14:58:36.190: INFO: Created: latency-svc-f9z7x
  May 11 14:58:36.234: INFO: Got endpoints: latency-svc-4tx2s [303.620941ms]
  May 11 14:58:36.234: INFO: Got endpoints: latency-svc-ttv62 [271.14604ms]
  May 11 14:58:36.234: INFO: Got endpoints: latency-svc-6rlpm [476.743757ms]
  May 11 14:58:36.234: INFO: Got endpoints: latency-svc-8phsd [290.303591ms]
  May 11 14:58:36.234: INFO: Got endpoints: latency-svc-bjrqs [477.274149ms]
  May 11 14:58:36.268: INFO: Got endpoints: latency-svc-zvw4j [511.029765ms]
  May 11 14:58:36.268: INFO: Got endpoints: latency-svc-rcr2f [432.435866ms]
  May 11 14:58:36.268: INFO: Got endpoints: latency-svc-7lsd4 [510.865989ms]
  May 11 14:58:36.269: INFO: Got endpoints: latency-svc-79chj [393.696356ms]
  May 11 14:58:36.269: INFO: Got endpoints: latency-svc-6j7wh [465.365309ms]
  May 11 14:58:36.283: INFO: Got endpoints: latency-svc-l5bxz [462.166858ms]
  May 11 14:58:36.299: INFO: Got endpoints: latency-svc-b9zkc [407.556282ms]
  May 11 14:58:36.300: INFO: Created: latency-svc-5frx7
  May 11 14:58:36.310: INFO: Got endpoints: latency-svc-9cwnm [552.779585ms]
  May 11 14:58:36.310: INFO: Got endpoints: latency-svc-rhvhw [399.113042ms]
  May 11 14:58:36.310: INFO: Got endpoints: latency-svc-f9z7x [456.584212ms]
  May 11 14:58:36.328: INFO: Got endpoints: latency-svc-5frx7 [94.275186ms]
  May 11 14:58:36.328: INFO: Created: latency-svc-k7b7x
  May 11 14:58:36.355: INFO: Created: latency-svc-fzz7m
  May 11 14:58:36.356: INFO: Got endpoints: latency-svc-k7b7x [121.557837ms]
  May 11 14:58:36.367: INFO: Got endpoints: latency-svc-fzz7m [132.55838ms]
  May 11 14:58:36.373: INFO: Created: latency-svc-nnljp
  May 11 14:58:36.380: INFO: Got endpoints: latency-svc-nnljp [145.455388ms]
  May 11 14:58:36.385: INFO: Created: latency-svc-2ws79
  May 11 14:58:36.392: INFO: Got endpoints: latency-svc-2ws79 [158.051601ms]
  May 11 14:58:36.399: INFO: Created: latency-svc-w79s6
  May 11 14:58:36.408: INFO: Got endpoints: latency-svc-w79s6 [139.509598ms]
  May 11 14:58:36.412: INFO: Created: latency-svc-gmhw4
  May 11 14:58:36.421: INFO: Got endpoints: latency-svc-gmhw4 [152.947978ms]
  May 11 14:58:36.427: INFO: Created: latency-svc-75plm
  May 11 14:58:36.435: INFO: Got endpoints: latency-svc-75plm [166.396709ms]
  May 11 14:58:36.439: INFO: Created: latency-svc-9l6n9
  May 11 14:58:36.454: INFO: Got endpoints: latency-svc-9l6n9 [185.097195ms]
  May 11 14:58:36.459: INFO: Created: latency-svc-fvnln
  May 11 14:58:36.468: INFO: Got endpoints: latency-svc-fvnln [199.304615ms]
  May 11 14:58:36.476: INFO: Created: latency-svc-x8btc
  May 11 14:58:36.485: INFO: Got endpoints: latency-svc-x8btc [201.187181ms]
  May 11 14:58:36.489: INFO: Created: latency-svc-g7dn7
  May 11 14:58:36.498: INFO: Got endpoints: latency-svc-g7dn7 [199.515788ms]
  May 11 14:58:36.504: INFO: Created: latency-svc-ht2dj
  May 11 14:58:36.517: INFO: Got endpoints: latency-svc-ht2dj [206.820449ms]
  May 11 14:58:36.521: INFO: Created: latency-svc-268kz
  May 11 14:58:36.530: INFO: Got endpoints: latency-svc-268kz [219.838419ms]
  May 11 14:58:36.545: INFO: Created: latency-svc-z68mz
  May 11 14:58:36.559: INFO: Got endpoints: latency-svc-z68mz [249.221865ms]
  May 11 14:58:36.567: INFO: Created: latency-svc-pcktw
  May 11 14:58:36.577: INFO: Got endpoints: latency-svc-pcktw [248.261054ms]
  May 11 14:58:36.584: INFO: Created: latency-svc-hq7t4
  May 11 14:58:36.587: INFO: Got endpoints: latency-svc-hq7t4 [231.414502ms]
  May 11 14:58:36.601: INFO: Created: latency-svc-27p6x
  May 11 14:58:36.606: INFO: Got endpoints: latency-svc-27p6x [238.735369ms]
  May 11 14:58:36.608: INFO: Created: latency-svc-gthcr
  May 11 14:58:36.640: INFO: Got endpoints: latency-svc-gthcr [259.959778ms]
  May 11 14:58:36.645: INFO: Created: latency-svc-rb528
  May 11 14:58:36.653: INFO: Got endpoints: latency-svc-rb528 [261.295138ms]
  May 11 14:58:36.667: INFO: Created: latency-svc-kwp4b
  May 11 14:58:36.675: INFO: Created: latency-svc-tqt6c
  May 11 14:58:36.676: INFO: Got endpoints: latency-svc-kwp4b [268.743007ms]
  May 11 14:58:36.696: INFO: Got endpoints: latency-svc-tqt6c [274.548881ms]
  May 11 14:58:36.703: INFO: Created: latency-svc-xztcs
  May 11 14:58:36.709: INFO: Got endpoints: latency-svc-xztcs [274.057765ms]
  May 11 14:58:36.716: INFO: Created: latency-svc-mx8cr
  May 11 14:58:36.767: INFO: Created: latency-svc-q57wt
  May 11 14:58:36.776: INFO: Got endpoints: latency-svc-mx8cr [322.446645ms]
  May 11 14:58:36.799: INFO: Created: latency-svc-9s27p
  May 11 14:58:36.817: INFO: Got endpoints: latency-svc-q57wt [348.939574ms]
  May 11 14:58:36.837: INFO: Created: latency-svc-h8fsf
  May 11 14:58:36.864: INFO: Got endpoints: latency-svc-9s27p [379.219693ms]
  May 11 14:58:36.871: INFO: Created: latency-svc-lbq8r
  May 11 14:58:36.914: INFO: Got endpoints: latency-svc-h8fsf [415.686312ms]
  May 11 14:58:36.922: INFO: Created: latency-svc-4frlk
  May 11 14:58:36.930: INFO: Created: latency-svc-8mf45
  May 11 14:58:36.942: INFO: Created: latency-svc-zpdtq
  May 11 14:58:36.951: INFO: Got endpoints: latency-svc-lbq8r [434.333862ms]
  May 11 14:58:36.957: INFO: Created: latency-svc-ff8ts
  May 11 14:58:36.971: INFO: Created: latency-svc-p47m4
  May 11 14:58:36.984: INFO: Created: latency-svc-4qc7x
  May 11 14:58:36.998: INFO: Created: latency-svc-t4qkq
  May 11 14:58:37.002: INFO: Got endpoints: latency-svc-4frlk [471.724464ms]
  May 11 14:58:37.007: INFO: Created: latency-svc-qfm5j
  May 11 14:58:37.022: INFO: Created: latency-svc-96stp
  May 11 14:58:37.033: INFO: Created: latency-svc-vtvs4
  May 11 14:58:37.044: INFO: Created: latency-svc-pd7d6
  May 11 14:58:37.051: INFO: Got endpoints: latency-svc-8mf45 [491.797744ms]
  May 11 14:58:37.058: INFO: Created: latency-svc-pg9sz
  May 11 14:58:37.097: INFO: Created: latency-svc-brmm6
  May 11 14:58:37.117: INFO: Got endpoints: latency-svc-zpdtq [540.35054ms]
  May 11 14:58:37.145: INFO: Created: latency-svc-vgzzn
  May 11 14:58:37.165: INFO: Got endpoints: latency-svc-ff8ts [577.320989ms]
  May 11 14:58:37.200: INFO: Created: latency-svc-tgvzf
  May 11 14:58:37.205: INFO: Got endpoints: latency-svc-p47m4 [599.629642ms]
  May 11 14:58:37.218: INFO: Created: latency-svc-crpk4
  May 11 14:58:37.227: INFO: Created: latency-svc-lqdm7
  May 11 14:58:37.236: INFO: Created: latency-svc-qc7zk
  May 11 14:58:37.270: INFO: Got endpoints: latency-svc-4qc7x [630.571029ms]
  May 11 14:58:37.290: INFO: Created: latency-svc-gbj59
  May 11 14:58:37.302: INFO: Got endpoints: latency-svc-t4qkq [648.440327ms]
  May 11 14:58:37.307: INFO: Created: latency-svc-gvpbw
  May 11 14:58:37.326: INFO: Created: latency-svc-kqbm6
  May 11 14:58:37.335: INFO: Created: latency-svc-kkb6s
  May 11 14:58:37.357: INFO: Got endpoints: latency-svc-qfm5j [680.592666ms]
  May 11 14:58:37.369: INFO: Created: latency-svc-5sfcd
  May 11 14:58:37.400: INFO: Got endpoints: latency-svc-96stp [704.183384ms]
  May 11 14:58:37.431: INFO: Created: latency-svc-2w44z
  May 11 14:58:37.452: INFO: Got endpoints: latency-svc-vtvs4 [743.594168ms]
  May 11 14:58:37.498: INFO: Created: latency-svc-7mrdh
  May 11 14:58:37.511: INFO: Got endpoints: latency-svc-pd7d6 [734.837509ms]
  May 11 14:58:37.545: INFO: Created: latency-svc-llxdw
  May 11 14:58:37.565: INFO: Got endpoints: latency-svc-pg9sz [748.200218ms]
  May 11 14:58:37.593: INFO: Created: latency-svc-2v8vk
  May 11 14:58:37.602: INFO: Got endpoints: latency-svc-brmm6 [737.668238ms]
  May 11 14:58:37.626: INFO: Created: latency-svc-lh5mw
  May 11 14:58:37.654: INFO: Got endpoints: latency-svc-vgzzn [740.124401ms]
  May 11 14:58:37.672: INFO: Created: latency-svc-9hzht
  May 11 14:58:37.701: INFO: Got endpoints: latency-svc-tgvzf [750.266659ms]
  May 11 14:58:37.725: INFO: Created: latency-svc-bxzhl
  May 11 14:58:37.751: INFO: Got endpoints: latency-svc-crpk4 [748.861567ms]
  May 11 14:58:37.777: INFO: Created: latency-svc-bth4n
  May 11 14:58:37.820: INFO: Got endpoints: latency-svc-lqdm7 [768.767453ms]
  May 11 14:58:37.835: INFO: Created: latency-svc-4bg5m
  May 11 14:58:37.854: INFO: Got endpoints: latency-svc-qc7zk [736.60324ms]
  May 11 14:58:37.868: INFO: Created: latency-svc-pxq6n
  May 11 14:58:37.901: INFO: Got endpoints: latency-svc-gbj59 [736.594912ms]
  May 11 14:58:37.917: INFO: Created: latency-svc-lbr5j
  May 11 14:58:37.955: INFO: Got endpoints: latency-svc-gvpbw [749.772078ms]
  May 11 14:58:37.968: INFO: Created: latency-svc-f8pnr
  May 11 14:58:38.026: INFO: Got endpoints: latency-svc-kqbm6 [755.716454ms]
  May 11 14:58:38.041: INFO: Created: latency-svc-7bkwx
  May 11 14:58:38.052: INFO: Got endpoints: latency-svc-kkb6s [750.179622ms]
  May 11 14:58:38.083: INFO: Created: latency-svc-cskrp
  May 11 14:58:38.102: INFO: Got endpoints: latency-svc-5sfcd [744.448061ms]
  May 11 14:58:38.120: INFO: Created: latency-svc-zww5p
  May 11 14:58:38.152: INFO: Got endpoints: latency-svc-2w44z [752.243675ms]
  May 11 14:58:38.168: INFO: Created: latency-svc-p9l69
  May 11 14:58:38.200: INFO: Got endpoints: latency-svc-7mrdh [747.349199ms]
  May 11 14:58:38.218: INFO: Created: latency-svc-5kn4n
  May 11 14:58:38.250: INFO: Got endpoints: latency-svc-llxdw [738.384849ms]
  May 11 14:58:38.283: INFO: Created: latency-svc-6jhdd
  May 11 14:58:38.300: INFO: Got endpoints: latency-svc-2v8vk [734.773818ms]
  May 11 14:58:38.320: INFO: Created: latency-svc-kj6tt
  May 11 14:58:38.350: INFO: Got endpoints: latency-svc-lh5mw [748.683861ms]
  May 11 14:58:38.365: INFO: Created: latency-svc-fnmxb
  May 11 14:58:38.401: INFO: Got endpoints: latency-svc-9hzht [746.190465ms]
  May 11 14:58:38.415: INFO: Created: latency-svc-qqhlt
  May 11 14:58:38.451: INFO: Got endpoints: latency-svc-bxzhl [749.634024ms]
  May 11 14:58:38.467: INFO: Created: latency-svc-7tq85
  May 11 14:58:38.502: INFO: Got endpoints: latency-svc-bth4n [751.404417ms]
  May 11 14:58:38.520: INFO: Created: latency-svc-tdxjt
  May 11 14:58:38.570: INFO: Got endpoints: latency-svc-4bg5m [749.6201ms]
  May 11 14:58:38.592: INFO: Created: latency-svc-5mz77
  May 11 14:58:38.602: INFO: Got endpoints: latency-svc-pxq6n [748.376473ms]
  May 11 14:58:38.619: INFO: Created: latency-svc-lc59r
  May 11 14:58:38.672: INFO: Got endpoints: latency-svc-lbr5j [771.03117ms]
  May 11 14:58:38.687: INFO: Created: latency-svc-m25bw
  May 11 14:58:38.700: INFO: Got endpoints: latency-svc-f8pnr [745.009614ms]
  May 11 14:58:38.714: INFO: Created: latency-svc-slxtw
  May 11 14:58:38.751: INFO: Got endpoints: latency-svc-7bkwx [724.918219ms]
  May 11 14:58:38.771: INFO: Created: latency-svc-29j8n
  May 11 14:58:38.807: INFO: Got endpoints: latency-svc-cskrp [754.432611ms]
  May 11 14:58:38.858: INFO: Got endpoints: latency-svc-zww5p [756.850078ms]
  May 11 14:58:38.874: INFO: Created: latency-svc-7qg5l
  May 11 14:58:38.905: INFO: Created: latency-svc-xk8gg
  May 11 14:58:38.910: INFO: Got endpoints: latency-svc-p9l69 [757.743723ms]
  May 11 14:58:38.976: INFO: Created: latency-svc-sbndh
  May 11 14:58:38.976: INFO: Got endpoints: latency-svc-5kn4n [776.19977ms]
  May 11 14:58:39.016: INFO: Got endpoints: latency-svc-6jhdd [766.233565ms]
  May 11 14:58:39.025: INFO: Created: latency-svc-97klm
  May 11 14:58:39.041: INFO: Created: latency-svc-2s8fh
  May 11 14:58:39.055: INFO: Got endpoints: latency-svc-kj6tt [754.545885ms]
  May 11 14:58:39.071: INFO: Created: latency-svc-96ft9
  May 11 14:58:39.100: INFO: Got endpoints: latency-svc-fnmxb [749.564975ms]
  May 11 14:58:39.117: INFO: Created: latency-svc-jm5mz
  May 11 14:58:39.154: INFO: Got endpoints: latency-svc-qqhlt [753.456677ms]
  May 11 14:58:39.176: INFO: Created: latency-svc-smnk9
  May 11 14:58:39.201: INFO: Got endpoints: latency-svc-7tq85 [749.526791ms]
  May 11 14:58:39.222: INFO: Created: latency-svc-4j5tl
  May 11 14:58:39.252: INFO: Got endpoints: latency-svc-tdxjt [750.211749ms]
  May 11 14:58:39.279: INFO: Created: latency-svc-jgbx6
  May 11 14:58:39.300: INFO: Got endpoints: latency-svc-5mz77 [729.983036ms]
  May 11 14:58:39.314: INFO: Created: latency-svc-fbh9w
  May 11 14:58:39.352: INFO: Got endpoints: latency-svc-lc59r [750.159415ms]
  May 11 14:58:39.369: INFO: Created: latency-svc-pfltn
  May 11 14:58:39.400: INFO: Got endpoints: latency-svc-m25bw [727.509708ms]
  May 11 14:58:39.415: INFO: Created: latency-svc-9mfzw
  May 11 14:58:39.451: INFO: Got endpoints: latency-svc-slxtw [750.697178ms]
  May 11 14:58:39.482: INFO: Created: latency-svc-svxhs
  May 11 14:58:39.498: INFO: Got endpoints: latency-svc-29j8n [746.709857ms]
  May 11 14:58:39.511: INFO: Created: latency-svc-4vf72
  May 11 14:58:39.552: INFO: Got endpoints: latency-svc-7qg5l [745.741969ms]
  May 11 14:58:39.566: INFO: Created: latency-svc-49dhj
  May 11 14:58:39.601: INFO: Got endpoints: latency-svc-xk8gg [742.936484ms]
  May 11 14:58:39.614: INFO: Created: latency-svc-sdn6d
  May 11 14:58:39.653: INFO: Got endpoints: latency-svc-sbndh [743.352917ms]
  May 11 14:58:39.685: INFO: Created: latency-svc-j24lh
  May 11 14:58:39.701: INFO: Got endpoints: latency-svc-97klm [724.933163ms]
  May 11 14:58:39.713: INFO: Created: latency-svc-997n8
  May 11 14:58:39.753: INFO: Got endpoints: latency-svc-2s8fh [736.639041ms]
  May 11 14:58:39.769: INFO: Created: latency-svc-w9q66
  May 11 14:58:39.800: INFO: Got endpoints: latency-svc-96ft9 [745.186974ms]
  May 11 14:58:39.817: INFO: Created: latency-svc-tx924
  May 11 14:58:39.849: INFO: Got endpoints: latency-svc-jm5mz [748.94693ms]
  May 11 14:58:39.862: INFO: Created: latency-svc-bxcq9
  May 11 14:58:39.900: INFO: Got endpoints: latency-svc-smnk9 [746.017044ms]
  May 11 14:58:39.917: INFO: Created: latency-svc-btvdm
  May 11 14:58:39.951: INFO: Got endpoints: latency-svc-4j5tl [750.097258ms]
  May 11 14:58:39.966: INFO: Created: latency-svc-7rr69
  May 11 14:58:40.001: INFO: Got endpoints: latency-svc-jgbx6 [748.809497ms]
  May 11 14:58:40.018: INFO: Created: latency-svc-n8jc6
  May 11 14:58:40.051: INFO: Got endpoints: latency-svc-fbh9w [751.350696ms]
  May 11 14:58:40.073: INFO: Created: latency-svc-9mdgf
  May 11 14:58:40.099: INFO: Got endpoints: latency-svc-pfltn [747.228068ms]
  May 11 14:58:40.112: INFO: Created: latency-svc-8s46g
  May 11 14:58:40.155: INFO: Got endpoints: latency-svc-9mfzw [755.129401ms]
  May 11 14:58:40.171: INFO: Created: latency-svc-wd94v
  May 11 14:58:40.221: INFO: Got endpoints: latency-svc-svxhs [769.814226ms]
  May 11 14:58:40.232: INFO: Created: latency-svc-5hgqk
  May 11 14:58:40.249: INFO: Got endpoints: latency-svc-4vf72 [750.871342ms]
  May 11 14:58:40.263: INFO: Created: latency-svc-lws2r
  May 11 14:58:40.303: INFO: Got endpoints: latency-svc-49dhj [750.330988ms]
  May 11 14:58:40.317: INFO: Created: latency-svc-vfdjf
  May 11 14:58:40.350: INFO: Got endpoints: latency-svc-sdn6d [748.622569ms]
  May 11 14:58:40.365: INFO: Created: latency-svc-zg9pr
  May 11 14:58:40.401: INFO: Got endpoints: latency-svc-j24lh [747.604704ms]
  May 11 14:58:40.420: INFO: Created: latency-svc-57hhp
  May 11 14:58:40.470: INFO: Got endpoints: latency-svc-997n8 [768.536029ms]
  May 11 14:58:40.485: INFO: Created: latency-svc-gcsrc
  May 11 14:58:40.500: INFO: Got endpoints: latency-svc-w9q66 [747.224323ms]
  May 11 14:58:40.515: INFO: Created: latency-svc-zppzj
  May 11 14:58:40.555: INFO: Got endpoints: latency-svc-tx924 [754.787848ms]
  May 11 14:58:40.568: INFO: Created: latency-svc-82p6v
  May 11 14:58:40.600: INFO: Got endpoints: latency-svc-bxcq9 [751.579379ms]
  May 11 14:58:40.613: INFO: Created: latency-svc-lghcd
  May 11 14:58:40.669: INFO: Got endpoints: latency-svc-btvdm [769.051504ms]
  May 11 14:58:40.682: INFO: Created: latency-svc-lst9m
  May 11 14:58:40.707: INFO: Got endpoints: latency-svc-7rr69 [755.712498ms]
  May 11 14:58:40.719: INFO: Created: latency-svc-4qc47
  May 11 14:58:40.750: INFO: Got endpoints: latency-svc-n8jc6 [748.823831ms]
  May 11 14:58:40.769: INFO: Created: latency-svc-8pw28
  May 11 14:58:40.803: INFO: Got endpoints: latency-svc-9mdgf [751.492731ms]
  May 11 14:58:40.844: INFO: Created: latency-svc-vmj2n
  May 11 14:58:40.852: INFO: Got endpoints: latency-svc-8s46g [752.163413ms]
  May 11 14:58:40.864: INFO: Created: latency-svc-8gwsg
  May 11 14:58:40.901: INFO: Got endpoints: latency-svc-wd94v [746.230335ms]
  May 11 14:58:40.913: INFO: Created: latency-svc-64ptd
  May 11 14:58:40.964: INFO: Got endpoints: latency-svc-5hgqk [742.943201ms]
  May 11 14:58:40.976: INFO: Created: latency-svc-dp4bz
  May 11 14:58:41.000: INFO: Got endpoints: latency-svc-lws2r [750.998895ms]
  May 11 14:58:41.015: INFO: Created: latency-svc-rcwvr
  May 11 14:58:41.070: INFO: Got endpoints: latency-svc-vfdjf [767.148462ms]
  May 11 14:58:41.104: INFO: Created: latency-svc-wt927
  May 11 14:58:41.106: INFO: Got endpoints: latency-svc-zg9pr [755.635255ms]
  May 11 14:58:41.136: INFO: Created: latency-svc-pttgk
  May 11 14:58:41.173: INFO: Got endpoints: latency-svc-57hhp [771.82982ms]
  May 11 14:58:41.189: INFO: Created: latency-svc-85pcm
  May 11 14:58:41.202: INFO: Got endpoints: latency-svc-gcsrc [732.745268ms]
  May 11 14:58:41.223: INFO: Created: latency-svc-mcg2b
  May 11 14:58:41.250: INFO: Got endpoints: latency-svc-zppzj [750.075954ms]
  May 11 14:58:41.266: INFO: Created: latency-svc-f752c
  May 11 14:58:41.300: INFO: Got endpoints: latency-svc-82p6v [745.458898ms]
  May 11 14:58:41.331: INFO: Created: latency-svc-sgpkv
  May 11 14:58:41.352: INFO: Got endpoints: latency-svc-lghcd [751.909438ms]
  May 11 14:58:41.383: INFO: Created: latency-svc-lsctv
  May 11 14:58:41.400: INFO: Got endpoints: latency-svc-lst9m [730.478663ms]
  May 11 14:58:41.412: INFO: Created: latency-svc-9zpct
  May 11 14:58:41.452: INFO: Got endpoints: latency-svc-4qc47 [745.322147ms]
  May 11 14:58:41.485: INFO: Created: latency-svc-j2w7q
  May 11 14:58:41.500: INFO: Got endpoints: latency-svc-8pw28 [750.362867ms]
  May 11 14:58:41.521: INFO: Created: latency-svc-2n6kp
  May 11 14:58:41.554: INFO: Got endpoints: latency-svc-vmj2n [750.935122ms]
  May 11 14:58:41.600: INFO: Created: latency-svc-kmjwc
  May 11 14:58:41.604: INFO: Got endpoints: latency-svc-8gwsg [752.195157ms]
  May 11 14:58:41.622: INFO: Created: latency-svc-6c82h
  May 11 14:58:41.670: INFO: Got endpoints: latency-svc-64ptd [768.892198ms]
  May 11 14:58:41.686: INFO: Created: latency-svc-9l4qb
  May 11 14:58:41.723: INFO: Got endpoints: latency-svc-dp4bz [759.184406ms]
  May 11 14:58:41.742: INFO: Created: latency-svc-mlk8m
  May 11 14:58:41.750: INFO: Got endpoints: latency-svc-rcwvr [750.737754ms]
  May 11 14:58:41.767: INFO: Created: latency-svc-lxmfs
  May 11 14:58:41.800: INFO: Got endpoints: latency-svc-wt927 [729.997588ms]
  May 11 14:58:41.816: INFO: Created: latency-svc-xr6zf
  May 11 14:58:41.854: INFO: Got endpoints: latency-svc-pttgk [748.078107ms]
  May 11 14:58:41.866: INFO: Created: latency-svc-qg7m2
  May 11 14:58:41.900: INFO: Got endpoints: latency-svc-85pcm [727.4401ms]
  May 11 14:58:41.925: INFO: Created: latency-svc-fb6xs
  May 11 14:58:41.949: INFO: Got endpoints: latency-svc-mcg2b [746.811321ms]
  May 11 14:58:41.960: INFO: Created: latency-svc-fw4gn
  May 11 14:58:42.001: INFO: Got endpoints: latency-svc-f752c [750.701131ms]
  May 11 14:58:42.019: INFO: Created: latency-svc-fcr2v
  May 11 14:58:42.052: INFO: Got endpoints: latency-svc-sgpkv [751.525524ms]
  May 11 14:58:42.065: INFO: Created: latency-svc-xj8mt
  May 11 14:58:42.120: INFO: Got endpoints: latency-svc-lsctv [768.085311ms]
  May 11 14:58:42.135: INFO: Created: latency-svc-mwcxt
  May 11 14:58:42.151: INFO: Got endpoints: latency-svc-9zpct [751.143506ms]
  May 11 14:58:42.181: INFO: Created: latency-svc-5vpmd
  May 11 14:58:42.200: INFO: Got endpoints: latency-svc-j2w7q [747.920562ms]
  May 11 14:58:42.216: INFO: Created: latency-svc-jjkwh
  May 11 14:58:42.250: INFO: Got endpoints: latency-svc-2n6kp [749.739969ms]
  May 11 14:58:42.265: INFO: Created: latency-svc-dz89j
  May 11 14:58:42.299: INFO: Got endpoints: latency-svc-kmjwc [745.461847ms]
  May 11 14:58:42.321: INFO: Created: latency-svc-p7p8l
  May 11 14:58:42.350: INFO: Got endpoints: latency-svc-6c82h [745.878481ms]
  May 11 14:58:42.363: INFO: Created: latency-svc-qkxlz
  May 11 14:58:42.399: INFO: Got endpoints: latency-svc-9l4qb [728.34226ms]
  May 11 14:58:42.412: INFO: Created: latency-svc-rc6jk
  May 11 14:58:42.453: INFO: Got endpoints: latency-svc-mlk8m [729.714327ms]
  May 11 14:58:42.464: INFO: Created: latency-svc-2qg22
  May 11 14:58:42.500: INFO: Got endpoints: latency-svc-lxmfs [749.795563ms]
  May 11 14:58:42.513: INFO: Created: latency-svc-fdkqp
  May 11 14:58:42.550: INFO: Got endpoints: latency-svc-xr6zf [750.313177ms]
  May 11 14:58:42.564: INFO: Created: latency-svc-lmlqf
  May 11 14:58:42.607: INFO: Got endpoints: latency-svc-qg7m2 [753.023981ms]
  May 11 14:58:42.676: INFO: Got endpoints: latency-svc-fb6xs [775.38735ms]
  May 11 14:58:42.682: INFO: Created: latency-svc-qwqnw
  May 11 14:58:42.720: INFO: Created: latency-svc-jmc8v
  May 11 14:58:42.721: INFO: Got endpoints: latency-svc-fw4gn [771.909544ms]
  May 11 14:58:42.755: INFO: Created: latency-svc-fvb27
  May 11 14:58:42.790: INFO: Got endpoints: latency-svc-fcr2v [789.617031ms]
  May 11 14:58:42.814: INFO: Got endpoints: latency-svc-xj8mt [762.523973ms]
  May 11 14:58:42.834: INFO: Created: latency-svc-l7wcr
  May 11 14:58:42.844: INFO: Created: latency-svc-jt2kh
  May 11 14:58:42.854: INFO: Got endpoints: latency-svc-mwcxt [733.259736ms]
  May 11 14:58:42.867: INFO: Created: latency-svc-4h67x
  May 11 14:58:42.901: INFO: Got endpoints: latency-svc-5vpmd [749.915091ms]
  May 11 14:58:42.917: INFO: Created: latency-svc-2rc6d
  May 11 14:58:42.972: INFO: Got endpoints: latency-svc-jjkwh [772.16197ms]
  May 11 14:58:42.986: INFO: Created: latency-svc-jjf9c
  May 11 14:58:42.999: INFO: Got endpoints: latency-svc-dz89j [749.01674ms]
  May 11 14:58:43.031: INFO: Created: latency-svc-x64cn
  May 11 14:58:43.051: INFO: Got endpoints: latency-svc-p7p8l [751.864238ms]
  May 11 14:58:43.080: INFO: Created: latency-svc-ft5m2
  May 11 14:58:43.105: INFO: Got endpoints: latency-svc-qkxlz [754.924641ms]
  May 11 14:58:43.126: INFO: Created: latency-svc-qgrns
  May 11 14:58:43.154: INFO: Got endpoints: latency-svc-rc6jk [755.814418ms]
  May 11 14:58:43.169: INFO: Created: latency-svc-k59dz
  May 11 14:58:43.198: INFO: Got endpoints: latency-svc-2qg22 [745.604725ms]
  May 11 14:58:43.229: INFO: Created: latency-svc-zbc5g
  May 11 14:58:43.253: INFO: Got endpoints: latency-svc-fdkqp [752.461953ms]
  May 11 14:58:43.265: INFO: Created: latency-svc-fvg4w
  May 11 14:58:43.313: INFO: Got endpoints: latency-svc-lmlqf [762.490097ms]
  May 11 14:58:43.326: INFO: Created: latency-svc-trvdt
  May 11 14:58:43.350: INFO: Got endpoints: latency-svc-qwqnw [743.014186ms]
  May 11 14:58:43.362: INFO: Created: latency-svc-q7bbs
  May 11 14:58:43.400: INFO: Got endpoints: latency-svc-jmc8v [723.769864ms]
  May 11 14:58:43.411: INFO: Created: latency-svc-bsrzq
  May 11 14:58:43.450: INFO: Got endpoints: latency-svc-fvb27 [728.876109ms]
  May 11 14:58:43.461: INFO: Created: latency-svc-6g656
  May 11 14:58:43.523: INFO: Got endpoints: latency-svc-l7wcr [732.396256ms]
  May 11 14:58:43.537: INFO: Created: latency-svc-554jt
  May 11 14:58:43.552: INFO: Got endpoints: latency-svc-jt2kh [737.905188ms]
  May 11 14:58:43.563: INFO: Created: latency-svc-lhf7l
  May 11 14:58:43.603: INFO: Got endpoints: latency-svc-4h67x [749.310708ms]
  May 11 14:58:43.650: INFO: Got endpoints: latency-svc-2rc6d [748.903514ms]
  May 11 14:58:43.700: INFO: Got endpoints: latency-svc-jjf9c [727.492369ms]
  May 11 14:58:43.750: INFO: Got endpoints: latency-svc-x64cn [750.389366ms]
  May 11 14:58:43.801: INFO: Got endpoints: latency-svc-ft5m2 [749.858975ms]
  May 11 14:58:43.851: INFO: Got endpoints: latency-svc-qgrns [746.158824ms]
  May 11 14:58:43.900: INFO: Got endpoints: latency-svc-k59dz [745.644207ms]
  May 11 14:58:43.948: INFO: Got endpoints: latency-svc-zbc5g [750.113212ms]
  May 11 14:58:44.001: INFO: Got endpoints: latency-svc-fvg4w [748.501339ms]
  May 11 14:58:44.052: INFO: Got endpoints: latency-svc-trvdt [739.070531ms]
  May 11 14:58:44.101: INFO: Got endpoints: latency-svc-q7bbs [751.328258ms]
  May 11 14:58:44.156: INFO: Got endpoints: latency-svc-bsrzq [755.969342ms]
  May 11 14:58:44.201: INFO: Got endpoints: latency-svc-6g656 [750.974413ms]
  May 11 14:58:44.250: INFO: Got endpoints: latency-svc-554jt [727.141755ms]
  May 11 14:58:44.299: INFO: Got endpoints: latency-svc-lhf7l [746.590101ms]
  May 11 14:58:44.299: INFO: Latencies: [46.893807ms 64.60427ms 79.446977ms 94.275186ms 96.838823ms 118.772527ms 121.557837ms 132.55838ms 135.1476ms 139.509598ms 145.455388ms 152.947978ms 154.674018ms 158.051601ms 166.396709ms 174.084863ms 185.097195ms 187.687755ms 199.304615ms 199.515788ms 201.187181ms 206.604137ms 206.820449ms 219.838419ms 231.414502ms 238.735369ms 248.261054ms 249.221865ms 259.959778ms 261.295138ms 268.743007ms 271.14604ms 274.057765ms 274.548881ms 290.303591ms 303.620941ms 322.446645ms 348.939574ms 379.219693ms 393.696356ms 399.113042ms 407.556282ms 415.686312ms 432.435866ms 434.333862ms 456.584212ms 462.166858ms 465.365309ms 471.724464ms 476.743757ms 477.274149ms 491.797744ms 510.865989ms 511.029765ms 540.35054ms 552.779585ms 577.320989ms 599.629642ms 630.571029ms 648.440327ms 680.592666ms 704.183384ms 723.769864ms 724.918219ms 724.933163ms 727.141755ms 727.4401ms 727.492369ms 727.509708ms 728.34226ms 728.876109ms 729.714327ms 729.983036ms 729.997588ms 730.478663ms 732.396256ms 732.745268ms 733.259736ms 734.773818ms 734.837509ms 736.594912ms 736.60324ms 736.639041ms 737.668238ms 737.905188ms 738.384849ms 739.070531ms 740.124401ms 742.936484ms 742.943201ms 743.014186ms 743.352917ms 743.594168ms 744.448061ms 745.009614ms 745.186974ms 745.322147ms 745.458898ms 745.461847ms 745.604725ms 745.644207ms 745.741969ms 745.878481ms 746.017044ms 746.158824ms 746.190465ms 746.230335ms 746.590101ms 746.709857ms 746.811321ms 747.224323ms 747.228068ms 747.349199ms 747.604704ms 747.920562ms 748.078107ms 748.200218ms 748.376473ms 748.501339ms 748.622569ms 748.683861ms 748.809497ms 748.823831ms 748.861567ms 748.903514ms 748.94693ms 749.01674ms 749.310708ms 749.526791ms 749.564975ms 749.6201ms 749.634024ms 749.739969ms 749.772078ms 749.795563ms 749.858975ms 749.915091ms 750.075954ms 750.097258ms 750.113212ms 750.159415ms 750.179622ms 750.211749ms 750.266659ms 750.313177ms 750.330988ms 750.362867ms 750.389366ms 750.697178ms 750.701131ms 750.737754ms 750.871342ms 750.935122ms 750.974413ms 750.998895ms 751.143506ms 751.328258ms 751.350696ms 751.404417ms 751.492731ms 751.525524ms 751.579379ms 751.864238ms 751.909438ms 752.163413ms 752.195157ms 752.243675ms 752.461953ms 753.023981ms 753.456677ms 754.432611ms 754.545885ms 754.787848ms 754.924641ms 755.129401ms 755.635255ms 755.712498ms 755.716454ms 755.814418ms 755.969342ms 756.850078ms 757.743723ms 759.184406ms 762.490097ms 762.523973ms 766.233565ms 767.148462ms 768.085311ms 768.536029ms 768.767453ms 768.892198ms 769.051504ms 769.814226ms 771.03117ms 771.82982ms 771.909544ms 772.16197ms 775.38735ms 776.19977ms 789.617031ms]
  May 11 14:58:44.299: INFO: 50 %ile: 745.644207ms
  May 11 14:58:44.299: INFO: 90 %ile: 756.850078ms
  May 11 14:58:44.299: INFO: 99 %ile: 776.19977ms
  May 11 14:58:44.299: INFO: Total sample count: 200
  May 11 14:58:44.299: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-5955" for this suite. @ 05/11/23 14:58:44.307
• [10.779 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/11/23 14:58:44.313
  May 11 14:58:44.313: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename projected @ 05/11/23 14:58:44.314
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:44.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:44.34
  STEP: Creating a pod to test downward API volume plugin @ 05/11/23 14:58:44.342
  STEP: Saw pod success @ 05/11/23 14:58:48.36
  May 11 14:58:48.362: INFO: Trying to get logs from node macpro-2 pod downwardapi-volume-b626bc0e-cf0f-44cd-b14f-dbc66490c395 container client-container: <nil>
  STEP: delete the pod @ 05/11/23 14:58:48.368
  May 11 14:58:48.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9004" for this suite. @ 05/11/23 14:58:48.394
• [4.087 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/11/23 14:58:48.4
  May 11 14:58:48.400: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename configmap @ 05/11/23 14:58:48.401
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:48.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:48.417
  STEP: Creating configMap with name configmap-test-volume-97cbe6fd-3dea-4022-8042-c65badc61382 @ 05/11/23 14:58:48.419
  STEP: Creating a pod to test consume configMaps @ 05/11/23 14:58:48.424
  STEP: Saw pod success @ 05/11/23 14:58:52.46
  May 11 14:58:52.468: INFO: Trying to get logs from node macpro-2 pod pod-configmaps-f481b391-3340-4342-9093-b713b7250319 container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 14:58:52.479
  May 11 14:58:52.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5678" for this suite. @ 05/11/23 14:58:52.513
• [4.122 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/11/23 14:58:52.523
  May 11 14:58:52.523: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename kubectl @ 05/11/23 14:58:52.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:52.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:52.548
  STEP: creating Agnhost RC @ 05/11/23 14:58:52.553
  May 11 14:58:52.553: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-4091 create -f -'
  May 11 14:58:54.447: INFO: stderr: ""
  May 11 14:58:54.447: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/11/23 14:58:54.447
  May 11 14:58:55.454: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 14:58:55.454: INFO: Found 0 / 1
  May 11 14:58:56.452: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 14:58:56.452: INFO: Found 1 / 1
  May 11 14:58:56.452: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/11/23 14:58:56.452
  May 11 14:58:56.459: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 14:58:56.460: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 11 14:58:56.460: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=kubectl-4091 patch pod agnhost-primary-4lzfc -p {"metadata":{"annotations":{"x":"y"}}}'
  May 11 14:58:56.598: INFO: stderr: ""
  May 11 14:58:56.598: INFO: stdout: "pod/agnhost-primary-4lzfc patched\n"
  STEP: checking annotations @ 05/11/23 14:58:56.598
  May 11 14:58:56.605: INFO: Selector matched 1 pods for map[app:agnhost]
  May 11 14:58:56.605: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 11 14:58:56.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4091" for this suite. @ 05/11/23 14:58:56.612
• [4.111 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/11/23 14:58:56.634
  May 11 14:58:56.634: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename emptydir @ 05/11/23 14:58:56.635
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:58:56.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:58:56.791
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/11/23 14:58:56.795
  STEP: Saw pod success @ 05/11/23 14:59:00.919
  May 11 14:59:00.922: INFO: Trying to get logs from node macpro-2 pod pod-43a646f9-1dbc-411f-99e0-bc7aa035ba3d container test-container: <nil>
  STEP: delete the pod @ 05/11/23 14:59:00.927
  May 11 14:59:00.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2569" for this suite. @ 05/11/23 14:59:00.946
• [4.317 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/11/23 14:59:00.953
  May 11 14:59:00.953: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename deployment @ 05/11/23 14:59:00.954
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:59:00.97
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:59:00.973
  May 11 14:59:00.975: INFO: Creating deployment "webserver-deployment"
  May 11 14:59:00.980: INFO: Waiting for observed generation 1
  May 11 14:59:02.986: INFO: Waiting for all required pods to come up
  May 11 14:59:02.990: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/11/23 14:59:02.99
  May 11 14:59:02.990: INFO: Waiting for deployment "webserver-deployment" to complete
  May 11 14:59:02.995: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May 11 14:59:03.003: INFO: Updating deployment webserver-deployment
  May 11 14:59:03.003: INFO: Waiting for observed generation 2
  May 11 14:59:05.017: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May 11 14:59:05.022: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May 11 14:59:05.025: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 11 14:59:05.033: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May 11 14:59:05.033: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May 11 14:59:05.035: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 11 14:59:05.040: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May 11 14:59:05.040: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May 11 14:59:05.048: INFO: Updating deployment webserver-deployment
  May 11 14:59:05.048: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May 11 14:59:05.067: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May 11 14:59:05.093: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  May 11 14:59:05.131: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-2980  323003a1-7b43-4d2c-a160-1730c89e664f 43508 3 2023-05-11 14:59:00 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053c8418 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-11 14:59:03 +0000 UTC,LastTransitionTime:2023-05-11 14:59:00 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-11 14:59:05 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  May 11 14:59:05.170: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-2980  af15aae2-3aae-4c22-b4bd-66940d49068c 43485 3 2023-05-11 14:59:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 323003a1-7b43-4d2c-a160-1730c89e664f 0xc0053c8947 0xc0053c8948}] [] [{kube-controller-manager Update apps/v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"323003a1-7b43-4d2c-a160-1730c89e664f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053c89f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 11 14:59:05.170: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May 11 14:59:05.170: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-2980  bf821adc-9d2c-49bf-b198-05f56246e1ca 43482 3 2023-05-11 14:59:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 323003a1-7b43-4d2c-a160-1730c89e664f 0xc0053c8857 0xc0053c8858}] [] [{kube-controller-manager Update apps/v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"323003a1-7b43-4d2c-a160-1730c89e664f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0053c88e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  May 11 14:59:05.211: INFO: Pod "webserver-deployment-67bd4bf6dc-2c6pm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2c6pm webserver-deployment-67bd4bf6dc- deployment-2980  38e2f2a3-be0b-438a-9389-73217ee09ada 43533 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c8f67 0xc0053c8f68}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8vkzk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8vkzk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.211: INFO: Pod "webserver-deployment-67bd4bf6dc-4899n" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4899n webserver-deployment-67bd4bf6dc- deployment-2980  113430fc-b662-46c5-9b1d-c28e3049f7ff 43540 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c9290 0xc0053c9291}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qbp76,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qbp76,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.11,PodIP:,StartTime:2023-05-11 14:59:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.211: INFO: Pod "webserver-deployment-67bd4bf6dc-674kp" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-674kp webserver-deployment-67bd4bf6dc- deployment-2980  5ea9848e-696e-4d68-840b-dfe3399890f0 43333 0 2023-05-11 14:59:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:a5ac9691dbc247269f6f4a7bf49ad321101b1463a0664492d102cd39f85cd5fb cni.projectcalico.org/podIP:192.168.151.221/32 cni.projectcalico.org/podIPs:192.168.151.221/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c9467 0xc0053c9468}] [] [{calico Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.151.221\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sv7nm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sv7nm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.11,PodIP:192.168.151.221,StartTime:2023-05-11 14:59:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:59:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://632e9489f89e95d48297e9152bbbea1bbc3839b6c4970a4e5a2db8afa14a0ba6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.151.221,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.211: INFO: Pod "webserver-deployment-67bd4bf6dc-8dgfj" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8dgfj webserver-deployment-67bd4bf6dc- deployment-2980  41c60b32-659b-4755-8028-1006c6fee533 43305 0 2023-05-11 14:59:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:eb8b73f19f4fa29d7dd136d8b7da731ce38656330ec7983165cbeb22807dc95f cni.projectcalico.org/podIP:192.168.153.44/32 cni.projectcalico.org/podIPs:192.168.153.44/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c9677 0xc0053c9678}] [] [{calico Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-44l7r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-44l7r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:192.168.153.44,StartTime:2023-05-11 14:59:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:59:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7267a1f6649ecef91047990fabd5148bad6f3cf1403da602a34d1b165df544f2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.44,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.211: INFO: Pod "webserver-deployment-67bd4bf6dc-8dm59" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8dm59 webserver-deployment-67bd4bf6dc- deployment-2980  7f51965e-e8a7-409a-a3ed-c3dbc1bd68d5 43367 0 2023-05-11 14:59:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:b022a8d0e096ed18a8804a6f0f90cc832df1f9b6c0d847711922f0a289dd894e cni.projectcalico.org/podIP:192.168.153.24/32 cni.projectcalico.org/podIPs:192.168.153.24/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c9877 0xc0053c9878}] [] [{calico Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2q4q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2q4q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:192.168.153.24,StartTime:2023-05-11 14:59:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:59:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://27c0a89386f2822af055a5fddd354e71ef335bd5a193d2b86bf089cb1cc3a150,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.24,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.212: INFO: Pod "webserver-deployment-67bd4bf6dc-8w7rb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-8w7rb webserver-deployment-67bd4bf6dc- deployment-2980  047db5f9-7d5a-4a75-97d6-fc7af1266798 43313 0 2023-05-11 14:59:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:3fda1f04a47c58507e75be8a3dd8a64611d3bb029eac5a58b54e80a76e62ce8a cni.projectcalico.org/podIP:192.168.153.26/32 cni.projectcalico.org/podIPs:192.168.153.26/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c9a77 0xc0053c9a78}] [] [{calico Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.26\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8zqcc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8zqcc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:192.168.153.26,StartTime:2023-05-11 14:59:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:59:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1d6bc27dcb8c0845cd86bd50f0ea0fda446dbd62bc7b9c1ab49cb57c0d041a32,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.26,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.212: INFO: Pod "webserver-deployment-67bd4bf6dc-fk974" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fk974 webserver-deployment-67bd4bf6dc- deployment-2980  0ae82e48-8ab4-4c99-9cff-14e6853989cb 43527 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c9c77 0xc0053c9c78}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hf2s9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hf2s9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.212: INFO: Pod "webserver-deployment-67bd4bf6dc-fq94m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-fq94m webserver-deployment-67bd4bf6dc- deployment-2980  e932a5c7-cb2f-4e19-9222-4ba4e463ff8f 43530 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c9de0 0xc0053c9de1}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rq85c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rq85c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.212: INFO: Pod "webserver-deployment-67bd4bf6dc-j2w4r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-j2w4r webserver-deployment-67bd4bf6dc- deployment-2980  e1b289ff-3099-4c9d-9b3a-82063ce0bae2 43523 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0053c9f40 0xc0053c9f41}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqwt6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqwt6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.212: INFO: Pod "webserver-deployment-67bd4bf6dc-jp6hd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jp6hd webserver-deployment-67bd4bf6dc- deployment-2980  baea1cd5-59d6-463b-9aca-c1985eaa3a8d 43526 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc004e8a1d7 0xc004e8a1d8}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bggrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bggrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.212: INFO: Pod "webserver-deployment-67bd4bf6dc-kd26h" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kd26h webserver-deployment-67bd4bf6dc- deployment-2980  fd3ef083-9662-4c97-b194-b787874d2262 43342 0 2023-05-11 14:59:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:90f0af5fbd595e351199c9eb55815d2db9a6b3ec2c6e5ae16aeb3ee58dd10984 cni.projectcalico.org/podIP:192.168.150.162/32 cni.projectcalico.org/podIPs:192.168.150.162/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc004e8b140 0xc004e8b141}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:00 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-11 14:59:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.162\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7h2sf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7h2sf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.162,StartTime:2023-05-11 14:59:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:59:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a3a3fee358804471a72132094066994afd000b81f10cd75a7b1d10e30eab945e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.162,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.213: INFO: Pod "webserver-deployment-67bd4bf6dc-nfknx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nfknx webserver-deployment-67bd4bf6dc- deployment-2980  87d00dc4-95a8-48aa-a5fa-329b50969510 43524 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc004e8b807 0xc004e8b808}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vx2hs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vx2hs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.213: INFO: Pod "webserver-deployment-67bd4bf6dc-pcnc8" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pcnc8 webserver-deployment-67bd4bf6dc- deployment-2980  b96207ea-7280-4a40-9699-9c80e28d1d1c 43336 0 2023-05-11 14:59:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:5c8c1813ddbc4845d5f77572a21b48ed5952d0999ff16e1e05f6ff84063707b6 cni.projectcalico.org/podIP:192.168.151.222/32 cni.projectcalico.org/podIPs:192.168.151.222/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc004e8bcb7 0xc004e8bcb8}] [] [{calico Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.151.222\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4h4zg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4h4zg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.11,PodIP:192.168.151.222,StartTime:2023-05-11 14:59:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:59:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://824f14079be1d38076d010183dabffb6f7e07db850980732140c175a017e37dc,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.151.222,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.213: INFO: Pod "webserver-deployment-67bd4bf6dc-ptnh5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ptnh5 webserver-deployment-67bd4bf6dc- deployment-2980  a3f30078-e97f-4944-ac95-88a71b283c6c 43365 0 2023-05-11 14:59:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f88ae678a0df06b7c2a3b0f1bc5a21e93ca7b31b13616c9e579b47c7ac840343 cni.projectcalico.org/podIP:192.168.153.41/32 cni.projectcalico.org/podIPs:192.168.153.41/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc005078137 0xc005078138}] [] [{calico Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-gpbqx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-gpbqx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:192.168.153.41,StartTime:2023-05-11 14:59:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:59:02 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://e95e26c4c4cba4cf9c281a18fc0f64bf29b0053377a3641c69213353e12f51da,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.41,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.213: INFO: Pod "webserver-deployment-67bd4bf6dc-s5d8n" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s5d8n webserver-deployment-67bd4bf6dc- deployment-2980  116b02f3-8347-4af2-91e9-120285840615 43337 0 2023-05-11 14:59:01 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:1b88dcaaeb0327ab7f229caf34459e1a762d2e338c228d17124428f48508ca1c cni.projectcalico.org/podIP:192.168.151.218/32 cni.projectcalico.org/podIPs:192.168.151.218/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc005078367 0xc005078368}] [] [{calico Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:01 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:02 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.151.218\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5swjp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5swjp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:02 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:01 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.11,PodIP:192.168.151.218,StartTime:2023-05-11 14:59:01 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 14:59:01 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://481185ab6341b3d7a100f71d614f7d7c2054a7699973df5f8b6db3637147f7a0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.151.218,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.213: INFO: Pod "webserver-deployment-67bd4bf6dc-tmtwt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tmtwt webserver-deployment-67bd4bf6dc- deployment-2980  c63d8e4c-9aae-473e-ad8e-7ea1657a47d4 43532 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc005078567 0xc005078568}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fk5dz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fk5dz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.213: INFO: Pod "webserver-deployment-67bd4bf6dc-w46lw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w46lw webserver-deployment-67bd4bf6dc- deployment-2980  dfa75093-e042-49c5-b040-497ab32e4785 43528 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc0050786d0 0xc0050786d1}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-77msq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-77msq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.213: INFO: Pod "webserver-deployment-67bd4bf6dc-ww4s8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ww4s8 webserver-deployment-67bd4bf6dc- deployment-2980  b5d2049d-b32c-4b38-b471-31c1a27726d9 43492 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc005078830 0xc005078831}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6lkv9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6lkv9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.214: INFO: Pod "webserver-deployment-67bd4bf6dc-xp7m8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xp7m8 webserver-deployment-67bd4bf6dc- deployment-2980  bc208e57-1932-4591-a0ad-db3aa2a71cfe 43500 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc005078990 0xc005078991}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bbt5g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bbt5g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:,StartTime:2023-05-11 14:59:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.214: INFO: Pod "webserver-deployment-67bd4bf6dc-xrh4m" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xrh4m webserver-deployment-67bd4bf6dc- deployment-2980  6e8771ab-bf3d-4d7e-9664-6b7dcd291a5d 43521 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc bf821adc-9d2c-49bf-b198-05f56246e1ca 0xc005078b57 0xc005078b58}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"bf821adc-9d2c-49bf-b198-05f56246e1ca\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jgb7b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jgb7b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:,StartTime:2023-05-11 14:59:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.214: INFO: Pod "webserver-deployment-7b75d79cf5-5n885" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-5n885 webserver-deployment-7b75d79cf5- deployment-2980  9e58a988-051e-4a09-8a5a-d39aa08b7140 43459 0 2023-05-11 14:59:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:63ae8a539e12cca43772e9ec2cdec6c2d396837ddd5368d85d1ded73072b230b cni.projectcalico.org/podIP:192.168.151.209/32 cni.projectcalico.org/podIPs:192.168.151.209/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005078d27 0xc005078d28}] [] [{calico Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.151.209\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zc5r5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zc5r5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.11,PodIP:192.168.151.209,StartTime:2023-05-11 14:59:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.151.209,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.214: INFO: Pod "webserver-deployment-7b75d79cf5-6czqt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-6czqt webserver-deployment-7b75d79cf5- deployment-2980  bbd136da-0e0c-4c06-bf51-d931ef65be5d 43519 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005078f57 0xc005078f58}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4gczj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4gczj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:,StartTime:2023-05-11 14:59:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.214: INFO: Pod "webserver-deployment-7b75d79cf5-7sk2d" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7sk2d webserver-deployment-7b75d79cf5- deployment-2980  4b8169e9-094f-4db1-9cfd-1f6cd561afd2 43529 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005079147 0xc005079148}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mq7tq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mq7tq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.214: INFO: Pod "webserver-deployment-7b75d79cf5-c99dd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-c99dd webserver-deployment-7b75d79cf5- deployment-2980  e0364f4c-98bc-4b83-92c6-3aadcd0d391d 43509 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc0050792d0 0xc0050792d1}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-67jfw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-67jfw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.214: INFO: Pod "webserver-deployment-7b75d79cf5-chf7h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-chf7h webserver-deployment-7b75d79cf5- deployment-2980  48915fb2-5440-43ab-8da7-efdb1bdd362f 43462 0 2023-05-11 14:59:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:9cc708f79f9b3c93f38a7924128046436debb5e86233b86909691e924937f9dd cni.projectcalico.org/podIP:192.168.150.178/32 cni.projectcalico.org/podIPs:192.168.150.178/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005079440 0xc005079441}] [] [{calico Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.178\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-s59bw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-s59bw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.178,StartTime:2023-05-11 14:59:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.178,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.215: INFO: Pod "webserver-deployment-7b75d79cf5-ddcp8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ddcp8 webserver-deployment-7b75d79cf5- deployment-2980  e0313ba2-fa3c-456d-bd1b-db76142ea4ae 43476 0 2023-05-11 14:59:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:187ea9ad6072fc98dc69cba05a3b5990467473707dc03e6b2b63b50eca00d593 cni.projectcalico.org/podIP:192.168.153.43/32 cni.projectcalico.org/podIPs:192.168.153.43/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc0050799b7 0xc0050799b8}] [] [{calico Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.43\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-plvd2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-plvd2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:192.168.153.43,StartTime:2023-05-11 14:59:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.43,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.215: INFO: Pod "webserver-deployment-7b75d79cf5-klfdf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-klfdf webserver-deployment-7b75d79cf5- deployment-2980  7266940a-de8a-4e8d-9a00-8fe32f3d1bc8 43501 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005079c47 0xc005079c48}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kt8xk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kt8xk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.11,PodIP:,StartTime:2023-05-11 14:59:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.215: INFO: Pod "webserver-deployment-7b75d79cf5-lqg5g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lqg5g webserver-deployment-7b75d79cf5- deployment-2980  269545ff-60e9-45e5-8080-efbb9951f5f6 43478 0 2023-05-11 14:59:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:66521c88b113a116f3008a7ff68863c099497721703873d613bfaadd884ced1b cni.projectcalico.org/podIP:192.168.153.35/32 cni.projectcalico.org/podIPs:192.168.153.35/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005079e37 0xc005079e38}] [] [{calico Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.153.35\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mdzf7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mdzf7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:192.168.153.35,StartTime:2023-05-11 14:59:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.153.35,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.215: INFO: Pod "webserver-deployment-7b75d79cf5-m6bvj" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-m6bvj webserver-deployment-7b75d79cf5- deployment-2980  bb658f49-0108-4e5c-8cdd-b5c60be7592e 43516 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc0052020e7 0xc0052020e8}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-l4rb2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-l4rb2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.215: INFO: Pod "webserver-deployment-7b75d79cf5-p4dp4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-p4dp4 webserver-deployment-7b75d79cf5- deployment-2980  3f488bd2-520a-42ec-b00e-eb47d56523c7 43541 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005202417 0xc005202418}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-69kfh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-69kfh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.215: INFO: Pod "webserver-deployment-7b75d79cf5-pt5mx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-pt5mx webserver-deployment-7b75d79cf5- deployment-2980  4fb1ca4e-e127-41b3-afbb-ccc2fbe6a1c4 43531 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005202737 0xc005202738}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rzw6w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rzw6w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.215: INFO: Pod "webserver-deployment-7b75d79cf5-tl7dx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tl7dx webserver-deployment-7b75d79cf5- deployment-2980  b77e2ab7-1596-4c0b-bdf3-318a837cc9dd 43535 0 2023-05-11 14:59:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005202b00 0xc005202b01}] [] [{kube-controller-manager Update v1 2023-05-11 14:59:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m9mnf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m9mnf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.216: INFO: Pod "webserver-deployment-7b75d79cf5-w5hlb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-w5hlb webserver-deployment-7b75d79cf5- deployment-2980  17a49292-9dd5-4f2a-85fa-ae94170339c5 43465 0 2023-05-11 14:59:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:16c929a11a99816eead4df6841b66651349dcce4c23c7adc53ca11a3197f9eb5 cni.projectcalico.org/podIP:192.168.150.174/32 cni.projectcalico.org/podIPs:192.168.150.174/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 af15aae2-3aae-4c22-b4bd-66940d49068c 0xc005202ca0 0xc005202ca1}] [] [{calico Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-11 14:59:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"af15aae2-3aae-4c22-b4bd-66940d49068c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 14:59:04 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.174\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tktmw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tktmw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 14:59:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.174,StartTime:2023-05-11 14:59:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.174,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 14:59:05.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2980" for this suite. @ 05/11/23 14:59:05.261
• [4.351 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/11/23 14:59:05.305
  May 11 14:59:05.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename daemonsets @ 05/11/23 14:59:05.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:59:05.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:59:05.419
  STEP: Creating simple DaemonSet "daemon-set" @ 05/11/23 14:59:05.45
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/11/23 14:59:05.455
  May 11 14:59:05.461: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:05.461: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:05.461: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:05.469: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:59:05.469: INFO: Node macpro-1 is running 0 daemon pod, expected 1
  May 11 14:59:06.476: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:06.476: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:06.476: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:06.480: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 11 14:59:06.480: INFO: Node macpro-3 is running 0 daemon pod, expected 1
  May 11 14:59:07.474: INFO: DaemonSet pods can't tolerate node master-1 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:07.474: INFO: DaemonSet pods can't tolerate node master-2 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:07.474: INFO: DaemonSet pods can't tolerate node master-3 with taints [{Key:node-role.kubernetes.io/control-plane Value: Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 11 14:59:07.477: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  May 11 14:59:07.477: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 05/11/23 14:59:07.482
  May 11 14:59:07.485: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/11/23 14:59:07.485
  May 11 14:59:07.493: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/11/23 14:59:07.493
  May 11 14:59:07.495: INFO: Observed &DaemonSet event: ADDED
  May 11 14:59:07.495: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.495: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.495: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.495: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.495: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.495: INFO: Found daemon set daemon-set in namespace daemonsets-9534 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 11 14:59:07.495: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/11/23 14:59:07.495
  STEP: watching for the daemon set status to be patched @ 05/11/23 14:59:07.504
  May 11 14:59:07.506: INFO: Observed &DaemonSet event: ADDED
  May 11 14:59:07.506: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.506: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.506: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.506: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.507: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.507: INFO: Observed daemon set daemon-set in namespace daemonsets-9534 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 11 14:59:07.507: INFO: Observed &DaemonSet event: MODIFIED
  May 11 14:59:07.507: INFO: Found daemon set daemon-set in namespace daemonsets-9534 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May 11 14:59:07.507: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/11/23 14:59:07.511
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-9534, will wait for the garbage collector to delete the pods @ 05/11/23 14:59:07.511
  May 11 14:59:07.572: INFO: Deleting DaemonSet.extensions daemon-set took: 8.180699ms
  May 11 14:59:07.673: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.873701ms
  May 11 14:59:10.033: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 11 14:59:10.033: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 11 14:59:10.035: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"43706"},"items":null}

  May 11 14:59:10.037: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"43706"},"items":null}

  May 11 14:59:10.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-9534" for this suite. @ 05/11/23 14:59:10.051
• [4.752 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/11/23 14:59:10.058
  May 11 14:59:10.058: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename proxy @ 05/11/23 14:59:10.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:59:10.09
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:59:10.093
  May 11 14:59:10.095: INFO: Creating pod...
  May 11 14:59:12.212: INFO: Creating service...
  May 11 14:59:12.243: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/pods/agnhost/proxy?method=DELETE
  May 11 14:59:12.257: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 11 14:59:12.257: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/pods/agnhost/proxy?method=OPTIONS
  May 11 14:59:12.398: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 11 14:59:12.398: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/pods/agnhost/proxy?method=PATCH
  May 11 14:59:12.421: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 11 14:59:12.421: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/pods/agnhost/proxy?method=POST
  May 11 14:59:12.459: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 11 14:59:12.459: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/pods/agnhost/proxy?method=PUT
  May 11 14:59:12.534: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 11 14:59:12.534: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/services/e2e-proxy-test-service/proxy?method=DELETE
  May 11 14:59:12.548: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 11 14:59:12.548: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May 11 14:59:12.567: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 11 14:59:12.567: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/services/e2e-proxy-test-service/proxy?method=PATCH
  May 11 14:59:12.584: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 11 14:59:12.584: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/services/e2e-proxy-test-service/proxy?method=POST
  May 11 14:59:12.603: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 11 14:59:12.603: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/services/e2e-proxy-test-service/proxy?method=PUT
  May 11 14:59:12.776: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 11 14:59:12.776: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/pods/agnhost/proxy?method=GET
  May 11 14:59:12.786: INFO: http.Client request:GET StatusCode:301
  May 11 14:59:12.786: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/services/e2e-proxy-test-service/proxy?method=GET
  May 11 14:59:12.806: INFO: http.Client request:GET StatusCode:301
  May 11 14:59:12.806: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/pods/agnhost/proxy?method=HEAD
  May 11 14:59:12.816: INFO: http.Client request:HEAD StatusCode:301
  May 11 14:59:12.816: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-56/services/e2e-proxy-test-service/proxy?method=HEAD
  May 11 14:59:12.836: INFO: http.Client request:HEAD StatusCode:301
  May 11 14:59:12.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-56" for this suite. @ 05/11/23 14:59:12.847
• [2.983 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/11/23 14:59:13.042
  May 11 14:59:13.042: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename replication-controller @ 05/11/23 14:59:13.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:59:13.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:59:13.089
  STEP: creating a ReplicationController @ 05/11/23 14:59:13.269
  STEP: waiting for RC to be added @ 05/11/23 14:59:13.285
  STEP: waiting for available Replicas @ 05/11/23 14:59:13.285
  STEP: patching ReplicationController @ 05/11/23 14:59:14.839
  STEP: waiting for RC to be modified @ 05/11/23 14:59:14.849
  STEP: patching ReplicationController status @ 05/11/23 14:59:14.849
  STEP: waiting for RC to be modified @ 05/11/23 14:59:14.856
  STEP: waiting for available Replicas @ 05/11/23 14:59:14.856
  STEP: fetching ReplicationController status @ 05/11/23 14:59:14.863
  STEP: patching ReplicationController scale @ 05/11/23 14:59:14.866
  STEP: waiting for RC to be modified @ 05/11/23 14:59:14.87
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/11/23 14:59:14.87
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/11/23 14:59:16.394
  STEP: updating ReplicationController status @ 05/11/23 14:59:16.397
  STEP: waiting for RC to be modified @ 05/11/23 14:59:16.402
  STEP: listing all ReplicationControllers @ 05/11/23 14:59:16.402
  STEP: checking that ReplicationController has expected values @ 05/11/23 14:59:16.408
  STEP: deleting ReplicationControllers by collection @ 05/11/23 14:59:16.408
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/11/23 14:59:16.414
  May 11 14:59:16.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0511 14:59:16.448830      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-4588" for this suite. @ 05/11/23 14:59:16.452
• [3.416 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/11/23 14:59:16.459
  May 11 14:59:16.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename init-container @ 05/11/23 14:59:16.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:59:16.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:59:16.483
  STEP: creating the pod @ 05/11/23 14:59:16.485
  May 11 14:59:16.485: INFO: PodSpec: initContainers in spec.initContainers
  E0511 14:59:17.449578      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:18.449669      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:19.450054      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:20.450151      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:21.450360      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:22.451282      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:23.452335      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:24.452813      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:25.453137      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:26.453285      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:27.453385      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:28.453532      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:29.453760      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:30.453916      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:31.454131      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:32.454755      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:33.454934      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:34.455523      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:35.455777      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:36.455976      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:37.456172      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:38.456935      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:39.457973      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:40.458179      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:41.458390      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:42.458537      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:43.458717      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:44.459365      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:45.460028      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:46.460188      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:47.460335      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:48.460454      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:49.460882      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:50.461085      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:51.461275      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:52.461448      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:53.461629      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:54.462320      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:55.462498      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:56.462651      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:57.462794      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:58.462932      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 14:59:59.463257      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 14:59:59.464: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-f810fda4-9516-41a6-b760-649f9ce2692f", GenerateName:"", Namespace:"init-container-4398", SelfLink:"", UID:"3f78a589-6fbc-46c5-807a-5a707b31f2d2", ResourceVersion:"44294", Generation:0, CreationTimestamp:time.Date(2023, time.May, 11, 14, 59, 16, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"485344120"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"f782779ee5169939ec33e43258277e10b0dbb6898d610ef85b55e9a46ac93afa", "cni.projectcalico.org/podIP":"192.168.150.173/32", "cni.projectcalico.org/podIPs":"192.168.150.173/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 11, 14, 59, 16, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00199ac60), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 11, 14, 59, 16, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00199acd8), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 11, 14, 59, 59, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc00199ad20), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-hxpmz", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0045df100), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hxpmz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hxpmz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-hxpmz", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004af14e8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"macpro-2", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0009e17a0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004af1570)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004af1590)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004af1598), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004af159c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00978a2b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 11, 14, 59, 16, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 11, 14, 59, 16, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 11, 14, 59, 16, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 11, 14, 59, 16, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"10.221.188.12", PodIP:"192.168.150.173", PodIPs:[]v1.PodIP{v1.PodIP{IP:"192.168.150.173"}}, StartTime:time.Date(2023, time.May, 11, 14, 59, 16, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009e1880)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0009e1960)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://879d3c0f4735efc9aa1b27f44b367fa7cc1a2dec843a6cff3185db299166e93e", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0045df180), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc0045df160), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004af1614), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May 11 14:59:59.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-4398" for this suite. @ 05/11/23 14:59:59.469
• [43.015 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/11/23 14:59:59.475
  May 11 14:59:59.475: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename gc @ 05/11/23 14:59:59.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 14:59:59.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 14:59:59.494
  May 11 14:59:59.530: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"f9f4e554-a463-4710-853a-d689fd4ff102", Controller:(*bool)(0xc004e3a6fa), BlockOwnerDeletion:(*bool)(0xc004e3a6fb)}}
  May 11 14:59:59.547: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"c7be6565-7a24-410d-9ca4-0b0e56e4ba89", Controller:(*bool)(0xc004c28846), BlockOwnerDeletion:(*bool)(0xc004c28847)}}
  May 11 14:59:59.558: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"162cd5d6-4724-4f93-8407-184bcb5e650b", Controller:(*bool)(0xc004c28ace), BlockOwnerDeletion:(*bool)(0xc004c28acf)}}
  E0511 15:00:00.463421      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:01.463690      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:02.463836      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:03.464094      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:04.464575      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:00:04.571: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3117" for this suite. @ 05/11/23 15:00:04.576
• [5.112 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/11/23 15:00:04.588
  May 11 15:00:04.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename sched-preemption @ 05/11/23 15:00:04.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:00:04.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:00:04.639
  May 11 15:00:04.667: INFO: Waiting up to 1m0s for all nodes to be ready
  E0511 15:00:05.465286      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:06.465576      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:07.466047      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:08.466223      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:09.466784      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:10.466965      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:11.467605      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:12.467770      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:13.468787      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:14.469414      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:15.470274      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:16.470418      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:17.471148      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:18.471732      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:19.472678      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:20.472825      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:21.473026      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:22.473178      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:23.473443      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:24.474129      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:25.474337      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:26.474505      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:27.475529      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:28.475752      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:29.476650      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:30.476813      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:31.477528      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:32.477657      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:33.478553      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:34.479251      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:35.479410      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:36.479600      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:37.479685      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:38.479886      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:39.480397      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:40.480590      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:41.480747      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:42.481000      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:43.481365      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:44.482078      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:45.482692      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:46.482834      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:47.483278      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:48.483400      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:49.483570      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:50.483784      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:51.484005      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:52.484179      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:53.484772      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:54.485373      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:55.485677      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:56.485799      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:57.486155      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:58.486293      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:00:59.486487      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:00.486696      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:01.487152      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:02.487355      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:03.488320      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:04.488813      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:01:04.701: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/11/23 15:01:04.704
  May 11 15:01:04.720: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 11 15:01:04.726: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 11 15:01:04.742: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 11 15:01:04.749: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  May 11 15:01:04.775: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  May 11 15:01:04.785: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/11/23 15:01:04.785
  E0511 15:01:05.489093      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:06.489508      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/11/23 15:01:06.807
  E0511 15:01:07.490442      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:08.490641      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:09.491401      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:10.491611      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:11.492571      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:12.492646      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:01:12.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1482" for this suite. @ 05/11/23 15:01:12.927
• [68.348 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/11/23 15:01:12.937
  May 11 15:01:12.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename webhook @ 05/11/23 15:01:12.937
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:01:12.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:01:12.956
  STEP: Setting up server cert @ 05/11/23 15:01:12.98
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/11/23 15:01:13.359
  STEP: Deploying the webhook pod @ 05/11/23 15:01:13.364
  STEP: Wait for the deployment to be ready @ 05/11/23 15:01:13.376
  May 11 15:01:13.388: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0511 15:01:13.493100      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:14.493634      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/11/23 15:01:15.398
  STEP: Verifying the service has paired with the endpoint @ 05/11/23 15:01:15.41
  E0511 15:01:15.494262      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:01:16.410: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/11/23 15:01:16.413
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/11/23 15:01:16.427
  STEP: Creating a dummy validating-webhook-configuration object @ 05/11/23 15:01:16.439
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/11/23 15:01:16.447
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/11/23 15:01:16.452
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/11/23 15:01:16.459
  May 11 15:01:16.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0511 15:01:16.494447      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-19" for this suite. @ 05/11/23 15:01:16.545
  STEP: Destroying namespace "webhook-markers-5906" for this suite. @ 05/11/23 15:01:16.55
• [3.619 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/11/23 15:01:16.556
  May 11 15:01:16.556: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename deployment @ 05/11/23 15:01:16.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:01:16.574
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:01:16.576
  May 11 15:01:16.578: INFO: Creating simple deployment test-new-deployment
  May 11 15:01:16.593: INFO: deployment "test-new-deployment" doesn't have the required revision set
  E0511 15:01:17.495441      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:18.495645      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 05/11/23 15:01:18.61
  STEP: updating a scale subresource @ 05/11/23 15:01:18.613
  STEP: verifying the deployment Spec.Replicas was modified @ 05/11/23 15:01:18.619
  STEP: Patch a scale subresource @ 05/11/23 15:01:18.623
  May 11 15:01:18.654: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-6310  5642c8fd-1b29-48d4-9a52-e32722651ba5 44882 3 2023-05-11 15:01:16 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-11 15:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 15:01:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d87758 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-11 15:01:17 +0000 UTC,LastTransitionTime:2023-05-11 15:01:17 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-11 15:01:17 +0000 UTC,LastTransitionTime:2023-05-11 15:01:16 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 11 15:01:18.666: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-6310  aa022b13-b336-47a8-8a1e-5c3b85056476 44886 3 2023-05-11 15:01:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 5642c8fd-1b29-48d4-9a52-e32722651ba5 0xc005d87b97 0xc005d87b98}] [] [{kube-controller-manager Update apps/v1 2023-05-11 15:01:17 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-05-11 15:01:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5642c8fd-1b29-48d4-9a52-e32722651ba5\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005d87c28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 11 15:01:18.682: INFO: Pod "test-new-deployment-67bd4bf6dc-bj5qp" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-bj5qp test-new-deployment-67bd4bf6dc- deployment-6310  b79f716b-480d-455b-a6a7-c94eb1d0a0f3 44889 0 2023-05-11 15:01:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc aa022b13-b336-47a8-8a1e-5c3b85056476 0xc005fb6017 0xc005fb6018}] [] [{kube-controller-manager Update v1 2023-05-11 15:01:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa022b13-b336-47a8-8a1e-5c3b85056476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-11 15:01:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9l6r7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9l6r7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:01:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:01:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:01:18 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:01:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.13,PodIP:,StartTime:2023-05-11 15:01:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 15:01:18.682: INFO: Pod "test-new-deployment-67bd4bf6dc-lm8th" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-lm8th test-new-deployment-67bd4bf6dc- deployment-6310  f73b025b-2352-4ef4-b416-f162e991feb4 44807 0 2023-05-11 15:01:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:8495979456f027105b7251c7f04c074642263c60b08a6c65ca4048426b9a0263 cni.projectcalico.org/podIP:192.168.150.188/32 cni.projectcalico.org/podIPs:192.168.150.188/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc aa022b13-b336-47a8-8a1e-5c3b85056476 0xc005fb61e7 0xc005fb61e8}] [] [{kube-controller-manager Update v1 2023-05-11 15:01:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aa022b13-b336-47a8-8a1e-5c3b85056476\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-11 15:01:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-11 15:01:17 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.188\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8kkc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8kkc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:01:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:01:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:01:17 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:01:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.188,StartTime:2023-05-11 15:01:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 15:01:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://8cb82b4cfb111181f9ce4a6525c8eb6f1ec43b10060a9fe4453e5119f6658f9c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.188,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 15:01:18.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6310" for this suite. @ 05/11/23 15:01:18.703
• [2.162 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/11/23 15:01:18.719
  May 11 15:01:18.719: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename containers @ 05/11/23 15:01:18.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:01:18.741
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:01:18.744
  STEP: Creating a pod to test override all @ 05/11/23 15:01:18.75
  E0511 15:01:19.496653      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:20.496813      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:21.497780      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:22.497963      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/11/23 15:01:22.788
  May 11 15:01:22.791: INFO: Trying to get logs from node macpro-2 pod client-containers-cbc7a796-5165-4e59-a5a3-07dc604e1bac container agnhost-container: <nil>
  STEP: delete the pod @ 05/11/23 15:01:22.804
  May 11 15:01:22.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5299" for this suite. @ 05/11/23 15:01:22.82
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/11/23 15:01:22.827
  May 11 15:01:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename services @ 05/11/23 15:01:22.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:01:22.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:01:22.844
  STEP: creating service in namespace services-517 @ 05/11/23 15:01:22.846
  STEP: creating service affinity-nodeport-transition in namespace services-517 @ 05/11/23 15:01:22.846
  STEP: creating replication controller affinity-nodeport-transition in namespace services-517 @ 05/11/23 15:01:22.862
  I0511 15:01:22.873906      24 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-517, replica count: 3
  E0511 15:01:23.498553      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:24.499628      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:25.499967      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0511 15:01:25.924553      24 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 11 15:01:25.935: INFO: Creating new exec pod
  E0511 15:01:26.500795      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:27.501516      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:28.501790      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:01:28.953: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-517 exec execpod-affinitytjh7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  May 11 15:01:29.085: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May 11 15:01:29.085: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 15:01:29.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-517 exec execpod-affinitytjh7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.108.147.140 80'
  May 11 15:01:29.198: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.108.147.140 80\nConnection to 10.108.147.140 80 port [tcp/http] succeeded!\n"
  May 11 15:01:29.198: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 15:01:29.198: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-517 exec execpod-affinitytjh7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.13 31841'
  May 11 15:01:29.311: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.13 31841\nConnection to 10.221.188.13 31841 port [tcp/*] succeeded!\n"
  May 11 15:01:29.311: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 15:01:29.311: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-517 exec execpod-affinitytjh7m -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.221.188.11 31841'
  May 11 15:01:29.427: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.221.188.11 31841\nConnection to 10.221.188.11 31841 port [tcp/*] succeeded!\n"
  May 11 15:01:29.427: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 11 15:01:29.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-517 exec execpod-affinitytjh7m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.221.188.11:31841/ ; done'
  E0511 15:01:29.502482      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:01:29.601: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n"
  May 11 15:01:29.601: INFO: stdout: "\naffinity-nodeport-transition-q2229\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-nx6q8\naffinity-nodeport-transition-q2229\naffinity-nodeport-transition-q2229\naffinity-nodeport-transition-nx6q8\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-q2229\naffinity-nodeport-transition-q2229\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-q2229\naffinity-nodeport-transition-nx6q8\naffinity-nodeport-transition-q2229\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc"
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-q2229
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-nx6q8
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-q2229
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-q2229
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-nx6q8
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-q2229
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-q2229
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-q2229
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-nx6q8
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-q2229
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.601: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.610: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2982172659 --namespace=services-517 exec execpod-affinitytjh7m -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.221.188.11:31841/ ; done'
  May 11 15:01:29.777: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.221.188.11:31841/\n"
  May 11 15:01:29.777: INFO: stdout: "\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc\naffinity-nodeport-transition-9bhdc"
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Received response from host: affinity-nodeport-transition-9bhdc
  May 11 15:01:29.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 11 15:01:29.782: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-517, will wait for the garbage collector to delete the pods @ 05/11/23 15:01:29.792
  May 11 15:01:29.857: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.181746ms
  May 11 15:01:29.958: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.957124ms
  E0511 15:01:30.503558      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:31.504053      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-517" for this suite. @ 05/11/23 15:01:32.081
• [9.259 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/11/23 15:01:32.087
  May 11 15:01:32.087: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename resourcequota @ 05/11/23 15:01:32.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:01:32.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:01:32.105
  STEP: Creating resourceQuota "e2e-rq-status-w9n9m" @ 05/11/23 15:01:32.111
  May 11 15:01:32.119: INFO: Resource quota "e2e-rq-status-w9n9m" reports spec: hard cpu limit of 500m
  May 11 15:01:32.119: INFO: Resource quota "e2e-rq-status-w9n9m" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-w9n9m" /status @ 05/11/23 15:01:32.119
  STEP: Confirm /status for "e2e-rq-status-w9n9m" resourceQuota via watch @ 05/11/23 15:01:32.126
  May 11 15:01:32.127: INFO: observed resourceQuota "e2e-rq-status-w9n9m" in namespace "resourcequota-3666" with hard status: v1.ResourceList(nil)
  May 11 15:01:32.127: INFO: Found resourceQuota "e2e-rq-status-w9n9m" in namespace "resourcequota-3666" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 11 15:01:32.127: INFO: ResourceQuota "e2e-rq-status-w9n9m" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/11/23 15:01:32.129
  May 11 15:01:32.135: INFO: Resource quota "e2e-rq-status-w9n9m" reports spec: hard cpu limit of 1
  May 11 15:01:32.135: INFO: Resource quota "e2e-rq-status-w9n9m" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-w9n9m" /status @ 05/11/23 15:01:32.135
  STEP: Confirm /status for "e2e-rq-status-w9n9m" resourceQuota via watch @ 05/11/23 15:01:32.143
  May 11 15:01:32.144: INFO: observed resourceQuota "e2e-rq-status-w9n9m" in namespace "resourcequota-3666" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 11 15:01:32.144: INFO: Found resourceQuota "e2e-rq-status-w9n9m" in namespace "resourcequota-3666" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May 11 15:01:32.144: INFO: ResourceQuota "e2e-rq-status-w9n9m" /status was patched
  STEP: Get "e2e-rq-status-w9n9m" /status @ 05/11/23 15:01:32.144
  May 11 15:01:32.150: INFO: Resourcequota "e2e-rq-status-w9n9m" reports status: hard cpu of 1
  May 11 15:01:32.150: INFO: Resourcequota "e2e-rq-status-w9n9m" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-w9n9m" /status before checking Spec is unchanged @ 05/11/23 15:01:32.153
  May 11 15:01:32.157: INFO: Resourcequota "e2e-rq-status-w9n9m" reports status: hard cpu of 2
  May 11 15:01:32.157: INFO: Resourcequota "e2e-rq-status-w9n9m" reports status: hard memory of 2Gi
  May 11 15:01:32.159: INFO: Found resourceQuota "e2e-rq-status-w9n9m" in namespace "resourcequota-3666" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  E0511 15:01:32.505046      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:33.505355      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:34.506200      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:35.506556      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:36.506730      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:37.506797      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:38.506927      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:39.507172      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:40.507365      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:41.507551      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:42.507557      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:43.507744      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:44.508474      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:45.508702      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:46.508855      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:47.509803      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:48.510883      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:49.511189      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:50.511352      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:51.511574      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:52.512359      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:53.512548      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:54.513250      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:55.513455      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:56.513611      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:57.514484      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:58.514596      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:01:59.514839      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:00.514950      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:01.515278      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:02.515579      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:03.515847      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:04.516498      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:05.516810      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:06.516958      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:07.517572      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:08.517689      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:09.517947      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:10.518090      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:11.518315      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:12.518495      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:13.518739      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:14.519366      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:15.519570      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:16.519754      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:17.520060      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:18.520223      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:19.520659      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:20.520801      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:21.520923      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:22.521859      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:23.522061      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:24.522678      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:25.522876      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:26.523026      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:27.523242      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:28.524222      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:29.524646      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:30.524774      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:31.524950      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:32.525894      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:33.526069      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:34.526985      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:35.527190      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:36.527348      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:37.527603      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:38.527839      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:39.528025      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:40.528189      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:41.528408      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:42.528464      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:43.528652      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:44.529302      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:45.529570      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:46.529733      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:47.530244      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:48.530386      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:49.530626      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:50.530781      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:51.530922      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:52.531033      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:53.531273      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:54.531709      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:55.531905      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:56.532054      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:57.533069      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:58.533257      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:02:59.533721      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:00.533866      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:01.534047      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:02.534079      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:03.534241      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:04.534750      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:05.534947      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:06.535108      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:07.535748      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:08.535927      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:09.536201      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:10.536372      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:11.536543      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:12.537585      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:13.537820      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:14.538322      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:15.538510      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:16.539304      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:17.540280      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:18.540798      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:19.541011      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:20.541219      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:21.541384      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:22.542274      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:23.542458      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:24.543150      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:25.543334      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:26.543479      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:27.543807      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:28.543913      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:29.544459      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:30.544578      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:31.544719      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:32.545739      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:33.545971      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:34.546617      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:35.546809      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:36.547012      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:37.547167      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:38.547268      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:39.547489      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:40.547620      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:41.547798      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:42.548541      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:43.548721      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:44.549383      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:45.549553      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:46.549695      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:47.549969      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:48.550192      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:49.550508      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:50.550728      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:51.550930      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:52.551734      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:53.551923      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:54.552420      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:55.552601      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:56.552742      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:57.553593      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:58.553712      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:03:59.554688      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:00.554823      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:01.554961      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:02.555294      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:03.555468      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:04.556046      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:05.556238      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:06.556407      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:07.556638      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:08.556783      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:09.557027      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:10.557261      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:11.557468      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:12.557446      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:13.557645      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:14.558440      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:15.558640      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:16.558831      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:17.559379      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:18.559526      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:19.559604      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:20.559757      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:21.559909      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:22.560001      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:23.560262      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:24.560764      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:25.560988      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:26.561140      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:27.561176      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:28.561285      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:29.561503      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:30.561608      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:31.561806      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:32.562343      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:33.562543      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:34.563133      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:35.563387      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:36.563539      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:37.564504      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:38.564616      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:39.564845      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:40.564969      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:41.565229      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:42.565778      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:43.565981      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:44.566505      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:45.566691      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:46.566839      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:47.567176      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:48.567292      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:49.567641      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:50.567783      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:51.567943      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:52.568622      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:53.568811      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:54.569509      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:55.569707      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:56.569899      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:57.570575      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:58.570717      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:04:59.570925      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:00.571146      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:01.571335      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:02.571920      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:03.572105      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:04.572545      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:05.572728      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:06.572879      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:07.573722      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:08.574239      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:09.574677      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:10.574833      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:11.575066      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:12.575653      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:13.575846      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:14.576440      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:15.576682      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:16.576870      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:17.577504      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:18.577617      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:19.577801      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:20.577928      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:21.578092      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:22.578742      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:23.578928      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:24.579477      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:25.579684      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:26.579799      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:27.580330      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:28.580473      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:29.580703      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:30.580834      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:31.581013      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:32.581421      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:33.582176      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:34.582704      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:35.582894      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:36.583053      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:37.583145      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:38.583256      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:39.584249      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:40.585263      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:41.585408      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:42.585731      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:43.585926      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:44.586514      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:45.586693      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:46.586834      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:47.587168      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:48.587294      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:49.587497      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:50.587634      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:51.587792      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:52.587867      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:53.588061      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:54.588628      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:55.588789      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:56.588950      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:57.589126      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:58.589243      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:05:59.589583      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:00.589702      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:01.589910      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:02.590347      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:03.590568      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:04.591267      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:05.591607      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:06.591746      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:07.592064      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:08.592204      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:09.592438      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:10.592572      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:11.592742      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:12.593373      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:13.593839      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:14.594534      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:15.594759      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:16.595232      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:17.596139      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:18.596236      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:19.596613      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:20.596749      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:21.596897      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:22.597027      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:23.597240      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:24.597698      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:25.597887      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:26.598202      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:27.599257      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:28.599382      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:29.599591      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:30.599733      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:31.599873      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:32.165: INFO: ResourceQuota "e2e-rq-status-w9n9m" Spec was unchanged and /status reset
  May 11 15:06:32.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3666" for this suite. @ 05/11/23 15:06:32.169
• [300.087 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/11/23 15:06:32.174
  May 11 15:06:32.174: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename downward-api @ 05/11/23 15:06:32.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:06:32.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:06:32.201
  STEP: Creating the pod @ 05/11/23 15:06:32.203
  E0511 15:06:32.600519      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:33.600775      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:34.601705      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:34.752: INFO: Successfully updated pod "labelsupdateb7414211-3f7d-4fe9-b5d6-75477612d2be"
  E0511 15:06:35.602659      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:36.602812      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:37.603717      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:38.603851      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:38.780: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2295" for this suite. @ 05/11/23 15:06:38.788
• [6.619 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/11/23 15:06:38.794
  May 11 15:06:38.794: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename deployment @ 05/11/23 15:06:38.795
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:06:38.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:06:38.812
  May 11 15:06:38.822: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0511 15:06:39.604766      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:40.605029      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:41.605269      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:42.605414      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:43.605608      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:43.827: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/11/23 15:06:43.827
  May 11 15:06:43.827: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/11/23 15:06:43.837
  E0511 15:06:44.606269      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:45.607259      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:45.864: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-6863  25ab5386-c219-4b09-a052-6461f8b303cd 45972 1 2023-05-11 15:06:43 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-11 15:06:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 15:06:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005298748 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-11 15:06:43 +0000 UTC,LastTransitionTime:2023-05-11 15:06:43 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-68b75d69f8" has successfully progressed.,LastUpdateTime:2023-05-11 15:06:45 +0000 UTC,LastTransitionTime:2023-05-11 15:06:43 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 11 15:06:45.866: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-6863  1d14a527-6492-459f-84ae-4cc1692423d8 45961 1 2023-05-11 15:06:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 25ab5386-c219-4b09-a052-6461f8b303cd 0xc00506abd7 0xc00506abd8}] [] [{kube-controller-manager Update apps/v1 2023-05-11 15:06:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"25ab5386-c219-4b09-a052-6461f8b303cd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-11 15:06:45 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00506ac88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 11 15:06:45.869: INFO: Pod "test-cleanup-deployment-68b75d69f8-6qkt4" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-6qkt4 test-cleanup-deployment-68b75d69f8- deployment-6863  e8093141-b72f-4466-a9ee-621d5ed436a5 45960 0 2023-05-11 15:06:43 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[cni.projectcalico.org/containerID:5d86bee6c3af3b376d89ba3e7d3f5faf97b2485eccc2a062885be93b1a5902f9 cni.projectcalico.org/podIP:192.168.150.149/32 cni.projectcalico.org/podIPs:192.168.150.149/32] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 1d14a527-6492-459f-84ae-4cc1692423d8 0xc00506b047 0xc00506b048}] [] [{kube-controller-manager Update v1 2023-05-11 15:06:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1d14a527-6492-459f-84ae-4cc1692423d8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-11 15:06:44 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-11 15:06:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"192.168.150.149\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7pz6k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7pz6k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:macpro-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:06:43 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:06:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:06:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-11 15:06:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:10.221.188.12,PodIP:192.168.150.149,StartTime:2023-05-11 15:06:43 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-11 15:06:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://33b4821546e93a8386c92afd3ca2edec59474540d6185f1cf7dca9dda0d1d914,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:192.168.150.149,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 11 15:06:45.869: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6863" for this suite. @ 05/11/23 15:06:45.873
• [7.086 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/11/23 15:06:45.88
  May 11 15:06:45.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename aggregator @ 05/11/23 15:06:45.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:06:45.912
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:06:45.914
  May 11 15:06:45.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Registering the sample API server. @ 05/11/23 15:06:45.917
  May 11 15:06:46.438: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May 11 15:06:46.474: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0511 15:06:46.607390      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:47.607561      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:48.551: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:06:48.607761      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:49.607996      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:50.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:06:50.608692      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:51.608884      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:52.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:06:52.609659      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:53.609859      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:54.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:06:54.610647      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:55.610874      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:56.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:06:56.611326      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:57.611480      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:06:58.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:06:58.611833      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:06:59.612022      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:07:00.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:07:00.612221      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:01.612392      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:07:02.555: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:07:02.613010      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:03.613193      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:07:04.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:07:04.614135      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:05.614327      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:07:06.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:07:06.615279      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:07.615463      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:07:08.556: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 11, 15, 6, 46, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0511 15:07:08.616327      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:09.616637      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:10.617517      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 11 15:07:10.675: INFO: Waited 109.789719ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/11/23 15:07:10.722
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/11/23 15:07:10.725
  STEP: List APIServices @ 05/11/23 15:07:10.732
  May 11 15:07:10.739: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/11/23 15:07:10.739
  May 11 15:07:10.752: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/11/23 15:07:10.753
  May 11 15:07:10.765: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 11, 15, 7, 10, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/11/23 15:07:10.765
  May 11 15:07:10.769: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-11 15:07:10 +0000 UTC Passed all checks passed}
  May 11 15:07:10.769: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 11 15:07:10.769: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/11/23 15:07:10.769
  May 11 15:07:10.780: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-496914690" @ 05/11/23 15:07:10.78
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/11/23 15:07:10.797
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/11/23 15:07:10.802
  STEP: Patch APIService Status @ 05/11/23 15:07:10.805
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/11/23 15:07:10.812
  May 11 15:07:10.815: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-11 15:07:10 +0000 UTC Passed all checks passed}
  May 11 15:07:10.815: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 11 15:07:10.815: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May 11 15:07:10.815: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/11/23 15:07:10.815
  STEP: Confirm that the generated APIService has been deleted @ 05/11/23 15:07:10.823
  May 11 15:07:10.823: INFO: Requesting list of APIServices to confirm quantity
  May 11 15:07:10.827: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May 11 15:07:10.827: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May 11 15:07:10.941: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-2942" for this suite. @ 05/11/23 15:07:10.966
• [25.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/11/23 15:07:10.973
  May 11 15:07:10.973: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename pods @ 05/11/23 15:07:10.974
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:07:10.993
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:07:10.995
  STEP: Create a pod @ 05/11/23 15:07:10.997
  E0511 15:07:11.618112      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:12.618277      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/11/23 15:07:13.016
  May 11 15:07:13.025: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May 11 15:07:13.025: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9758" for this suite. @ 05/11/23 15:07:13.029
• [2.062 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/11/23 15:07:13.036
  May 11 15:07:13.036: INFO: >>> kubeConfig: /tmp/kubeconfig-2982172659
  STEP: Building a namespace api object, basename subpath @ 05/11/23 15:07:13.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/11/23 15:07:13.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/11/23 15:07:13.052
  STEP: Setting up data @ 05/11/23 15:07:13.054
  STEP: Creating pod pod-subpath-test-projected-b45h @ 05/11/23 15:07:13.062
  STEP: Creating a pod to test atomic-volume-subpath @ 05/11/23 15:07:13.062
  E0511 15:07:13.619195      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:14.619562      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:15.619893      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:16.620038      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:17.620786      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:18.621043      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:19.621373      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:20.621509      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:21.622192      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:22.622305      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:23.622537      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:24.623091      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:25.623410      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:26.623617      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:27.623668      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:28.623796      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:29.623921      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:30.624143      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:31.624352      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:32.624857      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:33.624529      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:34.625178      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:35.625325      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0511 15:07:36.625543      24 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/11/23 15:07:37.122
  May 11 15:07:37.125: INFO: Trying to get logs from node macpro-3 pod pod-subpath-test-projected-b45h container test-container-subpath-projected-b45h: <nil>
  STEP: delete the pod @ 05/11/23 15:07:37.145
  STEP: Deleting pod pod-subpath-test-projected-b45h @ 05/11/23 15:07:37.163
  May 11 15:07:37.163: INFO: Deleting pod "pod-subpath-test-projected-b45h" in namespace "subpath-9866"
  May 11 15:07:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9866" for this suite. @ 05/11/23 15:07:37.172
• [24.146 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May 11 15:07:37.183: INFO: Running AfterSuite actions on node 1
  May 11 15:07:37.183: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.045 seconds]
------------------------------

Ran 378 of 7207 Specs in 5962.767 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h39m23.079395471s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

