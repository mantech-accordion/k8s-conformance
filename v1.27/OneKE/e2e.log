  I0512 12:31:56.534999      20 e2e.go:117] Starting e2e run "9ddd5405-7c85-47bc-86b2-34c9975c020e" on Ginkgo node 1
  May 12 12:31:56.555: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1683894716 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  May 12 12:31:56.694: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 12:31:56.695: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  May 12 12:31:56.736: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  May 12 12:31:56.744: INFO: 4 / 4 pods ready in namespace 'kube-system' in daemonset 'rke2-canal' (0 seconds elapsed)
  May 12 12:31:56.744: INFO: e2e test version: v1.27.1
  May 12 12:31:56.744: INFO: kube-apiserver version: v1.27.1+rke2r1
  May 12 12:31:56.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 12:31:56.747: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 05/12/23 12:31:56.99
  May 12 12:31:56.990: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 12:31:56.991
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:31:57.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:31:57.007
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 12:31:57.008
  STEP: Saw pod success @ 05/12/23 12:32:01.024
  May 12 12:32:01.025: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-fb54995f-bfdf-409a-9065-9b26f3e19d67 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 12:32:01.034
  May 12 12:32:01.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5407" for this suite. @ 05/12/23 12:32:01.05
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 05/12/23 12:32:01.062
  May 12 12:32:01.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replicaset @ 05/12/23 12:32:01.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:32:01.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:32:01.079
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 05/12/23 12:32:01.081
  May 12 12:32:01.087: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 12 12:32:06.099: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/23 12:32:06.101
  STEP: getting scale subresource @ 05/12/23 12:32:06.101
  STEP: updating a scale subresource @ 05/12/23 12:32:06.105
  STEP: verifying the replicaset Spec.Replicas was modified @ 05/12/23 12:32:06.111
  STEP: Patch a scale subresource @ 05/12/23 12:32:06.126
  May 12 12:32:06.144: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-679" for this suite. @ 05/12/23 12:32:06.163
• [5.123 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 05/12/23 12:32:06.197
  May 12 12:32:06.197: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename statefulset @ 05/12/23 12:32:06.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:32:06.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:32:06.217
  STEP: Creating service test in namespace statefulset-1690 @ 05/12/23 12:32:06.218
  STEP: Creating statefulset ss in namespace statefulset-1690 @ 05/12/23 12:32:06.222
  May 12 12:32:06.237: INFO: Found 0 stateful pods, waiting for 1
  May 12 12:32:16.245: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 05/12/23 12:32:16.258
  STEP: updating a scale subresource @ 05/12/23 12:32:16.264
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/12/23 12:32:16.277
  STEP: Patch a scale subresource @ 05/12/23 12:32:16.286
  STEP: verifying the statefulset Spec.Replicas was modified @ 05/12/23 12:32:16.298
  May 12 12:32:16.299: INFO: Deleting all statefulset in ns statefulset-1690
  May 12 12:32:16.303: INFO: Scaling statefulset ss to 0
  May 12 12:32:26.316: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 12:32:26.318: INFO: Deleting statefulset ss
  May 12 12:32:26.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1690" for this suite. @ 05/12/23 12:32:26.333
• [20.147 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 05/12/23 12:32:26.345
  May 12 12:32:26.345: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename disruption @ 05/12/23 12:32:26.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:32:26.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:32:26.363
  STEP: Waiting for the pdb to be processed @ 05/12/23 12:32:26.368
  STEP: Updating PodDisruptionBudget status @ 05/12/23 12:32:28.377
  STEP: Waiting for all pods to be running @ 05/12/23 12:32:28.399
  May 12 12:32:28.424: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 05/12/23 12:32:30.434
  STEP: Waiting for the pdb to be processed @ 05/12/23 12:32:30.466
  STEP: Patching PodDisruptionBudget status @ 05/12/23 12:32:30.474
  STEP: Waiting for the pdb to be processed @ 05/12/23 12:32:30.483
  May 12 12:32:30.486: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8520" for this suite. @ 05/12/23 12:32:30.489
• [4.156 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 05/12/23 12:32:30.502
  May 12 12:32:30.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 12:32:30.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:32:30.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:32:30.525
  STEP: creating the pod @ 05/12/23 12:32:30.526
  STEP: submitting the pod to kubernetes @ 05/12/23 12:32:30.527
  STEP: verifying the pod is in kubernetes @ 05/12/23 12:32:32.541
  STEP: updating the pod @ 05/12/23 12:32:32.542
  May 12 12:32:33.064: INFO: Successfully updated pod "pod-update-6167ed24-c5ed-4512-8f32-16f4a00283ae"
  STEP: verifying the updated pod is in kubernetes @ 05/12/23 12:32:33.068
  May 12 12:32:33.070: INFO: Pod update OK
  May 12 12:32:33.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4935" for this suite. @ 05/12/23 12:32:33.076
• [2.580 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 05/12/23 12:32:33.083
  May 12 12:32:33.083: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 12:32:33.083
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:32:33.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:32:33.145
  STEP: Discovering how many secrets are in namespace by default @ 05/12/23 12:32:33.146
  STEP: Counting existing ResourceQuota @ 05/12/23 12:32:38.148
  STEP: Creating a ResourceQuota @ 05/12/23 12:32:43.161
  STEP: Ensuring resource quota status is calculated @ 05/12/23 12:32:43.181
  STEP: Creating a Secret @ 05/12/23 12:32:45.184
  STEP: Ensuring resource quota status captures secret creation @ 05/12/23 12:32:45.192
  STEP: Deleting a secret @ 05/12/23 12:32:47.2
  STEP: Ensuring resource quota status released usage @ 05/12/23 12:32:47.21
  May 12 12:32:49.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7452" for this suite. @ 05/12/23 12:32:49.218
• [16.141 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 05/12/23 12:32:49.225
  May 12 12:32:49.226: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-pred @ 05/12/23 12:32:49.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:32:49.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:32:49.242
  May 12 12:32:49.244: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 12 12:32:49.248: INFO: Waiting for terminating namespaces to be deleted...
  May 12 12:32:49.250: INFO: 
  Logging pods the apiserver thinks is on node onekube-ip-172-16-100-5 before test
  May 12 12:32:49.259: INFO: helm-install-one-longhorn-nv79t from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.259: INFO: 	Container helm ready: false, restart count 0
  May 12 12:32:49.259: INFO: helm-install-one-metallb-6p76s from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container helm ready: false, restart count 0
  May 12 12:32:49.260: INFO: helm-install-one-traefik-qs2vx from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container helm ready: false, restart count 0
  May 12 12:32:49.260: INFO: helm-install-rke2-metrics-server-vb7g9 from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container helm ready: false, restart count 0
  May 12 12:32:49.260: INFO: helm-install-rke2-snapshot-controller-5cb8c from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container helm ready: false, restart count 1
  May 12 12:32:49.260: INFO: helm-install-rke2-snapshot-controller-crd-x6qww from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container helm ready: false, restart count 0
  May 12 12:32:49.260: INFO: helm-install-rke2-snapshot-validation-webhook-5ft26 from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container helm ready: false, restart count 0
  May 12 12:32:49.260: INFO: kube-proxy-onekube-ip-172-16-100-5 from kube-system started at 2023-05-12 10:27:24 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container kube-proxy ready: true, restart count 0
  May 12 12:32:49.260: INFO: rke2-canal-qjzfl from kube-system started at 2023-05-12 10:27:25 +0000 UTC (2 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container calico-node ready: true, restart count 0
  May 12 12:32:49.260: INFO: 	Container kube-flannel ready: true, restart count 0
  May 12 12:32:49.260: INFO: rke2-coredns-rke2-coredns-5896cccb79-95wmb from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container coredns ready: true, restart count 0
  May 12 12:32:49.260: INFO: rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-hg8jj from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container autoscaler ready: true, restart count 0
  May 12 12:32:49.260: INFO: rke2-metrics-server-6d45f6cb4d-h7dx4 from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container metrics-server ready: true, restart count 0
  May 12 12:32:49.260: INFO: rke2-snapshot-controller-7bf6d7bf5f-sg6ft from kube-system started at 2023-05-12 10:28:35 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container rke2-snapshot-controller ready: true, restart count 0
  May 12 12:32:49.260: INFO: rke2-snapshot-validation-webhook-b65d46c9f-8bqxq from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container rke2-snapshot-validation-webhook ready: true, restart count 0
  May 12 12:32:49.260: INFO: csi-attacher-79bf77b7d8-vr2c8 from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.260: INFO: 	Container csi-attacher ready: true, restart count 0
  May 12 12:32:49.261: INFO: csi-provisioner-566959ff99-rj4wp from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 12 12:32:49.261: INFO: csi-provisioner-566959ff99-srfx5 from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 12 12:32:49.261: INFO: csi-resizer-769c8fc86-4mp8g from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container csi-resizer ready: true, restart count 0
  May 12 12:32:49.261: INFO: csi-resizer-769c8fc86-5lvwx from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container csi-resizer ready: true, restart count 0
  May 12 12:32:49.261: INFO: csi-snapshotter-5677bd8f7f-2pqth from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 12 12:32:49.261: INFO: csi-snapshotter-5677bd8f7f-9dlsf from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 12 12:32:49.261: INFO: engine-image-ei-7fa7c208-kkhx7 from longhorn-system started at 2023-05-12 10:28:57 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container engine-image-ei-7fa7c208 ready: true, restart count 0
  May 12 12:32:49.261: INFO: instance-manager-e-1d0a2d1f86ac8de0e3ad611c8e7e88f7 from longhorn-system started at 2023-05-12 10:28:57 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container engine-manager ready: true, restart count 0
  May 12 12:32:49.261: INFO: longhorn-admission-webhook-9944c8788-5vs75 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container longhorn-admission-webhook ready: true, restart count 0
  May 12 12:32:49.261: INFO: longhorn-admission-webhook-9944c8788-n5t49 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container longhorn-admission-webhook ready: true, restart count 0
  May 12 12:32:49.261: INFO: longhorn-conversion-webhook-6c88c48f86-rwmr7 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.261: INFO: 	Container longhorn-conversion-webhook ready: true, restart count 0
  May 12 12:32:49.262: INFO: longhorn-conversion-webhook-6c88c48f86-x7rnk from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container longhorn-conversion-webhook ready: true, restart count 0
  May 12 12:32:49.262: INFO: longhorn-csi-plugin-8mtl4 from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (3 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container longhorn-csi-plugin ready: true, restart count 0
  May 12 12:32:49.262: INFO: 	Container longhorn-liveness-probe ready: true, restart count 0
  May 12 12:32:49.262: INFO: 	Container node-driver-registrar ready: true, restart count 0
  May 12 12:32:49.262: INFO: longhorn-manager-lsknf from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container longhorn-manager ready: true, restart count 0
  May 12 12:32:49.262: INFO: longhorn-recovery-backend-568bcc58fc-5d6pn from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container longhorn-recovery-backend ready: true, restart count 0
  May 12 12:32:49.262: INFO: longhorn-recovery-backend-568bcc58fc-lklhz from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container longhorn-recovery-backend ready: true, restart count 0
  May 12 12:32:49.262: INFO: one-metallb-controller-7965f57b86-zvww2 from metallb-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container controller ready: true, restart count 0
  May 12 12:32:49.262: INFO: one-metallb-speaker-tr56f from metallb-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container speaker ready: true, restart count 0
  May 12 12:32:49.262: INFO: sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-tc4w8 from sonobuoy started at 2023-05-12 12:31:55 +0000 UTC (2 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 12 12:32:49.262: INFO: 	Container systemd-logs ready: true, restart count 0
  May 12 12:32:49.262: INFO: one-traefik-66db8f599c-7j8zw from traefik-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.262: INFO: 	Container one-traefik ready: true, restart count 0
  May 12 12:32:49.262: INFO: 
  Logging pods the apiserver thinks is on node onekube-ip-172-16-100-7 before test
  May 12 12:32:49.269: INFO: kube-proxy-onekube-ip-172-16-100-7 from kube-system started at 2023-05-12 10:34:21 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.270: INFO: 	Container kube-proxy ready: true, restart count 0
  May 12 12:32:49.270: INFO: rke2-canal-58rgz from kube-system started at 2023-05-12 10:34:22 +0000 UTC (2 container statuses recorded)
  May 12 12:32:49.270: INFO: 	Container calico-node ready: true, restart count 0
  May 12 12:32:49.270: INFO: 	Container kube-flannel ready: true, restart count 0
  May 12 12:32:49.270: INFO: engine-image-ei-7fa7c208-vfzpk from longhorn-system started at 2023-05-12 11:40:17 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.270: INFO: 	Container engine-image-ei-7fa7c208 ready: true, restart count 0
  May 12 12:32:49.270: INFO: instance-manager-e-bb03c767b3d98d645b664cf2a3258c7f from longhorn-system started at 2023-05-12 11:40:14 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.271: INFO: 	Container engine-manager ready: true, restart count 0
  May 12 12:32:49.271: INFO: longhorn-csi-plugin-czs5s from longhorn-system started at 2023-05-12 11:40:12 +0000 UTC (3 container statuses recorded)
  May 12 12:32:49.271: INFO: 	Container longhorn-csi-plugin ready: true, restart count 0
  May 12 12:32:49.271: INFO: 	Container longhorn-liveness-probe ready: true, restart count 0
  May 12 12:32:49.271: INFO: 	Container node-driver-registrar ready: true, restart count 0
  May 12 12:32:49.272: INFO: longhorn-manager-48l97 from longhorn-system started at 2023-05-12 11:40:12 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.272: INFO: 	Container longhorn-manager ready: true, restart count 0
  May 12 12:32:49.272: INFO: one-metallb-speaker-dnsq8 from metallb-system started at 2023-05-12 11:40:12 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.272: INFO: 	Container speaker ready: true, restart count 0
  May 12 12:32:49.272: INFO: sonobuoy from sonobuoy started at 2023-05-12 12:31:54 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.272: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 12 12:32:49.273: INFO: sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-8q9cx from sonobuoy started at 2023-05-12 12:31:55 +0000 UTC (2 container statuses recorded)
  May 12 12:32:49.273: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 12 12:32:49.273: INFO: 	Container systemd-logs ready: true, restart count 0
  May 12 12:32:49.273: INFO: one-traefik-66db8f599c-xk9n7 from traefik-system started at 2023-05-12 11:40:16 +0000 UTC (1 container statuses recorded)
  May 12 12:32:49.273: INFO: 	Container one-traefik ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/12/23 12:32:49.273
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/12/23 12:32:51.297
  STEP: Trying to apply a random label on the found node. @ 05/12/23 12:32:51.329
  STEP: verifying the node has the label kubernetes.io/e2e-e7037f93-a950-4d8d-8efc-df3bd13974bb 95 @ 05/12/23 12:32:51.343
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 05/12/23 12:32:51.345
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 172.16.100.7 on the node which pod4 resides and expect not scheduled @ 05/12/23 12:32:53.371
  STEP: removing the label kubernetes.io/e2e-e7037f93-a950-4d8d-8efc-df3bd13974bb off the node onekube-ip-172-16-100-7 @ 05/12/23 12:37:53.405
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-e7037f93-a950-4d8d-8efc-df3bd13974bb @ 05/12/23 12:37:53.44
  May 12 12:37:53.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-3010" for this suite. @ 05/12/23 12:37:53.459
• [304.237 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 05/12/23 12:37:53.462
  May 12 12:37:53.462: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 12:37:53.463
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:37:53.476
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:37:53.478
  STEP: creating secret secrets-7742/secret-test-fb03c8e4-2f2c-4837-9a2b-a81dbe3c169e @ 05/12/23 12:37:53.479
  STEP: Creating a pod to test consume secrets @ 05/12/23 12:37:53.484
  STEP: Saw pod success @ 05/12/23 12:37:57.513
  May 12 12:37:57.518: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-8fddedc7-ec9f-4812-b2ca-630a3aa599e5 container env-test: <nil>
  STEP: delete the pod @ 05/12/23 12:37:57.528
  May 12 12:37:57.541: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7742" for this suite. @ 05/12/23 12:37:57.543
• [4.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 05/12/23 12:37:57.549
  May 12 12:37:57.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename disruption @ 05/12/23 12:37:57.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:37:57.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:37:57.566
  STEP: Waiting for the pdb to be processed @ 05/12/23 12:37:57.572
  STEP: Waiting for all pods to be running @ 05/12/23 12:37:57.613
  May 12 12:37:57.624: INFO: running pods: 0 < 3
  May 12 12:37:59.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-3356" for this suite. @ 05/12/23 12:37:59.653
• [2.112 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 05/12/23 12:37:59.662
  May 12 12:37:59.662: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replicaset @ 05/12/23 12:37:59.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:37:59.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:37:59.682
  May 12 12:37:59.684: INFO: Creating ReplicaSet my-hostname-basic-16a23247-e7a6-4b70-a224-131180de4c90
  May 12 12:37:59.690: INFO: Pod name my-hostname-basic-16a23247-e7a6-4b70-a224-131180de4c90: Found 0 pods out of 1
  May 12 12:38:04.702: INFO: Pod name my-hostname-basic-16a23247-e7a6-4b70-a224-131180de4c90: Found 1 pods out of 1
  May 12 12:38:04.703: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-16a23247-e7a6-4b70-a224-131180de4c90" is running
  May 12 12:38:04.713: INFO: Pod "my-hostname-basic-16a23247-e7a6-4b70-a224-131180de4c90-2fzbl" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-12 12:37:59 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-12 12:38:01 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-12 12:38:01 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-12 12:37:59 +0000 UTC Reason: Message:}])
  May 12 12:38:04.713: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/12/23 12:38:04.713
  May 12 12:38:04.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5467" for this suite. @ 05/12/23 12:38:04.738
• [5.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 05/12/23 12:38:04.748
  May 12 12:38:04.748: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename limitrange @ 05/12/23 12:38:04.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:04.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:04.769
  STEP: Creating a LimitRange @ 05/12/23 12:38:04.772
  STEP: Setting up watch @ 05/12/23 12:38:04.772
  STEP: Submitting a LimitRange @ 05/12/23 12:38:04.875
  STEP: Verifying LimitRange creation was observed @ 05/12/23 12:38:04.881
  STEP: Fetching the LimitRange to ensure it has proper values @ 05/12/23 12:38:04.881
  May 12 12:38:04.884: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 12 12:38:04.884: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 05/12/23 12:38:04.884
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 05/12/23 12:38:04.892
  May 12 12:38:04.898: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  May 12 12:38:04.898: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 05/12/23 12:38:04.898
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 05/12/23 12:38:04.906
  May 12 12:38:04.912: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  May 12 12:38:04.913: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 05/12/23 12:38:04.914
  STEP: Failing to create a Pod with more than max resources @ 05/12/23 12:38:04.917
  STEP: Updating a LimitRange @ 05/12/23 12:38:04.922
  STEP: Verifying LimitRange updating is effective @ 05/12/23 12:38:04.926
  STEP: Creating a Pod with less than former min resources @ 05/12/23 12:38:06.932
  STEP: Failing to create a Pod with more than max resources @ 05/12/23 12:38:06.95
  STEP: Deleting a LimitRange @ 05/12/23 12:38:06.968
  STEP: Verifying the LimitRange was deleted @ 05/12/23 12:38:06.973
  May 12 12:38:11.981: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 05/12/23 12:38:11.981
  May 12 12:38:11.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-2167" for this suite. @ 05/12/23 12:38:12.005
• [7.262 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 05/12/23 12:38:12.014
  May 12 12:38:12.014: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 12:38:12.015
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:12.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:12.028
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/12/23 12:38:12.03
  STEP: Saw pod success @ 05/12/23 12:38:16.053
  May 12 12:38:16.057: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-42119c06-89f0-494b-8b12-dcb3b72bcab1 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 12:38:16.066
  May 12 12:38:16.083: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8749" for this suite. @ 05/12/23 12:38:16.086
• [4.077 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 05/12/23 12:38:16.091
  May 12 12:38:16.091: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 12:38:16.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:16.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:16.107
  STEP: Setting up server cert @ 05/12/23 12:38:16.123
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 12:38:16.575
  STEP: Deploying the webhook pod @ 05/12/23 12:38:16.58
  STEP: Wait for the deployment to be ready @ 05/12/23 12:38:16.593
  May 12 12:38:16.617: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/23 12:38:18.646
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 12:38:18.67
  May 12 12:38:19.672: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 12 12:38:19.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-4168-crds.webhook.example.com via the AdmissionRegistration API @ 05/12/23 12:38:20.266
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/12/23 12:38:20.28
  May 12 12:38:22.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-708" for this suite. @ 05/12/23 12:38:22.95
  STEP: Destroying namespace "webhook-markers-2882" for this suite. @ 05/12/23 12:38:22.954
• [6.867 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 05/12/23 12:38:22.961
  May 12 12:38:22.961: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename events @ 05/12/23 12:38:22.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:22.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:22.987
  STEP: Create set of events @ 05/12/23 12:38:22.988
  May 12 12:38:22.990: INFO: created test-event-1
  May 12 12:38:22.992: INFO: created test-event-2
  May 12 12:38:22.994: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 05/12/23 12:38:22.994
  STEP: delete collection of events @ 05/12/23 12:38:22.995
  May 12 12:38:22.995: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/12/23 12:38:23.001
  May 12 12:38:23.001: INFO: requesting list of events to confirm quantity
  May 12 12:38:23.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4023" for this suite. @ 05/12/23 12:38:23.006
• [0.049 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 05/12/23 12:38:23.011
  May 12 12:38:23.011: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 12:38:23.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:23.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:23.026
  STEP: creating the pod @ 05/12/23 12:38:23.027
  May 12 12:38:23.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9340 create -f -'
  May 12 12:38:23.884: INFO: stderr: ""
  May 12 12:38:23.884: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 05/12/23 12:38:25.897
  May 12 12:38:25.898: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9340 label pods pause testing-label=testing-label-value'
  May 12 12:38:25.990: INFO: stderr: ""
  May 12 12:38:25.990: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 05/12/23 12:38:25.99
  May 12 12:38:25.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9340 get pod pause -L testing-label'
  May 12 12:38:26.059: INFO: stderr: ""
  May 12 12:38:26.059: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 05/12/23 12:38:26.059
  May 12 12:38:26.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9340 label pods pause testing-label-'
  May 12 12:38:26.114: INFO: stderr: ""
  May 12 12:38:26.114: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 05/12/23 12:38:26.114
  May 12 12:38:26.114: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9340 get pod pause -L testing-label'
  May 12 12:38:26.167: INFO: stderr: ""
  May 12 12:38:26.167: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 05/12/23 12:38:26.167
  May 12 12:38:26.167: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9340 delete --grace-period=0 --force -f -'
  May 12 12:38:26.222: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 12:38:26.222: INFO: stdout: "pod \"pause\" force deleted\n"
  May 12 12:38:26.223: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9340 get rc,svc -l name=pause --no-headers'
  May 12 12:38:26.280: INFO: stderr: "No resources found in kubectl-9340 namespace.\n"
  May 12 12:38:26.280: INFO: stdout: ""
  May 12 12:38:26.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9340 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 12 12:38:26.336: INFO: stderr: ""
  May 12 12:38:26.336: INFO: stdout: ""
  May 12 12:38:26.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9340" for this suite. @ 05/12/23 12:38:26.34
• [3.332 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 05/12/23 12:38:26.346
  May 12 12:38:26.346: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename namespaces @ 05/12/23 12:38:26.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:26.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:26.364
  STEP: Creating a test namespace @ 05/12/23 12:38:26.365
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:26.38
  STEP: Creating a pod in the namespace @ 05/12/23 12:38:26.381
  STEP: Waiting for the pod to have running status @ 05/12/23 12:38:26.389
  STEP: Deleting the namespace @ 05/12/23 12:38:28.403
  STEP: Waiting for the namespace to be removed. @ 05/12/23 12:38:28.412
  STEP: Recreating the namespace @ 05/12/23 12:38:39.42
  STEP: Verifying there are no pods in the namespace @ 05/12/23 12:38:39.459
  May 12 12:38:39.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-3119" for this suite. @ 05/12/23 12:38:39.467
  STEP: Destroying namespace "nsdeletetest-1253" for this suite. @ 05/12/23 12:38:39.472
  May 12 12:38:39.474: INFO: Namespace nsdeletetest-1253 was already deleted
  STEP: Destroying namespace "nsdeletetest-1970" for this suite. @ 05/12/23 12:38:39.474
• [13.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 05/12/23 12:38:39.48
  May 12 12:38:39.480: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 12:38:39.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:39.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:39.495
  May 12 12:38:39.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 05/12/23 12:38:40.799
  May 12 12:38:40.800: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 create -f -'
  May 12 12:38:41.539: INFO: stderr: ""
  May 12 12:38:41.539: INFO: stdout: "e2e-test-crd-publish-openapi-8919-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 12 12:38:41.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 delete e2e-test-crd-publish-openapi-8919-crds test-foo'
  May 12 12:38:41.591: INFO: stderr: ""
  May 12 12:38:41.592: INFO: stdout: "e2e-test-crd-publish-openapi-8919-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  May 12 12:38:41.592: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 apply -f -'
  May 12 12:38:41.811: INFO: stderr: ""
  May 12 12:38:41.811: INFO: stdout: "e2e-test-crd-publish-openapi-8919-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  May 12 12:38:41.811: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 delete e2e-test-crd-publish-openapi-8919-crds test-foo'
  May 12 12:38:41.862: INFO: stderr: ""
  May 12 12:38:41.862: INFO: stdout: "e2e-test-crd-publish-openapi-8919-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 05/12/23 12:38:41.863
  May 12 12:38:41.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 create -f -'
  May 12 12:38:42.076: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 05/12/23 12:38:42.076
  May 12 12:38:42.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 create -f -'
  May 12 12:38:42.415: INFO: rc: 1
  May 12 12:38:42.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 apply -f -'
  May 12 12:38:42.640: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 05/12/23 12:38:42.64
  May 12 12:38:42.640: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 create -f -'
  May 12 12:38:42.849: INFO: rc: 1
  May 12 12:38:42.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 --namespace=crd-publish-openapi-5146 apply -f -'
  May 12 12:38:43.051: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 05/12/23 12:38:43.051
  May 12 12:38:43.051: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 explain e2e-test-crd-publish-openapi-8919-crds'
  May 12 12:38:43.249: INFO: stderr: ""
  May 12 12:38:43.249: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8919-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 05/12/23 12:38:43.249
  May 12 12:38:43.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 explain e2e-test-crd-publish-openapi-8919-crds.metadata'
  May 12 12:38:43.466: INFO: stderr: ""
  May 12 12:38:43.466: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8919-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  May 12 12:38:43.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 explain e2e-test-crd-publish-openapi-8919-crds.spec'
  May 12 12:38:43.666: INFO: stderr: ""
  May 12 12:38:43.666: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8919-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  May 12 12:38:43.666: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 explain e2e-test-crd-publish-openapi-8919-crds.spec.bars'
  May 12 12:38:43.881: INFO: stderr: ""
  May 12 12:38:43.881: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-8919-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 05/12/23 12:38:43.881
  May 12 12:38:43.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-5146 explain e2e-test-crd-publish-openapi-8919-crds.spec.bars2'
  May 12 12:38:44.081: INFO: rc: 1
  May 12 12:38:45.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-5146" for this suite. @ 05/12/23 12:38:45.496
• [6.020 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 05/12/23 12:38:45.5
  May 12 12:38:45.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 12:38:45.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:45.512
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:45.515
  STEP: creating a Pod with a static label @ 05/12/23 12:38:45.523
  STEP: watching for Pod to be ready @ 05/12/23 12:38:45.53
  May 12 12:38:45.531: INFO: observed Pod pod-test in namespace pods-6065 in phase Pending with labels: map[test-pod-static:true] & conditions []
  May 12 12:38:45.535: INFO: observed Pod pod-test in namespace pods-6065 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC  }]
  May 12 12:38:45.542: INFO: observed Pod pod-test in namespace pods-6065 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC  }]
  May 12 12:38:45.985: INFO: observed Pod pod-test in namespace pods-6065 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC  }]
  May 12 12:38:46.510: INFO: Found Pod pod-test in namespace pods-6065 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:46 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:46 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:38:45 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 05/12/23 12:38:46.516
  STEP: getting the Pod and ensuring that it's patched @ 05/12/23 12:38:46.531
  STEP: replacing the Pod's status Ready condition to False @ 05/12/23 12:38:46.534
  STEP: check the Pod again to ensure its Ready conditions are False @ 05/12/23 12:38:46.542
  STEP: deleting the Pod via a Collection with a LabelSelector @ 05/12/23 12:38:46.542
  STEP: watching for the Pod to be deleted @ 05/12/23 12:38:46.551
  May 12 12:38:46.552: INFO: observed event type MODIFIED
  May 12 12:38:48.521: INFO: observed event type MODIFIED
  May 12 12:38:48.758: INFO: observed event type MODIFIED
  May 12 12:38:48.813: INFO: observed event type MODIFIED
  May 12 12:38:49.525: INFO: observed event type MODIFIED
  May 12 12:38:49.542: INFO: observed event type MODIFIED
  May 12 12:38:49.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6065" for this suite. @ 05/12/23 12:38:49.548
• [4.053 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 05/12/23 12:38:49.555
  May 12 12:38:49.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 12:38:49.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:49.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:49.57
  STEP: Creating configMap with name configmap-test-volume-map-aaeadd66-4f10-44a5-b0e1-7a4fd3901f9c @ 05/12/23 12:38:49.572
  STEP: Creating a pod to test consume configMaps @ 05/12/23 12:38:49.576
  STEP: Saw pod success @ 05/12/23 12:38:53.596
  May 12 12:38:53.597: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-b18af4cb-360c-458d-8465-bb9cde1e22d8 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 12:38:53.601
  May 12 12:38:53.614: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9865" for this suite. @ 05/12/23 12:38:53.617
• [4.079 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 05/12/23 12:38:53.635
  May 12 12:38:53.635: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename statefulset @ 05/12/23 12:38:53.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:38:53.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:38:53.649
  STEP: Creating service test in namespace statefulset-4160 @ 05/12/23 12:38:53.651
  STEP: Creating a new StatefulSet @ 05/12/23 12:38:53.656
  May 12 12:38:53.664: INFO: Found 0 stateful pods, waiting for 3
  May 12 12:39:03.675: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 12 12:39:03.675: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 12 12:39:03.675: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  May 12 12:39:03.698: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-4160 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 12:39:03.843: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 12:39:03.843: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 12:39:03.843: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/12/23 12:39:13.852
  May 12 12:39:13.868: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/12/23 12:39:13.868
  STEP: Updating Pods in reverse ordinal order @ 05/12/23 12:39:23.882
  May 12 12:39:23.886: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-4160 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 12:39:24.039: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 12 12:39:24.039: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 12:39:24.039: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  STEP: Rolling back to a previous revision @ 05/12/23 12:39:34.05
  May 12 12:39:34.050: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-4160 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 12:39:34.169: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 12:39:34.170: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 12:39:34.170: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 12:39:44.196: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 05/12/23 12:39:54.223
  May 12 12:39:54.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-4160 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 12:39:54.359: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 12 12:39:54.359: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 12:39:54.359: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 12 12:40:04.370: INFO: Deleting all statefulset in ns statefulset-4160
  May 12 12:40:04.370: INFO: Scaling statefulset ss2 to 0
  May 12 12:40:14.387: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 12:40:14.388: INFO: Deleting statefulset ss2
  May 12 12:40:14.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4160" for this suite. @ 05/12/23 12:40:14.402
• [80.772 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 05/12/23 12:40:14.407
  May 12 12:40:14.407: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/23 12:40:14.408
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:40:14.418
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:40:14.42
  May 12 12:40:14.431: INFO: created pod
  STEP: Saw pod success @ 05/12/23 12:40:18.446
  May 12 12:40:48.448: INFO: polling logs
  May 12 12:40:48.457: INFO: Pod logs: 
  I0512 12:40:15.116089       1 log.go:198] OK: Got token
  I0512 12:40:15.116154       1 log.go:198] validating with in-cluster discovery
  I0512 12:40:15.116431       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0512 12:40:15.116468       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4122:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683895814, NotBefore:1683895214, IssuedAt:1683895214, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4122", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a2668fbe-29f7-4c0b-8e0a-347b0f7c7561"}}}
  I0512 12:40:15.131226       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0512 12:40:15.137712       1 log.go:198] OK: Validated signature on JWT
  I0512 12:40:15.137878       1 log.go:198] OK: Got valid claims from token!
  I0512 12:40:15.137958       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-4122:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1683895814, NotBefore:1683895214, IssuedAt:1683895214, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-4122", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"a2668fbe-29f7-4c0b-8e0a-347b0f7c7561"}}}

  May 12 12:40:48.458: INFO: completed pod
  May 12 12:40:48.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4122" for this suite. @ 05/12/23 12:40:48.464
• [34.062 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 05/12/23 12:40:48.469
  May 12 12:40:48.469: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 12:40:48.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:40:48.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:40:48.486
  STEP: Setting up server cert @ 05/12/23 12:40:48.505
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 12:40:49.098
  STEP: Deploying the webhook pod @ 05/12/23 12:40:49.102
  STEP: Wait for the deployment to be ready @ 05/12/23 12:40:49.114
  May 12 12:40:49.121: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/23 12:40:51.13
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 12:40:51.158
  May 12 12:40:52.162: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/12/23 12:40:52.167
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 05/12/23 12:40:52.195
  STEP: Creating a dummy validating-webhook-configuration object @ 05/12/23 12:40:52.208
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 05/12/23 12:40:52.219
  STEP: Creating a dummy mutating-webhook-configuration object @ 05/12/23 12:40:52.226
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 05/12/23 12:40:52.231
  May 12 12:40:52.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2662" for this suite. @ 05/12/23 12:40:52.294
  STEP: Destroying namespace "webhook-markers-4698" for this suite. @ 05/12/23 12:40:52.307
• [3.870 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 05/12/23 12:40:52.34
  May 12 12:40:52.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 12:40:52.34
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:40:52.352
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:40:52.354
  STEP: creating service nodeport-test with type=NodePort in namespace services-5055 @ 05/12/23 12:40:52.356
  STEP: creating replication controller nodeport-test in namespace services-5055 @ 05/12/23 12:40:52.363
  I0512 12:40:52.374123      20 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-5055, replica count: 2
  I0512 12:40:55.425839      20 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 12:40:55.425: INFO: Creating new exec pod
  May 12 12:40:58.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5055 exec execpodb8dd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 12 12:40:58.594: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 12 12:40:58.594: INFO: stdout: ""
  May 12 12:40:59.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5055 exec execpodb8dd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 12 12:40:59.703: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostNameConnection to nodeport-test 80 port [tcp/http] succeeded!\n\n"
  May 12 12:40:59.703: INFO: stdout: ""
  May 12 12:41:00.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5055 exec execpodb8dd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 12 12:41:00.749: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 12 12:41:00.749: INFO: stdout: ""
  May 12 12:41:01.594: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5055 exec execpodb8dd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  May 12 12:41:01.734: INFO: stderr: "+ nc -v -t -w 2 nodeport-test 80\n+ echo hostName\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  May 12 12:41:01.734: INFO: stdout: "nodeport-test-xb7mg"
  May 12 12:41:01.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5055 exec execpodb8dd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.38.189 80'
  May 12 12:41:01.850: INFO: stderr: "+ nc -v -t -w 2 10.43.38.189 80\n+ echoConnection to 10.43.38.189 80 port [tcp/http] succeeded!\n hostName\n"
  May 12 12:41:01.850: INFO: stdout: "nodeport-test-x4j82"
  May 12 12:41:01.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5055 exec execpodb8dd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.5 31891'
  May 12 12:41:01.989: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.100.5 31891\nConnection to 172.16.100.5 31891 port [tcp/*] succeeded!\n"
  May 12 12:41:01.989: INFO: stdout: "nodeport-test-xb7mg"
  May 12 12:41:01.989: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5055 exec execpodb8dd4 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.7 31891'
  May 12 12:41:02.136: INFO: stderr: "+ nc -v -t -w 2 172.16.100.7 31891\n+ echoConnection to 172.16.100.7 31891 port [tcp/*] succeeded!\n hostName\n"
  May 12 12:41:02.136: INFO: stdout: "nodeport-test-x4j82"
  May 12 12:41:02.136: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5055" for this suite. @ 05/12/23 12:41:02.139
• [9.803 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 05/12/23 12:41:02.143
  May 12 12:41:02.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 12:41:02.143
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:02.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:02.182
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 12:41:02.201
  STEP: Saw pod success @ 05/12/23 12:41:06.225
  May 12 12:41:06.229: INFO: Trying to get logs from node onekube-ip-172-16-100-5 pod downwardapi-volume-7babfcf4-0ba7-4cc2-9286-dcb56984c963 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 12:41:06.243
  May 12 12:41:06.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5864" for this suite. @ 05/12/23 12:41:06.259
• [4.118 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 05/12/23 12:41:06.264
  May 12 12:41:06.264: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 12:41:06.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:06.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:06.279
  STEP: Setting up server cert @ 05/12/23 12:41:06.298
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 12:41:06.705
  STEP: Deploying the webhook pod @ 05/12/23 12:41:06.708
  STEP: Wait for the deployment to be ready @ 05/12/23 12:41:06.718
  May 12 12:41:06.722: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/23 12:41:08.727
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 12:41:08.74
  May 12 12:41:09.745: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 12 12:41:09.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9789-crds.webhook.example.com via the AdmissionRegistration API @ 05/12/23 12:41:10.316
  STEP: Creating a custom resource that should be mutated by the webhook @ 05/12/23 12:41:10.329
  May 12 12:41:12.377: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2952" for this suite. @ 05/12/23 12:41:13.066
  STEP: Destroying namespace "webhook-markers-5535" for this suite. @ 05/12/23 12:41:13.118
• [6.859 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 05/12/23 12:41:13.124
  May 12 12:41:13.124: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename daemonsets @ 05/12/23 12:41:13.124
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:13.138
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:13.14
  May 12 12:41:13.155: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 05/12/23 12:41:13.159
  May 12 12:41:13.161: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:13.161: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 05/12/23 12:41:13.161
  May 12 12:41:13.185: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:13.185: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 12:41:14.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:14.187: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 12:41:15.187: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:15.187: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 12:41:16.189: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 12:41:16.189: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 05/12/23 12:41:16.192
  May 12 12:41:16.225: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 12:41:16.226: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  May 12 12:41:17.228: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:17.228: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 05/12/23 12:41:17.228
  May 12 12:41:17.238: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:17.238: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 12:41:18.243: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:18.243: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 12:41:19.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:19.241: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 12:41:20.240: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 12:41:20.241: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/23 12:41:20.244
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3315, will wait for the garbage collector to delete the pods @ 05/12/23 12:41:20.244
  May 12 12:41:20.303: INFO: Deleting DaemonSet.extensions daemon-set took: 7.534372ms
  May 12 12:41:20.403: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.115565ms
  May 12 12:41:23.113: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:41:23.114: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 12 12:41:23.126: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"72176"},"items":null}

  May 12 12:41:23.129: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"72176"},"items":null}

  May 12 12:41:23.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3315" for this suite. @ 05/12/23 12:41:23.159
• [10.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 05/12/23 12:41:23.167
  May 12 12:41:23.167: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 12:41:23.167
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:23.179
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:23.181
  STEP: Setting up server cert @ 05/12/23 12:41:23.197
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 12:41:23.616
  STEP: Deploying the webhook pod @ 05/12/23 12:41:23.62
  STEP: Wait for the deployment to be ready @ 05/12/23 12:41:23.631
  May 12 12:41:23.642: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/23 12:41:25.661
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 12:41:25.689
  May 12 12:41:26.689: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 05/12/23 12:41:26.691
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/12/23 12:41:26.691
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 05/12/23 12:41:26.704
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 05/12/23 12:41:27.723
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/12/23 12:41:27.724
  STEP: Having no error when timeout is longer than webhook latency @ 05/12/23 12:41:28.751
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/12/23 12:41:28.751
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 05/12/23 12:41:33.796
  STEP: Registering slow webhook via the AdmissionRegistration API @ 05/12/23 12:41:33.796
  May 12 12:41:38.846: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5948" for this suite. @ 05/12/23 12:41:38.908
  STEP: Destroying namespace "webhook-markers-7287" for this suite. @ 05/12/23 12:41:38.913
• [15.751 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 05/12/23 12:41:38.917
  May 12 12:41:38.917: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename field-validation @ 05/12/23 12:41:38.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:38.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:38.935
  May 12 12:41:38.937: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 12:41:41.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7439" for this suite. @ 05/12/23 12:41:41.577
• [2.664 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 05/12/23 12:41:41.584
  May 12 12:41:41.584: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 12:41:41.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:41.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:41.599
  STEP: Creating configMap with name configmap-test-volume-a366f2c4-b30a-4037-b01b-3879eb5c460a @ 05/12/23 12:41:41.6
  STEP: Creating a pod to test consume configMaps @ 05/12/23 12:41:41.603
  STEP: Saw pod success @ 05/12/23 12:41:45.641
  May 12 12:41:45.650: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-9b94701f-35a3-4c2a-ac06-15ed59f3e16c container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 12:41:45.664
  May 12 12:41:45.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9055" for this suite. @ 05/12/23 12:41:45.684
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 05/12/23 12:41:45.689
  May 12 12:41:45.689: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 12:41:45.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:45.701
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:45.704
  May 12 12:41:45.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: creating the pod @ 05/12/23 12:41:45.705
  STEP: submitting the pod to kubernetes @ 05/12/23 12:41:45.706
  May 12 12:41:47.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8469" for this suite. @ 05/12/23 12:41:47.765
• [2.084 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 05/12/23 12:41:47.775
  May 12 12:41:47.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 12:41:47.777
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:47.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:47.793
  STEP: Creating the pod @ 05/12/23 12:41:47.794
  May 12 12:41:50.396: INFO: Successfully updated pod "annotationupdate3fbd3e62-df5c-4a02-922c-f1f45739dec5"
  May 12 12:41:54.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1006" for this suite. @ 05/12/23 12:41:54.415
• [6.644 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 05/12/23 12:41:54.42
  May 12 12:41:54.420: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename deployment @ 05/12/23 12:41:54.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:54.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:54.442
  STEP: creating a Deployment @ 05/12/23 12:41:54.446
  May 12 12:41:54.446: INFO: Creating simple deployment test-deployment-rr5x6
  May 12 12:41:54.490: INFO: deployment "test-deployment-rr5x6" doesn't have the required revision set
  STEP: Getting /status @ 05/12/23 12:41:56.496
  May 12 12:41:56.498: INFO: Deployment test-deployment-rr5x6 has Conditions: [{Available True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rr5x6-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 05/12/23 12:41:56.498
  May 12 12:41:56.504: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 41, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 41, 55, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 41, 55, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 41, 54, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-rr5x6-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 05/12/23 12:41:56.504
  May 12 12:41:56.506: INFO: Observed &Deployment event: ADDED
  May 12 12:41:56.506: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rr5x6-5994cf9475"}
  May 12 12:41:56.506: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.506: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rr5x6-5994cf9475"}
  May 12 12:41:56.506: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 12 12:41:56.506: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.507: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 12 12:41:56.507: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rr5x6-5994cf9475" is progressing.}
  May 12 12:41:56.507: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.507: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 12 12:41:56.507: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rr5x6-5994cf9475" has successfully progressed.}
  May 12 12:41:56.507: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.507: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 12 12:41:56.507: INFO: Observed Deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rr5x6-5994cf9475" has successfully progressed.}
  May 12 12:41:56.507: INFO: Found Deployment test-deployment-rr5x6 in namespace deployment-3000 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 12 12:41:56.507: INFO: Deployment test-deployment-rr5x6 has an updated status
  STEP: patching the Statefulset Status @ 05/12/23 12:41:56.507
  May 12 12:41:56.507: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 12 12:41:56.513: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 05/12/23 12:41:56.513
  May 12 12:41:56.515: INFO: Observed &Deployment event: ADDED
  May 12 12:41:56.515: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rr5x6-5994cf9475"}
  May 12 12:41:56.515: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.516: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-rr5x6-5994cf9475"}
  May 12 12:41:56.516: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 12 12:41:56.516: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.516: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  May 12 12:41:56.516: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:54 +0000 UTC 2023-05-12 12:41:54 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-rr5x6-5994cf9475" is progressing.}
  May 12 12:41:56.516: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.516: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 12 12:41:56.516: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rr5x6-5994cf9475" has successfully progressed.}
  May 12 12:41:56.517: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.517: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:55 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  May 12 12:41:56.517: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-05-12 12:41:55 +0000 UTC 2023-05-12 12:41:54 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-rr5x6-5994cf9475" has successfully progressed.}
  May 12 12:41:56.517: INFO: Observed deployment test-deployment-rr5x6 in namespace deployment-3000 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 12 12:41:56.517: INFO: Observed &Deployment event: MODIFIED
  May 12 12:41:56.517: INFO: Found deployment test-deployment-rr5x6 in namespace deployment-3000 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  May 12 12:41:56.517: INFO: Deployment test-deployment-rr5x6 has a patched status
  May 12 12:41:56.522: INFO: Deployment "test-deployment-rr5x6":
  &Deployment{ObjectMeta:{test-deployment-rr5x6  deployment-3000  c5ff48de-2f27-405d-8206-b70f51a44eb1 72541 1 2023-05-12 12:41:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-12 12:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-05-12 12:41:56 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-05-12 12:41:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0032e7b18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-rr5x6-5994cf9475",LastUpdateTime:2023-05-12 12:41:56 +0000 UTC,LastTransitionTime:2023-05-12 12:41:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 12 12:41:56.525: INFO: New ReplicaSet "test-deployment-rr5x6-5994cf9475" of Deployment "test-deployment-rr5x6":
  &ReplicaSet{ObjectMeta:{test-deployment-rr5x6-5994cf9475  deployment-3000  f2114d6f-56e9-4924-8acc-6f30c1507dbe 72534 1 2023-05-12 12:41:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-rr5x6 c5ff48de-2f27-405d-8206-b70f51a44eb1 0xc004cd5ac0 0xc004cd5ac1}] [] [{kube-controller-manager Update apps/v1 2023-05-12 12:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c5ff48de-2f27-405d-8206-b70f51a44eb1\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 12:41:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cd5b68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 12 12:41:56.532: INFO: Pod "test-deployment-rr5x6-5994cf9475-6jkpq" is available:
  &Pod{ObjectMeta:{test-deployment-rr5x6-5994cf9475-6jkpq test-deployment-rr5x6-5994cf9475- deployment-3000  621a59b8-387a-42fc-95db-81b9e8fa657d 72533 0 2023-05-12 12:41:54 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[cni.projectcalico.org/containerID:54fc77a7abbc1d0d613a33d7d91ccd951cecc9cbaa0824fba2c32c6c47d81796 cni.projectcalico.org/podIP:10.42.3.86/32 cni.projectcalico.org/podIPs:10.42.3.86/32] [{apps/v1 ReplicaSet test-deployment-rr5x6-5994cf9475 f2114d6f-56e9-4924-8acc-6f30c1507dbe 0xc0032e7f10 0xc0032e7f11}] [] [{calico Update v1 2023-05-12 12:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-12 12:41:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f2114d6f-56e9-4924-8acc-6f30c1507dbe\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 12:41:55 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.86\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hbrcp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hbrcp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 12:41:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 12:41:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 12:41:55 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 12:41:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.86,StartTime:2023-05-12 12:41:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 12:41:55 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://3a5107b694b66c99307a497f0d125c2a94a3d2f41a8c65367e2ee2bce2d4055b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.86,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 12:41:56.534: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3000" for this suite. @ 05/12/23 12:41:56.538
• [2.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 05/12/23 12:41:56.545
  May 12 12:41:56.545: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 12:41:56.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:41:56.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:41:56.561
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 05/12/23 12:41:56.562
  May 12 12:41:56.563: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 12:41:57.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 12:42:03.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3647" for this suite. @ 05/12/23 12:42:03.898
• [7.356 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 05/12/23 12:42:03.903
  May 12 12:42:03.903: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename deployment @ 05/12/23 12:42:03.904
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:42:03.918
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:42:03.926
  May 12 12:42:03.939: INFO: Pod name rollover-pod: Found 0 pods out of 1
  May 12 12:42:08.946: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/23 12:42:08.947
  May 12 12:42:08.948: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  May 12 12:42:10.956: INFO: Creating deployment "test-rollover-deployment"
  May 12 12:42:10.990: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  May 12 12:42:12.999: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  May 12 12:42:13.003: INFO: Ensure that both replica sets have 1 created replica
  May 12 12:42:13.007: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  May 12 12:42:13.014: INFO: Updating deployment test-rollover-deployment
  May 12 12:42:13.014: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  May 12 12:42:15.025: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  May 12 12:42:15.042: INFO: Make sure deployment "test-rollover-deployment" is complete
  May 12 12:42:15.063: INFO: all replica sets need to contain the pod-template-hash label
  May 12 12:42:15.064: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 12:42:17.096: INFO: all replica sets need to contain the pod-template-hash label
  May 12 12:42:17.097: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 12:42:19.068: INFO: all replica sets need to contain the pod-template-hash label
  May 12 12:42:19.069: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 12:42:21.075: INFO: all replica sets need to contain the pod-template-hash label
  May 12 12:42:21.075: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 12:42:23.067: INFO: all replica sets need to contain the pod-template-hash label
  May 12 12:42:23.067: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 12, 42, 14, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 12, 42, 11, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 12:42:25.068: INFO: 
  May 12 12:42:25.068: INFO: Ensure that both old replica sets have no replicas
  May 12 12:42:25.075: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-3267  54e443df-750d-40b9-9da0-f855950c02b5 72854 2 2023-05-12 12:42:10 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-12 12:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 12:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cd58d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-12 12:42:11 +0000 UTC,LastTransitionTime:2023-05-12 12:42:11 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-05-12 12:42:24 +0000 UTC,LastTransitionTime:2023-05-12 12:42:11 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 12 12:42:25.076: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-3267  85f3073a-3620-4e8f-a366-3f7c1e68a0f1 72844 2 2023-05-12 12:42:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 54e443df-750d-40b9-9da0-f855950c02b5 0xc004cd5d67 0xc004cd5d68}] [] [{kube-controller-manager Update apps/v1 2023-05-12 12:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54e443df-750d-40b9-9da0-f855950c02b5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 12:42:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cd5e18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 12 12:42:25.076: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  May 12 12:42:25.076: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-3267  7652710f-a6db-4654-bdbd-1a9a6860edea 72853 2 2023-05-12 12:42:03 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 54e443df-750d-40b9-9da0-f855950c02b5 0xc004cd5c37 0xc004cd5c38}] [] [{e2e.test Update apps/v1 2023-05-12 12:42:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 12:42:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54e443df-750d-40b9-9da0-f855950c02b5\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-12 12:42:24 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc004cd5cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 12 12:42:25.076: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-3267  edb7e012-027e-4f72-bd2c-234aac0e39c2 72762 2 2023-05-12 12:42:10 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 54e443df-750d-40b9-9da0-f855950c02b5 0xc004cd5e87 0xc004cd5e88}] [] [{kube-controller-manager Update apps/v1 2023-05-12 12:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"54e443df-750d-40b9-9da0-f855950c02b5\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 12:42:13 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004cd5f38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 12 12:42:25.078: INFO: Pod "test-rollover-deployment-57777854c9-mdml4" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-mdml4 test-rollover-deployment-57777854c9- deployment-3267  0a7d133e-786e-4abe-a8af-ce19fee32346 72785 0 2023-05-12 12:42:13 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[cni.projectcalico.org/containerID:bd0e648a6aa7017713d6a4cd3ece89bcd9e5821b0a819be10d28a4763f3f72da cni.projectcalico.org/podIP:10.42.3.89/32 cni.projectcalico.org/podIPs:10.42.3.89/32] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 85f3073a-3620-4e8f-a366-3f7c1e68a0f1 0xc003a49367 0xc003a49368}] [] [{calico Update v1 2023-05-12 12:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-12 12:42:13 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"85f3073a-3620-4e8f-a366-3f7c1e68a0f1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 12:42:14 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.89\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bdrvw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bdrvw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 12:42:13 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 12:42:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 12:42:14 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 12:42:13 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.89,StartTime:2023-05-12 12:42:13 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 12:42:13 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://7c958be8c4d3445e633b93a83d995f5fdf35eb6345c0ad5003d094140f123ee9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.89,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 12:42:25.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3267" for this suite. @ 05/12/23 12:42:25.081
• [21.187 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 05/12/23 12:42:25.09
  May 12 12:42:25.090: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 12:42:25.091
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:42:25.11
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:42:25.112
  STEP: Creating projection with secret that has name projected-secret-test-map-10d3c0f7-7a69-4607-ba1d-29a6850c5eca @ 05/12/23 12:42:25.114
  STEP: Creating a pod to test consume secrets @ 05/12/23 12:42:25.12
  STEP: Saw pod success @ 05/12/23 12:42:29.138
  May 12 12:42:29.139: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-secrets-01e59015-0283-4dee-b302-aba6b5cf3644 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 12:42:29.142
  May 12 12:42:29.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5661" for this suite. @ 05/12/23 12:42:29.156
• [4.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 05/12/23 12:42:29.163
  May 12 12:42:29.163: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename daemonsets @ 05/12/23 12:42:29.164
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:42:29.174
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:42:29.177
  STEP: Creating simple DaemonSet "daemon-set" @ 05/12/23 12:42:29.19
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/23 12:42:29.193
  May 12 12:42:29.198: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 12:42:29.198: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 12:42:29.199: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:42:29.199: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 12:42:30.203: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 12:42:30.203: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 12:42:30.208: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:42:30.209: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 12:42:31.217: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 12:42:31.218: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 12:42:31.226: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 12:42:31.226: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 05/12/23 12:42:31.23
  May 12 12:42:31.235: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 05/12/23 12:42:31.235
  May 12 12:42:31.244: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 05/12/23 12:42:31.244
  May 12 12:42:31.247: INFO: Observed &DaemonSet event: ADDED
  May 12 12:42:31.247: INFO: Observed &DaemonSet event: MODIFIED
  May 12 12:42:31.248: INFO: Observed &DaemonSet event: MODIFIED
  May 12 12:42:31.248: INFO: Observed &DaemonSet event: MODIFIED
  May 12 12:42:31.248: INFO: Found daemon set daemon-set in namespace daemonsets-3715 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 12 12:42:31.248: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 05/12/23 12:42:31.248
  STEP: watching for the daemon set status to be patched @ 05/12/23 12:42:31.255
  May 12 12:42:31.257: INFO: Observed &DaemonSet event: ADDED
  May 12 12:42:31.257: INFO: Observed &DaemonSet event: MODIFIED
  May 12 12:42:31.258: INFO: Observed &DaemonSet event: MODIFIED
  May 12 12:42:31.259: INFO: Observed &DaemonSet event: MODIFIED
  May 12 12:42:31.259: INFO: Observed daemon set daemon-set in namespace daemonsets-3715 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 12 12:42:31.260: INFO: Observed &DaemonSet event: MODIFIED
  May 12 12:42:31.260: INFO: Found daemon set daemon-set in namespace daemonsets-3715 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  May 12 12:42:31.260: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/23 12:42:31.262
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3715, will wait for the garbage collector to delete the pods @ 05/12/23 12:42:31.262
  May 12 12:42:31.339: INFO: Deleting DaemonSet.extensions daemon-set took: 25.158501ms
  May 12 12:42:31.440: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.833517ms
  May 12 12:42:33.451: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 12:42:33.451: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 12 12:42:33.456: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"73001"},"items":null}

  May 12 12:42:33.460: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"73001"},"items":null}

  May 12 12:42:33.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3715" for this suite. @ 05/12/23 12:42:33.477
• [4.319 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 05/12/23 12:42:33.483
  May 12 12:42:33.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename dns @ 05/12/23 12:42:33.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:42:33.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:42:33.498
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/12/23 12:42:33.499
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 05/12/23 12:42:33.5
  STEP: creating a pod to probe DNS @ 05/12/23 12:42:33.5
  STEP: submitting the pod to kubernetes @ 05/12/23 12:42:33.5
  STEP: retrieving the pod @ 05/12/23 12:42:35.521
  STEP: looking for the results for each expected name from probers @ 05/12/23 12:42:35.526
  May 12 12:42:35.544: INFO: DNS probes using dns-1496/dns-test-9111c490-a8ca-4130-abb7-36777ba05897 succeeded

  May 12 12:42:35.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 12:42:35.548
  STEP: Destroying namespace "dns-1496" for this suite. @ 05/12/23 12:42:35.559
• [2.080 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 05/12/23 12:42:35.565
  May 12 12:42:35.565: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename dns @ 05/12/23 12:42:35.566
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:42:35.577
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:42:35.579
  STEP: Creating a test headless service @ 05/12/23 12:42:35.58
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6158.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-6158.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 05/12/23 12:42:35.587
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-6158.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-6158.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 05/12/23 12:42:35.587
  STEP: creating a pod to probe DNS @ 05/12/23 12:42:35.587
  STEP: submitting the pod to kubernetes @ 05/12/23 12:42:35.587
  STEP: retrieving the pod @ 05/12/23 12:42:37.601
  STEP: looking for the results for each expected name from probers @ 05/12/23 12:42:37.602
  May 12 12:42:37.612: INFO: DNS probes using dns-6158/dns-test-5de613b5-9185-4173-8981-7297df11dbee succeeded

  May 12 12:42:37.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 12:42:37.615
  STEP: deleting the test headless service @ 05/12/23 12:42:37.627
  STEP: Destroying namespace "dns-6158" for this suite. @ 05/12/23 12:42:37.64
• [2.080 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 05/12/23 12:42:37.646
  May 12 12:42:37.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 12:42:37.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:42:37.661
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:42:37.662
  STEP: Counting existing ResourceQuota @ 05/12/23 12:42:54.673
  STEP: Creating a ResourceQuota @ 05/12/23 12:42:59.676
  STEP: Ensuring resource quota status is calculated @ 05/12/23 12:42:59.681
  STEP: Creating a ConfigMap @ 05/12/23 12:43:01.684
  STEP: Ensuring resource quota status captures configMap creation @ 05/12/23 12:43:01.69
  STEP: Deleting a ConfigMap @ 05/12/23 12:43:03.698
  STEP: Ensuring resource quota status released usage @ 05/12/23 12:43:03.725
  May 12 12:43:05.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8575" for this suite. @ 05/12/23 12:43:05.756
• [28.120 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 05/12/23 12:43:05.77
  May 12 12:43:05.770: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pod-network-test @ 05/12/23 12:43:05.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:43:05.786
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:43:05.787
  STEP: Performing setup for networking test in namespace pod-network-test-9026 @ 05/12/23 12:43:05.789
  STEP: creating a selector @ 05/12/23 12:43:05.789
  STEP: Creating the service pods in kubernetes @ 05/12/23 12:43:05.789
  May 12 12:43:05.790: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/12/23 12:43:17.896
  May 12 12:43:19.961: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 12 12:43:19.961: INFO: Going to poll 10.42.2.51 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May 12 12:43:19.965: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.2.51 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9026 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 12:43:19.965: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 12:43:19.966: INFO: ExecWithOptions: Clientset creation
  May 12 12:43:19.966: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9026/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.2.51+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 12 12:43:21.053: INFO: Found all 1 expected endpoints: [netserver-0]
  May 12 12:43:21.054: INFO: Going to poll 10.42.3.94 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  May 12 12:43:21.065: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.42.3.94 8081 | grep -v '^\s*$'] Namespace:pod-network-test-9026 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 12:43:21.066: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 12:43:21.070: INFO: ExecWithOptions: Clientset creation
  May 12 12:43:21.071: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-9026/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.42.3.94+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 12 12:43:22.166: INFO: Found all 1 expected endpoints: [netserver-1]
  May 12 12:43:22.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-9026" for this suite. @ 05/12/23 12:43:22.17
• [16.417 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 05/12/23 12:43:22.189
  May 12 12:43:22.190: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename field-validation @ 05/12/23 12:43:22.191
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:43:22.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:43:22.22
  May 12 12:43:22.223: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  W0512 12:43:24.873712      20 warnings.go:70] unknown field "alpha"
  W0512 12:43:24.873791      20 warnings.go:70] unknown field "beta"
  W0512 12:43:24.873823      20 warnings.go:70] unknown field "delta"
  W0512 12:43:24.873853      20 warnings.go:70] unknown field "epsilon"
  W0512 12:43:24.873883      20 warnings.go:70] unknown field "gamma"
  May 12 12:43:24.895: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8157" for this suite. @ 05/12/23 12:43:24.905
• [2.722 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 05/12/23 12:43:24.911
  May 12 12:43:24.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename field-validation @ 05/12/23 12:43:24.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:43:24.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:43:24.925
  May 12 12:43:24.927: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  W0512 12:43:27.514401      20 warnings.go:70] unknown field "alpha"
  W0512 12:43:27.526502      20 warnings.go:70] unknown field "beta"
  W0512 12:43:27.526590      20 warnings.go:70] unknown field "delta"
  W0512 12:43:27.526667      20 warnings.go:70] unknown field "epsilon"
  W0512 12:43:27.534350      20 warnings.go:70] unknown field "gamma"
  May 12 12:43:27.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2909" for this suite. @ 05/12/23 12:43:27.6
• [2.693 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 05/12/23 12:43:27.607
  May 12 12:43:27.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 12:43:27.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:43:27.619
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:43:27.62
  STEP: creating a secret @ 05/12/23 12:43:27.621
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 05/12/23 12:43:27.625
  STEP: patching the secret @ 05/12/23 12:43:27.634
  STEP: deleting the secret using a LabelSelector @ 05/12/23 12:43:27.64
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 05/12/23 12:43:27.645
  May 12 12:43:27.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3524" for this suite. @ 05/12/23 12:43:27.657
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 05/12/23 12:43:27.664
  May 12 12:43:27.664: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 12:43:27.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:43:27.677
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:43:27.678
  STEP: Creating a ResourceQuota with best effort scope @ 05/12/23 12:43:27.679
  STEP: Ensuring ResourceQuota status is calculated @ 05/12/23 12:43:27.683
  STEP: Creating a ResourceQuota with not best effort scope @ 05/12/23 12:43:29.693
  STEP: Ensuring ResourceQuota status is calculated @ 05/12/23 12:43:29.711
  STEP: Creating a best-effort pod @ 05/12/23 12:43:31.715
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 05/12/23 12:43:31.726
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 05/12/23 12:43:33.729
  STEP: Deleting the pod @ 05/12/23 12:43:35.739
  STEP: Ensuring resource quota status released the pod usage @ 05/12/23 12:43:35.775
  STEP: Creating a not best-effort pod @ 05/12/23 12:43:37.783
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 05/12/23 12:43:37.807
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 05/12/23 12:43:39.817
  STEP: Deleting the pod @ 05/12/23 12:43:41.828
  STEP: Ensuring resource quota status released the pod usage @ 05/12/23 12:43:41.862
  May 12 12:43:43.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7091" for this suite. @ 05/12/23 12:43:43.871
• [16.216 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 05/12/23 12:43:43.879
  May 12 12:43:43.879: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 12:43:43.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:43:43.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:43:43.896
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-22f55a9e-f04e-4d28-b693-402c9301ccc7 @ 05/12/23 12:43:43.901
  STEP: Creating the pod @ 05/12/23 12:43:43.905
  STEP: Updating configmap projected-configmap-test-upd-22f55a9e-f04e-4d28-b693-402c9301ccc7 @ 05/12/23 12:43:45.953
  STEP: waiting to observe update in volume @ 05/12/23 12:43:45.967
  May 12 12:44:30.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-976" for this suite. @ 05/12/23 12:44:30.313
• [46.439 seconds]
------------------------------
S
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 05/12/23 12:44:30.318
  May 12 12:44:30.318: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename statefulset @ 05/12/23 12:44:30.319
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:44:30.33
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:44:30.332
  STEP: Creating service test in namespace statefulset-2982 @ 05/12/23 12:44:30.333
  STEP: Creating stateful set ss in namespace statefulset-2982 @ 05/12/23 12:44:30.339
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-2982 @ 05/12/23 12:44:30.346
  May 12 12:44:30.349: INFO: Found 0 stateful pods, waiting for 1
  May 12 12:44:40.360: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 05/12/23 12:44:40.361
  May 12 12:44:40.368: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 12:44:40.511: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 12:44:40.511: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 12:44:40.511: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 12:44:40.513: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May 12 12:44:50.526: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 12 12:44:50.527: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 12:44:50.561: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
  May 12 12:44:50.563: INFO: ss-0  onekube-ip-172-16-100-7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:40 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:30 +0000 UTC  }]
  May 12 12:44:50.563: INFO: ss-1                           Pending         []
  May 12 12:44:50.563: INFO: 
  May 12 12:44:50.564: INFO: StatefulSet ss has not reached scale 3, at 2
  May 12 12:44:51.576: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.984932938s
  May 12 12:44:52.642: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.973459538s
  May 12 12:44:53.661: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.903201834s
  May 12 12:44:54.667: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.888134454s
  May 12 12:44:55.683: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.88181752s
  May 12 12:44:56.698: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.865120461s
  May 12 12:44:57.706: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.848540656s
  May 12 12:44:58.714: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.842797221s
  May 12 12:44:59.727: INFO: Verifying statefulset ss doesn't scale past 3 for another 833.508161ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-2982 @ 05/12/23 12:45:00.728
  May 12 12:45:00.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 12:45:00.868: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 12 12:45:00.868: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 12:45:00.868: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 12 12:45:00.868: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-2982 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 12:45:00.994: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 12 12:45:00.994: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 12:45:00.994: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 12 12:45:00.994: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-2982 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 12:45:01.110: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  May 12 12:45:01.110: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 12:45:01.110: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 12 12:45:01.113: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  May 12 12:45:11.130: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 12 12:45:11.130: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 12 12:45:11.130: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 05/12/23 12:45:11.131
  May 12 12:45:11.143: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-2982 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 12:45:11.257: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 12:45:11.257: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 12:45:11.257: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 12:45:11.257: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-2982 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 12:45:11.366: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 12:45:11.366: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 12:45:11.366: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 12:45:11.366: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-2982 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 12:45:11.479: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 12:45:11.479: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 12:45:11.479: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 12:45:11.479: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 12:45:11.481: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  May 12 12:45:21.497: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 12 12:45:21.499: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 12 12:45:21.500: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 12 12:45:21.525: INFO: POD   NODE                     PHASE    GRACE  CONDITIONS
  May 12 12:45:21.525: INFO: ss-0  onekube-ip-172-16-100-7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:30 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:45:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:45:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:30 +0000 UTC  }]
  May 12 12:45:21.525: INFO: ss-1  onekube-ip-172-16-100-5  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:45:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:45:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:50 +0000 UTC  }]
  May 12 12:45:21.525: INFO: ss-2  onekube-ip-172-16-100-7  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:50 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:45:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:45:11 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-05-12 12:44:50 +0000 UTC  }]
  May 12 12:45:21.525: INFO: 
  May 12 12:45:21.525: INFO: StatefulSet ss has not reached scale 0, at 3
  May 12 12:45:22.527: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.994560486s
  May 12 12:45:23.530: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.991222267s
  May 12 12:45:24.535: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.988412946s
  May 12 12:45:25.542: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.983998141s
  May 12 12:45:26.544: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.976595531s
  May 12 12:45:27.546: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.975317796s
  May 12 12:45:28.556: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.973122333s
  May 12 12:45:29.559: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.961961072s
  May 12 12:45:30.571: INFO: Verifying statefulset ss doesn't scale past 0 for another 959.165803ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-2982 @ 05/12/23 12:45:31.573
  May 12 12:45:31.586: INFO: Scaling statefulset ss to 0
  May 12 12:45:31.604: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 12:45:31.607: INFO: Deleting all statefulset in ns statefulset-2982
  May 12 12:45:31.609: INFO: Scaling statefulset ss to 0
  May 12 12:45:31.616: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 12:45:31.618: INFO: Deleting statefulset ss
  May 12 12:45:31.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2982" for this suite. @ 05/12/23 12:45:31.641
• [61.327 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 05/12/23 12:45:31.646
  May 12 12:45:31.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 12:45:31.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:45:31.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:45:31.662
  STEP: Creating secret with name secret-test-9cbef563-f26f-4bb8-a012-bda8538adeea @ 05/12/23 12:45:31.663
  STEP: Creating a pod to test consume secrets @ 05/12/23 12:45:31.668
  STEP: Saw pod success @ 05/12/23 12:45:35.696
  May 12 12:45:35.697: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-secrets-badb9a9d-9c34-4de3-ba87-e01d0531ec81 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 12:45:35.702
  May 12 12:45:35.715: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6460" for this suite. @ 05/12/23 12:45:35.718
• [4.077 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 05/12/23 12:45:35.727
  May 12 12:45:35.727: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 12:45:35.728
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:45:35.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:45:35.741
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/12/23 12:45:35.742
  STEP: Saw pod success @ 05/12/23 12:45:39.77
  May 12 12:45:39.786: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-df4adbb5-56bc-4c64-9242-5733756efcea container test-container: <nil>
  STEP: delete the pod @ 05/12/23 12:45:39.799
  May 12 12:45:39.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8030" for this suite. @ 05/12/23 12:45:39.821
• [4.107 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 05/12/23 12:45:39.834
  May 12 12:45:39.834: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 12:45:39.835
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:45:39.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:45:39.848
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 12:45:39.849
  STEP: Saw pod success @ 05/12/23 12:45:43.873
  May 12 12:45:43.880: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-ac046b97-b63b-4b68-8bfa-00bf5ede1ced container client-container: <nil>
  STEP: delete the pod @ 05/12/23 12:45:43.894
  May 12 12:45:43.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7797" for this suite. @ 05/12/23 12:45:43.918
• [4.089 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 05/12/23 12:45:43.926
  May 12 12:45:43.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replicaset @ 05/12/23 12:45:43.927
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:45:43.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:45:43.939
  STEP: Create a Replicaset @ 05/12/23 12:45:43.942
  STEP: Verify that the required pods have come up. @ 05/12/23 12:45:43.945
  May 12 12:45:43.947: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 12 12:45:48.950: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/23 12:45:48.951
  STEP: Getting /status @ 05/12/23 12:45:48.951
  May 12 12:45:48.953: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 05/12/23 12:45:48.954
  May 12 12:45:48.959: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 05/12/23 12:45:48.959
  May 12 12:45:48.960: INFO: Observed &ReplicaSet event: ADDED
  May 12 12:45:48.962: INFO: Observed &ReplicaSet event: MODIFIED
  May 12 12:45:48.963: INFO: Observed &ReplicaSet event: MODIFIED
  May 12 12:45:48.963: INFO: Observed &ReplicaSet event: MODIFIED
  May 12 12:45:48.963: INFO: Found replicaset test-rs in namespace replicaset-7644 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 12 12:45:48.963: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 05/12/23 12:45:48.963
  May 12 12:45:48.963: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 12 12:45:48.969: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 05/12/23 12:45:48.969
  May 12 12:45:48.973: INFO: Observed &ReplicaSet event: ADDED
  May 12 12:45:48.974: INFO: Observed &ReplicaSet event: MODIFIED
  May 12 12:45:48.975: INFO: Observed &ReplicaSet event: MODIFIED
  May 12 12:45:48.975: INFO: Observed &ReplicaSet event: MODIFIED
  May 12 12:45:48.979: INFO: Observed replicaset test-rs in namespace replicaset-7644 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 12 12:45:48.982: INFO: Observed &ReplicaSet event: MODIFIED
  May 12 12:45:48.984: INFO: Found replicaset test-rs in namespace replicaset-7644 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  May 12 12:45:48.990: INFO: Replicaset test-rs has a patched status
  May 12 12:45:48.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-7644" for this suite. @ 05/12/23 12:45:48.996
• [5.075 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 05/12/23 12:45:49.009
  May 12 12:45:49.009: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 12:45:49.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:45:49.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:45:49.029
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/12/23 12:45:49.034
  STEP: Saw pod success @ 05/12/23 12:45:53.083
  May 12 12:45:53.091: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-5dd8683c-03d4-4170-aa96-8a1c4b845508 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 12:45:53.162
  May 12 12:45:53.232: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9650" for this suite. @ 05/12/23 12:45:53.234
• [4.233 seconds]
------------------------------
SS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 05/12/23 12:45:53.242
  May 12 12:45:53.242: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename containers @ 05/12/23 12:45:53.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:45:53.256
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:45:53.26
  STEP: Creating a pod to test override arguments @ 05/12/23 12:45:53.262
  STEP: Saw pod success @ 05/12/23 12:45:57.286
  May 12 12:45:57.289: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod client-containers-64e32065-b7c4-463e-be36-8bf9e397cb09 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 12:45:57.295
  May 12 12:45:57.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-8826" for this suite. @ 05/12/23 12:45:57.321
• [4.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 05/12/23 12:45:57.329
  May 12 12:45:57.330: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 12:45:57.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:45:57.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:45:57.348
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 12:45:57.35
  STEP: Saw pod success @ 05/12/23 12:46:01.371
  May 12 12:46:01.373: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-153f2fec-9848-4fdc-b687-ada6ad770540 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 12:46:01.377
  May 12 12:46:01.391: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7353" for this suite. @ 05/12/23 12:46:01.393
• [4.069 seconds]
------------------------------
SSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 05/12/23 12:46:01.399
  May 12 12:46:01.399: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 12:46:01.4
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:46:01.413
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:46:01.414
  STEP: Creating a pod to test downward api env vars @ 05/12/23 12:46:01.416
  STEP: Saw pod success @ 05/12/23 12:46:05.431
  May 12 12:46:05.432: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downward-api-9fc47198-c4e4-4395-a53c-8e9c4da07e85 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 12:46:05.439
  May 12 12:46:05.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8805" for this suite. @ 05/12/23 12:46:05.455
• [4.059 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 05/12/23 12:46:05.46
  May 12 12:46:05.460: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename dns @ 05/12/23 12:46:05.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:46:05.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:46:05.475
  STEP: Creating a test headless service @ 05/12/23 12:46:05.476
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1813.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1813.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local;sleep 1; done
   @ 05/12/23 12:46:05.484
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-1813.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-1813.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local;sleep 1; done
   @ 05/12/23 12:46:05.484
  STEP: creating a pod to probe DNS @ 05/12/23 12:46:05.484
  STEP: submitting the pod to kubernetes @ 05/12/23 12:46:05.485
  STEP: retrieving the pod @ 05/12/23 12:46:07.512
  STEP: looking for the results for each expected name from probers @ 05/12/23 12:46:07.514
  May 12 12:46:07.522: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:07.523: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:07.526: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:07.528: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:07.530: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:07.533: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:07.537: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:07.542: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:07.542: INFO: Lookups using dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local]

  May 12 12:46:12.546: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:12.548: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:12.550: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:12.551: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:12.553: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:12.554: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:12.556: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:12.557: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:12.558: INFO: Lookups using dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local]

  May 12 12:46:17.545: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:17.547: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:17.549: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:17.554: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:17.557: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:17.559: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:17.563: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:17.566: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:17.566: INFO: Lookups using dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local]

  May 12 12:46:22.546: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:22.547: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:22.549: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:22.550: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:22.552: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:22.553: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:22.555: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:22.556: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:22.556: INFO: Lookups using dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local]

  May 12 12:46:27.545: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:27.547: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:27.548: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:27.550: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:27.552: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:27.553: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:27.554: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:27.556: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:27.556: INFO: Lookups using dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local]

  May 12 12:46:32.546: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:32.550: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:32.552: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:32.553: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:32.555: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:32.556: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:32.558: INFO: Unable to read jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:32.560: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local from pod dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609: the server could not find the requested resource (get pods dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609)
  May 12 12:46:32.560: INFO: Lookups using dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local wheezy_udp@dns-test-service-2.dns-1813.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-1813.svc.cluster.local jessie_udp@dns-test-service-2.dns-1813.svc.cluster.local jessie_tcp@dns-test-service-2.dns-1813.svc.cluster.local]

  May 12 12:46:37.563: INFO: DNS probes using dns-1813/dns-test-83cdd6b5-eed4-47f8-ad82-9f351608a609 succeeded

  May 12 12:46:37.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 12:46:37.565
  STEP: deleting the test headless service @ 05/12/23 12:46:37.581
  STEP: Destroying namespace "dns-1813" for this suite. @ 05/12/23 12:46:37.594
• [32.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 05/12/23 12:46:37.615
  May 12 12:46:37.615: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 12:46:37.616
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:46:37.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:46:37.634
  STEP: Creating secret with name secret-test-bc8c91c1-9197-4610-858a-4f81066ef612 @ 05/12/23 12:46:37.635
  STEP: Creating a pod to test consume secrets @ 05/12/23 12:46:37.642
  STEP: Saw pod success @ 05/12/23 12:46:41.668
  May 12 12:46:41.673: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-secrets-7738a7e1-3417-4810-b112-51e0845ce58c container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 12:46:41.682
  May 12 12:46:41.697: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7739" for this suite. @ 05/12/23 12:46:41.701
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 05/12/23 12:46:41.707
  May 12 12:46:41.708: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 12:46:41.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:46:41.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:46:41.724
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 12:46:41.725
  STEP: Saw pod success @ 05/12/23 12:46:45.756
  May 12 12:46:45.766: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-429a18eb-4dc9-4fe1-9449-284b285c8215 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 12:46:45.782
  May 12 12:46:45.801: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5753" for this suite. @ 05/12/23 12:46:45.805
• [4.105 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 05/12/23 12:46:45.814
  May 12 12:46:45.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/23 12:46:45.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:46:45.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:46:45.83
  May 12 12:46:45.841: INFO: Waiting up to 1m0s for all nodes to be ready
  May 12 12:47:45.870: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/12/23 12:47:45.872
  May 12 12:47:45.882: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 12 12:47:45.890: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 12 12:47:45.908: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 12 12:47:45.918: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/12/23 12:47:45.918
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 05/12/23 12:47:49.953
  May 12 12:47:54.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-1635" for this suite. @ 05/12/23 12:47:54.067
• [68.257 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 05/12/23 12:47:54.073
  May 12 12:47:54.073: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/12/23 12:47:54.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:47:54.086
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:47:54.087
  STEP: create the container to handle the HTTPGet hook request. @ 05/12/23 12:47:54.091
  STEP: create the pod with lifecycle hook @ 05/12/23 12:47:56.113
  STEP: delete the pod with lifecycle hook @ 05/12/23 12:47:58.141
  STEP: check prestop hook @ 05/12/23 12:48:02.156
  May 12 12:48:02.160: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-1806" for this suite. @ 05/12/23 12:48:02.166
• [8.103 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 05/12/23 12:48:02.182
  May 12 12:48:02.182: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 12:48:02.184
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:02.203
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:02.222
  STEP: Starting the proxy @ 05/12/23 12:48:02.246
  May 12 12:48:02.247: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-9029 proxy --unix-socket=/tmp/kubectl-proxy-unix786979415/test'
  STEP: retrieving proxy /api/ output @ 05/12/23 12:48:02.314
  May 12 12:48:02.315: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9029" for this suite. @ 05/12/23 12:48:02.319
• [0.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 05/12/23 12:48:02.327
  May 12 12:48:02.327: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 12:48:02.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:02.342
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:02.344
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-5250 @ 05/12/23 12:48:02.346
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/12/23 12:48:02.367
  STEP: creating service externalsvc in namespace services-5250 @ 05/12/23 12:48:02.367
  STEP: creating replication controller externalsvc in namespace services-5250 @ 05/12/23 12:48:02.378
  I0512 12:48:02.392237      20 runners.go:194] Created replication controller with name: externalsvc, namespace: services-5250, replica count: 2
  I0512 12:48:05.445195      20 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 05/12/23 12:48:05.453
  May 12 12:48:05.496: INFO: Creating new exec pod
  May 12 12:48:07.506: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5250 exec execpodxg9r2 -- /bin/sh -x -c nslookup nodeport-service.services-5250.svc.cluster.local'
  May 12 12:48:07.734: INFO: stderr: "+ nslookup nodeport-service.services-5250.svc.cluster.local\n"
  May 12 12:48:07.734: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nnodeport-service.services-5250.svc.cluster.local\tcanonical name = externalsvc.services-5250.svc.cluster.local.\nName:\texternalsvc.services-5250.svc.cluster.local\nAddress: 10.43.3.188\n\n"
  May 12 12:48:07.734: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-5250, will wait for the garbage collector to delete the pods @ 05/12/23 12:48:07.737
  May 12 12:48:07.811: INFO: Deleting ReplicationController externalsvc took: 20.627151ms
  May 12 12:48:07.911: INFO: Terminating ReplicationController externalsvc pods took: 100.660099ms
  May 12 12:48:10.162: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-5250" for this suite. @ 05/12/23 12:48:10.176
• [7.856 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 05/12/23 12:48:10.188
  May 12 12:48:10.188: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 12:48:10.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:10.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:10.203
  STEP: Counting existing ResourceQuota @ 05/12/23 12:48:10.204
  STEP: Creating a ResourceQuota @ 05/12/23 12:48:15.207
  STEP: Ensuring resource quota status is calculated @ 05/12/23 12:48:15.21
  STEP: Creating a Pod that fits quota @ 05/12/23 12:48:17.215
  STEP: Ensuring ResourceQuota status captures the pod usage @ 05/12/23 12:48:17.234
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 05/12/23 12:48:19.251
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 05/12/23 12:48:19.277
  STEP: Ensuring a pod cannot update its resource requirements @ 05/12/23 12:48:19.298
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 05/12/23 12:48:19.31
  STEP: Deleting the pod @ 05/12/23 12:48:21.313
  STEP: Ensuring resource quota status released the pod usage @ 05/12/23 12:48:21.326
  May 12 12:48:23.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1449" for this suite. @ 05/12/23 12:48:23.35
• [13.175 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 05/12/23 12:48:23.366
  May 12 12:48:23.366: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename discovery @ 05/12/23 12:48:23.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:23.386
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:23.387
  STEP: Setting up server cert @ 05/12/23 12:48:23.389
  May 12 12:48:23.709: INFO: Checking APIGroup: apiregistration.k8s.io
  May 12 12:48:23.710: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  May 12 12:48:23.710: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  May 12 12:48:23.710: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  May 12 12:48:23.710: INFO: Checking APIGroup: apps
  May 12 12:48:23.711: INFO: PreferredVersion.GroupVersion: apps/v1
  May 12 12:48:23.711: INFO: Versions found [{apps/v1 v1}]
  May 12 12:48:23.711: INFO: apps/v1 matches apps/v1
  May 12 12:48:23.711: INFO: Checking APIGroup: events.k8s.io
  May 12 12:48:23.711: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  May 12 12:48:23.711: INFO: Versions found [{events.k8s.io/v1 v1}]
  May 12 12:48:23.711: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  May 12 12:48:23.711: INFO: Checking APIGroup: authentication.k8s.io
  May 12 12:48:23.712: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  May 12 12:48:23.712: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  May 12 12:48:23.712: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  May 12 12:48:23.712: INFO: Checking APIGroup: authorization.k8s.io
  May 12 12:48:23.712: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  May 12 12:48:23.712: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  May 12 12:48:23.712: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  May 12 12:48:23.712: INFO: Checking APIGroup: autoscaling
  May 12 12:48:23.713: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  May 12 12:48:23.713: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  May 12 12:48:23.713: INFO: autoscaling/v2 matches autoscaling/v2
  May 12 12:48:23.713: INFO: Checking APIGroup: batch
  May 12 12:48:23.713: INFO: PreferredVersion.GroupVersion: batch/v1
  May 12 12:48:23.713: INFO: Versions found [{batch/v1 v1}]
  May 12 12:48:23.713: INFO: batch/v1 matches batch/v1
  May 12 12:48:23.713: INFO: Checking APIGroup: certificates.k8s.io
  May 12 12:48:23.714: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  May 12 12:48:23.714: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  May 12 12:48:23.714: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  May 12 12:48:23.714: INFO: Checking APIGroup: networking.k8s.io
  May 12 12:48:23.714: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  May 12 12:48:23.714: INFO: Versions found [{networking.k8s.io/v1 v1}]
  May 12 12:48:23.714: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  May 12 12:48:23.714: INFO: Checking APIGroup: policy
  May 12 12:48:23.715: INFO: PreferredVersion.GroupVersion: policy/v1
  May 12 12:48:23.715: INFO: Versions found [{policy/v1 v1}]
  May 12 12:48:23.715: INFO: policy/v1 matches policy/v1
  May 12 12:48:23.715: INFO: Checking APIGroup: rbac.authorization.k8s.io
  May 12 12:48:23.715: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  May 12 12:48:23.715: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  May 12 12:48:23.715: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  May 12 12:48:23.715: INFO: Checking APIGroup: storage.k8s.io
  May 12 12:48:23.716: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  May 12 12:48:23.716: INFO: Versions found [{storage.k8s.io/v1 v1}]
  May 12 12:48:23.716: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  May 12 12:48:23.716: INFO: Checking APIGroup: admissionregistration.k8s.io
  May 12 12:48:23.716: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  May 12 12:48:23.716: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  May 12 12:48:23.716: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  May 12 12:48:23.716: INFO: Checking APIGroup: apiextensions.k8s.io
  May 12 12:48:23.716: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  May 12 12:48:23.717: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  May 12 12:48:23.717: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  May 12 12:48:23.717: INFO: Checking APIGroup: scheduling.k8s.io
  May 12 12:48:23.717: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  May 12 12:48:23.717: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  May 12 12:48:23.718: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  May 12 12:48:23.718: INFO: Checking APIGroup: coordination.k8s.io
  May 12 12:48:23.718: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  May 12 12:48:23.718: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  May 12 12:48:23.719: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  May 12 12:48:23.719: INFO: Checking APIGroup: node.k8s.io
  May 12 12:48:23.719: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  May 12 12:48:23.719: INFO: Versions found [{node.k8s.io/v1 v1}]
  May 12 12:48:23.720: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  May 12 12:48:23.720: INFO: Checking APIGroup: discovery.k8s.io
  May 12 12:48:23.720: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  May 12 12:48:23.720: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  May 12 12:48:23.721: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  May 12 12:48:23.721: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  May 12 12:48:23.721: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  May 12 12:48:23.721: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  May 12 12:48:23.722: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  May 12 12:48:23.722: INFO: Checking APIGroup: crd.projectcalico.org
  May 12 12:48:23.722: INFO: PreferredVersion.GroupVersion: crd.projectcalico.org/v1
  May 12 12:48:23.722: INFO: Versions found [{crd.projectcalico.org/v1 v1}]
  May 12 12:48:23.723: INFO: crd.projectcalico.org/v1 matches crd.projectcalico.org/v1
  May 12 12:48:23.723: INFO: Checking APIGroup: helm.cattle.io
  May 12 12:48:23.723: INFO: PreferredVersion.GroupVersion: helm.cattle.io/v1
  May 12 12:48:23.724: INFO: Versions found [{helm.cattle.io/v1 v1}]
  May 12 12:48:23.724: INFO: helm.cattle.io/v1 matches helm.cattle.io/v1
  May 12 12:48:23.724: INFO: Checking APIGroup: k3s.cattle.io
  May 12 12:48:23.724: INFO: PreferredVersion.GroupVersion: k3s.cattle.io/v1
  May 12 12:48:23.725: INFO: Versions found [{k3s.cattle.io/v1 v1}]
  May 12 12:48:23.725: INFO: k3s.cattle.io/v1 matches k3s.cattle.io/v1
  May 12 12:48:23.725: INFO: Checking APIGroup: snapshot.storage.k8s.io
  May 12 12:48:23.725: INFO: PreferredVersion.GroupVersion: snapshot.storage.k8s.io/v1
  May 12 12:48:23.726: INFO: Versions found [{snapshot.storage.k8s.io/v1 v1} {snapshot.storage.k8s.io/v1beta1 v1beta1}]
  May 12 12:48:23.726: INFO: snapshot.storage.k8s.io/v1 matches snapshot.storage.k8s.io/v1
  May 12 12:48:23.726: INFO: Checking APIGroup: metallb.io
  May 12 12:48:23.726: INFO: PreferredVersion.GroupVersion: metallb.io/v1beta2
  May 12 12:48:23.726: INFO: Versions found [{metallb.io/v1beta2 v1beta2} {metallb.io/v1beta1 v1beta1} {metallb.io/v1alpha1 v1alpha1}]
  May 12 12:48:23.726: INFO: metallb.io/v1beta2 matches metallb.io/v1beta2
  May 12 12:48:23.727: INFO: Checking APIGroup: traefik.containo.us
  May 12 12:48:23.727: INFO: PreferredVersion.GroupVersion: traefik.containo.us/v1alpha1
  May 12 12:48:23.727: INFO: Versions found [{traefik.containo.us/v1alpha1 v1alpha1}]
  May 12 12:48:23.727: INFO: traefik.containo.us/v1alpha1 matches traefik.containo.us/v1alpha1
  May 12 12:48:23.727: INFO: Checking APIGroup: traefik.io
  May 12 12:48:23.728: INFO: PreferredVersion.GroupVersion: traefik.io/v1alpha1
  May 12 12:48:23.728: INFO: Versions found [{traefik.io/v1alpha1 v1alpha1}]
  May 12 12:48:23.728: INFO: traefik.io/v1alpha1 matches traefik.io/v1alpha1
  May 12 12:48:23.728: INFO: Checking APIGroup: longhorn.io
  May 12 12:48:23.729: INFO: PreferredVersion.GroupVersion: longhorn.io/v1beta2
  May 12 12:48:23.729: INFO: Versions found [{longhorn.io/v1beta2 v1beta2} {longhorn.io/v1beta1 v1beta1}]
  May 12 12:48:23.729: INFO: longhorn.io/v1beta2 matches longhorn.io/v1beta2
  May 12 12:48:23.729: INFO: Checking APIGroup: metrics.k8s.io
  May 12 12:48:23.729: INFO: PreferredVersion.GroupVersion: metrics.k8s.io/v1beta1
  May 12 12:48:23.729: INFO: Versions found [{metrics.k8s.io/v1beta1 v1beta1}]
  May 12 12:48:23.729: INFO: metrics.k8s.io/v1beta1 matches metrics.k8s.io/v1beta1
  May 12 12:48:23.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-2529" for this suite. @ 05/12/23 12:48:23.732
• [0.369 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 05/12/23 12:48:23.735
  May 12 12:48:23.735: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename var-expansion @ 05/12/23 12:48:23.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:23.758
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:23.761
  STEP: Creating a pod to test substitution in container's args @ 05/12/23 12:48:23.762
  STEP: Saw pod success @ 05/12/23 12:48:27.779
  May 12 12:48:27.789: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod var-expansion-c0733567-f798-4ddf-a9f3-4efc71172996 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 12:48:27.817
  May 12 12:48:27.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-602" for this suite. @ 05/12/23 12:48:27.834
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 05/12/23 12:48:27.84
  May 12 12:48:27.840: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename security-context-test @ 05/12/23 12:48:27.84
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:27.854
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:27.855
  May 12 12:48:31.894: INFO: Got logs for pod "busybox-privileged-false-9d3a7daf-c2c8-423a-a9c7-d83dc82efeab": "ip: RTNETLINK answers: Operation not permitted\n"
  May 12 12:48:31.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8519" for this suite. @ 05/12/23 12:48:31.904
• [4.072 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 05/12/23 12:48:31.916
  May 12 12:48:31.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 12:48:31.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:31.931
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:31.933
  STEP: creating Agnhost RC @ 05/12/23 12:48:31.934
  May 12 12:48:31.935: INFO: namespace kubectl-4048
  May 12 12:48:31.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-4048 create -f -'
  May 12 12:48:32.948: INFO: stderr: ""
  May 12 12:48:32.948: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/12/23 12:48:32.948
  May 12 12:48:33.957: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 12:48:33.957: INFO: Found 0 / 1
  May 12 12:48:34.955: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 12:48:34.955: INFO: Found 1 / 1
  May 12 12:48:34.955: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 12 12:48:34.961: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 12:48:34.961: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 12 12:48:34.961: INFO: wait on agnhost-primary startup in kubectl-4048 
  May 12 12:48:34.961: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-4048 logs agnhost-primary-c45cr agnhost-primary'
  May 12 12:48:35.036: INFO: stderr: ""
  May 12 12:48:35.036: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 05/12/23 12:48:35.036
  May 12 12:48:35.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-4048 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  May 12 12:48:35.152: INFO: stderr: ""
  May 12 12:48:35.152: INFO: stdout: "service/rm2 exposed\n"
  May 12 12:48:35.155: INFO: Service rm2 in namespace kubectl-4048 found.
  STEP: exposing service @ 05/12/23 12:48:37.172
  May 12 12:48:37.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-4048 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  May 12 12:48:37.284: INFO: stderr: ""
  May 12 12:48:37.284: INFO: stdout: "service/rm3 exposed\n"
  May 12 12:48:37.286: INFO: Service rm3 in namespace kubectl-4048 found.
  May 12 12:48:39.289: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4048" for this suite. @ 05/12/23 12:48:39.292
• [7.385 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 05/12/23 12:48:39.301
  May 12 12:48:39.301: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/23 12:48:39.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:39.316
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:39.317
  May 12 12:48:39.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4667" for this suite. @ 05/12/23 12:48:39.346
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 05/12/23 12:48:39.351
  May 12 12:48:39.351: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename watch @ 05/12/23 12:48:39.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:39.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:39.366
  STEP: getting a starting resourceVersion @ 05/12/23 12:48:39.367
  STEP: starting a background goroutine to produce watch events @ 05/12/23 12:48:39.368
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 05/12/23 12:48:39.368
  May 12 12:48:42.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1954" for this suite. @ 05/12/23 12:48:42.205
• [2.910 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 05/12/23 12:48:42.261
  May 12 12:48:42.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename endpointslice @ 05/12/23 12:48:42.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:48:42.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:48:42.279
  STEP: referencing a single matching pod @ 05/12/23 12:48:47.366
  STEP: referencing matching pods with named port @ 05/12/23 12:48:52.37
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 05/12/23 12:48:57.374
  STEP: recreating EndpointSlices after they've been deleted @ 05/12/23 12:49:02.378
  May 12 12:49:02.390: INFO: EndpointSlice for Service endpointslice-5066/example-named-port not found
  May 12 12:49:12.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5066" for this suite. @ 05/12/23 12:49:12.397
• [30.139 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 05/12/23 12:49:12.403
  May 12 12:49:12.403: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 12:49:12.403
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:12.42
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:12.421
  STEP: Creating projection with secret that has name projected-secret-test-map-fdfb1f0a-09b1-488e-9225-094a4bd67c83 @ 05/12/23 12:49:12.423
  STEP: Creating a pod to test consume secrets @ 05/12/23 12:49:12.425
  STEP: Saw pod success @ 05/12/23 12:49:16.457
  May 12 12:49:16.459: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-secrets-436ddc87-255a-49be-beba-cbc1e5b5ab67 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 12:49:16.465
  May 12 12:49:16.475: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-714" for this suite. @ 05/12/23 12:49:16.478
• [4.081 seconds]
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 05/12/23 12:49:16.484
  May 12 12:49:16.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename endpointslice @ 05/12/23 12:49:16.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:16.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:16.499
  STEP: getting /apis @ 05/12/23 12:49:16.5
  STEP: getting /apis/discovery.k8s.io @ 05/12/23 12:49:16.504
  STEP: getting /apis/discovery.k8s.iov1 @ 05/12/23 12:49:16.504
  STEP: creating @ 05/12/23 12:49:16.505
  STEP: getting @ 05/12/23 12:49:16.514
  STEP: listing @ 05/12/23 12:49:16.517
  STEP: watching @ 05/12/23 12:49:16.518
  May 12 12:49:16.518: INFO: starting watch
  STEP: cluster-wide listing @ 05/12/23 12:49:16.519
  STEP: cluster-wide watching @ 05/12/23 12:49:16.52
  May 12 12:49:16.520: INFO: starting watch
  STEP: patching @ 05/12/23 12:49:16.521
  STEP: updating @ 05/12/23 12:49:16.524
  May 12 12:49:16.528: INFO: waiting for watch events with expected annotations
  May 12 12:49:16.528: INFO: saw patched and updated annotations
  STEP: deleting @ 05/12/23 12:49:16.528
  STEP: deleting a collection @ 05/12/23 12:49:16.535
  May 12 12:49:16.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6341" for this suite. @ 05/12/23 12:49:16.546
• [0.069 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 05/12/23 12:49:16.554
  May 12 12:49:16.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 12:49:16.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:16.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:16.567
  STEP: creating service in namespace services-7163 @ 05/12/23 12:49:16.568
  STEP: creating service affinity-nodeport-transition in namespace services-7163 @ 05/12/23 12:49:16.569
  STEP: creating replication controller affinity-nodeport-transition in namespace services-7163 @ 05/12/23 12:49:16.578
  I0512 12:49:16.586023      20 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-7163, replica count: 3
  I0512 12:49:19.650962      20 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 12:49:19.667: INFO: Creating new exec pod
  May 12 12:49:22.687: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-7163 exec execpod-affinitywfnlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  May 12 12:49:22.899: INFO: stderr: "+ nc -v -t -w 2 affinity-nodeport-transition 80\n+ echo hostName\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  May 12 12:49:22.899: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 12:49:22.899: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-7163 exec execpod-affinitywfnlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.238.74 80'
  May 12 12:49:23.015: INFO: stderr: "+ + nc -vecho -t -w hostName 2\n 10.43.238.74 80\nConnection to 10.43.238.74 80 port [tcp/http] succeeded!\n"
  May 12 12:49:23.015: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 12:49:23.015: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-7163 exec execpod-affinitywfnlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.5 31053'
  May 12 12:49:23.125: INFO: stderr: "+ nc -v -t -w 2 172.16.100.5 31053\n+ echo hostName\nConnection to 172.16.100.5 31053 port [tcp/*] succeeded!\n"
  May 12 12:49:23.125: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 12:49:23.125: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-7163 exec execpod-affinitywfnlv -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.7 31053'
  May 12 12:49:23.248: INFO: stderr: "+ nc -v -t -w 2 172.16.100.7 31053\nConnection to 172.16.100.7 31053 port [tcp/*] succeeded!\n+ echo hostName\n"
  May 12 12:49:23.248: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 12:49:23.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-7163 exec execpod-affinitywfnlv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.100.5:31053/ ; done'
  May 12 12:49:23.432: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n"
  May 12 12:49:23.432: INFO: stdout: "\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-dzwrf\naffinity-nodeport-transition-dzwrf\naffinity-nodeport-transition-wr49b\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-wr49b\naffinity-nodeport-transition-wr49b\naffinity-nodeport-transition-wr49b\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-wr49b\naffinity-nodeport-transition-wr49b"
  May 12 12:49:23.432: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-dzwrf
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-dzwrf
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-wr49b
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-wr49b
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-wr49b
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-wr49b
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-wr49b
  May 12 12:49:23.433: INFO: Received response from host: affinity-nodeport-transition-wr49b
  May 12 12:49:23.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-7163 exec execpod-affinitywfnlv -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.100.5:31053/ ; done'
  May 12 12:49:23.677: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31053/\n"
  May 12 12:49:23.679: INFO: stdout: "\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9\naffinity-nodeport-transition-x8rx9"
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.679: INFO: Received response from host: affinity-nodeport-transition-x8rx9
  May 12 12:49:23.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 12:49:23.683: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-7163, will wait for the garbage collector to delete the pods @ 05/12/23 12:49:23.695
  May 12 12:49:23.751: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.062601ms
  May 12 12:49:23.853: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.636123ms
  STEP: Destroying namespace "services-7163" for this suite. @ 05/12/23 12:49:25.702
• [9.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 05/12/23 12:49:25.717
  May 12 12:49:25.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 12:49:25.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:25.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:25.735
  STEP: Creating configMap that has name configmap-test-emptyKey-7f35f82c-b351-4e17-be22-4eaf8559bd92 @ 05/12/23 12:49:25.736
  May 12 12:49:25.737: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3594" for this suite. @ 05/12/23 12:49:25.74
• [0.025 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 05/12/23 12:49:25.745
  May 12 12:49:25.745: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 12:49:25.745
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:25.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:25.758
  STEP: Creating a pod to test downward api env vars @ 05/12/23 12:49:25.759
  STEP: Saw pod success @ 05/12/23 12:49:29.772
  May 12 12:49:29.773: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downward-api-1629b487-1db6-44a2-a514-fd1ffd601930 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 12:49:29.778
  May 12 12:49:29.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1165" for this suite. @ 05/12/23 12:49:29.793
• [4.053 seconds]
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 05/12/23 12:49:29.797
  May 12 12:49:29.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename dns @ 05/12/23 12:49:29.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:29.811
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:29.812
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8304.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8304.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 05/12/23 12:49:29.814
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8304.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8304.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 05/12/23 12:49:29.814
  STEP: creating a pod to probe /etc/hosts @ 05/12/23 12:49:29.814
  STEP: submitting the pod to kubernetes @ 05/12/23 12:49:29.814
  STEP: retrieving the pod @ 05/12/23 12:49:31.826
  STEP: looking for the results for each expected name from probers @ 05/12/23 12:49:31.827
  May 12 12:49:31.837: INFO: DNS probes using dns-8304/dns-test-55353ffe-f1f9-4657-8351-e6c8f695ed0a succeeded

  May 12 12:49:31.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 12:49:31.841
  STEP: Destroying namespace "dns-8304" for this suite. @ 05/12/23 12:49:31.851
• [2.064 seconds]
------------------------------
SS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 05/12/23 12:49:31.862
  May 12 12:49:31.862: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename prestop @ 05/12/23 12:49:31.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:31.878
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:31.881
  STEP: Creating server pod server in namespace prestop-5699 @ 05/12/23 12:49:31.883
  STEP: Waiting for pods to come up. @ 05/12/23 12:49:31.888
  STEP: Creating tester pod tester in namespace prestop-5699 @ 05/12/23 12:49:33.917
  STEP: Deleting pre-stop pod @ 05/12/23 12:49:35.948
  May 12 12:49:40.964: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  May 12 12:49:40.965: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 05/12/23 12:49:40.968
  STEP: Destroying namespace "prestop-5699" for this suite. @ 05/12/23 12:49:40.98
• [9.122 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 05/12/23 12:49:40.986
  May 12 12:49:40.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-runtime @ 05/12/23 12:49:40.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:41.001
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:41.003
  STEP: create the container @ 05/12/23 12:49:41.005
  W0512 12:49:41.009364      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/12/23 12:49:41.009
  STEP: get the container status @ 05/12/23 12:49:44.034
  STEP: the container should be terminated @ 05/12/23 12:49:44.036
  STEP: the termination message should be set @ 05/12/23 12:49:44.036
  May 12 12:49:44.036: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 05/12/23 12:49:44.037
  May 12 12:49:44.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8046" for this suite. @ 05/12/23 12:49:44.057
• [3.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 05/12/23 12:49:44.063
  May 12 12:49:44.063: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename dns @ 05/12/23 12:49:44.063
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:49:44.077
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:49:44.078
  STEP: Creating a test headless service @ 05/12/23 12:49:44.08
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6025.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6025.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6025.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6025.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-6025.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 113.242.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.242.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.242.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.242.113_tcp@PTR;sleep 1; done
   @ 05/12/23 12:49:44.094
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-6025.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-6025.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-6025.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-6025.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-6025.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-6025.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-6025.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-6025.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 113.242.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.242.113_udp@PTR;check="$$(dig +tcp +noall +answer +search 113.242.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.242.113_tcp@PTR;sleep 1; done
   @ 05/12/23 12:49:44.095
  STEP: creating a pod to probe DNS @ 05/12/23 12:49:44.095
  STEP: submitting the pod to kubernetes @ 05/12/23 12:49:44.095
  STEP: retrieving the pod @ 05/12/23 12:49:46.126
  STEP: looking for the results for each expected name from probers @ 05/12/23 12:49:46.128
  May 12 12:49:46.133: INFO: Unable to read wheezy_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:46.136: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:46.138: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:46.141: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:46.151: INFO: Unable to read jessie_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:46.153: INFO: Unable to read jessie_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:46.155: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:46.158: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:46.166: INFO: Lookups using dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f failed for: [wheezy_udp@dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_udp@dns-test-service.dns-6025.svc.cluster.local jessie_tcp@dns-test-service.dns-6025.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local]

  May 12 12:49:51.178: INFO: Unable to read wheezy_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:51.189: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:51.195: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:51.199: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:51.212: INFO: Unable to read jessie_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:51.214: INFO: Unable to read jessie_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:51.216: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:51.218: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:51.242: INFO: Lookups using dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f failed for: [wheezy_udp@dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_udp@dns-test-service.dns-6025.svc.cluster.local jessie_tcp@dns-test-service.dns-6025.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local]

  May 12 12:49:56.177: INFO: Unable to read wheezy_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:56.182: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:56.187: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:56.191: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:56.215: INFO: Unable to read jessie_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:56.219: INFO: Unable to read jessie_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:56.225: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:56.227: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:49:56.235: INFO: Lookups using dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f failed for: [wheezy_udp@dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_udp@dns-test-service.dns-6025.svc.cluster.local jessie_tcp@dns-test-service.dns-6025.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local]

  May 12 12:50:01.173: INFO: Unable to read wheezy_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:01.175: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:01.177: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:01.179: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:01.188: INFO: Unable to read jessie_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:01.190: INFO: Unable to read jessie_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:01.193: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:01.195: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:01.202: INFO: Lookups using dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f failed for: [wheezy_udp@dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_udp@dns-test-service.dns-6025.svc.cluster.local jessie_tcp@dns-test-service.dns-6025.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local]

  May 12 12:50:06.183: INFO: Unable to read wheezy_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:06.192: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:06.202: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:06.207: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:06.223: INFO: Unable to read jessie_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:06.225: INFO: Unable to read jessie_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:06.227: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:06.229: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:06.236: INFO: Lookups using dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f failed for: [wheezy_udp@dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_udp@dns-test-service.dns-6025.svc.cluster.local jessie_tcp@dns-test-service.dns-6025.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local]

  May 12 12:50:11.190: INFO: Unable to read wheezy_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:11.194: INFO: Unable to read wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:11.197: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:11.200: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:11.211: INFO: Unable to read jessie_udp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:11.213: INFO: Unable to read jessie_tcp@dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:11.215: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:11.217: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:11.223: INFO: Lookups using dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f failed for: [wheezy_udp@dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@dns-test-service.dns-6025.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_udp@dns-test-service.dns-6025.svc.cluster.local jessie_tcp@dns-test-service.dns-6025.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local]

  May 12 12:50:16.185: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local from pod dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f: the server could not find the requested resource (get pods dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f)
  May 12 12:50:16.216: INFO: Lookups using dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f failed for: [wheezy_tcp@_http._tcp.dns-test-service.dns-6025.svc.cluster.local]

  May 12 12:50:21.216: INFO: DNS probes using dns-6025/dns-test-381d17cd-276d-4bf7-95ba-e0888c1ce85f succeeded

  May 12 12:50:21.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 12:50:21.219
  STEP: deleting the test service @ 05/12/23 12:50:21.234
  STEP: deleting the test headless service @ 05/12/23 12:50:21.268
  STEP: Destroying namespace "dns-6025" for this suite. @ 05/12/23 12:50:21.289
• [37.235 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 05/12/23 12:50:21.3
  May 12 12:50:21.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 12:50:21.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:50:21.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:50:21.323
  STEP: Creating resourceQuota "e2e-rq-status-m2vcq" @ 05/12/23 12:50:21.326
  May 12 12:50:21.332: INFO: Resource quota "e2e-rq-status-m2vcq" reports spec: hard cpu limit of 500m
  May 12 12:50:21.332: INFO: Resource quota "e2e-rq-status-m2vcq" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-m2vcq" /status @ 05/12/23 12:50:21.332
  STEP: Confirm /status for "e2e-rq-status-m2vcq" resourceQuota via watch @ 05/12/23 12:50:21.336
  May 12 12:50:21.337: INFO: observed resourceQuota "e2e-rq-status-m2vcq" in namespace "resourcequota-6382" with hard status: v1.ResourceList(nil)
  May 12 12:50:21.337: INFO: Found resourceQuota "e2e-rq-status-m2vcq" in namespace "resourcequota-6382" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 12 12:50:21.337: INFO: ResourceQuota "e2e-rq-status-m2vcq" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 05/12/23 12:50:21.338
  May 12 12:50:21.343: INFO: Resource quota "e2e-rq-status-m2vcq" reports spec: hard cpu limit of 1
  May 12 12:50:21.343: INFO: Resource quota "e2e-rq-status-m2vcq" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-m2vcq" /status @ 05/12/23 12:50:21.343
  STEP: Confirm /status for "e2e-rq-status-m2vcq" resourceQuota via watch @ 05/12/23 12:50:21.347
  May 12 12:50:21.348: INFO: observed resourceQuota "e2e-rq-status-m2vcq" in namespace "resourcequota-6382" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  May 12 12:50:21.348: INFO: Found resourceQuota "e2e-rq-status-m2vcq" in namespace "resourcequota-6382" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  May 12 12:50:21.348: INFO: ResourceQuota "e2e-rq-status-m2vcq" /status was patched
  STEP: Get "e2e-rq-status-m2vcq" /status @ 05/12/23 12:50:21.348
  May 12 12:50:21.349: INFO: Resourcequota "e2e-rq-status-m2vcq" reports status: hard cpu of 1
  May 12 12:50:21.349: INFO: Resourcequota "e2e-rq-status-m2vcq" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-m2vcq" /status before checking Spec is unchanged @ 05/12/23 12:50:21.35
  May 12 12:50:21.353: INFO: Resourcequota "e2e-rq-status-m2vcq" reports status: hard cpu of 2
  May 12 12:50:21.353: INFO: Resourcequota "e2e-rq-status-m2vcq" reports status: hard memory of 2Gi
  May 12 12:50:21.354: INFO: Found resourceQuota "e2e-rq-status-m2vcq" in namespace "resourcequota-6382" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  May 12 12:55:06.362: INFO: ResourceQuota "e2e-rq-status-m2vcq" Spec was unchanged and /status reset
  May 12 12:55:06.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6382" for this suite. @ 05/12/23 12:55:06.365
• [285.068 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 05/12/23 12:55:06.369
  May 12 12:55:06.369: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 12:55:06.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:55:06.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:55:06.383
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 05/12/23 12:55:06.386
  STEP: Saw pod success @ 05/12/23 12:55:10.415
  May 12 12:55:10.423: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-76fa6a9e-63c2-401b-b197-e2f1b05633e7 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 12:55:10.46
  May 12 12:55:10.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8366" for this suite. @ 05/12/23 12:55:10.48
• [4.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 05/12/23 12:55:10.487
  May 12 12:55:10.487: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 12:55:10.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:55:10.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:55:10.502
  STEP: Creating secret with name s-test-opt-del-02b1cca4-bd14-4082-87a4-d898727f6b79 @ 05/12/23 12:55:10.505
  STEP: Creating secret with name s-test-opt-upd-dbc17f87-cbdf-47d0-a4c0-990b519dfae9 @ 05/12/23 12:55:10.509
  STEP: Creating the pod @ 05/12/23 12:55:10.514
  STEP: Deleting secret s-test-opt-del-02b1cca4-bd14-4082-87a4-d898727f6b79 @ 05/12/23 12:55:12.544
  STEP: Updating secret s-test-opt-upd-dbc17f87-cbdf-47d0-a4c0-990b519dfae9 @ 05/12/23 12:55:12.548
  STEP: Creating secret with name s-test-opt-create-0adc3937-4d91-435d-940d-6863d1ff2b25 @ 05/12/23 12:55:12.553
  STEP: waiting to observe update in volume @ 05/12/23 12:55:12.557
  May 12 12:55:14.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4956" for this suite. @ 05/12/23 12:55:14.609
• [4.130 seconds]
------------------------------
S
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 05/12/23 12:55:14.618
  May 12 12:55:14.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 12:55:14.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:55:14.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:55:14.638
  STEP: Creating projection with secret that has name projected-secret-test-24351a74-08ee-46f2-bc71-4ff48d2b1260 @ 05/12/23 12:55:14.639
  STEP: Creating a pod to test consume secrets @ 05/12/23 12:55:14.644
  STEP: Saw pod success @ 05/12/23 12:55:18.675
  May 12 12:55:18.687: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-secrets-00c9c514-f8e7-4f91-beeb-aacaa27ac3cb container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 12:55:18.701
  May 12 12:55:18.721: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7241" for this suite. @ 05/12/23 12:55:18.726
• [4.113 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 05/12/23 12:55:18.732
  May 12 12:55:18.732: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 12:55:18.733
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:55:18.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:55:18.746
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 12:55:18.747
  STEP: Saw pod success @ 05/12/23 12:55:22.763
  May 12 12:55:22.768: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-3cf78826-182c-4a21-afdc-eccdcd644f1d container client-container: <nil>
  STEP: delete the pod @ 05/12/23 12:55:22.775
  May 12 12:55:22.816: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-176" for this suite. @ 05/12/23 12:55:22.819
• [4.091 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 05/12/23 12:55:22.827
  May 12 12:55:22.827: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename namespaces @ 05/12/23 12:55:22.828
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:55:22.84
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:55:22.842
  STEP: creating a Namespace @ 05/12/23 12:55:22.843
  STEP: patching the Namespace @ 05/12/23 12:55:22.861
  STEP: get the Namespace and ensuring it has the label @ 05/12/23 12:55:22.864
  May 12 12:55:22.866: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4883" for this suite. @ 05/12/23 12:55:22.869
  STEP: Destroying namespace "nspatchtest-bccd03e2-d0d8-4762-860e-e449931c09a1-485" for this suite. @ 05/12/23 12:55:22.873
• [0.050 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 05/12/23 12:55:22.88
  May 12 12:55:22.880: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename cronjob @ 05/12/23 12:55:22.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 12:55:22.892
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 12:55:22.895
  STEP: Creating a suspended cronjob @ 05/12/23 12:55:22.897
  STEP: Ensuring no jobs are scheduled @ 05/12/23 12:55:22.901
  STEP: Ensuring no job exists by listing jobs explicitly @ 05/12/23 13:00:22.912
  STEP: Removing cronjob @ 05/12/23 13:00:22.914
  May 12 13:00:22.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3891" for this suite. @ 05/12/23 13:00:22.925
• [300.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 05/12/23 13:00:22.932
  May 12 13:00:22.932: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 13:00:22.933
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:22.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:22.946
  STEP: Creating a ResourceQuota @ 05/12/23 13:00:22.948
  STEP: Getting a ResourceQuota @ 05/12/23 13:00:22.952
  STEP: Updating a ResourceQuota @ 05/12/23 13:00:22.954
  STEP: Verifying a ResourceQuota was modified @ 05/12/23 13:00:22.956
  STEP: Deleting a ResourceQuota @ 05/12/23 13:00:22.958
  STEP: Verifying the deleted ResourceQuota @ 05/12/23 13:00:22.96
  May 12 13:00:22.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1885" for this suite. @ 05/12/23 13:00:22.963
• [0.035 seconds]
------------------------------
S
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 05/12/23 13:00:22.967
  May 12 13:00:22.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 13:00:22.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:22.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:22.982
  STEP: Creating the pod @ 05/12/23 13:00:22.984
  May 12 13:00:25.516: INFO: Successfully updated pod "labelsupdate10df9ad1-4988-4bdf-a869-aba9aeb82423"
  May 12 13:00:29.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5153" for this suite. @ 05/12/23 13:00:29.545
• [6.585 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 05/12/23 13:00:29.555
  May 12 13:00:29.555: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:00:29.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:29.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:29.571
  STEP: Creating configMap with name projected-configmap-test-volume-map-ee6163e0-2461-4e02-8766-20eae7141f7f @ 05/12/23 13:00:29.572
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:00:29.575
  STEP: Saw pod success @ 05/12/23 13:00:33.589
  May 12 13:00:33.594: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-configmaps-521f0e78-65ff-467a-8b71-02ef785aae02 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:00:33.598
  May 12 13:00:33.613: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1556" for this suite. @ 05/12/23 13:00:33.617
• [4.068 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 05/12/23 13:00:33.626
  May 12 13:00:33.626: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replication-controller @ 05/12/23 13:00:33.627
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:33.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:33.645
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 05/12/23 13:00:33.646
  STEP: When a replication controller with a matching selector is created @ 05/12/23 13:00:35.659
  STEP: Then the orphan pod is adopted @ 05/12/23 13:00:35.664
  May 12 13:00:36.681: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7459" for this suite. @ 05/12/23 13:00:36.697
• [3.079 seconds]
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 05/12/23 13:00:36.705
  May 12 13:00:36.705: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:00:36.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:36.723
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:36.725
  May 12 13:00:36.747: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-887" for this suite. @ 05/12/23 13:00:36.75
• [0.048 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 05/12/23 13:00:36.754
  May 12 13:00:36.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/23 13:00:36.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:36.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:36.768
  May 12 13:00:40.779: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1888" for this suite. @ 05/12/23 13:00:40.782
• [4.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 05/12/23 13:00:40.789
  May 12 13:00:40.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename daemonsets @ 05/12/23 13:00:40.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:40.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:40.806
  STEP: Creating a simple DaemonSet "daemon-set" @ 05/12/23 13:00:40.819
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/23 13:00:40.821
  May 12 13:00:40.828: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:00:40.828: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:00:40.831: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:00:40.831: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 13:00:41.833: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:00:41.833: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:00:41.835: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 13:00:41.835: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 05/12/23 13:00:41.837
  May 12 13:00:41.858: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:00:41.859: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:00:41.878: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 13:00:41.878: INFO: Node onekube-ip-172-16-100-7 is running 0 daemon pod, expected 1
  May 12 13:00:42.890: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:00:42.891: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:00:42.901: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 13:00:42.902: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 05/12/23 13:00:42.902
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/23 13:00:42.913
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-1224, will wait for the garbage collector to delete the pods @ 05/12/23 13:00:42.913
  May 12 13:00:42.986: INFO: Deleting DaemonSet.extensions daemon-set took: 19.07039ms
  May 12 13:00:43.086: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.308233ms
  May 12 13:00:45.889: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:00:45.889: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 12 13:00:45.891: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"80983"},"items":null}

  May 12 13:00:45.892: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"80983"},"items":null}

  May 12 13:00:45.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-1224" for this suite. @ 05/12/23 13:00:45.9
• [5.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 05/12/23 13:00:45.91
  May 12 13:00:45.910: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:00:45.911
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:45.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:45.932
  STEP: Creating projection with secret that has name projected-secret-test-5aa38cc8-0788-4558-9ae9-65badc581fff @ 05/12/23 13:00:45.935
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:00:45.939
  STEP: Saw pod success @ 05/12/23 13:00:49.973
  May 12 13:00:49.979: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-secrets-9e3d66a7-6b09-48ba-94c8-8204ca6e6c5b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:00:49.999
  May 12 13:00:50.033: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9722" for this suite. @ 05/12/23 13:00:50.037
• [4.133 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 05/12/23 13:00:50.043
  May 12 13:00:50.043: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename aggregator @ 05/12/23 13:00:50.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:00:50.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:00:50.061
  May 12 13:00:50.062: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Registering the sample API server. @ 05/12/23 13:00:50.063
  May 12 13:00:50.382: INFO: Found ClusterRoles; assuming RBAC is enabled.
  May 12 13:00:50.399: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  May 12 13:00:52.452: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:00:54.460: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:00:56.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:00:58.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:00.459: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:02.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:04.462: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:06.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:08.464: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:10.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:12.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:14.463: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:16.456: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:18.466: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:20.468: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:22.454: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:24.467: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:26.455: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 0, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  May 12 13:01:28.636: INFO: Waited 131.409892ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 05/12/23 13:01:28.674
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 05/12/23 13:01:28.675
  STEP: List APIServices @ 05/12/23 13:01:28.681
  May 12 13:01:28.688: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 05/12/23 13:01:28.689
  May 12 13:01:28.698: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 05/12/23 13:01:28.698
  May 12 13:01:28.704: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.May, 12, 13, 1, 28, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 05/12/23 13:01:28.705
  May 12 13:01:28.708: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-05-12 13:01:28 +0000 UTC Passed all checks passed}
  May 12 13:01:28.708: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 12 13:01:28.708: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 05/12/23 13:01:28.709
  May 12 13:01:28.715: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1273251827" @ 05/12/23 13:01:28.715
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 05/12/23 13:01:28.735
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 05/12/23 13:01:28.739
  STEP: Patch APIService Status @ 05/12/23 13:01:28.741
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 05/12/23 13:01:28.745
  May 12 13:01:28.754: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-05-12 13:01:28 +0000 UTC Passed all checks passed}
  May 12 13:01:28.754: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 12 13:01:28.754: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  May 12 13:01:28.755: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 05/12/23 13:01:28.755
  STEP: Confirm that the generated APIService has been deleted @ 05/12/23 13:01:28.759
  May 12 13:01:28.759: INFO: Requesting list of APIServices to confirm quantity
  May 12 13:01:28.761: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  May 12 13:01:28.762: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  May 12 13:01:28.850: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-9543" for this suite. @ 05/12/23 13:01:28.862
• [38.823 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 05/12/23 13:01:28.872
  May 12 13:01:28.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-runtime @ 05/12/23 13:01:28.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:01:28.884
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:01:28.887
  STEP: create the container @ 05/12/23 13:01:28.889
  W0512 13:01:28.893046      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 05/12/23 13:01:28.893
  STEP: get the container status @ 05/12/23 13:01:31.906
  STEP: the container should be terminated @ 05/12/23 13:01:31.908
  STEP: the termination message should be set @ 05/12/23 13:01:31.908
  May 12 13:01:31.908: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/12/23 13:01:31.908
  May 12 13:01:31.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9603" for this suite. @ 05/12/23 13:01:31.921
• [3.053 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 05/12/23 13:01:31.926
  May 12 13:01:31.926: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:01:31.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:01:31.94
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:01:31.942
  STEP: Creating configMap with name configmap-test-volume-a7a678b4-5898-409a-b1e7-b5bca6c29ff5 @ 05/12/23 13:01:31.943
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:01:31.946
  STEP: Saw pod success @ 05/12/23 13:01:35.967
  May 12 13:01:35.976: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-8ee0f597-95ef-40ab-9cfe-4c1156f0fd59 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:01:36.006
  May 12 13:01:36.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9625" for this suite. @ 05/12/23 13:01:36.026
• [4.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 05/12/23 13:01:36.034
  May 12 13:01:36.034: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:01:36.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:01:36.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:01:36.049
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 05/12/23 13:01:36.051
  STEP: Saw pod success @ 05/12/23 13:01:40.08
  May 12 13:01:40.088: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-c5d66700-9ac4-47fa-ac68-3cbc07bb6e37 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:01:40.099
  May 12 13:01:40.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6106" for this suite. @ 05/12/23 13:01:40.122
• [4.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 05/12/23 13:01:40.13
  May 12 13:01:40.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:01:40.131
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:01:40.144
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:01:40.146
  STEP: Creating configMap with name projected-configmap-test-volume-f7a9b3b2-1a7e-48a8-b951-61128cdcb206 @ 05/12/23 13:01:40.148
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:01:40.151
  STEP: Saw pod success @ 05/12/23 13:01:44.193
  May 12 13:01:44.196: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-configmaps-3ce7c40b-8441-4d2e-b5e9-ac51535f8a05 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:01:44.2
  May 12 13:01:44.212: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3087" for this suite. @ 05/12/23 13:01:44.216
• [4.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 05/12/23 13:01:44.221
  May 12 13:01:44.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:01:44.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:01:44.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:01:44.236
  STEP: creating all guestbook components @ 05/12/23 13:01:44.237
  May 12 13:01:44.237: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  May 12 13:01:44.238: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 create -f -'
  May 12 13:01:44.611: INFO: stderr: ""
  May 12 13:01:44.611: INFO: stdout: "service/agnhost-replica created\n"
  May 12 13:01:44.611: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  May 12 13:01:44.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 create -f -'
  May 12 13:01:44.887: INFO: stderr: ""
  May 12 13:01:44.887: INFO: stdout: "service/agnhost-primary created\n"
  May 12 13:01:44.888: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  May 12 13:01:44.888: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 create -f -'
  May 12 13:01:45.246: INFO: stderr: ""
  May 12 13:01:45.246: INFO: stdout: "service/frontend created\n"
  May 12 13:01:45.246: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  May 12 13:01:45.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 create -f -'
  May 12 13:01:45.551: INFO: stderr: ""
  May 12 13:01:45.551: INFO: stdout: "deployment.apps/frontend created\n"
  May 12 13:01:45.551: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 12 13:01:45.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 create -f -'
  May 12 13:01:45.891: INFO: stderr: ""
  May 12 13:01:45.891: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  May 12 13:01:45.891: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  May 12 13:01:45.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 create -f -'
  May 12 13:01:46.212: INFO: stderr: ""
  May 12 13:01:46.212: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 05/12/23 13:01:46.212
  May 12 13:01:46.213: INFO: Waiting for all frontend pods to be Running.
  May 12 13:01:51.274: INFO: Waiting for frontend to serve content.
  May 12 13:01:51.283: INFO: Trying to add a new entry to the guestbook.
  May 12 13:01:51.291: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 05/12/23 13:01:51.296
  May 12 13:01:51.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 delete --grace-period=0 --force -f -'
  May 12 13:01:51.367: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 13:01:51.367: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/23 13:01:51.367
  May 12 13:01:51.367: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 delete --grace-period=0 --force -f -'
  May 12 13:01:51.437: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 13:01:51.437: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/23 13:01:51.437
  May 12 13:01:51.438: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 delete --grace-period=0 --force -f -'
  May 12 13:01:51.501: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 13:01:51.501: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/23 13:01:51.502
  May 12 13:01:51.502: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 delete --grace-period=0 --force -f -'
  May 12 13:01:51.570: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 13:01:51.570: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/23 13:01:51.57
  May 12 13:01:51.570: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 delete --grace-period=0 --force -f -'
  May 12 13:01:51.627: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 13:01:51.627: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 05/12/23 13:01:51.627
  May 12 13:01:51.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3853 delete --grace-period=0 --force -f -'
  May 12 13:01:51.692: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 13:01:51.692: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  May 12 13:01:51.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3853" for this suite. @ 05/12/23 13:01:51.703
• [7.486 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 05/12/23 13:01:51.709
  May 12 13:01:51.709: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:01:51.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:01:51.722
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:01:51.724
  STEP: Setting up server cert @ 05/12/23 13:01:51.739
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:01:52.437
  STEP: Deploying the webhook pod @ 05/12/23 13:01:52.444
  STEP: Wait for the deployment to be ready @ 05/12/23 13:01:52.455
  May 12 13:01:52.467: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/23 13:01:54.489
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:01:54.516
  May 12 13:01:55.518: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 05/12/23 13:01:55.526
  STEP: create a configmap that should be updated by the webhook @ 05/12/23 13:01:55.566
  May 12 13:01:55.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8992" for this suite. @ 05/12/23 13:01:55.63
  STEP: Destroying namespace "webhook-markers-9411" for this suite. @ 05/12/23 13:01:55.637
• [3.931 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 05/12/23 13:01:55.642
  May 12 13:01:55.642: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename containers @ 05/12/23 13:01:55.642
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:01:55.654
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:01:55.656
  STEP: Creating a pod to test override command @ 05/12/23 13:01:55.657
  STEP: Saw pod success @ 05/12/23 13:01:59.679
  May 12 13:01:59.690: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod client-containers-51534b8e-f0c0-42ad-a8d0-57641ff52bab container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:01:59.71
  May 12 13:01:59.732: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9928" for this suite. @ 05/12/23 13:01:59.737
• [4.099 seconds]
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 05/12/23 13:01:59.742
  May 12 13:01:59.742: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sysctl @ 05/12/23 13:01:59.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:01:59.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:01:59.757
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 05/12/23 13:01:59.759
  STEP: Watching for error events or started pod @ 05/12/23 13:01:59.764
  STEP: Waiting for pod completion @ 05/12/23 13:02:01.767
  STEP: Checking that the pod succeeded @ 05/12/23 13:02:03.772
  STEP: Getting logs from the pod @ 05/12/23 13:02:03.772
  STEP: Checking that the sysctl is actually updated @ 05/12/23 13:02:03.775
  May 12 13:02:03.775: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-5424" for this suite. @ 05/12/23 13:02:03.792
• [4.057 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 05/12/23 13:02:03.802
  May 12 13:02:03.802: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename deployment @ 05/12/23 13:02:03.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:03.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:03.827
  May 12 13:02:03.831: INFO: Creating simple deployment test-new-deployment
  May 12 13:02:03.854: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 05/12/23 13:02:05.877
  STEP: updating a scale subresource @ 05/12/23 13:02:05.883
  STEP: verifying the deployment Spec.Replicas was modified @ 05/12/23 13:02:05.894
  STEP: Patch a scale subresource @ 05/12/23 13:02:05.902
  May 12 13:02:05.933: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-4761  314f86a8-aaef-4d1d-a5a3-199566acecbe 82034 3 2023-05-12 13:02:03 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-05-12 13:02:03 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0052dd6f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-05-12 13:02:05 +0000 UTC,LastTransitionTime:2023-05-12 13:02:03 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-12 13:02:05 +0000 UTC,LastTransitionTime:2023-05-12 13:02:05 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 12 13:02:05.942: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-4761  c65b042f-1839-4be8-9028-a484add44fd0 82031 3 2023-05-12 13:02:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:4 deployment.kubernetes.io/max-replicas:5 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 314f86a8-aaef-4d1d-a5a3-199566acecbe 0xc00533dc67 0xc00533dc68}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"314f86a8-aaef-4d1d-a5a3-199566acecbe\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:02:05 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00533dcf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 12 13:02:05.952: INFO: Pod "test-new-deployment-67bd4bf6dc-677n2" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-677n2 test-new-deployment-67bd4bf6dc- deployment-4761  b880cc78-0c91-442a-84e7-54f4f012b3b2 82037 0 2023-05-12 13:02:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc c65b042f-1839-4be8-9028-a484add44fd0 0xc0052ddb27 0xc0052ddb28}] [] [{kube-controller-manager Update v1 2023-05-12 13:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c65b042f-1839-4be8-9028-a484add44fd0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjxjw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjxjw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:02:05.953: INFO: Pod "test-new-deployment-67bd4bf6dc-8g8bc" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-8g8bc test-new-deployment-67bd4bf6dc- deployment-4761  b97e40c7-8cd9-4928-844f-a9c3eaa562a1 82032 0 2023-05-12 13:02:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc c65b042f-1839-4be8-9028-a484add44fd0 0xc0052ddc80 0xc0052ddc81}] [] [{kube-controller-manager Update v1 2023-05-12 13:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c65b042f-1839-4be8-9028-a484add44fd0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:02:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q5rnr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q5rnr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:05 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:02:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:02:05.953: INFO: Pod "test-new-deployment-67bd4bf6dc-lr85j" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-lr85j test-new-deployment-67bd4bf6dc- deployment-4761  1980e479-8ba6-48e0-8c0a-b9a91107507f 82016 0 2023-05-12 13:02:03 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:40892e3e22fff90e6575ac7e1f56f744e0450672c124ef49f9d36b601857903b cni.projectcalico.org/podIP:10.42.3.153/32 cni.projectcalico.org/podIPs:10.42.3.153/32] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc c65b042f-1839-4be8-9028-a484add44fd0 0xc0052dde57 0xc0052dde58}] [] [{kube-controller-manager Update v1 2023-05-12 13:02:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c65b042f-1839-4be8-9028-a484add44fd0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:02:04 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:02:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.153\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nxd5r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nxd5r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.153,StartTime:2023-05-12 13:02:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:02:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://a4ab97bc0348a686913ab7a6209b7c488a223c16099ec0e022540cbdd178d346,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.153,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:02:05.953: INFO: Pod "test-new-deployment-67bd4bf6dc-vwntc" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-vwntc test-new-deployment-67bd4bf6dc- deployment-4761  f29afc89-4766-44a7-9c44-c1f36f30f93a 82038 0 2023-05-12 13:02:05 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc c65b042f-1839-4be8-9028-a484add44fd0 0xc0053c2050 0xc0053c2051}] [] [{kube-controller-manager Update v1 2023-05-12 13:02:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c65b042f-1839-4be8-9028-a484add44fd0\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2z6r9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2z6r9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:02:05.954: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4761" for this suite. @ 05/12/23 13:02:05.968
• [2.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 05/12/23 13:02:06.007
  May 12 13:02:06.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 13:02:06.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:06.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:06.035
  STEP: Creating secret with name secret-test-d454abe1-4aaa-435f-86f9-68b500e41ece @ 05/12/23 13:02:06.036
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:02:06.041
  STEP: Saw pod success @ 05/12/23 13:02:10.077
  May 12 13:02:10.086: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-secrets-40b7d51a-40e5-4bf6-ad40-57388534ce0c container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:02:10.102
  May 12 13:02:10.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-294" for this suite. @ 05/12/23 13:02:10.12
• [4.117 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 05/12/23 13:02:10.126
  May 12 13:02:10.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename limitrange @ 05/12/23 13:02:10.127
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:10.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:10.141
  STEP: Creating LimitRange "e2e-limitrange-sk9qj" in namespace "limitrange-6228" @ 05/12/23 13:02:10.142
  STEP: Creating another limitRange in another namespace @ 05/12/23 13:02:10.145
  May 12 13:02:10.155: INFO: Namespace "e2e-limitrange-sk9qj-1333" created
  May 12 13:02:10.155: INFO: Creating LimitRange "e2e-limitrange-sk9qj" in namespace "e2e-limitrange-sk9qj-1333"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-sk9qj" @ 05/12/23 13:02:10.159
  May 12 13:02:10.161: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-sk9qj" in "limitrange-6228" namespace @ 05/12/23 13:02:10.161
  May 12 13:02:10.164: INFO: LimitRange "e2e-limitrange-sk9qj" has been patched
  STEP: Delete LimitRange "e2e-limitrange-sk9qj" by Collection with labelSelector: "e2e-limitrange-sk9qj=patched" @ 05/12/23 13:02:10.164
  STEP: Confirm that the limitRange "e2e-limitrange-sk9qj" has been deleted @ 05/12/23 13:02:10.167
  May 12 13:02:10.167: INFO: Requesting list of LimitRange to confirm quantity
  May 12 13:02:10.168: INFO: Found 0 LimitRange with label "e2e-limitrange-sk9qj=patched"
  May 12 13:02:10.168: INFO: LimitRange "e2e-limitrange-sk9qj" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-sk9qj" @ 05/12/23 13:02:10.168
  May 12 13:02:10.169: INFO: Found 1 limitRange
  May 12 13:02:10.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-6228" for this suite. @ 05/12/23 13:02:10.172
  STEP: Destroying namespace "e2e-limitrange-sk9qj-1333" for this suite. @ 05/12/23 13:02:10.177
• [0.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 05/12/23 13:02:10.182
  May 12 13:02:10.183: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename containers @ 05/12/23 13:02:10.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:10.196
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:10.198
  STEP: Creating a pod to test override all @ 05/12/23 13:02:10.199
  STEP: Saw pod success @ 05/12/23 13:02:14.223
  May 12 13:02:14.231: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod client-containers-fafc7d72-bc6a-4759-b672-46fd3bce8134 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:02:14.241
  May 12 13:02:14.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-1687" for this suite. @ 05/12/23 13:02:14.259
• [4.082 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 05/12/23 13:02:14.266
  May 12 13:02:14.266: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename proxy @ 05/12/23 13:02:14.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:14.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:14.283
  STEP: starting an echo server on multiple ports @ 05/12/23 13:02:14.297
  STEP: creating replication controller proxy-service-hlz76 in namespace proxy-5134 @ 05/12/23 13:02:14.297
  I0512 13:02:14.309677      20 runners.go:194] Created replication controller with name: proxy-service-hlz76, namespace: proxy-5134, replica count: 1
  I0512 13:02:15.360157      20 runners.go:194] proxy-service-hlz76 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0512 13:02:16.361244      20 runners.go:194] proxy-service-hlz76 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 13:02:16.363: INFO: setup took 2.078108125s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 05/12/23 13:02:16.363
  May 12 13:02:16.401: INFO: (0) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 37.840959ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 38.951815ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 39.009456ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 36.865092ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 38.875191ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 38.790831ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 38.821811ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 38.851958ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 36.85699ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 38.77105ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 38.973766ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 38.746235ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 38.721709ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 38.81602ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 38.959402ms)
  May 12 13:02:16.402: INFO: (0) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 38.864981ms)
  May 12 13:02:16.416: INFO: (1) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 9.688715ms)
  May 12 13:02:16.416: INFO: (1) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 12.19981ms)
  May 12 13:02:16.418: INFO: (1) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 13.348301ms)
  May 12 13:02:16.418: INFO: (1) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 14.834622ms)
  May 12 13:02:16.418: INFO: (1) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 13.809546ms)
  May 12 13:02:16.418: INFO: (1) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 15.06225ms)
  May 12 13:02:16.418: INFO: (1) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 11.884214ms)
  May 12 13:02:16.418: INFO: (1) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 13.626864ms)
  May 12 13:02:16.418: INFO: (1) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 13.564652ms)
  May 12 13:02:16.421: INFO: (1) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 14.902128ms)
  May 12 13:02:16.421: INFO: (1) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 15.091838ms)
  May 12 13:02:16.421: INFO: (1) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 15.064725ms)
  May 12 13:02:16.421: INFO: (1) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 15.060341ms)
  May 12 13:02:16.421: INFO: (1) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 15.146422ms)
  May 12 13:02:16.425: INFO: (1) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 11.265833ms)
  May 12 13:02:16.425: INFO: (1) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 11.334098ms)
  May 12 13:02:16.434: INFO: (2) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 7.376185ms)
  May 12 13:02:16.434: INFO: (2) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 4.534809ms)
  May 12 13:02:16.434: INFO: (2) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.039316ms)
  May 12 13:02:16.434: INFO: (2) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 7.558732ms)
  May 12 13:02:16.434: INFO: (2) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 8.895159ms)
  May 12 13:02:16.434: INFO: (2) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 7.411789ms)
  May 12 13:02:16.434: INFO: (2) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 4.95816ms)
  May 12 13:02:16.434: INFO: (2) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 4.534854ms)
  May 12 13:02:16.438: INFO: (2) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 8.069558ms)
  May 12 13:02:16.438: INFO: (2) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 8.103186ms)
  May 12 13:02:16.438: INFO: (2) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 8.122191ms)
  May 12 13:02:16.438: INFO: (2) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 8.100597ms)
  May 12 13:02:16.438: INFO: (2) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 8.05198ms)
  May 12 13:02:16.438: INFO: (2) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 8.10514ms)
  May 12 13:02:16.438: INFO: (2) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 8.092632ms)
  May 12 13:02:16.438: INFO: (2) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 8.125021ms)
  May 12 13:02:16.442: INFO: (3) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 3.402737ms)
  May 12 13:02:16.447: INFO: (3) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 7.967147ms)
  May 12 13:02:16.447: INFO: (3) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 8.020717ms)
  May 12 13:02:16.447: INFO: (3) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 7.979857ms)
  May 12 13:02:16.448: INFO: (3) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 8.736302ms)
  May 12 13:02:16.448: INFO: (3) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 8.750135ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 10.692613ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 10.68145ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 10.975436ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 10.901197ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 11.141161ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 11.125653ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 11.204843ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 11.204616ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 11.352779ms)
  May 12 13:02:16.450: INFO: (3) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 11.466836ms)
  May 12 13:02:16.457: INFO: (4) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 6.161799ms)
  May 12 13:02:16.457: INFO: (4) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 6.349358ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 7.573282ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 7.548487ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 7.501287ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 7.570594ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 7.648068ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 7.50351ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 7.640265ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 7.535986ms)
  May 12 13:02:16.458: INFO: (4) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 7.571316ms)
  May 12 13:02:16.462: INFO: (4) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 11.018572ms)
  May 12 13:02:16.462: INFO: (4) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 11.122117ms)
  May 12 13:02:16.462: INFO: (4) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 11.548666ms)
  May 12 13:02:16.463: INFO: (4) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 11.697437ms)
  May 12 13:02:16.463: INFO: (4) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 11.738042ms)
  May 12 13:02:16.472: INFO: (5) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.364975ms)
  May 12 13:02:16.472: INFO: (5) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 9.448192ms)
  May 12 13:02:16.472: INFO: (5) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 9.559602ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 9.358669ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 9.883845ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 9.661178ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 9.50722ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.786178ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.690558ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 9.866758ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.753935ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 10.106417ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 10.285583ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 10.287543ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 10.188767ms)
  May 12 13:02:16.473: INFO: (5) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 10.536915ms)
  May 12 13:02:16.476: INFO: (6) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 2.55613ms)
  May 12 13:02:16.479: INFO: (6) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 4.875812ms)
  May 12 13:02:16.479: INFO: (6) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 5.0686ms)
  May 12 13:02:16.485: INFO: (6) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 10.121325ms)
  May 12 13:02:16.486: INFO: (6) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 11.258993ms)
  May 12 13:02:16.486: INFO: (6) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 11.435101ms)
  May 12 13:02:16.486: INFO: (6) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 11.032339ms)
  May 12 13:02:16.486: INFO: (6) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 11.677068ms)
  May 12 13:02:16.486: INFO: (6) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 11.7736ms)
  May 12 13:02:16.486: INFO: (6) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 12.846434ms)
  May 12 13:02:16.486: INFO: (6) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 11.295147ms)
  May 12 13:02:16.487: INFO: (6) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 11.949845ms)
  May 12 13:02:16.487: INFO: (6) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 11.863212ms)
  May 12 13:02:16.487: INFO: (6) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 12.039051ms)
  May 12 13:02:16.487: INFO: (6) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 12.139724ms)
  May 12 13:02:16.487: INFO: (6) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 12.174432ms)
  May 12 13:02:16.495: INFO: (7) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 7.740799ms)
  May 12 13:02:16.498: INFO: (7) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 10.827738ms)
  May 12 13:02:16.498: INFO: (7) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 10.894535ms)
  May 12 13:02:16.498: INFO: (7) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 10.938954ms)
  May 12 13:02:16.498: INFO: (7) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 10.882609ms)
  May 12 13:02:16.498: INFO: (7) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 8.061252ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 8.081094ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 8.086727ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 11.301104ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 8.102536ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 11.205797ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 11.23472ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 8.223621ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 10.027428ms)
  May 12 13:02:16.499: INFO: (7) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.083926ms)
  May 12 13:02:16.500: INFO: (7) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 11.269259ms)
  May 12 13:02:16.507: INFO: (8) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 6.793461ms)
  May 12 13:02:16.507: INFO: (8) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 7.324268ms)
  May 12 13:02:16.508: INFO: (8) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 7.509622ms)
  May 12 13:02:16.509: INFO: (8) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 8.685864ms)
  May 12 13:02:16.509: INFO: (8) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 8.949725ms)
  May 12 13:02:16.509: INFO: (8) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 8.946835ms)
  May 12 13:02:16.509: INFO: (8) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 9.20366ms)
  May 12 13:02:16.509: INFO: (8) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.185303ms)
  May 12 13:02:16.509: INFO: (8) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 9.344322ms)
  May 12 13:02:16.510: INFO: (8) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 9.584252ms)
  May 12 13:02:16.510: INFO: (8) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 9.595726ms)
  May 12 13:02:16.514: INFO: (8) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 13.546917ms)
  May 12 13:02:16.514: INFO: (8) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 13.520788ms)
  May 12 13:02:16.514: INFO: (8) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 13.539237ms)
  May 12 13:02:16.514: INFO: (8) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 13.554641ms)
  May 12 13:02:16.514: INFO: (8) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 13.519702ms)
  May 12 13:02:16.519: INFO: (9) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 4.548748ms)
  May 12 13:02:16.519: INFO: (9) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 5.322719ms)
  May 12 13:02:16.520: INFO: (9) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 5.077879ms)
  May 12 13:02:16.520: INFO: (9) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 5.317128ms)
  May 12 13:02:16.520: INFO: (9) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 5.438853ms)
  May 12 13:02:16.520: INFO: (9) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 5.574278ms)
  May 12 13:02:16.524: INFO: (9) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 9.170252ms)
  May 12 13:02:16.524: INFO: (9) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 9.571999ms)
  May 12 13:02:16.524: INFO: (9) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 9.864524ms)
  May 12 13:02:16.524: INFO: (9) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 9.639891ms)
  May 12 13:02:16.524: INFO: (9) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.207899ms)
  May 12 13:02:16.525: INFO: (9) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 10.943685ms)
  May 12 13:02:16.525: INFO: (9) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 11.004602ms)
  May 12 13:02:16.525: INFO: (9) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 10.804226ms)
  May 12 13:02:16.525: INFO: (9) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 10.863101ms)
  May 12 13:02:16.525: INFO: (9) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 10.584822ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 8.901264ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 8.980148ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 9.330867ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.100849ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 9.974273ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.086619ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 9.151904ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.205024ms)
  May 12 13:02:16.535: INFO: (10) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 9.298056ms)
  May 12 13:02:16.537: INFO: (10) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 11.508818ms)
  May 12 13:02:16.537: INFO: (10) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 11.113261ms)
  May 12 13:02:16.538: INFO: (10) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 12.247907ms)
  May 12 13:02:16.539: INFO: (10) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 12.352679ms)
  May 12 13:02:16.539: INFO: (10) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 12.40231ms)
  May 12 13:02:16.540: INFO: (10) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 13.73009ms)
  May 12 13:02:16.540: INFO: (10) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 13.792432ms)
  May 12 13:02:16.549: INFO: (11) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 9.66279ms)
  May 12 13:02:16.550: INFO: (11) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 9.128267ms)
  May 12 13:02:16.550: INFO: (11) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.473945ms)
  May 12 13:02:16.550: INFO: (11) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 9.993173ms)
  May 12 13:02:16.550: INFO: (11) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 9.792059ms)
  May 12 13:02:16.550: INFO: (11) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 10.147996ms)
  May 12 13:02:16.551: INFO: (11) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 10.247184ms)
  May 12 13:02:16.551: INFO: (11) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 10.212349ms)
  May 12 13:02:16.551: INFO: (11) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 10.92606ms)
  May 12 13:02:16.551: INFO: (11) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 11.15744ms)
  May 12 13:02:16.551: INFO: (11) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 11.071933ms)
  May 12 13:02:16.551: INFO: (11) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 11.295296ms)
  May 12 13:02:16.552: INFO: (11) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 5.247428ms)
  May 12 13:02:16.552: INFO: (11) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 5.333113ms)
  May 12 13:02:16.552: INFO: (11) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 11.404694ms)
  May 12 13:02:16.552: INFO: (11) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 5.499612ms)
  May 12 13:02:16.558: INFO: (12) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 5.885476ms)
  May 12 13:02:16.560: INFO: (12) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 7.504952ms)
  May 12 13:02:16.560: INFO: (12) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 7.561851ms)
  May 12 13:02:16.561: INFO: (12) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 7.831397ms)
  May 12 13:02:16.561: INFO: (12) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 8.140235ms)
  May 12 13:02:16.561: INFO: (12) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 8.443019ms)
  May 12 13:02:16.561: INFO: (12) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 7.96542ms)
  May 12 13:02:16.564: INFO: (12) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 10.915023ms)
  May 12 13:02:16.564: INFO: (12) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 10.834565ms)
  May 12 13:02:16.564: INFO: (12) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 11.59857ms)
  May 12 13:02:16.565: INFO: (12) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 11.520585ms)
  May 12 13:02:16.565: INFO: (12) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 11.582976ms)
  May 12 13:02:16.565: INFO: (12) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 12.459809ms)
  May 12 13:02:16.565: INFO: (12) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 11.677251ms)
  May 12 13:02:16.565: INFO: (12) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 12.645538ms)
  May 12 13:02:16.565: INFO: (12) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 11.767918ms)
  May 12 13:02:16.574: INFO: (13) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 7.527073ms)
  May 12 13:02:16.575: INFO: (13) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 8.936189ms)
  May 12 13:02:16.575: INFO: (13) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.180753ms)
  May 12 13:02:16.575: INFO: (13) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 9.552526ms)
  May 12 13:02:16.575: INFO: (13) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 9.107712ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 9.699642ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 9.599862ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 9.441185ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 9.333765ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 9.875956ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.715248ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 9.644318ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 9.47874ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 9.469047ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.740296ms)
  May 12 13:02:16.576: INFO: (13) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.473406ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 8.345242ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 8.300601ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 8.278367ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 8.414587ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 8.368348ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 8.317087ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 8.257466ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 8.328842ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 8.313385ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 8.317202ms)
  May 12 13:02:16.586: INFO: (14) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 8.349821ms)
  May 12 13:02:16.587: INFO: (14) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.782275ms)
  May 12 13:02:16.587: INFO: (14) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 9.956129ms)
  May 12 13:02:16.587: INFO: (14) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 9.916483ms)
  May 12 13:02:16.588: INFO: (14) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 9.971171ms)
  May 12 13:02:16.588: INFO: (14) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 10.077566ms)
  May 12 13:02:16.590: INFO: (15) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 2.146098ms)
  May 12 13:02:16.594: INFO: (15) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 5.714755ms)
  May 12 13:02:16.594: INFO: (15) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 5.701087ms)
  May 12 13:02:16.594: INFO: (15) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 5.724143ms)
  May 12 13:02:16.594: INFO: (15) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 5.750251ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 11.166717ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 11.323974ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 11.284479ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 11.308625ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 11.335812ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 11.350765ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 11.262963ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 11.327025ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 11.291675ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 11.330631ms)
  May 12 13:02:16.599: INFO: (15) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 11.277148ms)
  May 12 13:02:16.603: INFO: (16) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 3.977163ms)
  May 12 13:02:16.609: INFO: (16) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 9.419851ms)
  May 12 13:02:16.609: INFO: (16) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.407488ms)
  May 12 13:02:16.609: INFO: (16) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 9.653717ms)
  May 12 13:02:16.609: INFO: (16) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 9.883075ms)
  May 12 13:02:16.609: INFO: (16) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 9.864396ms)
  May 12 13:02:16.609: INFO: (16) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 9.776541ms)
  May 12 13:02:16.609: INFO: (16) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 9.718724ms)
  May 12 13:02:16.609: INFO: (16) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 9.857319ms)
  May 12 13:02:16.610: INFO: (16) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 9.791216ms)
  May 12 13:02:16.610: INFO: (16) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 9.907364ms)
  May 12 13:02:16.610: INFO: (16) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 9.764755ms)
  May 12 13:02:16.610: INFO: (16) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 9.768126ms)
  May 12 13:02:16.610: INFO: (16) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.955618ms)
  May 12 13:02:16.610: INFO: (16) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 9.942985ms)
  May 12 13:02:16.610: INFO: (16) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 10.210212ms)
  May 12 13:02:16.618: INFO: (17) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 4.940604ms)
  May 12 13:02:16.619: INFO: (17) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 4.550732ms)
  May 12 13:02:16.619: INFO: (17) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 6.097456ms)
  May 12 13:02:16.619: INFO: (17) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 5.776564ms)
  May 12 13:02:16.624: INFO: (17) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 8.470898ms)
  May 12 13:02:16.624: INFO: (17) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 11.775256ms)
  May 12 13:02:16.625: INFO: (17) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 10.100981ms)
  May 12 13:02:16.625: INFO: (17) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 8.637331ms)
  May 12 13:02:16.625: INFO: (17) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 9.624927ms)
  May 12 13:02:16.626: INFO: (17) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 10.761636ms)
  May 12 13:02:16.626: INFO: (17) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 10.274407ms)
  May 12 13:02:16.626: INFO: (17) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 10.423142ms)
  May 12 13:02:16.627: INFO: (17) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 10.855556ms)
  May 12 13:02:16.627: INFO: (17) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 11.405146ms)
  May 12 13:02:16.628: INFO: (17) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 11.869041ms)
  May 12 13:02:16.628: INFO: (17) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 12.512158ms)
  May 12 13:02:16.634: INFO: (18) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 5.64165ms)
  May 12 13:02:16.634: INFO: (18) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 5.894994ms)
  May 12 13:02:16.634: INFO: (18) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 5.563883ms)
  May 12 13:02:16.635: INFO: (18) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 6.440874ms)
  May 12 13:02:16.635: INFO: (18) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 6.450433ms)
  May 12 13:02:16.635: INFO: (18) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 6.480423ms)
  May 12 13:02:16.635: INFO: (18) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 6.847662ms)
  May 12 13:02:16.635: INFO: (18) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 6.75193ms)
  May 12 13:02:16.639: INFO: (18) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 10.038307ms)
  May 12 13:02:16.641: INFO: (18) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 12.207907ms)
  May 12 13:02:16.641: INFO: (18) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 12.469534ms)
  May 12 13:02:16.641: INFO: (18) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 12.657094ms)
  May 12 13:02:16.641: INFO: (18) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 12.721447ms)
  May 12 13:02:16.641: INFO: (18) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 12.763892ms)
  May 12 13:02:16.641: INFO: (18) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 12.849605ms)
  May 12 13:02:16.641: INFO: (18) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 12.851811ms)
  May 12 13:02:16.647: INFO: (19) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:1080/proxy/rewriteme">test<... (200; 5.044493ms)
  May 12 13:02:16.648: INFO: (19) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:162/proxy/: bar (200; 5.808545ms)
  May 12 13:02:16.648: INFO: (19) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:162/proxy/: bar (200; 5.102291ms)
  May 12 13:02:16.648: INFO: (19) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:1080/proxy/rewriteme">... (200; 5.841119ms)
  May 12 13:02:16.648: INFO: (19) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw/proxy/rewriteme">test</a> (200; 6.334837ms)
  May 12 13:02:16.648: INFO: (19) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:462/proxy/: tls qux (200; 6.453104ms)
  May 12 13:02:16.649: INFO: (19) /api/v1/namespaces/proxy-5134/pods/proxy-service-hlz76-j62hw:160/proxy/: foo (200; 7.683184ms)
  May 12 13:02:16.650: INFO: (19) /api/v1/namespaces/proxy-5134/pods/http:proxy-service-hlz76-j62hw:160/proxy/: foo (200; 7.974524ms)
  May 12 13:02:16.650: INFO: (19) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:460/proxy/: tls baz (200; 8.321094ms)
  May 12 13:02:16.652: INFO: (19) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname1/proxy/: foo (200; 9.859317ms)
  May 12 13:02:16.652: INFO: (19) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname1/proxy/: foo (200; 10.194335ms)
  May 12 13:02:16.652: INFO: (19) /api/v1/namespaces/proxy-5134/services/http:proxy-service-hlz76:portname2/proxy/: bar (200; 10.173116ms)
  May 12 13:02:16.652: INFO: (19) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname2/proxy/: tls qux (200; 10.338509ms)
  May 12 13:02:16.652: INFO: (19) /api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/: <a href="/api/v1/namespaces/proxy-5134/pods/https:proxy-service-hlz76-j62hw:443/proxy/tlsrewritem... (200; 10.526387ms)
  May 12 13:02:16.652: INFO: (19) /api/v1/namespaces/proxy-5134/services/proxy-service-hlz76:portname2/proxy/: bar (200; 10.548735ms)
  May 12 13:02:16.653: INFO: (19) /api/v1/namespaces/proxy-5134/services/https:proxy-service-hlz76:tlsportname1/proxy/: tls baz (200; 10.869358ms)
  May 12 13:02:16.653: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-hlz76 in namespace proxy-5134, will wait for the garbage collector to delete the pods @ 05/12/23 13:02:16.655
  May 12 13:02:16.721: INFO: Deleting ReplicationController proxy-service-hlz76 took: 13.508147ms
  May 12 13:02:16.822: INFO: Terminating ReplicationController proxy-service-hlz76 pods took: 101.810428ms
  STEP: Destroying namespace "proxy-5134" for this suite. @ 05/12/23 13:02:19.324
• [5.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 05/12/23 13:02:19.337
  May 12 13:02:19.337: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/23 13:02:19.338
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:19.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:19.353
  May 12 13:02:19.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-3575" for this suite. @ 05/12/23 13:02:19.398
• [0.069 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 05/12/23 13:02:19.406
  May 12 13:02:19.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename deployment @ 05/12/23 13:02:19.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:19.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:19.43
  May 12 13:02:19.438: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  May 12 13:02:19.450: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 12 13:02:24.550: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/23 13:02:24.55
  May 12 13:02:24.550: INFO: Creating deployment "test-rolling-update-deployment"
  May 12 13:02:24.564: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  May 12 13:02:24.573: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  May 12 13:02:26.576: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  May 12 13:02:26.577: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  May 12 13:02:26.580: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-8536  87345cb0-7880-4b35-b721-bb03fcab8ce7 82393 1 2023-05-12 13:02:24 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-05-12 13:02:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:02:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003eafa98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-12 13:02:24 +0000 UTC,LastTransitionTime:2023-05-12 13:02:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-05-12 13:02:26 +0000 UTC,LastTransitionTime:2023-05-12 13:02:24 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 12 13:02:26.582: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-8536  79c0f17d-0323-4172-936b-7c415832b916 82383 1 2023-05-12 13:02:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment 87345cb0-7880-4b35-b721-bb03fcab8ce7 0xc005b71387 0xc005b71388}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:02:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87345cb0-7880-4b35-b721-bb03fcab8ce7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:02:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005b71438 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 12 13:02:26.583: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  May 12 13:02:26.583: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-8536  8e4d7874-10b4-46ab-81b6-b48301baf04d 82392 2 2023-05-12 13:02:19 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment 87345cb0-7880-4b35-b721-bb03fcab8ce7 0xc005b71257 0xc005b71258}] [] [{e2e.test Update apps/v1 2023-05-12 13:02:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:02:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"87345cb0-7880-4b35-b721-bb03fcab8ce7\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:02:26 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc005b71318 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 12 13:02:26.584: INFO: Pod "test-rolling-update-deployment-656d657cd8-rlwpt" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-rlwpt test-rolling-update-deployment-656d657cd8- deployment-8536  d76a6cea-aa0e-4860-8b10-b66f97fddf72 82382 0 2023-05-12 13:02:24 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[cni.projectcalico.org/containerID:36180f4197dfc93e643407bdebee9206b6c0daa076003e1aae18366846c8d503 cni.projectcalico.org/podIP:10.42.3.158/32 cni.projectcalico.org/podIPs:10.42.3.158/32] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 79c0f17d-0323-4172-936b-7c415832b916 0xc005b718c7 0xc005b718c8}] [] [{kube-controller-manager Update v1 2023-05-12 13:02:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"79c0f17d-0323-4172-936b-7c415832b916\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:02:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:02:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.158\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-pjn8p,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-pjn8p,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:02:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.158,StartTime:2023-05-12 13:02:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:02:25 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://74984c223412913e9d7324c652da07240792d3f919d8a3b040dd1125b75a5488,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.158,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:02:26.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-8536" for this suite. @ 05/12/23 13:02:26.589
• [7.188 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 05/12/23 13:02:26.596
  May 12 13:02:26.596: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename tables @ 05/12/23 13:02:26.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:26.608
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:26.609
  May 12 13:02:26.611: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-2021" for this suite. @ 05/12/23 13:02:26.613
• [0.020 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 05/12/23 13:02:26.616
  May 12 13:02:26.617: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:02:26.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:26.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:26.628
  STEP: Setting up server cert @ 05/12/23 13:02:26.644
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:02:27.126
  STEP: Deploying the webhook pod @ 05/12/23 13:02:27.134
  STEP: Wait for the deployment to be ready @ 05/12/23 13:02:27.149
  May 12 13:02:27.183: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:0, Replicas:0, UpdatedReplicas:0, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:0, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 2, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 2, 27, 0, time.Local), Reason:"NewReplicaSetCreated", Message:"Created new replica set \"sample-webhook-deployment-7497495989\""}}, CollisionCount:(*int32)(nil)}
  May 12 13:02:29.186: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 2, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 2, 27, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 2, 27, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 2, 27, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 05/12/23 13:02:31.186
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:02:31.202
  May 12 13:02:32.204: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/12/23 13:02:32.304
  STEP: Creating a configMap that should be mutated @ 05/12/23 13:02:32.315
  STEP: Deleting the collection of validation webhooks @ 05/12/23 13:02:32.34
  STEP: Creating a configMap that should not be mutated @ 05/12/23 13:02:32.363
  May 12 13:02:32.369: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8995" for this suite. @ 05/12/23 13:02:32.436
  STEP: Destroying namespace "webhook-markers-5827" for this suite. @ 05/12/23 13:02:32.445
• [5.833 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 05/12/23 13:02:32.451
  May 12 13:02:32.451: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replicaset @ 05/12/23 13:02:32.452
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:32.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:32.467
  STEP: Create a ReplicaSet @ 05/12/23 13:02:32.47
  STEP: Verify that the required pods have come up @ 05/12/23 13:02:32.475
  May 12 13:02:32.478: INFO: Pod name sample-pod: Found 0 pods out of 3
  May 12 13:02:37.484: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 05/12/23 13:02:37.484
  May 12 13:02:37.490: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 05/12/23 13:02:37.49
  STEP: DeleteCollection of the ReplicaSets @ 05/12/23 13:02:37.495
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 05/12/23 13:02:37.5
  May 12 13:02:37.508: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1283" for this suite. @ 05/12/23 13:02:37.527
• [5.092 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 05/12/23 13:02:37.545
  May 12 13:02:37.546: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replication-controller @ 05/12/23 13:02:37.547
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:37.594
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:37.599
  STEP: Creating replication controller my-hostname-basic-f096ca9f-d079-422a-a916-d71c60d89115 @ 05/12/23 13:02:37.603
  May 12 13:02:37.613: INFO: Pod name my-hostname-basic-f096ca9f-d079-422a-a916-d71c60d89115: Found 0 pods out of 1
  May 12 13:02:42.615: INFO: Pod name my-hostname-basic-f096ca9f-d079-422a-a916-d71c60d89115: Found 1 pods out of 1
  May 12 13:02:42.616: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-f096ca9f-d079-422a-a916-d71c60d89115" are running
  May 12 13:02:42.617: INFO: Pod "my-hostname-basic-f096ca9f-d079-422a-a916-d71c60d89115-mml8l" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-12 13:02:37 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-12 13:02:38 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-12 13:02:38 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-05-12 13:02:37 +0000 UTC Reason: Message:}])
  May 12 13:02:42.618: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 05/12/23 13:02:42.618
  May 12 13:02:42.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6934" for this suite. @ 05/12/23 13:02:42.629
• [5.090 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 05/12/23 13:02:42.636
  May 12 13:02:42.636: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:02:42.637
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:42.655
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:42.661
  STEP: creating service multi-endpoint-test in namespace services-5626 @ 05/12/23 13:02:42.675
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5626 to expose endpoints map[] @ 05/12/23 13:02:42.688
  May 12 13:02:42.702: INFO: successfully validated that service multi-endpoint-test in namespace services-5626 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-5626 @ 05/12/23 13:02:42.704
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5626 to expose endpoints map[pod1:[100]] @ 05/12/23 13:02:44.735
  May 12 13:02:44.754: INFO: successfully validated that service multi-endpoint-test in namespace services-5626 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-5626 @ 05/12/23 13:02:44.754
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5626 to expose endpoints map[pod1:[100] pod2:[101]] @ 05/12/23 13:02:46.786
  May 12 13:02:46.811: INFO: successfully validated that service multi-endpoint-test in namespace services-5626 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 05/12/23 13:02:46.811
  May 12 13:02:46.811: INFO: Creating new exec pod
  May 12 13:02:49.830: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5626 exec execpod2hzgf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  May 12 13:02:49.984: INFO: stderr: "+ nc+  -v -techo -w 2 hostName multi-endpoint-test\n 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  May 12 13:02:49.984: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:02:49.984: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5626 exec execpod2hzgf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.150.250 80'
  May 12 13:02:50.098: INFO: stderr: "+ nc -v -t+ echo hostName\n -w 2 10.43.150.250 80\nConnection to 10.43.150.250 80 port [tcp/http] succeeded!\n"
  May 12 13:02:50.098: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:02:50.098: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5626 exec execpod2hzgf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  May 12 13:02:50.210: INFO: stderr: "+ + echonc hostName -v\n -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  May 12 13:02:50.210: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:02:50.210: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5626 exec execpod2hzgf -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.150.250 81'
  May 12 13:02:50.316: INFO: stderr: "+ nc -v -t -w 2 10.43.150.250 81\n+ echo hostName\nConnection to 10.43.150.250 81 port [tcp/*] succeeded!\n"
  May 12 13:02:50.316: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-5626 @ 05/12/23 13:02:50.316
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5626 to expose endpoints map[pod2:[101]] @ 05/12/23 13:02:50.336
  May 12 13:02:50.349: INFO: successfully validated that service multi-endpoint-test in namespace services-5626 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-5626 @ 05/12/23 13:02:50.349
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-5626 to expose endpoints map[] @ 05/12/23 13:02:50.367
  May 12 13:02:50.374: INFO: successfully validated that service multi-endpoint-test in namespace services-5626 exposes endpoints map[]
  May 12 13:02:50.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5626" for this suite. @ 05/12/23 13:02:50.395
• [7.765 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 05/12/23 13:02:50.401
  May 12 13:02:50.401: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:02:50.402
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:50.416
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:50.418
  STEP: Setting up server cert @ 05/12/23 13:02:50.437
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:02:50.797
  STEP: Deploying the webhook pod @ 05/12/23 13:02:50.8
  STEP: Wait for the deployment to be ready @ 05/12/23 13:02:50.811
  May 12 13:02:50.821: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/23 13:02:52.828
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:02:52.85
  May 12 13:02:53.850: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 05/12/23 13:02:53.861
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 05/12/23 13:02:53.868
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 05/12/23 13:02:53.868
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 05/12/23 13:02:53.868
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 05/12/23 13:02:53.871
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/12/23 13:02:53.873
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 05/12/23 13:02:53.876
  May 12 13:02:53.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9356" for this suite. @ 05/12/23 13:02:53.93
  STEP: Destroying namespace "webhook-markers-9376" for this suite. @ 05/12/23 13:02:53.941
• [3.549 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 05/12/23 13:02:53.951
  May 12 13:02:53.951: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/23 13:02:53.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:53.962
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:53.964
  May 12 13:02:53.968: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:02:57.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1623" for this suite. @ 05/12/23 13:02:57.135
• [3.188 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 05/12/23 13:02:57.145
  May 12 13:02:57.145: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/23 13:02:57.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:57.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:57.167
  May 12 13:02:57.169: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:02:57.758: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4051" for this suite. @ 05/12/23 13:02:57.763
• [0.622 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 05/12/23 13:02:57.769
  May 12 13:02:57.769: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename taint-single-pod @ 05/12/23 13:02:57.769
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:02:57.78
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:02:57.783
  May 12 13:02:57.784: INFO: Waiting up to 1m0s for all nodes to be ready
  May 12 13:03:57.847: INFO: Waiting for terminating namespaces to be deleted...
  May 12 13:03:57.849: INFO: Starting informer...
  STEP: Starting pod... @ 05/12/23 13:03:57.849
  May 12 13:03:58.073: INFO: Pod is running on onekube-ip-172-16-100-7. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/12/23 13:03:58.073
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/12/23 13:03:58.117
  STEP: Waiting short time to make sure Pod is queued for deletion @ 05/12/23 13:03:58.129
  May 12 13:03:58.135: INFO: Pod wasn't evicted. Proceeding
  May 12 13:03:58.146: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/12/23 13:03:58.31
  STEP: Waiting some time to make sure that toleration time passed. @ 05/12/23 13:03:58.387
  May 12 13:05:13.388: INFO: Pod wasn't evicted. Test successful
  May 12 13:05:13.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-4890" for this suite. @ 05/12/23 13:05:13.397
• [135.644 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 05/12/23 13:05:13.419
  May 12 13:05:13.419: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename security-context-test @ 05/12/23 13:05:13.421
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:05:13.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:05:13.445
  May 12 13:05:17.495: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-8104" for this suite. @ 05/12/23 13:05:17.501
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 05/12/23 13:05:17.511
  May 12 13:05:17.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 13:05:17.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:05:17.522
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:05:17.525
  STEP: Creating pod busybox-cf84243e-ec6c-4036-9dcc-8a98957282f5 in namespace container-probe-2995 @ 05/12/23 13:05:17.527
  May 12 13:05:19.543: INFO: Started pod busybox-cf84243e-ec6c-4036-9dcc-8a98957282f5 in namespace container-probe-2995
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/23 13:05:19.543
  May 12 13:05:19.544: INFO: Initial restart count of pod busybox-cf84243e-ec6c-4036-9dcc-8a98957282f5 is 0
  May 12 13:06:09.703: INFO: Restart count of pod container-probe-2995/busybox-cf84243e-ec6c-4036-9dcc-8a98957282f5 is now 1 (50.159031307s elapsed)
  May 12 13:06:09.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 13:06:09.707
  STEP: Destroying namespace "container-probe-2995" for this suite. @ 05/12/23 13:06:09.718
• [52.212 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 05/12/23 13:06:09.728
  May 12 13:06:09.728: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:06:09.729
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:06:09.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:06:09.799
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 13:06:09.8
  STEP: Saw pod success @ 05/12/23 13:06:13.825
  May 12 13:06:13.827: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-d2961624-f528-4e1c-b143-203d935a0d48 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 13:06:13.833
  May 12 13:06:13.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8608" for this suite. @ 05/12/23 13:06:13.851
• [4.129 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 05/12/23 13:06:13.86
  May 12 13:06:13.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename disruption @ 05/12/23 13:06:13.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:06:13.874
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:06:13.875
  STEP: Creating a kubernetes client @ 05/12/23 13:06:13.877
  May 12 13:06:13.877: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename disruption-2 @ 05/12/23 13:06:13.878
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:06:13.89
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:06:13.892
  STEP: Waiting for the pdb to be processed @ 05/12/23 13:06:13.897
  STEP: Waiting for the pdb to be processed @ 05/12/23 13:06:13.904
  STEP: Waiting for the pdb to be processed @ 05/12/23 13:06:13.915
  STEP: listing a collection of PDBs across all namespaces @ 05/12/23 13:06:13.918
  STEP: listing a collection of PDBs in namespace disruption-6705 @ 05/12/23 13:06:13.92
  STEP: deleting a collection of PDBs @ 05/12/23 13:06:13.922
  STEP: Waiting for the PDB collection to be deleted @ 05/12/23 13:06:13.928
  May 12 13:06:13.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:06:13.931: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-615" for this suite. @ 05/12/23 13:06:13.934
  STEP: Destroying namespace "disruption-6705" for this suite. @ 05/12/23 13:06:13.938
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 05/12/23 13:06:13.946
  May 12 13:06:13.946: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/23 13:06:13.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:06:13.957
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:06:13.958
  May 12 13:06:13.969: INFO: Waiting up to 1m0s for all nodes to be ready
  May 12 13:07:14.011: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 05/12/23 13:07:14.014
  May 12 13:07:14.036: INFO: Created pod: pod0-0-sched-preemption-low-priority
  May 12 13:07:14.045: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  May 12 13:07:14.076: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  May 12 13:07:14.080: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 05/12/23 13:07:14.08
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 05/12/23 13:07:16.092
  May 12 13:07:22.139: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2108" for this suite. @ 05/12/23 13:07:22.195
• [68.256 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 05/12/23 13:07:22.204
  May 12 13:07:22.204: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename proxy @ 05/12/23 13:07:22.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:07:22.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:07:22.218
  May 12 13:07:22.219: INFO: Creating pod...
  May 12 13:07:24.243: INFO: Creating service...
  May 12 13:07:24.288: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/pods/agnhost/proxy/some/path/with/DELETE
  May 12 13:07:24.296: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 12 13:07:24.296: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/pods/agnhost/proxy/some/path/with/GET
  May 12 13:07:24.301: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 12 13:07:24.301: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/pods/agnhost/proxy/some/path/with/HEAD
  May 12 13:07:24.304: INFO: http.Client request:HEAD | StatusCode:200
  May 12 13:07:24.304: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/pods/agnhost/proxy/some/path/with/OPTIONS
  May 12 13:07:24.307: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 12 13:07:24.307: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/pods/agnhost/proxy/some/path/with/PATCH
  May 12 13:07:24.308: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 12 13:07:24.308: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/pods/agnhost/proxy/some/path/with/POST
  May 12 13:07:24.310: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 12 13:07:24.310: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/pods/agnhost/proxy/some/path/with/PUT
  May 12 13:07:24.312: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 12 13:07:24.312: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/services/test-service/proxy/some/path/with/DELETE
  May 12 13:07:24.314: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 12 13:07:24.314: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/services/test-service/proxy/some/path/with/GET
  May 12 13:07:24.317: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  May 12 13:07:24.317: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/services/test-service/proxy/some/path/with/HEAD
  May 12 13:07:24.319: INFO: http.Client request:HEAD | StatusCode:200
  May 12 13:07:24.319: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/services/test-service/proxy/some/path/with/OPTIONS
  May 12 13:07:24.322: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 12 13:07:24.322: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/services/test-service/proxy/some/path/with/PATCH
  May 12 13:07:24.324: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 12 13:07:24.324: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/services/test-service/proxy/some/path/with/POST
  May 12 13:07:24.326: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 12 13:07:24.326: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-7530/services/test-service/proxy/some/path/with/PUT
  May 12 13:07:24.328: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 12 13:07:24.328: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7530" for this suite. @ 05/12/23 13:07:24.33
• [2.131 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 05/12/23 13:07:24.336
  May 12 13:07:24.336: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename watch @ 05/12/23 13:07:24.337
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:07:24.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:07:24.35
  STEP: creating a watch on configmaps with label A @ 05/12/23 13:07:24.352
  STEP: creating a watch on configmaps with label B @ 05/12/23 13:07:24.353
  STEP: creating a watch on configmaps with label A or B @ 05/12/23 13:07:24.354
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 05/12/23 13:07:24.364
  May 12 13:07:24.373: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5538  9840db18-4f7b-4cc9-98ce-3f0325cd6753 84815 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:07:24.373: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5538  9840db18-4f7b-4cc9-98ce-3f0325cd6753 84815 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 05/12/23 13:07:24.373
  May 12 13:07:24.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5538  9840db18-4f7b-4cc9-98ce-3f0325cd6753 84816 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:07:24.379: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5538  9840db18-4f7b-4cc9-98ce-3f0325cd6753 84816 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 05/12/23 13:07:24.38
  May 12 13:07:24.383: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5538  9840db18-4f7b-4cc9-98ce-3f0325cd6753 84817 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:07:24.384: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5538  9840db18-4f7b-4cc9-98ce-3f0325cd6753 84817 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 05/12/23 13:07:24.385
  May 12 13:07:24.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5538  9840db18-4f7b-4cc9-98ce-3f0325cd6753 84818 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:07:24.388: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-5538  9840db18-4f7b-4cc9-98ce-3f0325cd6753 84818 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 05/12/23 13:07:24.388
  May 12 13:07:24.391: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5538  1f745bec-ee39-4eb5-9690-adceb75ca939 84819 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:07:24.392: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5538  1f745bec-ee39-4eb5-9690-adceb75ca939 84819 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 05/12/23 13:07:34.394
  May 12 13:07:34.418: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5538  1f745bec-ee39-4eb5-9690-adceb75ca939 84943 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:07:34.419: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-5538  1f745bec-ee39-4eb5-9690-adceb75ca939 84943 0 2023-05-12 13:07:24 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-05-12 13:07:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:07:44.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-5538" for this suite. @ 05/12/23 13:07:44.424
• [20.092 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 05/12/23 13:07:44.43
  May 12 13:07:44.430: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename lease-test @ 05/12/23 13:07:44.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:07:44.445
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:07:44.448
  May 12 13:07:44.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-3983" for this suite. @ 05/12/23 13:07:44.475
• [0.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 05/12/23 13:07:44.483
  May 12 13:07:44.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename gc @ 05/12/23 13:07:44.484
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:07:44.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:07:44.498
  STEP: create the rc @ 05/12/23 13:07:44.501
  W0512 13:07:44.506314      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 05/12/23 13:07:50.51
  STEP: wait for the rc to be deleted @ 05/12/23 13:07:50.518
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 05/12/23 13:07:55.534
  STEP: Gathering metrics @ 05/12/23 13:08:25.562
  May 12 13:08:25.622: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 12 13:08:25.626: INFO: Deleting pod "simpletest.rc-2bn26" in namespace "gc-494"
  May 12 13:08:25.639: INFO: Deleting pod "simpletest.rc-2dj57" in namespace "gc-494"
  May 12 13:08:25.652: INFO: Deleting pod "simpletest.rc-2hx96" in namespace "gc-494"
  May 12 13:08:25.661: INFO: Deleting pod "simpletest.rc-47qcp" in namespace "gc-494"
  May 12 13:08:25.676: INFO: Deleting pod "simpletest.rc-4bfhr" in namespace "gc-494"
  May 12 13:08:25.688: INFO: Deleting pod "simpletest.rc-4n7hw" in namespace "gc-494"
  May 12 13:08:25.700: INFO: Deleting pod "simpletest.rc-4pm2h" in namespace "gc-494"
  May 12 13:08:25.714: INFO: Deleting pod "simpletest.rc-5d96f" in namespace "gc-494"
  May 12 13:08:25.732: INFO: Deleting pod "simpletest.rc-5dwqv" in namespace "gc-494"
  May 12 13:08:25.754: INFO: Deleting pod "simpletest.rc-5g5t9" in namespace "gc-494"
  May 12 13:08:25.767: INFO: Deleting pod "simpletest.rc-5vh4b" in namespace "gc-494"
  May 12 13:08:25.777: INFO: Deleting pod "simpletest.rc-65hdr" in namespace "gc-494"
  May 12 13:08:25.791: INFO: Deleting pod "simpletest.rc-68qkp" in namespace "gc-494"
  May 12 13:08:25.804: INFO: Deleting pod "simpletest.rc-69zmf" in namespace "gc-494"
  May 12 13:08:25.814: INFO: Deleting pod "simpletest.rc-6s88b" in namespace "gc-494"
  May 12 13:08:25.825: INFO: Deleting pod "simpletest.rc-6zpn2" in namespace "gc-494"
  May 12 13:08:25.839: INFO: Deleting pod "simpletest.rc-72t7v" in namespace "gc-494"
  May 12 13:08:25.850: INFO: Deleting pod "simpletest.rc-7hbbt" in namespace "gc-494"
  May 12 13:08:25.860: INFO: Deleting pod "simpletest.rc-7mtjm" in namespace "gc-494"
  May 12 13:08:25.871: INFO: Deleting pod "simpletest.rc-7vbzg" in namespace "gc-494"
  May 12 13:08:25.879: INFO: Deleting pod "simpletest.rc-7vcx7" in namespace "gc-494"
  May 12 13:08:25.891: INFO: Deleting pod "simpletest.rc-7x8bz" in namespace "gc-494"
  May 12 13:08:25.901: INFO: Deleting pod "simpletest.rc-88rbc" in namespace "gc-494"
  May 12 13:08:25.911: INFO: Deleting pod "simpletest.rc-88rs6" in namespace "gc-494"
  May 12 13:08:25.924: INFO: Deleting pod "simpletest.rc-8jb98" in namespace "gc-494"
  May 12 13:08:25.936: INFO: Deleting pod "simpletest.rc-8m88h" in namespace "gc-494"
  May 12 13:08:25.950: INFO: Deleting pod "simpletest.rc-8vvgv" in namespace "gc-494"
  May 12 13:08:25.961: INFO: Deleting pod "simpletest.rc-b7zmt" in namespace "gc-494"
  May 12 13:08:25.978: INFO: Deleting pod "simpletest.rc-bkc45" in namespace "gc-494"
  May 12 13:08:25.986: INFO: Deleting pod "simpletest.rc-blxbq" in namespace "gc-494"
  May 12 13:08:26.000: INFO: Deleting pod "simpletest.rc-bqb6w" in namespace "gc-494"
  May 12 13:08:26.010: INFO: Deleting pod "simpletest.rc-btbcs" in namespace "gc-494"
  May 12 13:08:26.018: INFO: Deleting pod "simpletest.rc-ct9hr" in namespace "gc-494"
  May 12 13:08:26.029: INFO: Deleting pod "simpletest.rc-cvvhc" in namespace "gc-494"
  May 12 13:08:26.037: INFO: Deleting pod "simpletest.rc-dg86l" in namespace "gc-494"
  May 12 13:08:26.052: INFO: Deleting pod "simpletest.rc-dgrcr" in namespace "gc-494"
  May 12 13:08:26.061: INFO: Deleting pod "simpletest.rc-dmh44" in namespace "gc-494"
  May 12 13:08:26.072: INFO: Deleting pod "simpletest.rc-dng2n" in namespace "gc-494"
  May 12 13:08:26.081: INFO: Deleting pod "simpletest.rc-fm9t5" in namespace "gc-494"
  May 12 13:08:26.092: INFO: Deleting pod "simpletest.rc-fsd7f" in namespace "gc-494"
  May 12 13:08:26.100: INFO: Deleting pod "simpletest.rc-fsrsd" in namespace "gc-494"
  May 12 13:08:26.133: INFO: Deleting pod "simpletest.rc-gn6qn" in namespace "gc-494"
  May 12 13:08:26.146: INFO: Deleting pod "simpletest.rc-gqmbz" in namespace "gc-494"
  May 12 13:08:26.161: INFO: Deleting pod "simpletest.rc-gr7hm" in namespace "gc-494"
  May 12 13:08:26.172: INFO: Deleting pod "simpletest.rc-gzzbp" in namespace "gc-494"
  May 12 13:08:26.182: INFO: Deleting pod "simpletest.rc-h4rb6" in namespace "gc-494"
  May 12 13:08:26.196: INFO: Deleting pod "simpletest.rc-hqhk5" in namespace "gc-494"
  May 12 13:08:26.211: INFO: Deleting pod "simpletest.rc-ht967" in namespace "gc-494"
  May 12 13:08:26.221: INFO: Deleting pod "simpletest.rc-jzc5m" in namespace "gc-494"
  May 12 13:08:26.231: INFO: Deleting pod "simpletest.rc-k47qh" in namespace "gc-494"
  May 12 13:08:26.243: INFO: Deleting pod "simpletest.rc-kbdg6" in namespace "gc-494"
  May 12 13:08:26.260: INFO: Deleting pod "simpletest.rc-khdwv" in namespace "gc-494"
  May 12 13:08:26.273: INFO: Deleting pod "simpletest.rc-kzbcd" in namespace "gc-494"
  May 12 13:08:26.284: INFO: Deleting pod "simpletest.rc-l2tct" in namespace "gc-494"
  May 12 13:08:26.293: INFO: Deleting pod "simpletest.rc-lcg52" in namespace "gc-494"
  May 12 13:08:26.300: INFO: Deleting pod "simpletest.rc-ldbpv" in namespace "gc-494"
  May 12 13:08:26.317: INFO: Deleting pod "simpletest.rc-lg4hw" in namespace "gc-494"
  May 12 13:08:26.329: INFO: Deleting pod "simpletest.rc-lnkqh" in namespace "gc-494"
  May 12 13:08:26.340: INFO: Deleting pod "simpletest.rc-m6549" in namespace "gc-494"
  May 12 13:08:26.352: INFO: Deleting pod "simpletest.rc-m7tsv" in namespace "gc-494"
  May 12 13:08:26.363: INFO: Deleting pod "simpletest.rc-mlwzj" in namespace "gc-494"
  May 12 13:08:26.374: INFO: Deleting pod "simpletest.rc-mscb4" in namespace "gc-494"
  May 12 13:08:26.389: INFO: Deleting pod "simpletest.rc-mtll4" in namespace "gc-494"
  May 12 13:08:26.401: INFO: Deleting pod "simpletest.rc-n84tr" in namespace "gc-494"
  May 12 13:08:26.451: INFO: Deleting pod "simpletest.rc-nqknb" in namespace "gc-494"
  May 12 13:08:26.506: INFO: Deleting pod "simpletest.rc-pd7jp" in namespace "gc-494"
  May 12 13:08:26.553: INFO: Deleting pod "simpletest.rc-qb6xk" in namespace "gc-494"
  May 12 13:08:26.601: INFO: Deleting pod "simpletest.rc-qv2qc" in namespace "gc-494"
  May 12 13:08:26.652: INFO: Deleting pod "simpletest.rc-qwl6h" in namespace "gc-494"
  May 12 13:08:26.704: INFO: Deleting pod "simpletest.rc-rgdvx" in namespace "gc-494"
  May 12 13:08:26.756: INFO: Deleting pod "simpletest.rc-rkmkc" in namespace "gc-494"
  May 12 13:08:26.803: INFO: Deleting pod "simpletest.rc-s94k9" in namespace "gc-494"
  May 12 13:08:26.853: INFO: Deleting pod "simpletest.rc-sjfpz" in namespace "gc-494"
  May 12 13:08:26.902: INFO: Deleting pod "simpletest.rc-sthmg" in namespace "gc-494"
  May 12 13:08:26.953: INFO: Deleting pod "simpletest.rc-t92n5" in namespace "gc-494"
  May 12 13:08:27.003: INFO: Deleting pod "simpletest.rc-tknf5" in namespace "gc-494"
  May 12 13:08:27.055: INFO: Deleting pod "simpletest.rc-tmvzq" in namespace "gc-494"
  May 12 13:08:27.118: INFO: Deleting pod "simpletest.rc-tpx8c" in namespace "gc-494"
  May 12 13:08:27.161: INFO: Deleting pod "simpletest.rc-tr9cv" in namespace "gc-494"
  May 12 13:08:27.206: INFO: Deleting pod "simpletest.rc-trmrb" in namespace "gc-494"
  May 12 13:08:27.264: INFO: Deleting pod "simpletest.rc-tv525" in namespace "gc-494"
  May 12 13:08:27.303: INFO: Deleting pod "simpletest.rc-v56bv" in namespace "gc-494"
  May 12 13:08:27.352: INFO: Deleting pod "simpletest.rc-vgjhh" in namespace "gc-494"
  May 12 13:08:27.403: INFO: Deleting pod "simpletest.rc-vqqqf" in namespace "gc-494"
  May 12 13:08:27.454: INFO: Deleting pod "simpletest.rc-vrgt5" in namespace "gc-494"
  May 12 13:08:27.509: INFO: Deleting pod "simpletest.rc-w6qrx" in namespace "gc-494"
  May 12 13:08:27.560: INFO: Deleting pod "simpletest.rc-wchmb" in namespace "gc-494"
  May 12 13:08:27.601: INFO: Deleting pod "simpletest.rc-wcjwg" in namespace "gc-494"
  May 12 13:08:27.652: INFO: Deleting pod "simpletest.rc-wq6hf" in namespace "gc-494"
  May 12 13:08:27.700: INFO: Deleting pod "simpletest.rc-wq7vp" in namespace "gc-494"
  May 12 13:08:27.755: INFO: Deleting pod "simpletest.rc-wrrv8" in namespace "gc-494"
  May 12 13:08:27.805: INFO: Deleting pod "simpletest.rc-x48c7" in namespace "gc-494"
  May 12 13:08:27.853: INFO: Deleting pod "simpletest.rc-x4b7d" in namespace "gc-494"
  May 12 13:08:27.904: INFO: Deleting pod "simpletest.rc-x8dmg" in namespace "gc-494"
  May 12 13:08:27.955: INFO: Deleting pod "simpletest.rc-xbgfl" in namespace "gc-494"
  May 12 13:08:28.002: INFO: Deleting pod "simpletest.rc-xldgv" in namespace "gc-494"
  May 12 13:08:28.053: INFO: Deleting pod "simpletest.rc-xzrwx" in namespace "gc-494"
  May 12 13:08:28.102: INFO: Deleting pod "simpletest.rc-zg4mx" in namespace "gc-494"
  May 12 13:08:28.152: INFO: Deleting pod "simpletest.rc-zw2vn" in namespace "gc-494"
  May 12 13:08:28.204: INFO: Deleting pod "simpletest.rc-zwb67" in namespace "gc-494"
  May 12 13:08:28.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-494" for this suite. @ 05/12/23 13:08:28.296
• [43.864 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 05/12/23 13:08:28.348
  May 12 13:08:28.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename hostport @ 05/12/23 13:08:28.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:08:28.36
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:08:28.362
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 05/12/23 13:08:28.367
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 172.16.100.7 on the node which pod1 resides and expect scheduled @ 05/12/23 13:08:34.383
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 172.16.100.7 but use UDP protocol on the node which pod2 resides @ 05/12/23 13:08:36.396
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 05/12/23 13:08:42.428
  May 12 13:08:42.428: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 172.16.100.7 http://127.0.0.1:54323/hostname] Namespace:hostport-5842 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:08:42.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:08:42.429: INFO: ExecWithOptions: Clientset creation
  May 12 13:08:42.429: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-5842/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+172.16.100.7+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.100.7, port: 54323 @ 05/12/23 13:08:42.507
  May 12 13:08:42.507: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://172.16.100.7:54323/hostname] Namespace:hostport-5842 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:08:42.507: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:08:42.508: INFO: ExecWithOptions: Clientset creation
  May 12 13:08:42.508: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-5842/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F172.16.100.7%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 172.16.100.7, port: 54323 UDP @ 05/12/23 13:08:42.595
  May 12 13:08:42.595: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 172.16.100.7 54323] Namespace:hostport-5842 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:08:42.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:08:42.596: INFO: ExecWithOptions: Clientset creation
  May 12 13:08:42.596: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/hostport-5842/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+172.16.100.7+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  May 12 13:08:47.675: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-5842" for this suite. @ 05/12/23 13:08:47.684
• [19.343 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 05/12/23 13:08:47.692
  May 12 13:08:47.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:08:47.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:08:47.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:08:47.718
  STEP: Setting up server cert @ 05/12/23 13:08:47.736
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:08:48.112
  STEP: Deploying the webhook pod @ 05/12/23 13:08:48.117
  STEP: Wait for the deployment to be ready @ 05/12/23 13:08:48.129
  May 12 13:08:48.136: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/23 13:08:50.158
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:08:50.19
  May 12 13:08:51.190: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/12/23 13:08:51.202
  STEP: create a pod @ 05/12/23 13:08:51.235
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 05/12/23 13:08:53.255
  May 12 13:08:53.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=webhook-9778 attach --namespace=webhook-9778 to-be-attached-pod -i -c=container1'
  May 12 13:08:53.318: INFO: rc: 1
  May 12 13:08:53.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9778" for this suite. @ 05/12/23 13:08:53.362
  STEP: Destroying namespace "webhook-markers-9640" for this suite. @ 05/12/23 13:08:53.375
• [5.687 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 05/12/23 13:08:53.38
  May 12 13:08:53.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename deployment @ 05/12/23 13:08:53.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:08:53.394
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:08:53.395
  May 12 13:08:53.401: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  May 12 13:08:58.426: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/23 13:08:58.426
  May 12 13:08:58.426: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 05/12/23 13:08:58.437
  May 12 13:09:00.471: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-2566  8a93f7b7-7858-414a-b468-ed09b87b13bd 87516 1 2023-05-12 13:08:58 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-05-12 13:08:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:09:00 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467a378 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-05-12 13:08:58 +0000 UTC,LastTransitionTime:2023-05-12 13:08:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-cleanup-deployment-68b75d69f8" has successfully progressed.,LastUpdateTime:2023-05-12 13:09:00 +0000 UTC,LastTransitionTime:2023-05-12 13:08:58 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  May 12 13:09:00.478: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-2566  a40b8d15-4a00-4198-8d66-42a4272c78e8 87502 1 2023-05-12 13:08:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 8a93f7b7-7858-414a-b468-ed09b87b13bd 0xc00467a747 0xc00467a748}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:08:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"8a93f7b7-7858-414a-b468-ed09b87b13bd\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:08:59 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00467a7f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  May 12 13:09:00.483: INFO: Pod "test-cleanup-deployment-68b75d69f8-6d6rj" is available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-6d6rj test-cleanup-deployment-68b75d69f8- deployment-2566  d3ea87d5-7d4d-43f9-a9c6-97edecfa4884 87501 0 2023-05-12 13:08:58 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[cni.projectcalico.org/containerID:ebb00b072df127149d9d4aa4de57054f299e6f76fb13245141235164402017ea cni.projectcalico.org/podIP:10.42.3.234/32 cni.projectcalico.org/podIPs:10.42.3.234/32] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 a40b8d15-4a00-4198-8d66-42a4272c78e8 0xc00467abb7 0xc00467abb8}] [] [{calico Update v1 2023-05-12 13:08:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-12 13:08:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a40b8d15-4a00-4198-8d66-42a4272c78e8\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:08:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.234\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8wv2w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8wv2w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:08:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:08:59 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:08:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.234,StartTime:2023-05-12 13:08:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:08:59 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:containerd://a6dbfd558dec9555c1ea1d65b5b3e02f46c068d2658e84a428f91c720fc2e069,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.234,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:09:00.484: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2566" for this suite. @ 05/12/23 13:09:00.488
• [7.114 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 05/12/23 13:09:00.496
  May 12 13:09:00.496: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename namespaces @ 05/12/23 13:09:00.497
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:00.508
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:00.512
  STEP: Creating namespace "e2e-ns-7kw8r" @ 05/12/23 13:09:00.514
  May 12 13:09:00.524: INFO: Namespace "e2e-ns-7kw8r-7241" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-7kw8r-7241" @ 05/12/23 13:09:00.524
  May 12 13:09:00.528: INFO: Namespace "e2e-ns-7kw8r-7241" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-7kw8r-7241" @ 05/12/23 13:09:00.529
  May 12 13:09:00.535: INFO: Namespace "e2e-ns-7kw8r-7241" has []v1.FinalizerName{"kubernetes"}
  May 12 13:09:00.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8260" for this suite. @ 05/12/23 13:09:00.538
  STEP: Destroying namespace "e2e-ns-7kw8r-7241" for this suite. @ 05/12/23 13:09:00.544
• [0.053 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 05/12/23 13:09:00.55
  May 12 13:09:00.550: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename subjectreview @ 05/12/23 13:09:00.551
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:00.563
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:00.564
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-887" @ 05/12/23 13:09:00.566
  May 12 13:09:00.569: INFO: saUsername: "system:serviceaccount:subjectreview-887:e2e"
  May 12 13:09:00.569: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-887"}
  May 12 13:09:00.570: INFO: saUID: "b30a148c-0324-4722-8a38-646a74463c87"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-887:e2e" @ 05/12/23 13:09:00.57
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-887:e2e" @ 05/12/23 13:09:00.57
  May 12 13:09:00.571: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-887:e2e" api 'list' configmaps in "subjectreview-887" namespace @ 05/12/23 13:09:00.571
  May 12 13:09:00.572: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-887:e2e" @ 05/12/23 13:09:00.572
  May 12 13:09:00.573: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  May 12 13:09:00.573: INFO: LocalSubjectAccessReview has been verified
  May 12 13:09:00.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-887" for this suite. @ 05/12/23 13:09:00.576
• [0.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 05/12/23 13:09:00.582
  May 12 13:09:00.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:09:00.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:00.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:00.595
  STEP: Creating configMap with name projected-configmap-test-volume-6cac6ecd-a844-436c-b3df-5eab695041ce @ 05/12/23 13:09:00.596
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:09:00.601
  STEP: Saw pod success @ 05/12/23 13:09:04.614
  May 12 13:09:04.616: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-configmaps-6064cadd-0f3b-45a8-9364-9d580aa21eb2 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:09:04.624
  May 12 13:09:04.638: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8488" for this suite. @ 05/12/23 13:09:04.641
• [4.063 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 05/12/23 13:09:04.646
  May 12 13:09:04.646: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 13:09:04.647
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:04.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:04.66
  STEP: creating the pod @ 05/12/23 13:09:04.662
  STEP: submitting the pod to kubernetes @ 05/12/23 13:09:04.662
  W0512 13:09:04.667012      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 05/12/23 13:09:06.673
  STEP: updating the pod @ 05/12/23 13:09:06.674
  May 12 13:09:07.184: INFO: Successfully updated pod "pod-update-activedeadlineseconds-302e1da2-e489-4b9e-a072-0e33f55f69b4"
  May 12 13:09:11.191: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6856" for this suite. @ 05/12/23 13:09:11.194
• [6.552 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 05/12/23 13:09:11.2
  May 12 13:09:11.200: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:09:11.201
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:11.212
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:11.217
  STEP: Creating configMap configmap-8687/configmap-test-45f82bd0-0f6c-4b50-bf10-1b593b3e4b4c @ 05/12/23 13:09:11.218
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:09:11.222
  STEP: Saw pod success @ 05/12/23 13:09:15.244
  May 12 13:09:15.246: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-6c960678-e25d-4852-b699-297d55b57f35 container env-test: <nil>
  STEP: delete the pod @ 05/12/23 13:09:15.252
  May 12 13:09:15.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8687" for this suite. @ 05/12/23 13:09:15.267
• [4.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 05/12/23 13:09:15.276
  May 12 13:09:15.276: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename security-context-test @ 05/12/23 13:09:15.277
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:15.29
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:15.291
  May 12 13:09:19.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7056" for this suite. @ 05/12/23 13:09:19.339
• [4.077 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 05/12/23 13:09:19.362
  May 12 13:09:19.362: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/23 13:09:19.363
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:19.375
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:19.376
  May 12 13:09:19.378: INFO: Got root ca configmap in namespace "svcaccounts-3766"
  May 12 13:09:19.381: INFO: Deleted root ca configmap in namespace "svcaccounts-3766"
  STEP: waiting for a new root ca configmap created @ 05/12/23 13:09:19.882
  May 12 13:09:19.891: INFO: Recreated root ca configmap in namespace "svcaccounts-3766"
  May 12 13:09:19.911: INFO: Updated root ca configmap in namespace "svcaccounts-3766"
  STEP: waiting for the root ca configmap reconciled @ 05/12/23 13:09:20.412
  May 12 13:09:20.413: INFO: Reconciled root ca configmap in namespace "svcaccounts-3766"
  May 12 13:09:20.413: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3766" for this suite. @ 05/12/23 13:09:20.417
• [1.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 05/12/23 13:09:20.431
  May 12 13:09:20.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 13:09:20.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:20.448
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:20.45
  May 12 13:09:42.520: INFO: Container started at 2023-05-12 13:09:21 +0000 UTC, pod became ready at 2023-05-12 13:09:40 +0000 UTC
  May 12 13:09:42.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-7870" for this suite. @ 05/12/23 13:09:42.523
• [22.095 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 05/12/23 13:09:42.531
  May 12 13:09:42.531: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/23 13:09:42.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:42.547
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:42.552
  STEP: Deleting RuntimeClass runtimeclass-1329-delete-me @ 05/12/23 13:09:42.564
  STEP: Waiting for the RuntimeClass to disappear @ 05/12/23 13:09:42.568
  May 12 13:09:42.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1329" for this suite. @ 05/12/23 13:09:42.575
• [0.049 seconds]
------------------------------
SSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 05/12/23 13:09:42.58
  May 12 13:09:42.581: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename statefulset @ 05/12/23 13:09:42.581
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:09:42.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:09:42.597
  STEP: Creating service test in namespace statefulset-7367 @ 05/12/23 13:09:42.598
  May 12 13:09:42.614: INFO: Found 0 stateful pods, waiting for 1
  May 12 13:09:52.616: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 05/12/23 13:09:52.619
  W0512 13:09:52.624867      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 12 13:09:52.630: INFO: Found 1 stateful pods, waiting for 2
  May 12 13:10:02.633: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 12 13:10:02.633: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 05/12/23 13:10:02.636
  STEP: Delete all of the StatefulSets @ 05/12/23 13:10:02.638
  STEP: Verify that StatefulSets have been deleted @ 05/12/23 13:10:02.641
  May 12 13:10:02.643: INFO: Deleting all statefulset in ns statefulset-7367
  May 12 13:10:02.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-7367" for this suite. @ 05/12/23 13:10:02.678
• [20.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 05/12/23 13:10:02.685
  May 12 13:10:02.685: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:10:02.686
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:10:02.7
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:10:02.702
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/12/23 13:10:02.703
  STEP: Saw pod success @ 05/12/23 13:10:06.731
  May 12 13:10:06.740: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-1ca88b0e-bc4b-4a13-859c-071f34a344c5 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:10:06.757
  May 12 13:10:06.777: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1031" for this suite. @ 05/12/23 13:10:06.781
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 05/12/23 13:10:06.791
  May 12 13:10:06.791: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename statefulset @ 05/12/23 13:10:06.792
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:10:06.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:10:06.805
  STEP: Creating service test in namespace statefulset-6222 @ 05/12/23 13:10:06.806
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 05/12/23 13:10:06.812
  STEP: Creating stateful set ss in namespace statefulset-6222 @ 05/12/23 13:10:06.817
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-6222 @ 05/12/23 13:10:06.822
  May 12 13:10:06.825: INFO: Found 0 stateful pods, waiting for 1
  May 12 13:10:16.833: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 05/12/23 13:10:16.835
  May 12 13:10:16.843: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-6222 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 13:10:16.994: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 13:10:16.994: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 13:10:16.994: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 13:10:16.996: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  May 12 13:10:27.004: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 12 13:10:27.004: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 13:10:27.043: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999461s
  May 12 13:10:28.047: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.990814458s
  May 12 13:10:29.050: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98702414s
  May 12 13:10:30.055: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.984051887s
  May 12 13:10:31.058: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.978458844s
  May 12 13:10:32.062: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.975921583s
  May 12 13:10:33.074: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.972184204s
  May 12 13:10:34.080: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.959267019s
  May 12 13:10:35.083: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.953855162s
  May 12 13:10:36.085: INFO: Verifying statefulset ss doesn't scale past 1 for another 951.63971ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-6222 @ 05/12/23 13:10:37.085
  May 12 13:10:37.095: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-6222 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 13:10:37.265: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 12 13:10:37.265: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 13:10:37.265: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 12 13:10:37.267: INFO: Found 1 stateful pods, waiting for 3
  May 12 13:10:47.271: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  May 12 13:10:47.271: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  May 12 13:10:47.271: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 05/12/23 13:10:47.271
  STEP: Scale down will halt with unhealthy stateful pod @ 05/12/23 13:10:47.271
  May 12 13:10:47.276: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-6222 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 13:10:47.401: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 13:10:47.401: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 13:10:47.401: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 13:10:47.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-6222 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 13:10:47.529: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 13:10:47.529: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 13:10:47.529: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 13:10:47.529: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-6222 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  May 12 13:10:47.664: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  May 12 13:10:47.664: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  May 12 13:10:47.664: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  May 12 13:10:47.664: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 13:10:47.666: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  May 12 13:10:57.671: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  May 12 13:10:57.671: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  May 12 13:10:57.671: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  May 12 13:10:57.682: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.999999768s
  May 12 13:10:58.699: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.989743376s
  May 12 13:10:59.703: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.9782376s
  May 12 13:11:00.715: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.974786913s
  May 12 13:11:01.718: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.962695441s
  May 12 13:11:02.728: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.95953241s
  May 12 13:11:03.740: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.945226606s
  May 12 13:11:04.749: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.937763469s
  May 12 13:11:05.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.927967515s
  May 12 13:11:06.792: INFO: Verifying statefulset ss doesn't scale past 3 for another 904.686111ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-6222 @ 05/12/23 13:11:07.793
  May 12 13:11:07.823: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-6222 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 13:11:07.971: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 12 13:11:07.971: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 13:11:07.971: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 12 13:11:07.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-6222 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 13:11:08.099: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 12 13:11:08.099: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 13:11:08.099: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 12 13:11:08.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=statefulset-6222 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  May 12 13:11:08.214: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  May 12 13:11:08.214: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  May 12 13:11:08.214: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  May 12 13:11:08.214: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 05/12/23 13:11:18.232
  May 12 13:11:18.235: INFO: Deleting all statefulset in ns statefulset-6222
  May 12 13:11:18.246: INFO: Scaling statefulset ss to 0
  May 12 13:11:18.260: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 13:11:18.263: INFO: Deleting statefulset ss
  May 12 13:11:18.272: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6222" for this suite. @ 05/12/23 13:11:18.276
• [71.489 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 05/12/23 13:11:18.28
  May 12 13:11:18.280: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replicaset @ 05/12/23 13:11:18.281
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:11:18.294
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:11:18.295
  May 12 13:11:18.304: INFO: Pod name sample-pod: Found 0 pods out of 1
  May 12 13:11:23.317: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 05/12/23 13:11:23.317
  STEP: Scaling up "test-rs" replicaset  @ 05/12/23 13:11:23.318
  May 12 13:11:23.327: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 05/12/23 13:11:23.327
  W0512 13:11:23.342942      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 12 13:11:23.350: INFO: observed ReplicaSet test-rs in namespace replicaset-3476 with ReadyReplicas 1, AvailableReplicas 1
  May 12 13:11:23.356: INFO: observed ReplicaSet test-rs in namespace replicaset-3476 with ReadyReplicas 1, AvailableReplicas 1
  May 12 13:11:23.377: INFO: observed ReplicaSet test-rs in namespace replicaset-3476 with ReadyReplicas 1, AvailableReplicas 1
  May 12 13:11:23.382: INFO: observed ReplicaSet test-rs in namespace replicaset-3476 with ReadyReplicas 1, AvailableReplicas 1
  May 12 13:11:24.550: INFO: observed ReplicaSet test-rs in namespace replicaset-3476 with ReadyReplicas 2, AvailableReplicas 2
  May 12 13:11:24.612: INFO: observed Replicaset test-rs in namespace replicaset-3476 with ReadyReplicas 3 found true
  May 12 13:11:24.612: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3476" for this suite. @ 05/12/23 13:11:24.615
• [6.338 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 05/12/23 13:11:24.624
  May 12 13:11:24.624: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:11:24.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:11:24.638
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:11:24.639
  STEP: Creating the pod @ 05/12/23 13:11:24.64
  May 12 13:11:27.194: INFO: Successfully updated pod "labelsupdate33186f7b-8f22-457f-bcd6-c8b11020bb3b"
  May 12 13:11:31.223: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9952" for this suite. @ 05/12/23 13:11:31.226
• [6.606 seconds]
------------------------------
SSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 05/12/23 13:11:31.231
  May 12 13:11:31.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename certificates @ 05/12/23 13:11:31.232
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:11:31.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:11:31.248
  STEP: getting /apis @ 05/12/23 13:11:31.661
  STEP: getting /apis/certificates.k8s.io @ 05/12/23 13:11:31.665
  STEP: getting /apis/certificates.k8s.io/v1 @ 05/12/23 13:11:31.666
  STEP: creating @ 05/12/23 13:11:31.667
  STEP: getting @ 05/12/23 13:11:31.677
  STEP: listing @ 05/12/23 13:11:31.678
  STEP: watching @ 05/12/23 13:11:31.68
  May 12 13:11:31.680: INFO: starting watch
  STEP: patching @ 05/12/23 13:11:31.681
  STEP: updating @ 05/12/23 13:11:31.684
  May 12 13:11:31.688: INFO: waiting for watch events with expected annotations
  May 12 13:11:31.688: INFO: saw patched and updated annotations
  STEP: getting /approval @ 05/12/23 13:11:31.688
  STEP: patching /approval @ 05/12/23 13:11:31.69
  STEP: updating /approval @ 05/12/23 13:11:31.694
  STEP: getting /status @ 05/12/23 13:11:31.698
  STEP: patching /status @ 05/12/23 13:11:31.7
  STEP: updating /status @ 05/12/23 13:11:31.705
  STEP: deleting @ 05/12/23 13:11:31.709
  STEP: deleting a collection @ 05/12/23 13:11:31.714
  May 12 13:11:31.720: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-5630" for this suite. @ 05/12/23 13:11:31.724
• [0.497 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 05/12/23 13:11:31.738
  May 12 13:11:31.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pod-network-test @ 05/12/23 13:11:31.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:11:31.75
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:11:31.753
  STEP: Performing setup for networking test in namespace pod-network-test-7995 @ 05/12/23 13:11:31.754
  STEP: creating a selector @ 05/12/23 13:11:31.755
  STEP: Creating the service pods in kubernetes @ 05/12/23 13:11:31.755
  May 12 13:11:31.755: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 05/12/23 13:11:43.848
  May 12 13:11:45.877: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 12 13:11:45.877: INFO: Breadth first check of 10.42.2.120 on host 172.16.100.5...
  May 12 13:11:45.884: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.249:9080/dial?request=hostname&protocol=http&host=10.42.2.120&port=8083&tries=1'] Namespace:pod-network-test-7995 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:11:45.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:11:45.887: INFO: ExecWithOptions: Clientset creation
  May 12 13:11:45.887: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7995/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.249%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.2.120%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 12 13:11:45.977: INFO: Waiting for responses: map[]
  May 12 13:11:45.977: INFO: reached 10.42.2.120 after 0/1 tries
  May 12 13:11:45.977: INFO: Breadth first check of 10.42.3.248 on host 172.16.100.7...
  May 12 13:11:45.979: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.249:9080/dial?request=hostname&protocol=http&host=10.42.3.248&port=8083&tries=1'] Namespace:pod-network-test-7995 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:11:45.979: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:11:45.980: INFO: ExecWithOptions: Clientset creation
  May 12 13:11:45.980: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-7995/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.249%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.42.3.248%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 12 13:11:46.044: INFO: Waiting for responses: map[]
  May 12 13:11:46.044: INFO: reached 10.42.3.248 after 0/1 tries
  May 12 13:11:46.044: INFO: Going to retry 0 out of 2 pods....
  May 12 13:11:46.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7995" for this suite. @ 05/12/23 13:11:46.047
• [14.311 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 05/12/23 13:11:46.052
  May 12 13:11:46.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 13:11:46.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:11:46.065
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:11:46.066
  STEP: Saw pod success @ 05/12/23 13:11:52.2
  May 12 13:11:52.213: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod client-envvars-c1bb12a6-60f7-42de-b471-11b0db82bb68 container env3cont: <nil>
  STEP: delete the pod @ 05/12/23 13:11:52.23
  May 12 13:11:52.246: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3267" for this suite. @ 05/12/23 13:11:52.25
• [6.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 05/12/23 13:11:52.255
  May 12 13:11:52.255: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 13:11:52.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:11:52.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:11:52.27
  STEP: Creating secret with name secret-test-a88807d0-422c-473c-a741-73ca9990258b @ 05/12/23 13:11:52.271
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:11:52.274
  STEP: Saw pod success @ 05/12/23 13:11:56.291
  May 12 13:11:56.296: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-secrets-59f0977a-3d11-4a83-9edc-5a9af4b0367d container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:11:56.301
  May 12 13:11:56.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6529" for this suite. @ 05/12/23 13:11:56.32
• [4.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 05/12/23 13:11:56.329
  May 12 13:11:56.329: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/12/23 13:11:56.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:11:56.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:11:56.348
  STEP: Creating 50 configmaps @ 05/12/23 13:11:56.349
  STEP: Creating RC which spawns configmap-volume pods @ 05/12/23 13:11:56.586
  May 12 13:11:56.766: INFO: Pod name wrapped-volume-race-49b4dcc5-7d2f-43a7-936e-f141aff03d2e: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/12/23 13:11:56.766
  STEP: Creating RC which spawns configmap-volume pods @ 05/12/23 13:12:00.811
  May 12 13:12:00.830: INFO: Pod name wrapped-volume-race-b0cb322b-0aac-4b39-ab87-2b29a7fb142c: Found 0 pods out of 5
  May 12 13:12:05.857: INFO: Pod name wrapped-volume-race-b0cb322b-0aac-4b39-ab87-2b29a7fb142c: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/12/23 13:12:05.857
  STEP: Creating RC which spawns configmap-volume pods @ 05/12/23 13:12:05.881
  May 12 13:12:05.905: INFO: Pod name wrapped-volume-race-311411fa-6f58-451e-b458-b3d18daafa7d: Found 1 pods out of 5
  May 12 13:12:10.913: INFO: Pod name wrapped-volume-race-311411fa-6f58-451e-b458-b3d18daafa7d: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 05/12/23 13:12:10.913
  May 12 13:12:10.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-311411fa-6f58-451e-b458-b3d18daafa7d in namespace emptydir-wrapper-8203, will wait for the garbage collector to delete the pods @ 05/12/23 13:12:10.937
  May 12 13:12:11.006: INFO: Deleting ReplicationController wrapped-volume-race-311411fa-6f58-451e-b458-b3d18daafa7d took: 16.858161ms
  May 12 13:12:11.114: INFO: Terminating ReplicationController wrapped-volume-race-311411fa-6f58-451e-b458-b3d18daafa7d pods took: 107.537875ms
  STEP: deleting ReplicationController wrapped-volume-race-b0cb322b-0aac-4b39-ab87-2b29a7fb142c in namespace emptydir-wrapper-8203, will wait for the garbage collector to delete the pods @ 05/12/23 13:12:13.015
  May 12 13:12:13.086: INFO: Deleting ReplicationController wrapped-volume-race-b0cb322b-0aac-4b39-ab87-2b29a7fb142c took: 17.857902ms
  May 12 13:12:13.187: INFO: Terminating ReplicationController wrapped-volume-race-b0cb322b-0aac-4b39-ab87-2b29a7fb142c pods took: 100.858998ms
  STEP: deleting ReplicationController wrapped-volume-race-49b4dcc5-7d2f-43a7-936e-f141aff03d2e in namespace emptydir-wrapper-8203, will wait for the garbage collector to delete the pods @ 05/12/23 13:12:15.191
  May 12 13:12:15.253: INFO: Deleting ReplicationController wrapped-volume-race-49b4dcc5-7d2f-43a7-936e-f141aff03d2e took: 5.20686ms
  May 12 13:12:15.354: INFO: Terminating ReplicationController wrapped-volume-race-49b4dcc5-7d2f-43a7-936e-f141aff03d2e pods took: 101.056909ms
  STEP: Cleaning up the configMaps @ 05/12/23 13:12:17.156
  STEP: Destroying namespace "emptydir-wrapper-8203" for this suite. @ 05/12/23 13:12:17.369
• [21.046 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 05/12/23 13:12:17.376
  May 12 13:12:17.376: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename server-version @ 05/12/23 13:12:17.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:12:17.393
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:12:17.396
  STEP: Request ServerVersion @ 05/12/23 13:12:17.398
  STEP: Confirm major version @ 05/12/23 13:12:17.399
  May 12 13:12:17.399: INFO: Major version: 1
  STEP: Confirm minor version @ 05/12/23 13:12:17.399
  May 12 13:12:17.399: INFO: cleanMinorVersion: 27
  May 12 13:12:17.399: INFO: Minor version: 27
  May 12 13:12:17.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-8425" for this suite. @ 05/12/23 13:12:17.403
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 05/12/23 13:12:17.411
  May 12 13:12:17.411: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename namespaces @ 05/12/23 13:12:17.412
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:12:17.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:12:17.429
  STEP: Creating a test namespace @ 05/12/23 13:12:17.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:12:17.451
  STEP: Creating a service in the namespace @ 05/12/23 13:12:17.453
  STEP: Deleting the namespace @ 05/12/23 13:12:17.466
  STEP: Waiting for the namespace to be removed. @ 05/12/23 13:12:17.481
  STEP: Recreating the namespace @ 05/12/23 13:12:23.488
  STEP: Verifying there is no service in the namespace @ 05/12/23 13:12:23.525
  May 12 13:12:23.530: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-1152" for this suite. @ 05/12/23 13:12:23.536
  STEP: Destroying namespace "nsdeletetest-4767" for this suite. @ 05/12/23 13:12:23.541
  May 12 13:12:23.543: INFO: Namespace nsdeletetest-4767 was already deleted
  STEP: Destroying namespace "nsdeletetest-4484" for this suite. @ 05/12/23 13:12:23.543
• [6.135 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 05/12/23 13:12:23.547
  May 12 13:12:23.547: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:12:23.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:12:23.559
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:12:23.561
  May 12 13:12:23.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-2079" for this suite. @ 05/12/23 13:12:23.565
• [0.021 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 05/12/23 13:12:23.57
  May 12 13:12:23.570: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename cronjob @ 05/12/23 13:12:23.57
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:12:23.58
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:12:23.582
  STEP: Creating a ForbidConcurrent cronjob @ 05/12/23 13:12:23.583
  STEP: Ensuring a job is scheduled @ 05/12/23 13:12:23.586
  STEP: Ensuring exactly one is scheduled @ 05/12/23 13:13:01.595
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/12/23 13:13:01.605
  STEP: Ensuring no more jobs are scheduled @ 05/12/23 13:13:01.614
  STEP: Removing cronjob @ 05/12/23 13:18:01.621
  May 12 13:18:01.626: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2117" for this suite. @ 05/12/23 13:18:01.637
• [338.082 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 05/12/23 13:18:01.652
  May 12 13:18:01.652: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:18:01.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:01.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:01.677
  STEP: Creating configMap with name configmap-test-volume-map-872e46dc-5115-48c6-ad58-a2a983a62bc0 @ 05/12/23 13:18:01.679
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:18:01.682
  STEP: Saw pod success @ 05/12/23 13:18:05.711
  May 12 13:18:05.715: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-2b8132d5-a2bf-4197-a299-ea56e6236bb8 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:18:05.733
  May 12 13:18:05.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5106" for this suite. @ 05/12/23 13:18:05.747
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 05/12/23 13:18:05.752
  May 12 13:18:05.752: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:18:05.753
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:05.765
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:05.766
  STEP: creating service in namespace services-2721 @ 05/12/23 13:18:05.767
  STEP: creating service affinity-clusterip in namespace services-2721 @ 05/12/23 13:18:05.768
  STEP: creating replication controller affinity-clusterip in namespace services-2721 @ 05/12/23 13:18:05.774
  I0512 13:18:05.791808      20 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-2721, replica count: 3
  I0512 13:18:08.844791      20 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 13:18:08.854: INFO: Creating new exec pod
  May 12 13:18:11.871: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-2721 exec execpod-affinityrtfzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  May 12 13:18:12.025: INFO: stderr: "+ + ncecho hostName\n -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  May 12 13:18:12.025: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:18:12.025: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-2721 exec execpod-affinityrtfzx -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.146.9 80'
  May 12 13:18:12.171: INFO: stderr: "+ nc -v -t -w 2 10.43.146.9 80\nConnection to 10.43.146.9 80 port [tcp/http] succeeded!\n+ echo hostName\n"
  May 12 13:18:12.171: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:18:12.172: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-2721 exec execpod-affinityrtfzx -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.146.9:80/ ; done'
  May 12 13:18:12.401: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.146.9:80/\n"
  May 12 13:18:12.401: INFO: stdout: "\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb\naffinity-clusterip-59gbb"
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Received response from host: affinity-clusterip-59gbb
  May 12 13:18:12.401: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:18:12.404: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-2721, will wait for the garbage collector to delete the pods @ 05/12/23 13:18:12.415
  May 12 13:18:12.471: INFO: Deleting ReplicationController affinity-clusterip took: 3.167972ms
  May 12 13:18:12.572: INFO: Terminating ReplicationController affinity-clusterip pods took: 101.233612ms
  STEP: Destroying namespace "services-2721" for this suite. @ 05/12/23 13:18:14.816
• [9.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 05/12/23 13:18:14.826
  May 12 13:18:14.826: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:18:14.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:14.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:14.84
  STEP: Setting up server cert @ 05/12/23 13:18:14.854
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:18:15.297
  STEP: Deploying the webhook pod @ 05/12/23 13:18:15.301
  STEP: Wait for the deployment to be ready @ 05/12/23 13:18:15.311
  May 12 13:18:15.320: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 05/12/23 13:18:17.329
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:18:17.337
  May 12 13:18:18.337: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 05/12/23 13:18:18.339
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 05/12/23 13:18:18.353
  STEP: Creating a configMap that should not be mutated @ 05/12/23 13:18:18.357
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 05/12/23 13:18:18.364
  STEP: Creating a configMap that should be mutated @ 05/12/23 13:18:18.373
  May 12 13:18:18.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6823" for this suite. @ 05/12/23 13:18:18.44
  STEP: Destroying namespace "webhook-markers-6523" for this suite. @ 05/12/23 13:18:18.45
• [3.631 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 05/12/23 13:18:18.458
  May 12 13:18:18.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename deployment @ 05/12/23 13:18:18.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:18.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:18.474
  May 12 13:18:18.475: INFO: Creating deployment "webserver-deployment"
  May 12 13:18:18.480: INFO: Waiting for observed generation 1
  May 12 13:18:20.492: INFO: Waiting for all required pods to come up
  May 12 13:18:20.511: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 05/12/23 13:18:20.512
  May 12 13:18:22.532: INFO: Waiting for deployment "webserver-deployment" to complete
  May 12 13:18:22.535: INFO: Updating deployment "webserver-deployment" with a non-existent image
  May 12 13:18:22.541: INFO: Updating deployment webserver-deployment
  May 12 13:18:22.541: INFO: Waiting for observed generation 2
  May 12 13:18:24.545: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  May 12 13:18:24.547: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  May 12 13:18:24.548: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 12 13:18:24.552: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  May 12 13:18:24.552: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  May 12 13:18:24.554: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  May 12 13:18:24.559: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  May 12 13:18:24.559: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  May 12 13:18:24.568: INFO: Updating deployment webserver-deployment
  May 12 13:18:24.568: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  May 12 13:18:24.581: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  May 12 13:18:26.607: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  May 12 13:18:26.612: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-137  2aead6bd-69f1-4023-8c7b-bb4fafc3914e 92568 3 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:18:26 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0055e8018 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:10,UnavailableReplicas:23,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-12 13:18:24 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-05-12 13:18:26 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,},},ReadyReplicas:10,CollisionCount:nil,},}

  May 12 13:18:26.615: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-137  d5ed371f-ce61-48f6-b81e-2ec05f961f43 92461 3 2023-05-12 13:18:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 2aead6bd-69f1-4023-8c7b-bb4fafc3914e 0xc003844df7 0xc003844df8}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2aead6bd-69f1-4023-8c7b-bb4fafc3914e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003844e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 12 13:18:26.616: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  May 12 13:18:26.616: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-137  06612204-9c4c-4222-a35c-42a79b06b882 92567 3 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 2aead6bd-69f1-4023-8c7b-bb4fafc3914e 0xc003844d07 0xc003844d08}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"2aead6bd-69f1-4023-8c7b-bb4fafc3914e\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:18:26 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003844d98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:10,AvailableReplicas:10,Conditions:[]ReplicaSetCondition{},},}
  May 12 13:18:26.622: INFO: Pod "webserver-deployment-67bd4bf6dc-2txf6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2txf6 webserver-deployment-67bd4bf6dc- deployment-137  bc3e1d8b-0496-4bb3-af54-2af15a054e9d 92535 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:feceeb9924534f42badd269615c86739044632a5f3e4a90883c6050d35a56132 cni.projectcalico.org/podIP:10.42.3.28/32 cni.projectcalico.org/podIPs:10.42.3.28/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e8457 0xc0055e8458}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-75lkj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-75lkj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.623: INFO: Pod "webserver-deployment-67bd4bf6dc-582n6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-582n6 webserver-deployment-67bd4bf6dc- deployment-137  a7d07425-940d-4fb0-855c-dd24382cc7b0 92503 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:d43991b70cd7dfee24556c50fd34f493267fb8de36a95b4f23cf4c3304297a01 cni.projectcalico.org/podIP:10.42.2.139/32 cni.projectcalico.org/podIPs:10.42.2.139/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e8657 0xc0055e8658}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-62mnb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-62mnb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.623: INFO: Pod "webserver-deployment-67bd4bf6dc-5z2hb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5z2hb webserver-deployment-67bd4bf6dc- deployment-137  f6382d14-df09-4ef5-a2e5-504f667e39f3 92566 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:885fd554e5b17998829e6c3e585bca128753eaddec886bf401b26969e5a514fd cni.projectcalico.org/podIP:10.42.3.19/32 cni.projectcalico.org/podIPs:10.42.3.19/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e8857 0xc0055e8858}] [] [{calico Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.19\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lxd9t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lxd9t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.19,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://1b2ef4dc137a0cf6a4013f8908a3c4a30a5a370ae18a8a7fa92814e29cf0b027,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.19,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.623: INFO: Pod "webserver-deployment-67bd4bf6dc-6lqxk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6lqxk webserver-deployment-67bd4bf6dc- deployment-137  a91901dc-f92f-45c1-a203-5cdb4bee6ed2 92536 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:61e08f85c3ee9166ec7db9140e34c07cb13748dc7dcd103bdc1e1197b8e97e13 cni.projectcalico.org/podIP:10.42.2.146/32 cni.projectcalico.org/podIPs:10.42.2.146/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e8a60 0xc0055e8a61}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bkbdg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bkbdg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.624: INFO: Pod "webserver-deployment-67bd4bf6dc-7rvqm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7rvqm webserver-deployment-67bd4bf6dc- deployment-137  e41bfd35-2b35-4547-82ee-b433688f2189 92515 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:f61e4d73985ed30601df0259cd1cad8cf8ebfc0b9dded05fdbc26155d8615fe1 cni.projectcalico.org/podIP:10.42.3.21/32 cni.projectcalico.org/podIPs:10.42.3.21/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e8c67 0xc0055e8c68}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tqs6h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tqs6h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.624: INFO: Pod "webserver-deployment-67bd4bf6dc-977r4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-977r4 webserver-deployment-67bd4bf6dc- deployment-137  17387656-9039-4ad5-a8dd-a12abf8c607d 92530 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:06a8f67b0f51d740aed6d63ccb3deec47ca3c1f1286a45b80ad852a44cde9b98 cni.projectcalico.org/podIP:10.42.2.144/32 cni.projectcalico.org/podIPs:10.42.2.144/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e8e67 0xc0055e8e68}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-65xgw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-65xgw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.627: INFO: Pod "webserver-deployment-67bd4bf6dc-bjtz7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-bjtz7 webserver-deployment-67bd4bf6dc- deployment-137  023da47f-ab74-42bc-9159-00ce4501ccca 92264 0 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:fbb25dfde09774d68435280ed3f566468d2ba76eaff15fce9e157fb05834f968 cni.projectcalico.org/podIP:10.42.2.134/32 cni.projectcalico.org/podIPs:10.42.2.134/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e9067 0xc0055e9068}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.134\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g999v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g999v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:10.42.2.134,StartTime:2023-05-12 13:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://71e2cc5d6389c46a6fa8abf1e1ec218eca43d97f99a69c21c1bac98d2f83331f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.134,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.627: INFO: Pod "webserver-deployment-67bd4bf6dc-dqscm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dqscm webserver-deployment-67bd4bf6dc- deployment-137  3983fc0a-05e7-4c0c-bf30-56dc45cc8d69 92512 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:9ea9b9f65a7d2ca038f8df63b9aeb4ca99d9de1f397d3b45a1760eff84dcf83b cni.projectcalico.org/podIP:10.42.2.141/32 cni.projectcalico.org/podIPs:10.42.2.141/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e9280 0xc0055e9281}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xw9k2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xw9k2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.628: INFO: Pod "webserver-deployment-67bd4bf6dc-gv2vk" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-gv2vk webserver-deployment-67bd4bf6dc- deployment-137  03b17c3f-844e-4256-8b9a-dffb4fdc0c41 92245 0 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:3b5c133fa933dc608f13a2103a0526dbdcfc7e515e7230abf6454c7043a9b804 cni.projectcalico.org/podIP:10.42.3.13/32 cni.projectcalico.org/podIPs:10.42.3.13/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e9477 0xc0055e9478}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.13\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k87wx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k87wx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.13,StartTime:2023-05-12 13:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://f68f3dbb7dfae597df085444f0349c5117ad9dbaab43bd679462a7abcc20dc5e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.13,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.628: INFO: Pod "webserver-deployment-67bd4bf6dc-jnsg5" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jnsg5 webserver-deployment-67bd4bf6dc- deployment-137  ef7303ce-24d4-47b0-9a1c-0618da7a2fe8 92266 0 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:fff35b19ee24583a736af1fe83827c4b839be3af85450c90473161ea0b1a7e1c cni.projectcalico.org/podIP:10.42.2.135/32 cni.projectcalico.org/podIPs:10.42.2.135/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e9690 0xc0055e9691}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sg24f,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sg24f,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:10.42.2.135,StartTime:2023-05-12 13:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://c494f7fddfec4f2915f791bec35b0bc1e87df92f8bab245b5363971ca33f6ff9,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.135,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.629: INFO: Pod "webserver-deployment-67bd4bf6dc-k5fp4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-k5fp4 webserver-deployment-67bd4bf6dc- deployment-137  2c8a4f7c-c022-4e66-8763-1b20f0698715 92529 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:5b7cad3a7f3c78f3aa47fec9e04282ac80b0ea6386ac4ea327f09d8feaa9f229 cni.projectcalico.org/podIP:10.42.2.143/32 cni.projectcalico.org/podIPs:10.42.2.143/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e98c0 0xc0055e98c1}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b5wkw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b5wkw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.629: INFO: Pod "webserver-deployment-67bd4bf6dc-k77gn" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-k77gn webserver-deployment-67bd4bf6dc- deployment-137  af3cb757-4236-4dd0-bbd6-703f91dc45d3 92249 0 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:daa4d948d5716338ae78f6a60fd867ff31b130bb53f90f9bbb2e7c3541b49e7f cni.projectcalico.org/podIP:10.42.3.15/32 cni.projectcalico.org/podIPs:10.42.3.15/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e9ab7 0xc0055e9ab8}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.15\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t26gm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t26gm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.15,StartTime:2023-05-12 13:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://b42ec1f9f833c9ef98f342a1039d9597c3184a23e5006bcb03f351c7a8e03be0,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.15,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.630: INFO: Pod "webserver-deployment-67bd4bf6dc-ljt2k" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ljt2k webserver-deployment-67bd4bf6dc- deployment-137  d9895a2a-ed48-45b6-af96-ec0dd78561c9 92269 0 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:a8723161f912faa099c8081e54b5cdc632facc85351ea22662a90abea399ce3e cni.projectcalico.org/podIP:10.42.2.136/32 cni.projectcalico.org/podIPs:10.42.2.136/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e9cd0 0xc0055e9cd1}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.136\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lhdmj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lhdmj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:10.42.2.136,StartTime:2023-05-12 13:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://6c7e623701f9e68e0659e2f08ec89691a0801ec6f7d178ffc33a3e64e585f710,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.136,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.630: INFO: Pod "webserver-deployment-67bd4bf6dc-n45d7" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-n45d7 webserver-deployment-67bd4bf6dc- deployment-137  9f661fbc-792d-41d9-bb3e-dff7bebe2ba2 92261 0 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:7105a6d18bf28eb2937e8df7c2490c390795bbfd4794a51ed166253b81ce97d6 cni.projectcalico.org/podIP:10.42.2.133/32 cni.projectcalico.org/podIPs:10.42.2.133/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0055e9ee0 0xc0055e9ee1}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.133\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b772w,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b772w,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:10.42.2.133,StartTime:2023-05-12 13:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://5c95faec69117e98e30229e72a0b19b908e6a4cc8fb13f5146f071c6f5cbbe17,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.133,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.631: INFO: Pod "webserver-deployment-67bd4bf6dc-nv9kr" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nv9kr webserver-deployment-67bd4bf6dc- deployment-137  4d22cb35-c8f1-460d-80b7-f652655da89a 92244 0 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:73f96186981d7ba1f5ccfca6907b3feacc1044114b1e39e2c104f2cef4dcc989 cni.projectcalico.org/podIP:10.42.3.12/32 cni.projectcalico.org/podIPs:10.42.3.12/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc0054180f0 0xc0054180f1}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.12\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vb625,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vb625,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.12,StartTime:2023-05-12 13:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://7a45577fbfd6124ac70e5ab4b20aa57942d650da3b1d28cb12351c0f9975f6ac,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.12,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.631: INFO: Pod "webserver-deployment-67bd4bf6dc-pg45b" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-pg45b webserver-deployment-67bd4bf6dc- deployment-137  2dfe6ea2-0f04-4edc-b047-f17395bf00de 92562 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:d4ea1c7c03ed54ee1813ae0ef9be714437efbe07764e56a839c31c940b5c2227 cni.projectcalico.org/podIP:10.42.3.20/32 cni.projectcalico.org/podIPs:10.42.3.20/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc005418310 0xc005418311}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:26 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.20\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rl4jf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rl4jf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:26 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.20,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:26 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://568a7a14674d750af4fd834f72e67d185dbeda48e887b08052451792d2a8d6c1,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.20,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.631: INFO: Pod "webserver-deployment-67bd4bf6dc-ps9h8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-ps9h8 webserver-deployment-67bd4bf6dc- deployment-137  99696e2d-2836-49a0-9541-02af6e5947bd 92533 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:4a768bd5617dfb9c004709bc3d38a35aa29704502dbb9f386435bd908a01cbff cni.projectcalico.org/podIP:10.42.2.148/32 cni.projectcalico.org/podIPs:10.42.2.148/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc005418520 0xc005418521}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hzbdf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hzbdf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.632: INFO: Pod "webserver-deployment-67bd4bf6dc-r84jv" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-r84jv webserver-deployment-67bd4bf6dc- deployment-137  b0970a79-8207-4235-8d84-cd1b8a4761db 92253 0 2023-05-12 13:18:18 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:d5feaf750f8abcee911be8f8e0f3367a20cbd0790ea4e45fab3ef04dd0ea7d40 cni.projectcalico.org/podIP:10.42.3.14/32 cni.projectcalico.org/podIPs:10.42.3.14/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc005418737 0xc005418738}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:18 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:19 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.14\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w5jbd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w5jbd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:18 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.14,StartTime:2023-05-12 13:18:18 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:18:20 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://254655b649c007810124690fa80bbae7659b26679e3c3b19a38f5e6295e0c3c7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.14,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.632: INFO: Pod "webserver-deployment-67bd4bf6dc-x942x" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-x942x webserver-deployment-67bd4bf6dc- deployment-137  409f3f73-38d8-42ad-b532-f7058a556d9a 92532 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:fa4e2168a0db42ce87f66913a88a2b57149ecdf8e681fff79cfdcd0e72490517 cni.projectcalico.org/podIP:10.42.3.27/32 cni.projectcalico.org/podIPs:10.42.3.27/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc005418950 0xc005418951}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-58ctv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-58ctv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.634: INFO: Pod "webserver-deployment-67bd4bf6dc-z9qsw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-z9qsw webserver-deployment-67bd4bf6dc- deployment-137  146ea676-0aa3-4a39-8aa3-b21b23b05755 92510 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[cni.projectcalico.org/containerID:d75cde5abab63eb041a4f453532b877e67dfd413a6a90d7c86dbeef9d2641918 cni.projectcalico.org/podIP:10.42.3.23/32 cni.projectcalico.org/podIPs:10.42.3.23/32] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 06612204-9c4c-4222-a35c-42a79b06b882 0xc005418b47 0xc005418b48}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"06612204-9c4c-4222-a35c-42a79b06b882\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5rfs8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5rfs8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.635: INFO: Pod "webserver-deployment-7b75d79cf5-22knx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-22knx webserver-deployment-7b75d79cf5- deployment-137  1e4e5c55-18b5-4166-8871-6097d3a5707d 92528 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:f4ae6ab5b305767087758f776d15103688c1bdaff053d090fb8f7f11e7816061 cni.projectcalico.org/podIP:10.42.3.25/32 cni.projectcalico.org/podIPs:10.42.3.25/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc005418d67 0xc005418d68}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-j7dl5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-j7dl5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.636: INFO: Pod "webserver-deployment-7b75d79cf5-2bl8g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2bl8g webserver-deployment-7b75d79cf5- deployment-137  a28d867d-c2f7-4b65-92dc-660444a1b06f 92537 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:13d017505a1f5570fa11f97e7a3ef5c1c30de9199aaf08c4937132190daa4cf3 cni.projectcalico.org/podIP:10.42.2.145/32 cni.projectcalico.org/podIPs:10.42.2.145/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc005418f97 0xc005418f98}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-n7ztj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-n7ztj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.636: INFO: Pod "webserver-deployment-7b75d79cf5-79f6r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-79f6r webserver-deployment-7b75d79cf5- deployment-137  64595c5e-5ecb-4c78-a68c-9696b43cae95 92534 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:d764db37de4d22e037dc2b7ec8e35922193b808197d4e8f3291441ec90ad74c1 cni.projectcalico.org/podIP:10.42.3.26/32 cni.projectcalico.org/podIPs:10.42.3.26/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc005419207 0xc005419208}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7cwm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7cwm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.637: INFO: Pod "webserver-deployment-7b75d79cf5-fkb8f" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-fkb8f webserver-deployment-7b75d79cf5- deployment-137  8510d558-a67f-4a82-9e4a-9aba341d7fd2 92521 0 2023-05-12 13:18:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:2e8521d9da81dac068f930e384bf61f6499bee27d60ed63ccf5c09cb78751a09 cni.projectcalico.org/podIP:10.42.3.18/32 cni.projectcalico.org/podIPs:10.42.3.18/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc005419447 0xc005419448}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.18\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wsqvv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wsqvv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.18,StartTime:2023-05-12 13:18:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.18,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.637: INFO: Pod "webserver-deployment-7b75d79cf5-fn9zb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-fn9zb webserver-deployment-7b75d79cf5- deployment-137  3d35fb92-757f-4634-a826-7b3c1c230580 92495 0 2023-05-12 13:18:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:aa338d3e940ca3075931c6861204f0626ad1e8d50eab43c8e27fbface2df8cd3 cni.projectcalico.org/podIP:10.42.2.138/32 cni.projectcalico.org/podIPs:10.42.2.138/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc005419690 0xc005419691}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.138\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cf5jh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cf5jh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:10.42.2.138,StartTime:2023-05-12 13:18:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.138,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.637: INFO: Pod "webserver-deployment-7b75d79cf5-fw8z4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-fw8z4 webserver-deployment-7b75d79cf5- deployment-137  8225a298-2f37-42a4-86a8-fc357ce5f720 92531 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:75fd5f0e8a3d538278148f993c69a765fd733057d03a2a6c94814c3e483d6ef0 cni.projectcalico.org/podIP:10.42.2.147/32 cni.projectcalico.org/podIPs:10.42.2.147/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc0054198d0 0xc0054198d1}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rwjsx,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rwjsx,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.637: INFO: Pod "webserver-deployment-7b75d79cf5-lpr8w" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lpr8w webserver-deployment-7b75d79cf5- deployment-137  c1fc12e6-24ad-4dcf-977a-88db8bcfc34a 92516 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:1ebda260b519fa1ab16ee260f1214151579259c2a0b826735ae6d08f05156ff4 cni.projectcalico.org/podIP:10.42.3.24/32 cni.projectcalico.org/podIPs:10.42.3.24/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc005419ae7 0xc005419ae8}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-zj7tr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-zj7tr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.638: INFO: Pod "webserver-deployment-7b75d79cf5-ts4bh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-ts4bh webserver-deployment-7b75d79cf5- deployment-137  04b01c36-fd24-49df-bcf4-b42063434b1d 92518 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:32f095a513525c9f81ad6dd6f18bd181d773124f1f0e42ebf082d655bedb66e3 cni.projectcalico.org/podIP:10.42.3.22/32 cni.projectcalico.org/podIPs:10.42.3.22/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc005419d07 0xc005419d08}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vk9vz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vk9vz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.638: INFO: Pod "webserver-deployment-7b75d79cf5-tz55j" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tz55j webserver-deployment-7b75d79cf5- deployment-137  36166643-29cc-4e07-bc05-3c18a6526438 92520 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:9441d5829f0f1ba30581302de938b812c937b7d786bdb6cf1b728c5542cc7ec8 cni.projectcalico.org/podIP:10.42.2.142/32 cni.projectcalico.org/podIPs:10.42.2.142/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc005419f37 0xc005419f38}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-m8sq5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-m8sq5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.638: INFO: Pod "webserver-deployment-7b75d79cf5-wgjtp" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wgjtp webserver-deployment-7b75d79cf5- deployment-137  6cbe397a-a6ff-4882-bcb2-1b98a4845e6a 92505 0 2023-05-12 13:18:24 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:fe2aa64ef182571592c517f5ac070f5910823128450cba6dd359b337debf4f75 cni.projectcalico.org/podIP:10.42.2.140/32 cni.projectcalico.org/podIPs:10.42.2.140/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc0034bc177 0xc0034bc178}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status} {calico Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bzlsl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bzlsl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:,StartTime:2023-05-12 13:18:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.639: INFO: Pod "webserver-deployment-7b75d79cf5-xrg7h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xrg7h webserver-deployment-7b75d79cf5- deployment-137  03677c77-6fec-442b-a9da-d0742223fe35 92524 0 2023-05-12 13:18:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:387b5c3c2ee5e108cfa94d21747f99d27dbe770b11298fdaec357c60d668b9cf cni.projectcalico.org/podIP:10.42.3.17/32 cni.projectcalico.org/podIPs:10.42.3.17/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc0034bc3b7 0xc0034bc3b8}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8jl4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8jl4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.17,StartTime:2023-05-12 13:18:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.17,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.639: INFO: Pod "webserver-deployment-7b75d79cf5-xxjf2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-xxjf2 webserver-deployment-7b75d79cf5- deployment-137  e278cb96-dbe3-43a8-8a15-16134398a2f5 92496 0 2023-05-12 13:18:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:185595b6969ec925e6288185051672cc178bb2c3916a4e9f2889328b85119783 cni.projectcalico.org/podIP:10.42.2.137/32 cni.projectcalico.org/podIPs:10.42.2.137/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc0034bc900 0xc0034bc901}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.137\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qht7q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qht7q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:10.42.2.137,StartTime:2023-05-12 13:18:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = failed to pull and unpack image "docker.io/library/webserver:404": failed to resolve reference "docker.io/library/webserver:404": pull access denied, repository does not exist or may require authorization: server message: insufficient_scope: authorization failed,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.137,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.640: INFO: Pod "webserver-deployment-7b75d79cf5-zdds9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zdds9 webserver-deployment-7b75d79cf5- deployment-137  8abecba3-480a-4151-9e8e-bc971fab7d70 92514 0 2023-05-12 13:18:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[cni.projectcalico.org/containerID:e85406dec2d84ccda629df152c5dcf439a7fc74070cf5117a0995fc5625d4474 cni.projectcalico.org/podIP:10.42.3.16/32 cni.projectcalico.org/podIPs:10.42.3.16/32] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 d5ed371f-ce61-48f6-b81e-2ec05f961f43 0xc0034bce30 0xc0034bce31}] [] [{kube-controller-manager Update v1 2023-05-12 13:18:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d5ed371f-ce61-48f6-b81e-2ec05f961f43\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {calico Update v1 2023-05-12 13:18:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kubelet Update v1 2023-05-12 13:18:25 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.16\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tc5cr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tc5cr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:18:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.16,StartTime:2023-05-12 13:18:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ImagePullBackOff,Message:Back-off pulling image "webserver:404",},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.16,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:18:26.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-137" for this suite. @ 05/12/23 13:18:26.643
• [8.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 05/12/23 13:18:26.654
  May 12 13:18:26.654: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:18:26.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:26.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:26.669
  May 12 13:18:26.671: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-7841 create -f -'
  May 12 13:18:27.263: INFO: stderr: ""
  May 12 13:18:27.263: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  May 12 13:18:27.263: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-7841 create -f -'
  May 12 13:18:27.614: INFO: stderr: ""
  May 12 13:18:27.614: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/12/23 13:18:27.614
  May 12 13:18:28.621: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 13:18:28.622: INFO: Found 1 / 1
  May 12 13:18:28.622: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  May 12 13:18:28.629: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 13:18:28.629: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 12 13:18:28.629: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-7841 describe pod agnhost-primary-vk6j7'
  May 12 13:18:28.723: INFO: stderr: ""
  May 12 13:18:28.723: INFO: stdout: "Name:             agnhost-primary-vk6j7\nNamespace:        kubectl-7841\nPriority:         0\nService Account:  default\nNode:             onekube-ip-172-16-100-7/172.16.100.7\nStart Time:       Fri, 12 May 2023 13:18:27 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      cni.projectcalico.org/containerID: 94e9d0ceb7b30c154b428cf3f8141436df68331012df1779e7beef9c1cca8aaa\n                  cni.projectcalico.org/podIP: 10.42.3.29/32\n                  cni.projectcalico.org/podIPs: 10.42.3.29/32\nStatus:           Running\nIP:               10.42.3.29\nIPs:\n  IP:           10.42.3.29\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   containerd://b3a910daaea5c24c461ad6414bfe93b9ad39fe78c6ef1bc685b4e1e1afded50f\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 12 May 2023 13:18:27 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-2bqdf (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-2bqdf:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-7841/agnhost-primary-vk6j7 to onekube-ip-172-16-100-7\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  May 12 13:18:28.723: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-7841 describe rc agnhost-primary'
  May 12 13:18:28.781: INFO: stderr: ""
  May 12 13:18:28.781: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-7841\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-vk6j7\n"
  May 12 13:18:28.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-7841 describe service agnhost-primary'
  May 12 13:18:28.845: INFO: stderr: ""
  May 12 13:18:28.845: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-7841\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.43.62.146\nIPs:               10.43.62.146\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.42.3.29:6379\nSession Affinity:  None\nEvents:            <none>\n"
  May 12 13:18:28.848: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-7841 describe node onekube-ip-172-16-100-4'
  May 12 13:18:28.925: INFO: stderr: ""
  May 12 13:18:28.925: INFO: stdout: "Name:               onekube-ip-172-16-100-4\nRoles:              control-plane,etcd,master\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/instance-type=rke2\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=onekube-ip-172-16-100-4\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=true\n                    node-role.kubernetes.io/etcd=true\n                    node-role.kubernetes.io/master=true\n                    node.kubernetes.io/instance-type=rke2\nAnnotations:        etcd.rke2.cattle.io/node-address: 172.16.100.4\n                    etcd.rke2.cattle.io/node-name: onekube-ip-172-16-100-4-017bf921\n                    flannel.alpha.coreos.com/backend-data: {\"VNI\":1,\"VtepMAC\":\"22:50:e1:f1:ba:b3\"}\n                    flannel.alpha.coreos.com/backend-type: vxlan\n                    flannel.alpha.coreos.com/kube-subnet-manager: true\n                    flannel.alpha.coreos.com/public-ip: 172.16.100.4\n                    node.alpha.kubernetes.io/ttl: 0\n                    projectcalico.org/IPv4Address: 172.16.100.4/24\n                    projectcalico.org/IPv4IPIPTunnelAddr: 10.42.0.1\n                    rke2.io/encryption-config-hash: start-b8d52bd9ccb548b398f507405187c563b4db0b4c81d08d6818a60888f76d50e1\n                    rke2.io/hostname: onekube-ip-172-16-100-4\n                    rke2.io/internal-ip: 172.16.100.4\n                    rke2.io/node-args:\n                      [\"server\",\"--node-name\",\"onekube-ip-172-16-100-4\",\"--token\",\"********\",\"--tls-san\",\"localhost\",\"--tls-san\",\"127.0.0.1\",\"--tls-san\",\"172.16...\n                    rke2.io/node-config-hash: F6LCDZWT3UPDZ4O4ALNPNET56CKVVDMQC5PXSJUAJYBAERCKAS3Q====\n                    rke2.io/node-env: {}\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 12 May 2023 10:24:50 +0000\nTaints:             CriticalAddonsOnly=true:NoExecute\nUnschedulable:      false\nLease:\n  HolderIdentity:  onekube-ip-172-16-100-4\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 12 May 2023 13:18:19 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Fri, 12 May 2023 10:25:45 +0000   Fri, 12 May 2023 10:25:45 +0000   FlannelIsUp                  Flannel is running on this node\n  MemoryPressure       False   Fri, 12 May 2023 13:17:38 +0000   Fri, 12 May 2023 10:24:50 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Fri, 12 May 2023 13:17:38 +0000   Fri, 12 May 2023 10:24:50 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Fri, 12 May 2023 13:17:38 +0000   Fri, 12 May 2023 10:24:50 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Fri, 12 May 2023 13:17:38 +0000   Fri, 12 May 2023 10:25:40 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  172.16.100.4\n  Hostname:    onekube-ip-172-16-100-4\nCapacity:\n  cpu:                2\n  ephemeral-storage:  20134592Ki\n  hugepages-2Mi:      0\n  memory:             3052240Ki\n  pods:               110\nAllocatable:\n  cpu:                2\n  ephemeral-storage:  19586931083\n  hugepages-2Mi:      0\n  memory:             3052240Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 0d44024809414007a292524d697e4109\n  System UUID:                0d440248-0941-4007-a292-524d697e4109\n  Boot ID:                    847316cd-7cef-4550-a548-be44c09ac3c0\n  Kernel Version:             5.15.0-71-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  containerd://1.6.19-k3s1\n  Kubelet Version:            v1.27.1+rke2r1\n  Kube-Proxy Version:         v1.27.1+rke2r1\nPodCIDR:                      10.42.0.0/24\nPodCIDRs:                     10.42.0.0/24\nProviderID:                   rke2://onekube-ip-172-16-100-4\nNon-terminated Pods:          (10 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cloud-controller-manager-onekube-ip-172-16-100-4           100m (5%)     0 (0%)      128Mi (4%)       0 (0%)         172m\n  kube-system                 etcd-onekube-ip-172-16-100-4                               200m (10%)    0 (0%)      512Mi (17%)      0 (0%)         172m\n  kube-system                 kube-apiserver-onekube-ip-172-16-100-4                     250m (12%)    0 (0%)      1Gi (34%)        0 (0%)         172m\n  kube-system                 kube-controller-manager-onekube-ip-172-16-100-4            200m (10%)    0 (0%)      256Mi (8%)       0 (0%)         172m\n  kube-system                 kube-proxy-onekube-ip-172-16-100-4                         250m (12%)    0 (0%)      128Mi (4%)       0 (0%)         172m\n  kube-system                 kube-scheduler-onekube-ip-172-16-100-4                     100m (5%)     0 (0%)      128Mi (4%)       0 (0%)         172m\n  kube-system                 rke2-canal-ct97k                                           250m (12%)    0 (0%)      0 (0%)           0 (0%)         172m\n  kube-system                 rke2-coredns-rke2-coredns-5896cccb79-2gv4j                 100m (5%)     100m (5%)   128Mi (4%)       128Mi (4%)     172m\n  sonobuoy                    sonobuoy-e2e-job-616d3f24a0804a06                          0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-dd6j6    0 (0%)        0 (0%)      0 (0%)           0 (0%)         46m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests      Limits\n  --------           --------      ------\n  cpu                1450m (72%)   100m (5%)\n  memory             2304Mi (77%)  128Mi (4%)\n  ephemeral-storage  0 (0%)        0 (0%)\n  hugepages-2Mi      0 (0%)        0 (0%)\nEvents:              <none>\n"
  May 12 13:18:28.925: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-7841 describe namespace kubectl-7841'
  May 12 13:18:28.976: INFO: stderr: ""
  May 12 13:18:28.976: INFO: stdout: "Name:         kubectl-7841\nLabels:       e2e-framework=kubectl\n              e2e-run=9ddd5405-7c85-47bc-86b2-34c9975c020e\n              kubernetes.io/metadata.name=kubectl-7841\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  May 12 13:18:28.976: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7841" for this suite. @ 05/12/23 13:18:28.979
• [2.329 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 05/12/23 13:18:28.983
  May 12 13:18:28.983: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:18:28.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:28.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:28.999
  STEP: Creating projection with secret that has name projected-secret-test-7e0e1ef6-5a68-496d-93e2-bdf5a3174a2e @ 05/12/23 13:18:29.001
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:18:29.006
  STEP: Saw pod success @ 05/12/23 13:18:33.032
  May 12 13:18:33.033: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-secrets-8f6a8d40-cd11-4743-8d45-a53839ecadfd container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:18:33.037
  May 12 13:18:33.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2426" for this suite. @ 05/12/23 13:18:33.054
• [4.075 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 05/12/23 13:18:33.059
  May 12 13:18:33.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replication-controller @ 05/12/23 13:18:33.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:33.08
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:33.082
  STEP: Creating ReplicationController "e2e-rc-ks82q" @ 05/12/23 13:18:33.085
  May 12 13:18:33.088: INFO: Get Replication Controller "e2e-rc-ks82q" to confirm replicas
  May 12 13:18:34.093: INFO: Get Replication Controller "e2e-rc-ks82q" to confirm replicas
  May 12 13:18:34.096: INFO: Found 1 replicas for "e2e-rc-ks82q" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-ks82q" @ 05/12/23 13:18:34.096
  STEP: Updating a scale subresource @ 05/12/23 13:18:34.098
  STEP: Verifying replicas where modified for replication controller "e2e-rc-ks82q" @ 05/12/23 13:18:34.104
  May 12 13:18:34.104: INFO: Get Replication Controller "e2e-rc-ks82q" to confirm replicas
  May 12 13:18:35.110: INFO: Get Replication Controller "e2e-rc-ks82q" to confirm replicas
  May 12 13:18:35.112: INFO: Found 2 replicas for "e2e-rc-ks82q" replication controller
  May 12 13:18:35.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2157" for this suite. @ 05/12/23 13:18:35.122
• [2.075 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 05/12/23 13:18:35.135
  May 12 13:18:35.135: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:18:35.135
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:35.149
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:35.151
  STEP: Creating configMap with name configmap-projected-all-test-volume-9fbbb2af-8e6f-4c1d-a682-dae1fc353447 @ 05/12/23 13:18:35.152
  STEP: Creating secret with name secret-projected-all-test-volume-4c7fcfc2-df5b-4555-848d-aef383cf1b66 @ 05/12/23 13:18:35.155
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 05/12/23 13:18:35.16
  STEP: Saw pod success @ 05/12/23 13:18:39.186
  May 12 13:18:39.196: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod projected-volume-b57618c7-8e81-4f8a-8568-b12607acabbe container projected-all-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:18:39.203
  May 12 13:18:39.215: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5041" for this suite. @ 05/12/23 13:18:39.218
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 05/12/23 13:18:39.225
  May 12 13:18:39.225: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename ingress @ 05/12/23 13:18:39.225
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:39.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:39.242
  STEP: getting /apis @ 05/12/23 13:18:39.243
  STEP: getting /apis/networking.k8s.io @ 05/12/23 13:18:39.247
  STEP: getting /apis/networking.k8s.iov1 @ 05/12/23 13:18:39.248
  STEP: creating @ 05/12/23 13:18:39.248
  STEP: getting @ 05/12/23 13:18:39.258
  STEP: listing @ 05/12/23 13:18:39.26
  STEP: watching @ 05/12/23 13:18:39.261
  May 12 13:18:39.261: INFO: starting watch
  STEP: cluster-wide listing @ 05/12/23 13:18:39.262
  STEP: cluster-wide watching @ 05/12/23 13:18:39.263
  May 12 13:18:39.263: INFO: starting watch
  STEP: patching @ 05/12/23 13:18:39.263
  STEP: updating @ 05/12/23 13:18:39.267
  May 12 13:18:39.270: INFO: waiting for watch events with expected annotations
  May 12 13:18:39.270: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/12/23 13:18:39.271
  STEP: updating /status @ 05/12/23 13:18:39.273
  STEP: get /status @ 05/12/23 13:18:39.28
  STEP: deleting @ 05/12/23 13:18:39.281
  STEP: deleting a collection @ 05/12/23 13:18:39.286
  May 12 13:18:39.291: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-7097" for this suite. @ 05/12/23 13:18:39.294
• [0.074 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 05/12/23 13:18:39.3
  May 12 13:18:39.300: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename endpointslice @ 05/12/23 13:18:39.301
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:39.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:39.314
  May 12 13:18:39.319: INFO: Endpoints addresses: [172.16.100.4] , ports: [6443]
  May 12 13:18:39.319: INFO: EndpointSlices addresses: [172.16.100.4] , ports: [6443]
  May 12 13:18:39.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8837" for this suite. @ 05/12/23 13:18:39.321
• [0.024 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 05/12/23 13:18:39.325
  May 12 13:18:39.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replication-controller @ 05/12/23 13:18:39.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:39.336
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:39.338
  STEP: creating a ReplicationController @ 05/12/23 13:18:39.34
  STEP: waiting for RC to be added @ 05/12/23 13:18:39.345
  STEP: waiting for available Replicas @ 05/12/23 13:18:39.345
  STEP: patching ReplicationController @ 05/12/23 13:18:40.719
  STEP: waiting for RC to be modified @ 05/12/23 13:18:40.725
  STEP: patching ReplicationController status @ 05/12/23 13:18:40.725
  STEP: waiting for RC to be modified @ 05/12/23 13:18:40.729
  STEP: waiting for available Replicas @ 05/12/23 13:18:40.73
  STEP: fetching ReplicationController status @ 05/12/23 13:18:40.732
  STEP: patching ReplicationController scale @ 05/12/23 13:18:40.733
  STEP: waiting for RC to be modified @ 05/12/23 13:18:40.738
  STEP: waiting for ReplicationController's scale to be the max amount @ 05/12/23 13:18:40.738
  STEP: fetching ReplicationController; ensuring that it's patched @ 05/12/23 13:18:41.968
  STEP: updating ReplicationController status @ 05/12/23 13:18:41.97
  STEP: waiting for RC to be modified @ 05/12/23 13:18:41.976
  STEP: listing all ReplicationControllers @ 05/12/23 13:18:41.976
  STEP: checking that ReplicationController has expected values @ 05/12/23 13:18:41.981
  STEP: deleting ReplicationControllers by collection @ 05/12/23 13:18:41.981
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 05/12/23 13:18:41.986
  May 12 13:18:42.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0512 13:18:42.011918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-1559" for this suite. @ 05/12/23 13:18:42.014
• [2.692 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 05/12/23 13:18:42.019
  May 12 13:18:42.019: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:18:42.02
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:42.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:42.033
  STEP: Creating the pod @ 05/12/23 13:18:42.035
  E0512 13:18:43.012248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:44.012539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:18:44.589: INFO: Successfully updated pod "annotationupdatecabc68c4-ccc6-4321-8044-4bf3449e4c1e"
  E0512 13:18:45.016041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:46.016600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:47.017033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:48.017118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:18:48.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7069" for this suite. @ 05/12/23 13:18:48.65
• [6.638 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 05/12/23 13:18:48.658
  May 12 13:18:48.658: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 13:18:48.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:48.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:48.674
  STEP: Counting existing ResourceQuota @ 05/12/23 13:18:48.676
  E0512 13:18:49.017031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:50.017802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:51.019030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:52.019500      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:53.019767      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/12/23 13:18:53.683
  STEP: Ensuring resource quota status is calculated @ 05/12/23 13:18:53.696
  E0512 13:18:54.019834      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:55.020143      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 05/12/23 13:18:55.699
  STEP: Creating a NodePort Service @ 05/12/23 13:18:55.714
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 05/12/23 13:18:55.741
  STEP: Ensuring resource quota status captures service creation @ 05/12/23 13:18:55.758
  E0512 13:18:56.021292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:57.021636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 05/12/23 13:18:57.76
  STEP: Ensuring resource quota status released usage @ 05/12/23 13:18:57.795
  E0512 13:18:58.021904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:18:59.022360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:18:59.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7768" for this suite. @ 05/12/23 13:18:59.8
• [11.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 05/12/23 13:18:59.808
  May 12 13:18:59.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/12/23 13:18:59.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:59.822
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:59.824
  STEP: creating @ 05/12/23 13:18:59.825
  STEP: getting @ 05/12/23 13:18:59.84
  STEP: listing in namespace @ 05/12/23 13:18:59.846
  STEP: patching @ 05/12/23 13:18:59.848
  STEP: deleting @ 05/12/23 13:18:59.854
  May 12 13:18:59.862: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-1721" for this suite. @ 05/12/23 13:18:59.865
• [0.074 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 05/12/23 13:18:59.883
  May 12 13:18:59.884: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:18:59.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:18:59.901
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:18:59.903
  STEP: creating service in namespace services-4152 @ 05/12/23 13:18:59.904
  STEP: creating service affinity-clusterip-transition in namespace services-4152 @ 05/12/23 13:18:59.904
  STEP: creating replication controller affinity-clusterip-transition in namespace services-4152 @ 05/12/23 13:18:59.914
  I0512 13:18:59.924532      20 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-4152, replica count: 3
  E0512 13:19:00.023125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:01.023644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:02.023818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0512 13:19:02.979272      20 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 13:19:02.983: INFO: Creating new exec pod
  E0512 13:19:03.024008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:04.024596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:05.024906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:19:06.005: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-4152 exec execpod-affinitylgv2g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  E0512 13:19:06.025861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:19:06.136: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  May 12 13:19:06.136: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:19:06.136: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-4152 exec execpod-affinitylgv2g -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.244.87 80'
  May 12 13:19:06.257: INFO: stderr: "+ nc -v -t -w 2 10.43.244.87 80\n+ echo hostName\nConnection to 10.43.244.87 80 port [tcp/http] succeeded!\n"
  May 12 13:19:06.257: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:19:06.264: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-4152 exec execpod-affinitylgv2g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.244.87:80/ ; done'
  May 12 13:19:06.444: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n"
  May 12 13:19:06.444: INFO: stdout: "\naffinity-clusterip-transition-7krvb\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-7krvb\naffinity-clusterip-transition-r6b86\naffinity-clusterip-transition-7krvb\naffinity-clusterip-transition-r6b86\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-7krvb\naffinity-clusterip-transition-7krvb\naffinity-clusterip-transition-r6b86\naffinity-clusterip-transition-7krvb\naffinity-clusterip-transition-r6b86\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-7krvb\naffinity-clusterip-transition-x62nf"
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-7krvb
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-7krvb
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-r6b86
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-7krvb
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-r6b86
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-7krvb
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-7krvb
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-r6b86
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-7krvb
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-r6b86
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-7krvb
  May 12 13:19:06.444: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.449: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-4152 exec execpod-affinitylgv2g -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.43.244.87:80/ ; done'
  May 12 13:19:06.622: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.43.244.87:80/\n"
  May 12 13:19:06.622: INFO: stdout: "\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf\naffinity-clusterip-transition-x62nf"
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.622: INFO: Received response from host: affinity-clusterip-transition-x62nf
  May 12 13:19:06.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:19:06.626: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4152, will wait for the garbage collector to delete the pods @ 05/12/23 13:19:06.64
  May 12 13:19:06.697: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.970019ms
  May 12 13:19:06.797: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.462989ms
  E0512 13:19:07.028071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:08.026759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4152" for this suite. @ 05/12/23 13:19:08.915
• [9.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 05/12/23 13:19:08.925
  May 12 13:19:08.925: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:19:08.925
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:19:08.939
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:19:08.941
  STEP: Setting up server cert @ 05/12/23 13:19:08.964
  E0512 13:19:09.027087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:19:09.431
  STEP: Deploying the webhook pod @ 05/12/23 13:19:09.438
  STEP: Wait for the deployment to be ready @ 05/12/23 13:19:09.451
  May 12 13:19:09.460: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 13:19:10.027253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:11.028188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 13:19:11.464
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:19:11.475
  E0512 13:19:12.029022      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:19:12.476: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 05/12/23 13:19:12.477
  STEP: create a pod that should be denied by the webhook @ 05/12/23 13:19:12.49
  STEP: create a pod that causes the webhook to hang @ 05/12/23 13:19:12.5
  E0512 13:19:13.029359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:14.029152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:15.030469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:16.030946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:17.031051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:18.031682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:19.031523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:20.031668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:21.031739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:22.032321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 05/12/23 13:19:22.504
  STEP: create a configmap that should be admitted by the webhook @ 05/12/23 13:19:22.516
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/12/23 13:19:22.527
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 05/12/23 13:19:22.531
  STEP: create a namespace that bypass the webhook @ 05/12/23 13:19:22.534
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 05/12/23 13:19:22.544
  May 12 13:19:22.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5168" for this suite. @ 05/12/23 13:19:22.613
  STEP: Destroying namespace "webhook-markers-7350" for this suite. @ 05/12/23 13:19:22.621
  STEP: Destroying namespace "exempted-namespace-3908" for this suite. @ 05/12/23 13:19:22.626
• [13.707 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 05/12/23 13:19:22.632
  May 12 13:19:22.632: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename var-expansion @ 05/12/23 13:19:22.633
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:19:22.643
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:19:22.645
  STEP: Creating a pod to test env composition @ 05/12/23 13:19:22.647
  E0512 13:19:23.032813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:24.032396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:25.032497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:26.032957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:19:26.679
  May 12 13:19:26.688: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod var-expansion-89e55c11-5fb8-4bd2-a52b-81d031556ff6 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 13:19:26.715
  May 12 13:19:26.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3944" for this suite. @ 05/12/23 13:19:26.733
• [4.106 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 05/12/23 13:19:26.738
  May 12 13:19:26.738: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename taint-multiple-pods @ 05/12/23 13:19:26.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:19:26.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:19:26.752
  May 12 13:19:26.754: INFO: Waiting up to 1m0s for all nodes to be ready
  E0512 13:19:27.033235      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:28.033229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:29.033446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:30.034132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:31.034464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:32.035097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:33.035707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:34.036563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:35.037494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:36.038014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:37.037980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:38.038905      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:39.038902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:40.039268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:41.040333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:42.041499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:43.042293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:44.042991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:45.043844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:46.043933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:47.044214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:48.044934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:49.045608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:50.046025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:51.047012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:52.047431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:53.048411      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:54.048431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:55.048783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:56.049547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:57.050444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:58.050614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:19:59.050858      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:00.051299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:01.052168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:02.052285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:03.052726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:04.053371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:05.053450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:06.053823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:07.054010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:08.054699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:09.054849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:10.055842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:11.056573      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:12.057549      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:13.057803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:14.058068      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:15.058240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:16.058189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:17.058884      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:18.059853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:19.059899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:20.060442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:21.060489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:22.061105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:23.062618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:24.062047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:25.063037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:26.063012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:20:26.811: INFO: Waiting for terminating namespaces to be deleted...
  May 12 13:20:26.813: INFO: Starting informer...
  STEP: Starting pods... @ 05/12/23 13:20:26.813
  May 12 13:20:27.033: INFO: Pod1 is running on onekube-ip-172-16-100-7. Tainting Node
  E0512 13:20:27.063330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:28.065016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:29.064312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:20:29.275: INFO: Pod2 is running on onekube-ip-172-16-100-7. Tainting Node
  STEP: Trying to apply a taint on the Node @ 05/12/23 13:20:29.276
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/12/23 13:20:29.329
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 05/12/23 13:20:29.366
  E0512 13:20:30.064401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:31.065242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:32.065509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:33.065845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:34.067879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:35.068267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:20:35.279: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  E0512 13:20:36.068409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:37.068856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:38.068998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:39.070051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:40.070668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:41.070786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:42.071450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:43.071452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:44.071920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:45.074636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:46.074376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:47.074400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:48.074764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:49.074742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:50.075418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:51.075846      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:52.077495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:53.077591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:54.078109      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:55.078250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:20:55.350: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  May 12 13:20:55.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 05/12/23 13:20:55.367
  STEP: Destroying namespace "taint-multiple-pods-4496" for this suite. @ 05/12/23 13:20:55.38
• [88.677 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 05/12/23 13:20:55.416
  May 12 13:20:55.416: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:20:55.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:20:55.473
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:20:55.478
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5868 @ 05/12/23 13:20:55.489
  STEP: changing the ExternalName service to type=ClusterIP @ 05/12/23 13:20:55.492
  STEP: creating replication controller externalname-service in namespace services-5868 @ 05/12/23 13:20:55.502
  I0512 13:20:55.513968      20 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5868, replica count: 2
  E0512 13:20:56.079220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:57.080496      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:20:58.081319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0512 13:20:58.564830      20 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 13:20:58.566: INFO: Creating new exec pod
  E0512 13:20:59.081960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:00.082811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:01.083390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:21:01.611: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5868 exec execpodtblxn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 12 13:21:01.740: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 12 13:21:01.740: INFO: stdout: ""
  E0512 13:21:02.083980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:21:02.741: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5868 exec execpodtblxn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 12 13:21:02.851: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 12 13:21:02.851: INFO: stdout: "externalname-service-vtf8l"
  May 12 13:21:02.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-5868 exec execpodtblxn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.13.98 80'
  May 12 13:21:02.983: INFO: stderr: "+ nc -v -t -w 2 10.43.13.98 80\n+ echo hostName\nConnection to 10.43.13.98 80 port [tcp/http] succeeded!\n"
  May 12 13:21:02.983: INFO: stdout: "externalname-service-4tcft"
  May 12 13:21:02.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:21:02.988: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-5868" for this suite. @ 05/12/23 13:21:03.011
• [7.601 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 05/12/23 13:21:03.018
  May 12 13:21:03.018: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-watch @ 05/12/23 13:21:03.018
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:21:03.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:21:03.036
  May 12 13:21:03.037: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 13:21:03.084028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:04.084641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:05.085472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 05/12/23 13:21:05.596
  May 12 13:21:05.606: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-12T13:21:05Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-12T13:21:05Z]] name:name1 resourceVersion:94584 uid:bf87947a-de4b-4046-a26c-ca532945bd12] num:map[num1:9223372036854775807 num2:1000000]]}
  E0512 13:21:06.086420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:07.086271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:08.086362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:09.086447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:10.086551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:11.087346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:12.087662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:13.087694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:14.087845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:15.088003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 05/12/23 13:21:15.608
  May 12 13:21:15.623: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-12T13:21:15Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-12T13:21:15Z]] name:name2 resourceVersion:94680 uid:e40bb180-b655-442e-b89b-aaa6ca016b37] num:map[num1:9223372036854775807 num2:1000000]]}
  E0512 13:21:16.088395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:17.088569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:18.088743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:19.089297      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:20.089990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:21.089915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:22.089833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:23.090724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:24.090358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:25.090492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 05/12/23 13:21:25.628
  May 12 13:21:25.640: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-12T13:21:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-12T13:21:25Z]] name:name1 resourceVersion:94734 uid:bf87947a-de4b-4046-a26c-ca532945bd12] num:map[num1:9223372036854775807 num2:1000000]]}
  E0512 13:21:26.090614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:27.091473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:28.091968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:29.091965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:30.092364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:31.093784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:32.093984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:33.094818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:34.095640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:35.095515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 05/12/23 13:21:35.641
  May 12 13:21:35.645: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-12T13:21:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-12T13:21:35Z]] name:name2 resourceVersion:94786 uid:e40bb180-b655-442e-b89b-aaa6ca016b37] num:map[num1:9223372036854775807 num2:1000000]]}
  E0512 13:21:36.095710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:37.096501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:38.096863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:39.096786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:40.098819      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:41.099153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:42.101331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:43.101866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:44.102431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:45.102324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 05/12/23 13:21:45.645
  May 12 13:21:45.660: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-12T13:21:05Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-12T13:21:25Z]] name:name1 resourceVersion:94833 uid:bf87947a-de4b-4046-a26c-ca532945bd12] num:map[num1:9223372036854775807 num2:1000000]]}
  E0512 13:21:46.103102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:47.103443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:48.103861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:49.103866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:50.104106      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:51.105094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:52.104932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:53.104798      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:54.105186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:55.105482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 05/12/23 13:21:55.661
  May 12 13:21:55.678: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-05-12T13:21:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-05-12T13:21:35Z]] name:name2 resourceVersion:94885 uid:e40bb180-b655-442e-b89b-aaa6ca016b37] num:map[num1:9223372036854775807 num2:1000000]]}
  E0512 13:21:56.107185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:57.107339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:58.110449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:21:59.110474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:00.112322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:01.112758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:02.116942      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:03.114880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:04.115792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:05.117209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:06.118118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:22:06.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-8268" for this suite. @ 05/12/23 13:22:06.257
• [63.244 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 05/12/23 13:22:06.262
  May 12 13:22:06.262: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:22:06.263
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:06.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:06.278
  STEP: Creating configMap with name configmap-test-upd-df873aff-78eb-4552-a828-d2ca82c0d453 @ 05/12/23 13:22:06.281
  STEP: Creating the pod @ 05/12/23 13:22:06.285
  E0512 13:22:07.118274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:08.119196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-df873aff-78eb-4552-a828-d2ca82c0d453 @ 05/12/23 13:22:08.339
  STEP: waiting to observe update in volume @ 05/12/23 13:22:08.349
  E0512 13:22:09.119444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:10.119479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:22:10.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9048" for this suite. @ 05/12/23 13:22:10.379
• [4.125 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 05/12/23 13:22:10.392
  May 12 13:22:10.392: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 13:22:10.394
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:10.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:10.414
  STEP: Creating a pod to test downward api env vars @ 05/12/23 13:22:10.415
  E0512 13:22:11.119826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:12.120758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:13.121680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:14.122076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:22:14.434
  May 12 13:22:14.435: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downward-api-b954f3c1-05b5-4ea2-8957-71fe827c8e3a container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 13:22:14.439
  May 12 13:22:14.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8380" for this suite. @ 05/12/23 13:22:14.453
• [4.065 seconds]
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 05/12/23 13:22:14.457
  May 12 13:22:14.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename csistoragecapacity @ 05/12/23 13:22:14.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:14.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:14.473
  STEP: getting /apis @ 05/12/23 13:22:14.474
  STEP: getting /apis/storage.k8s.io @ 05/12/23 13:22:14.478
  STEP: getting /apis/storage.k8s.io/v1 @ 05/12/23 13:22:14.479
  STEP: creating @ 05/12/23 13:22:14.48
  STEP: watching @ 05/12/23 13:22:14.487
  May 12 13:22:14.487: INFO: starting watch
  STEP: getting @ 05/12/23 13:22:14.491
  STEP: listing in namespace @ 05/12/23 13:22:14.492
  STEP: listing across namespaces @ 05/12/23 13:22:14.494
  STEP: patching @ 05/12/23 13:22:14.496
  STEP: updating @ 05/12/23 13:22:14.499
  May 12 13:22:14.502: INFO: waiting for watch events with expected annotations in namespace
  May 12 13:22:14.502: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 05/12/23 13:22:14.502
  STEP: deleting a collection @ 05/12/23 13:22:14.507
  May 12 13:22:14.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-2607" for this suite. @ 05/12/23 13:22:14.515
• [0.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 05/12/23 13:22:14.521
  May 12 13:22:14.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:22:14.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:14.536
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:14.538
  STEP: Creating configMap configmap-9217/configmap-test-c74fd93c-f194-4e1c-9e6b-5fcd1150758b @ 05/12/23 13:22:14.54
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:22:14.543
  E0512 13:22:15.125061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:16.125122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:17.125250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:18.125342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:22:18.561
  May 12 13:22:18.567: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-a1ece107-32e7-4c8d-a765-e01c9577c70f container env-test: <nil>
  STEP: delete the pod @ 05/12/23 13:22:18.585
  May 12 13:22:18.609: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9217" for this suite. @ 05/12/23 13:22:18.613
• [4.097 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 05/12/23 13:22:18.621
  May 12 13:22:18.622: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename events @ 05/12/23 13:22:18.622
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:18.636
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:18.638
  STEP: creating a test event @ 05/12/23 13:22:18.639
  STEP: listing all events in all namespaces @ 05/12/23 13:22:18.642
  STEP: patching the test event @ 05/12/23 13:22:18.646
  STEP: fetching the test event @ 05/12/23 13:22:18.649
  STEP: updating the test event @ 05/12/23 13:22:18.65
  STEP: getting the test event @ 05/12/23 13:22:18.655
  STEP: deleting the test event @ 05/12/23 13:22:18.656
  STEP: listing all events in all namespaces @ 05/12/23 13:22:18.659
  May 12 13:22:18.663: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9775" for this suite. @ 05/12/23 13:22:18.665
• [0.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 05/12/23 13:22:18.672
  May 12 13:22:18.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename job @ 05/12/23 13:22:18.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:18.685
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:18.687
  STEP: Creating a suspended job @ 05/12/23 13:22:18.689
  STEP: Patching the Job @ 05/12/23 13:22:18.693
  STEP: Watching for Job to be patched @ 05/12/23 13:22:18.703
  May 12 13:22:18.705: INFO: Event ADDED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 12 13:22:18.705: INFO: Event MODIFIED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5] and annotations: map[batch.kubernetes.io/job-tracking:]
  May 12 13:22:18.705: INFO: Event MODIFIED found for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 05/12/23 13:22:18.705
  STEP: Watching for Job to be updated @ 05/12/23 13:22:18.71
  May 12 13:22:18.711: INFO: Event MODIFIED found for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 12 13:22:18.711: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 05/12/23 13:22:18.711
  May 12 13:22:18.720: INFO: Job: e2e-sjwv5 as labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched]
  STEP: Waiting for job to complete @ 05/12/23 13:22:18.72
  E0512 13:22:19.125467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:20.126014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:21.126612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:22.126323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:23.126450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:24.128837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:25.129704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:26.130308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:27.134551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:28.134910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Delete a job collection with a labelselector @ 05/12/23 13:22:28.73
  STEP: Watching for Job to be deleted @ 05/12/23 13:22:28.741
  May 12 13:22:28.748: INFO: Event MODIFIED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 12 13:22:28.749: INFO: Event MODIFIED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 12 13:22:28.749: INFO: Event MODIFIED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 12 13:22:28.750: INFO: Event MODIFIED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 12 13:22:28.750: INFO: Event MODIFIED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 12 13:22:28.750: INFO: Event MODIFIED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 12 13:22:28.752: INFO: Event MODIFIED observed for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  May 12 13:22:28.752: INFO: Event DELETED found for Job e2e-sjwv5 in namespace job-8587 with labels: map[e2e-job-label:e2e-sjwv5 e2e-sjwv5:patched] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 05/12/23 13:22:28.752
  May 12 13:22:28.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-8587" for this suite. @ 05/12/23 13:22:28.761
• [10.096 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 05/12/23 13:22:28.773
  May 12 13:22:28.773: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:22:28.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:28.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:28.797
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/12/23 13:22:28.799
  E0512 13:22:29.135676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:30.136072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:31.136371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:32.137182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:22:32.817
  May 12 13:22:32.825: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-4f687f30-aeed-4acc-b2bd-e0758ea8a109 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:22:32.836
  May 12 13:22:32.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1275" for this suite. @ 05/12/23 13:22:32.859
• [4.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 05/12/23 13:22:32.867
  May 12 13:22:32.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:22:32.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:32.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:32.883
  STEP: validating cluster-info @ 05/12/23 13:22:32.884
  May 12 13:22:32.885: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-7463 cluster-info'
  May 12 13:22:32.942: INFO: stderr: ""
  May 12 13:22:32.942: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.43.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  May 12 13:22:32.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7463" for this suite. @ 05/12/23 13:22:32.946
• [0.084 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 05/12/23 13:22:32.952
  May 12 13:22:32.952: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename gc @ 05/12/23 13:22:32.953
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:32.966
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:32.968
  May 12 13:22:33.017: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"7494c2df-645c-4311-8b45-3f5863457a7e", Controller:(*bool)(0xc0034bc0e6), BlockOwnerDeletion:(*bool)(0xc0034bc0e7)}}
  May 12 13:22:33.023: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"28223d21-7301-4fe4-a2b1-9eae03ef8f62", Controller:(*bool)(0xc0034bc376), BlockOwnerDeletion:(*bool)(0xc0034bc377)}}
  May 12 13:22:33.033: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"d28ceff8-7192-4c9c-973d-55cee77efbcc", Controller:(*bool)(0xc0034bc85a), BlockOwnerDeletion:(*bool)(0xc0034bc85b)}}
  E0512 13:22:33.138726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:34.139335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:35.142263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:36.142918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:37.142994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:22:38.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-1779" for this suite. @ 05/12/23 13:22:38.046
• [5.099 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 05/12/23 13:22:38.052
  May 12 13:22:38.052: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replication-controller @ 05/12/23 13:22:38.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:38.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:38.073
  May 12 13:22:38.081: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 05/12/23 13:22:38.099
  STEP: Checking rc "condition-test" has the desired failure condition set @ 05/12/23 13:22:38.11
  E0512 13:22:38.146343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 05/12/23 13:22:39.128
  May 12 13:22:39.141: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 05/12/23 13:22:39.15
  E0512 13:22:39.150482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:22:39.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-9233" for this suite. @ 05/12/23 13:22:39.213
• [1.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 05/12/23 13:22:39.254
  May 12 13:22:39.254: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename security-context-test @ 05/12/23 13:22:39.255
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:39.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:39.277
  E0512 13:22:40.151223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:41.157015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:42.165417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:43.166102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:22:43.303: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-456" for this suite. @ 05/12/23 13:22:43.305
• [4.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 05/12/23 13:22:43.31
  May 12 13:22:43.310: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:22:43.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:43.326
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:43.327
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 13:22:43.329
  E0512 13:22:44.166356      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:45.166437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:46.166845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:47.168700      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:22:47.356
  May 12 13:22:47.357: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-cbb764ef-c353-4c66-a7df-ae54c377168d container client-container: <nil>
  STEP: delete the pod @ 05/12/23 13:22:47.361
  May 12 13:22:47.372: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6272" for this suite. @ 05/12/23 13:22:47.375
• [4.070 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 05/12/23 13:22:47.38
  May 12 13:22:47.380: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 13:22:47.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:47.4
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:47.402
  STEP: Counting existing ResourceQuota @ 05/12/23 13:22:47.404
  E0512 13:22:48.168835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:49.169408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:50.169912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:51.171527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:52.172303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/12/23 13:22:52.416
  STEP: Ensuring resource quota status is calculated @ 05/12/23 13:22:52.419
  E0512 13:22:53.172982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:54.173181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:22:54.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4980" for this suite. @ 05/12/23 13:22:54.423
• [7.046 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 05/12/23 13:22:54.431
  May 12 13:22:54.431: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:22:54.432
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:22:54.443
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:22:54.446
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/12/23 13:22:54.448
  May 12 13:22:54.448: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3135 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 12 13:22:54.508: INFO: stderr: ""
  May 12 13:22:54.508: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 05/12/23 13:22:54.508
  E0512 13:22:55.173253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:56.174561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:57.174339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:58.175082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:22:59.175688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/12/23 13:22:59.56
  May 12 13:22:59.560: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3135 get pod e2e-test-httpd-pod -o json'
  May 12 13:22:59.646: INFO: stderr: ""
  May 12 13:22:59.646: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"annotations\": {\n            \"cni.projectcalico.org/containerID\": \"c5b7abfb49a1869a9ca84bd29487dc8475e6f18fdc9540aa48883ceef0b73baf\",\n            \"cni.projectcalico.org/podIP\": \"10.42.3.62/32\",\n            \"cni.projectcalico.org/podIPs\": \"10.42.3.62/32\"\n        },\n        \"creationTimestamp\": \"2023-05-12T13:22:54Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-3135\",\n        \"resourceVersion\": \"95681\",\n        \"uid\": \"e3bc4826-f2f7-4fee-9f26-b37aeb50bd0b\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-nzwwl\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"onekube-ip-172-16-100-7\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-nzwwl\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-12T13:22:54Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-12T13:22:55Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-12T13:22:55Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-05-12T13:22:54Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"containerd://9309e8f3a47b140e8422e9a96b2f38dd6f36eabc154d381f76b0e6a25c7c2441\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-05-12T13:22:55Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"172.16.100.7\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.42.3.62\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.42.3.62\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-05-12T13:22:54Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 05/12/23 13:22:59.646
  May 12 13:22:59.646: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3135 replace -f -'
  May 12 13:23:00.026: INFO: stderr: ""
  May 12 13:23:00.026: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 05/12/23 13:23:00.026
  May 12 13:23:00.028: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-3135 delete pods e2e-test-httpd-pod'
  E0512 13:23:00.175820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:01.176912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:01.923: INFO: stderr: ""
  May 12 13:23:01.923: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 12 13:23:01.923: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3135" for this suite. @ 05/12/23 13:23:01.926
• [7.500 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 05/12/23 13:23:01.933
  May 12 13:23:01.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:23:01.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:23:01.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:23:01.946
  STEP: Setting up server cert @ 05/12/23 13:23:01.962
  E0512 13:23:02.178915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:23:02.666
  STEP: Deploying the webhook pod @ 05/12/23 13:23:02.673
  STEP: Wait for the deployment to be ready @ 05/12/23 13:23:02.684
  May 12 13:23:02.695: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 13:23:03.179263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:04.179231      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:04.700: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 23, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 23, 2, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 23, 2, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 23, 2, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 13:23:05.179347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:06.179822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 13:23:06.712
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:23:06.75
  E0512 13:23:07.179976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:07.757: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 05/12/23 13:23:07.765
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/23 13:23:07.803
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 05/12/23 13:23:07.814
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/23 13:23:07.821
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 05/12/23 13:23:07.827
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/23 13:23:07.832
  May 12 13:23:07.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-933" for this suite. @ 05/12/23 13:23:08.013
  STEP: Destroying namespace "webhook-markers-9799" for this suite. @ 05/12/23 13:23:08.016
• [6.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 05/12/23 13:23:08.022
  May 12 13:23:08.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replicaset @ 05/12/23 13:23:08.023
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:23:08.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:23:08.037
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 05/12/23 13:23:08.039
  E0512 13:23:08.181040      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:09.180903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 05/12/23 13:23:10.052
  STEP: Then the orphan pod is adopted @ 05/12/23 13:23:10.056
  STEP: When the matched label of one of its pods change @ 05/12/23 13:23:10.065
  May 12 13:23:10.069: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/12/23 13:23:10.083
  May 12 13:23:10.088: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2399" for this suite. @ 05/12/23 13:23:10.098
• [2.089 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 05/12/23 13:23:10.111
  May 12 13:23:10.111: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:23:10.112
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:23:10.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:23:10.131
  STEP: creating a replication controller @ 05/12/23 13:23:10.132
  May 12 13:23:10.132: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 create -f -'
  E0512 13:23:10.181639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:10.410: INFO: stderr: ""
  May 12 13:23:10.410: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/12/23 13:23:10.41
  May 12 13:23:10.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 12 13:23:10.503: INFO: stderr: ""
  May 12 13:23:10.503: INFO: stdout: "update-demo-nautilus-k26gm update-demo-nautilus-zblbk "
  May 12 13:23:10.503: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-k26gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:23:10.555: INFO: stderr: ""
  May 12 13:23:10.555: INFO: stdout: ""
  May 12 13:23:10.555: INFO: update-demo-nautilus-k26gm is created but not running
  E0512 13:23:11.182521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:12.183469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:13.185507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:14.185939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:15.186000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:15.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 12 13:23:15.613: INFO: stderr: ""
  May 12 13:23:15.613: INFO: stdout: "update-demo-nautilus-k26gm update-demo-nautilus-zblbk "
  May 12 13:23:15.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-k26gm -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:23:15.660: INFO: stderr: ""
  May 12 13:23:15.660: INFO: stdout: "true"
  May 12 13:23:15.660: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-k26gm -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 12 13:23:15.710: INFO: stderr: ""
  May 12 13:23:15.710: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 12 13:23:15.710: INFO: validating pod update-demo-nautilus-k26gm
  May 12 13:23:15.714: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 12 13:23:15.714: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 12 13:23:15.714: INFO: update-demo-nautilus-k26gm is verified up and running
  May 12 13:23:15.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-zblbk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:23:15.767: INFO: stderr: ""
  May 12 13:23:15.767: INFO: stdout: "true"
  May 12 13:23:15.767: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-zblbk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 12 13:23:15.826: INFO: stderr: ""
  May 12 13:23:15.826: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 12 13:23:15.826: INFO: validating pod update-demo-nautilus-zblbk
  May 12 13:23:15.831: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 12 13:23:15.831: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 12 13:23:15.831: INFO: update-demo-nautilus-zblbk is verified up and running
  STEP: scaling down the replication controller @ 05/12/23 13:23:15.831
  May 12 13:23:15.833: INFO: scanned /root for discovery docs: <nil>
  May 12 13:23:15.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0512 13:23:16.188060      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:16.916: INFO: stderr: ""
  May 12 13:23:16.916: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/12/23 13:23:16.916
  May 12 13:23:16.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 12 13:23:16.977: INFO: stderr: ""
  May 12 13:23:16.977: INFO: stdout: "update-demo-nautilus-k26gm update-demo-nautilus-zblbk "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 05/12/23 13:23:16.977
  E0512 13:23:17.189176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:18.189341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:19.189745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:20.189917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:21.190491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:21.978: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 12 13:23:22.036: INFO: stderr: ""
  May 12 13:23:22.036: INFO: stdout: "update-demo-nautilus-zblbk "
  May 12 13:23:22.036: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-zblbk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:23:22.123: INFO: stderr: ""
  May 12 13:23:22.123: INFO: stdout: "true"
  May 12 13:23:22.123: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-zblbk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  E0512 13:23:22.194965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:22.227: INFO: stderr: ""
  May 12 13:23:22.227: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 12 13:23:22.227: INFO: validating pod update-demo-nautilus-zblbk
  May 12 13:23:22.230: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 12 13:23:22.230: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 12 13:23:22.230: INFO: update-demo-nautilus-zblbk is verified up and running
  STEP: scaling up the replication controller @ 05/12/23 13:23:22.23
  May 12 13:23:22.232: INFO: scanned /root for discovery docs: <nil>
  May 12 13:23:22.232: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  May 12 13:23:22.345: INFO: stderr: ""
  May 12 13:23:22.345: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/12/23 13:23:22.345
  May 12 13:23:22.345: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 12 13:23:22.401: INFO: stderr: ""
  May 12 13:23:22.401: INFO: stdout: "update-demo-nautilus-k2m6n update-demo-nautilus-zblbk "
  May 12 13:23:22.401: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-k2m6n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:23:22.461: INFO: stderr: ""
  May 12 13:23:22.461: INFO: stdout: ""
  May 12 13:23:22.461: INFO: update-demo-nautilus-k2m6n is created but not running
  E0512 13:23:23.195473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:24.195495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:25.195753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:26.196222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:27.196130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:27.462: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 12 13:23:27.511: INFO: stderr: ""
  May 12 13:23:27.511: INFO: stdout: "update-demo-nautilus-k2m6n update-demo-nautilus-zblbk "
  May 12 13:23:27.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-k2m6n -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:23:27.556: INFO: stderr: ""
  May 12 13:23:27.557: INFO: stdout: "true"
  May 12 13:23:27.557: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-k2m6n -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 12 13:23:27.608: INFO: stderr: ""
  May 12 13:23:27.608: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 12 13:23:27.608: INFO: validating pod update-demo-nautilus-k2m6n
  May 12 13:23:27.612: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 12 13:23:27.612: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 12 13:23:27.612: INFO: update-demo-nautilus-k2m6n is verified up and running
  May 12 13:23:27.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-zblbk -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:23:27.659: INFO: stderr: ""
  May 12 13:23:27.659: INFO: stdout: "true"
  May 12 13:23:27.659: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods update-demo-nautilus-zblbk -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 12 13:23:27.705: INFO: stderr: ""
  May 12 13:23:27.705: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 12 13:23:27.705: INFO: validating pod update-demo-nautilus-zblbk
  May 12 13:23:27.707: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 12 13:23:27.707: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 12 13:23:27.707: INFO: update-demo-nautilus-zblbk is verified up and running
  STEP: using delete to clean up resources @ 05/12/23 13:23:27.707
  May 12 13:23:27.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 delete --grace-period=0 --force -f -'
  May 12 13:23:27.771: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 13:23:27.771: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 12 13:23:27.771: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get rc,svc -l name=update-demo --no-headers'
  May 12 13:23:27.834: INFO: stderr: "No resources found in kubectl-8598 namespace.\n"
  May 12 13:23:27.834: INFO: stdout: ""
  May 12 13:23:27.834: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8598 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 12 13:23:27.886: INFO: stderr: ""
  May 12 13:23:27.886: INFO: stdout: ""
  May 12 13:23:27.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8598" for this suite. @ 05/12/23 13:23:27.889
• [17.783 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 05/12/23 13:23:27.894
  May 12 13:23:27.894: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:23:27.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:23:27.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:23:27.909
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 05/12/23 13:23:27.91
  E0512 13:23:28.196372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:29.196894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:30.197118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:31.197555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:23:31.936
  May 12 13:23:31.946: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-86adec1c-869d-4f60-a444-9ca58816de87 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:23:31.973
  May 12 13:23:31.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4062" for this suite. @ 05/12/23 13:23:31.994
• [4.104 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 05/12/23 13:23:31.999
  May 12 13:23:31.999: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 13:23:32
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:23:32.013
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:23:32.014
  STEP: set up a multi version CRD @ 05/12/23 13:23:32.016
  May 12 13:23:32.016: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 13:23:32.200415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:33.206274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:34.208605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:35.209621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: rename a version @ 05/12/23 13:23:35.673
  STEP: check the new version name is served @ 05/12/23 13:23:35.686
  E0512 13:23:36.218269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:37.220008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the old version name is removed @ 05/12/23 13:23:37.315
  STEP: check the other version is not changed @ 05/12/23 13:23:38.028
  E0512 13:23:38.220802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:39.221725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:40.222720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:23:40.765: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4531" for this suite. @ 05/12/23 13:23:40.769
• [8.775 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 05/12/23 13:23:40.774
  May 12 13:23:40.774: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/23 13:23:40.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:23:40.79
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:23:40.792
  STEP: Creating ServiceAccount "e2e-sa-g8c5s"  @ 05/12/23 13:23:40.794
  May 12 13:23:40.796: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-g8c5s"  @ 05/12/23 13:23:40.796
  May 12 13:23:40.802: INFO: AutomountServiceAccountToken: true
  May 12 13:23:40.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7828" for this suite. @ 05/12/23 13:23:40.806
• [0.037 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 05/12/23 13:23:40.812
  May 12 13:23:40.812: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:23:40.812
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:23:40.826
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:23:40.827
  STEP: Creating a pod to test emptydir volume type on node default medium @ 05/12/23 13:23:40.829
  E0512 13:23:41.223191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:42.223237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:43.223399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:44.224608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:23:44.856
  May 12 13:23:44.861: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-fd3129eb-fd19-472f-bef6-96ddf6189fcf container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:23:44.868
  May 12 13:23:44.885: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6267" for this suite. @ 05/12/23 13:23:44.888
• [4.081 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 05/12/23 13:23:44.896
  May 12 13:23:44.896: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-runtime @ 05/12/23 13:23:44.897
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:23:44.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:23:44.91
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 05/12/23 13:23:44.917
  E0512 13:23:45.224751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:46.225611      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:47.226495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:48.227783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:49.227825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:50.228240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:51.228270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:52.234323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:53.235095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:54.235252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:55.235858      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:56.236013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:57.237097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:58.237990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:23:59.239139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:00.241967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:01.242044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 05/12/23 13:24:02.003
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 05/12/23 13:24:02.004
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 05/12/23 13:24:02.007
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 05/12/23 13:24:02.007
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 05/12/23 13:24:02.024
  E0512 13:24:02.242737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:03.242926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:04.243204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 05/12/23 13:24:05.038
  E0512 13:24:05.243702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 05/12/23 13:24:06.055
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 05/12/23 13:24:06.069
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 05/12/23 13:24:06.07
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 05/12/23 13:24:06.102
  E0512 13:24:06.244238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 05/12/23 13:24:07.11
  E0512 13:24:07.244226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:08.245150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 05/12/23 13:24:09.117
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 05/12/23 13:24:09.12
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 05/12/23 13:24:09.12
  May 12 13:24:09.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8673" for this suite. @ 05/12/23 13:24:09.139
• [24.246 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 05/12/23 13:24:09.143
  May 12 13:24:09.143: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:24:09.144
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:24:09.156
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:24:09.158
  STEP: creating a ConfigMap @ 05/12/23 13:24:09.159
  STEP: fetching the ConfigMap @ 05/12/23 13:24:09.163
  STEP: patching the ConfigMap @ 05/12/23 13:24:09.165
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 05/12/23 13:24:09.168
  STEP: deleting the ConfigMap by collection with a label selector @ 05/12/23 13:24:09.172
  STEP: listing all ConfigMaps in test namespace @ 05/12/23 13:24:09.175
  May 12 13:24:09.178: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5113" for this suite. @ 05/12/23 13:24:09.18
• [0.041 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 05/12/23 13:24:09.184
  May 12 13:24:09.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:24:09.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:24:09.199
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:24:09.201
  STEP: Creating configMap with name configmap-test-upd-07b6f5cd-8348-4017-a01b-d8679e447023 @ 05/12/23 13:24:09.204
  STEP: Creating the pod @ 05/12/23 13:24:09.208
  E0512 13:24:09.245741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:10.246803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod with text data @ 05/12/23 13:24:11.226
  STEP: Waiting for pod with binary data @ 05/12/23 13:24:11.246
  E0512 13:24:11.251293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:24:11.262: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9848" for this suite. @ 05/12/23 13:24:11.268
• [2.091 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 05/12/23 13:24:11.277
  May 12 13:24:11.278: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename endpointslicemirroring @ 05/12/23 13:24:11.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:24:11.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:24:11.293
  STEP: mirroring a new custom Endpoint @ 05/12/23 13:24:11.303
  May 12 13:24:11.318: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0512 13:24:12.251305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:13.251653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 05/12/23 13:24:13.33
  May 12 13:24:13.359: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0512 13:24:14.252275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:15.253033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 05/12/23 13:24:15.37
  May 12 13:24:15.402: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-6890" for this suite. @ 05/12/23 13:24:15.41
• [4.140 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 05/12/23 13:24:15.423
  May 12 13:24:15.423: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 13:24:15.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:24:15.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:24:15.437
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 13:24:15.438
  E0512 13:24:16.253182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:17.254077      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:18.254379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:19.254554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:24:19.456
  May 12 13:24:19.467: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-02c187aa-00ae-45ec-a079-0bff4165e83d container client-container: <nil>
  STEP: delete the pod @ 05/12/23 13:24:19.481
  May 12 13:24:19.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8137" for this suite. @ 05/12/23 13:24:19.505
• [4.087 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 05/12/23 13:24:19.511
  May 12 13:24:19.511: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename var-expansion @ 05/12/23 13:24:19.512
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:24:19.525
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:24:19.527
  STEP: Creating a pod to test substitution in container's command @ 05/12/23 13:24:19.528
  E0512 13:24:20.255346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:21.256572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:22.257002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:23.258592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:24:23.559
  May 12 13:24:23.567: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod var-expansion-0b6275a9-6eea-4ddc-b958-313064269a54 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 13:24:23.585
  May 12 13:24:23.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2176" for this suite. @ 05/12/23 13:24:23.606
• [4.100 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 05/12/23 13:24:23.612
  May 12 13:24:23.612: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 13:24:23.613
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:24:23.627
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:24:23.629
  STEP: Create set of pods @ 05/12/23 13:24:23.63
  May 12 13:24:23.636: INFO: created test-pod-1
  May 12 13:24:23.645: INFO: created test-pod-2
  May 12 13:24:23.652: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 05/12/23 13:24:23.652
  E0512 13:24:24.258218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:25.258331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 05/12/23 13:24:25.687
  May 12 13:24:25.690: INFO: Pod quantity 3 is different from expected quantity 0
  E0512 13:24:26.259421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:24:26.693: INFO: Pod quantity 3 is different from expected quantity 0
  E0512 13:24:27.260304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:24:27.696: INFO: Pod quantity 3 is different from expected quantity 0
  E0512 13:24:28.260943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:24:28.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2148" for this suite. @ 05/12/23 13:24:28.697
• [5.090 seconds]
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 05/12/23 13:24:28.704
  May 12 13:24:28.704: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:24:28.705
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:24:28.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:24:28.721
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 05/12/23 13:24:28.724
  E0512 13:24:29.261778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:30.262337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:31.262996      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:32.263376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:24:32.761
  May 12 13:24:32.771: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-bb284075-7ea1-416c-aebc-b4a0d8242655 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:24:32.786
  May 12 13:24:32.808: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6603" for this suite. @ 05/12/23 13:24:32.812
• [4.112 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 05/12/23 13:24:32.817
  May 12 13:24:32.817: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 13:24:32.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:24:32.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:24:32.831
  STEP: Creating pod test-grpc-037fec7d-e468-49aa-bf92-421d76cba6a6 in namespace container-probe-6137 @ 05/12/23 13:24:32.832
  E0512 13:24:33.263554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:34.273945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:24:34.852: INFO: Started pod test-grpc-037fec7d-e468-49aa-bf92-421d76cba6a6 in namespace container-probe-6137
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/23 13:24:34.852
  May 12 13:24:34.859: INFO: Initial restart count of pod test-grpc-037fec7d-e468-49aa-bf92-421d76cba6a6 is 0
  E0512 13:24:35.265132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:36.265899      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:37.265909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:38.266372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:39.267752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:40.267824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:41.268468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:42.268457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:43.269712      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:44.270127      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:45.270317      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:46.270831      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:47.271292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:48.271533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:49.272588      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:50.273065      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:51.274105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:52.273757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:53.274233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:54.274119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:55.275526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:56.274911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:57.276018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:58.276724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:24:59.277339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:00.277550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:01.278760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:02.278743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:03.280079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:04.279923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:05.282709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:06.281475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:07.282259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:08.282533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:09.283708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:10.284739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:11.285055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:12.284917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:13.286598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:14.286736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:15.287487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:16.287807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:17.288892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:18.289169      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:19.289512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:20.290453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:21.291165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:22.291576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:23.291693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:24.291737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:25.292940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:26.293371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:27.293338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:28.294072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:29.294540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:30.294765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:31.295501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:32.295663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:33.296111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:34.296828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:35.297797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:36.298386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:37.298621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:38.301413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:39.299811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:40.300087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:41.300889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:42.301810      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:43.302861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:44.304110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:45.305154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:46.305516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:47.306482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:48.307958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:49.309062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:50.309324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:51.310485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:52.311574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:53.313312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:54.313096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:55.314128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:56.314641      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:57.315507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:58.316708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:25:59.316400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:00.316964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:01.316708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:02.316774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:03.318283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:04.318701      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:05.319357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:06.319994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:07.320910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:08.321853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:09.322359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:10.323023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:11.324024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:12.323806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:13.324923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:14.324431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:15.325355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:16.325345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:17.326227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:18.326284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:19.327082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:20.327493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:21.328138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:22.328170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:23.328838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:24.329364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:25.329811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:26.330049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:27.330995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:28.330974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:29.331612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:30.331615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:31.331992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:32.333599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:33.334134      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:34.336772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:35.340289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:36.338637      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:37.338745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:38.338926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:39.339630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:40.342410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:41.342840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:42.343327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:43.343546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:44.344437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:45.345435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:46.346283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:47.346735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:48.347655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:49.347620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:50.348721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:51.348919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:52.348275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:53.348780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:54.348941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:55.349509      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:56.349529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:57.350695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:58.350578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:26:59.351753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:00.351994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:01.353743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:02.353954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:03.354814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:04.355325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:05.355769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:06.356385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:07.357223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:08.357392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:09.357929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:10.358029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:11.358858      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:12.359475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:13.359629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:14.359684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:15.366246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:16.366318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:17.366447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:18.367626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:19.368465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:20.381341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:21.377415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:22.377947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:23.378372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:24.378561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:25.378807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:26.378915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:27.379075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:28.379858      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:29.379970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:30.380908      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:31.381066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:32.381548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:33.381839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:34.381874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:35.382050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:36.382113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:37.382282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:38.382671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:39.386413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:40.386540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:41.386550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:42.387585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:43.387768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:44.388201      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:45.389433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:46.389465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:47.389801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:48.389779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:49.390519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:50.390721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:51.391806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:52.391508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:53.392161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:54.392993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:55.392799      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:56.392956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:57.393043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:58.393689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:27:59.394250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:00.394765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:01.394569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:02.395469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:03.396031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:04.396620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:05.396546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:06.397190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:07.398009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:08.399110      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:09.400095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:10.400154      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:11.400974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:12.401939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:13.402644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:14.402633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:15.402717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:16.403766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:17.404714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:18.405128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:19.412390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:20.407887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:21.408572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:22.409423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:23.410345      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:24.410216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:25.410254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:26.410376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:27.410458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:28.411129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:29.412355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:30.412770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:31.414061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:32.414174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:33.414443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:34.414921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:35.415569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:28:35.658: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 13:28:35.673
  STEP: Destroying namespace "container-probe-6137" for this suite. @ 05/12/23 13:28:35.7
• [242.888 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 05/12/23 13:28:35.707
  May 12 13:28:35.707: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 13:28:35.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:28:35.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:28:35.725
  STEP: Creating a pod to test downward api env vars @ 05/12/23 13:28:35.728
  E0512 13:28:36.415189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:37.415464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:38.416091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:39.416793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:28:39.756
  May 12 13:28:39.765: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downward-api-5382625d-0907-49a5-b98b-b5f98c7537f3 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 13:28:39.791
  May 12 13:28:39.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1723" for this suite. @ 05/12/23 13:28:39.815
• [4.111 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 05/12/23 13:28:39.821
  May 12 13:28:39.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename events @ 05/12/23 13:28:39.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:28:39.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:28:39.837
  STEP: Create set of events @ 05/12/23 13:28:39.838
  STEP: get a list of Events with a label in the current namespace @ 05/12/23 13:28:39.844
  STEP: delete a list of events @ 05/12/23 13:28:39.846
  May 12 13:28:39.846: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 05/12/23 13:28:39.853
  May 12 13:28:39.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5920" for this suite. @ 05/12/23 13:28:39.856
• [0.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 05/12/23 13:28:39.86
  May 12 13:28:39.860: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename watch @ 05/12/23 13:28:39.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:28:39.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:28:39.874
  STEP: creating a watch on configmaps @ 05/12/23 13:28:39.876
  STEP: creating a new configmap @ 05/12/23 13:28:39.876
  STEP: modifying the configmap once @ 05/12/23 13:28:39.879
  STEP: closing the watch once it receives two notifications @ 05/12/23 13:28:39.883
  May 12 13:28:39.883: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1509  5e8b3404-9cbe-4686-9b9c-2338f9dbbbdf 98155 0 2023-05-12 13:28:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-12 13:28:39 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:28:39.884: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1509  5e8b3404-9cbe-4686-9b9c-2338f9dbbbdf 98156 0 2023-05-12 13:28:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-12 13:28:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 05/12/23 13:28:39.884
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 05/12/23 13:28:39.888
  STEP: deleting the configmap @ 05/12/23 13:28:39.889
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 05/12/23 13:28:39.892
  May 12 13:28:39.892: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1509  5e8b3404-9cbe-4686-9b9c-2338f9dbbbdf 98157 0 2023-05-12 13:28:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-12 13:28:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:28:39.892: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1509  5e8b3404-9cbe-4686-9b9c-2338f9dbbbdf 98158 0 2023-05-12 13:28:39 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-05-12 13:28:39 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:28:39.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1509" for this suite. @ 05/12/23 13:28:39.895
• [0.038 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 05/12/23 13:28:39.899
  May 12 13:28:39.899: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 13:28:39.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:28:39.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:28:39.914
  May 12 13:28:39.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 13:28:40.417013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:41.417186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/12/23 13:28:41.731
  May 12 13:28:41.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6562 --namespace=crd-publish-openapi-6562 create -f -'
  E0512 13:28:42.417944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:28:42.554: INFO: stderr: ""
  May 12 13:28:42.554: INFO: stdout: "e2e-test-crd-publish-openapi-7081-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 12 13:28:42.554: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6562 --namespace=crd-publish-openapi-6562 delete e2e-test-crd-publish-openapi-7081-crds test-cr'
  May 12 13:28:42.603: INFO: stderr: ""
  May 12 13:28:42.603: INFO: stdout: "e2e-test-crd-publish-openapi-7081-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  May 12 13:28:42.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6562 --namespace=crd-publish-openapi-6562 apply -f -'
  May 12 13:28:42.851: INFO: stderr: ""
  May 12 13:28:42.851: INFO: stdout: "e2e-test-crd-publish-openapi-7081-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  May 12 13:28:42.851: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6562 --namespace=crd-publish-openapi-6562 delete e2e-test-crd-publish-openapi-7081-crds test-cr'
  May 12 13:28:42.906: INFO: stderr: ""
  May 12 13:28:42.906: INFO: stdout: "e2e-test-crd-publish-openapi-7081-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/12/23 13:28:42.906
  May 12 13:28:42.906: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6562 explain e2e-test-crd-publish-openapi-7081-crds'
  May 12 13:28:43.133: INFO: stderr: ""
  May 12 13:28:43.133: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-7081-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0512 13:28:43.418354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:44.430408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:28:45.117: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6562" for this suite. @ 05/12/23 13:28:45.152
• [5.263 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 05/12/23 13:28:45.162
  May 12 13:28:45.162: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-runtime @ 05/12/23 13:28:45.171
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:28:45.192
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:28:45.209
  STEP: create the container @ 05/12/23 13:28:45.211
  W0512 13:28:45.228678      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/12/23 13:28:45.228
  E0512 13:28:45.431956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:46.432295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:47.433408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/12/23 13:28:48.246
  STEP: the container should be terminated @ 05/12/23 13:28:48.248
  STEP: the termination message should be set @ 05/12/23 13:28:48.248
  May 12 13:28:48.248: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 05/12/23 13:28:48.248
  May 12 13:28:48.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-9917" for this suite. @ 05/12/23 13:28:48.261
• [3.104 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 05/12/23 13:28:48.267
  May 12 13:28:48.267: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-webhook @ 05/12/23 13:28:48.268
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:28:48.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:28:48.281
  STEP: Setting up server cert @ 05/12/23 13:28:48.282
  E0512 13:28:48.433868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/12/23 13:28:48.575
  STEP: Deploying the custom resource conversion webhook pod @ 05/12/23 13:28:48.58
  STEP: Wait for the deployment to be ready @ 05/12/23 13:28:48.591
  May 12 13:28:48.600: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0512 13:28:49.434375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:50.434486      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:28:50.605: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 28, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 28, 48, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 28, 48, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 28, 48, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-crd-conversion-webhook-deployment-5969648595\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 13:28:51.434576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:52.434672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 13:28:52.61
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:28:52.629
  E0512 13:28:53.435334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:28:53.643: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 12 13:28:53.650: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 13:28:54.448612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:55.447773      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/12/23 13:28:56.271
  STEP: Create a v2 custom resource @ 05/12/23 13:28:56.295
  STEP: List CRs in v1 @ 05/12/23 13:28:56.313
  STEP: List CRs in v2 @ 05/12/23 13:28:56.316
  May 12 13:28:56.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0512 13:28:56.447282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-5679" for this suite. @ 05/12/23 13:28:56.888
• [8.631 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 05/12/23 13:28:56.9
  May 12 13:28:56.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename deployment @ 05/12/23 13:28:56.901
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:28:56.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:28:56.917
  May 12 13:28:56.919: INFO: Creating deployment "test-recreate-deployment"
  May 12 13:28:56.925: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  May 12 13:28:56.942: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0512 13:28:57.447332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:28:58.447469      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:28:58.945: INFO: Waiting deployment "test-recreate-deployment" to complete
  May 12 13:28:58.947: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  May 12 13:28:58.953: INFO: Updating deployment test-recreate-deployment
  May 12 13:28:58.954: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  May 12 13:28:59.021: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-2338  6ba3e983-1151-4314-b2d8-6cf1348af51d 98435 2 2023-05-12 13:28:56 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-05-12 13:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:28:59 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038a4eb8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-05-12 13:28:58 +0000 UTC,LastTransitionTime:2023-05-12 13:28:58 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-05-12 13:28:59 +0000 UTC,LastTransitionTime:2023-05-12 13:28:56 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  May 12 13:28:59.024: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-2338  41e6a26f-aa2e-4082-a36d-97306629a9a1 98432 1 2023-05-12 13:28:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 6ba3e983-1151-4314-b2d8-6cf1348af51d 0xc0038a5737 0xc0038a5738}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ba3e983-1151-4314-b2d8-6cf1348af51d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:28:58 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038a57d8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 12 13:28:59.024: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  May 12 13:28:59.025: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-2338  6bdd435a-bf36-4a2c-999d-1e3ee1c02843 98423 2 2023-05-12 13:28:56 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 6ba3e983-1151-4314-b2d8-6cf1348af51d 0xc0038a5847 0xc0038a5848}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6ba3e983-1151-4314-b2d8-6cf1348af51d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:28:58 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0038a58f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  May 12 13:28:59.026: INFO: Pod "test-recreate-deployment-54757ffd6c-mmxdv" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-mmxdv test-recreate-deployment-54757ffd6c- deployment-2338  48f70c4e-2599-4a51-8439-12807d68d4fc 98434 0 2023-05-12 13:28:58 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c 41e6a26f-aa2e-4082-a36d-97306629a9a1 0xc0038a5db7 0xc0038a5db8}] [] [{kube-controller-manager Update v1 2023-05-12 13:28:58 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"41e6a26f-aa2e-4082-a36d-97306629a9a1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:28:59 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-g2pwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-g2pwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:28:58 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:28:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:28:58 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:28:58 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:,StartTime:2023-05-12 13:28:58 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  May 12 13:28:59.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2338" for this suite. @ 05/12/23 13:28:59.029
• [2.134 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 05/12/23 13:28:59.039
  May 12 13:28:59.039: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:28:59.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:28:59.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:28:59.052
  STEP: Setting up server cert @ 05/12/23 13:28:59.069
  E0512 13:28:59.447608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:28:59.491
  STEP: Deploying the webhook pod @ 05/12/23 13:28:59.495
  STEP: Wait for the deployment to be ready @ 05/12/23 13:28:59.505
  May 12 13:28:59.512: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 13:29:00.447752      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:01.448870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 13:29:01.532
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:29:01.558
  E0512 13:29:02.448861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:02.563: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 12 13:29:02.582: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-2420-crds.webhook.example.com via the AdmissionRegistration API @ 05/12/23 13:29:03.116
  STEP: Creating a custom resource while v1 is storage version @ 05/12/23 13:29:03.166
  E0512 13:29:03.450015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:04.450803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 05/12/23 13:29:05.186
  STEP: Patching the custom resource while v2 is storage version @ 05/12/23 13:29:05.194
  May 12 13:29:05.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0512 13:29:05.451194      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-7964" for this suite. @ 05/12/23 13:29:05.779
  STEP: Destroying namespace "webhook-markers-5893" for this suite. @ 05/12/23 13:29:05.784
• [6.750 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 05/12/23 13:29:05.789
  May 12 13:29:05.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename daemonsets @ 05/12/23 13:29:05.79
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:05.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:05.805
  May 12 13:29:05.817: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/23 13:29:05.821
  May 12 13:29:05.827: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:05.827: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:29:05.828: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:29:05.828: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:29:06.451442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:06.840: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:06.840: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:29:06.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:29:06.850: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:29:07.451685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:07.851: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:07.852: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:29:07.860: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 13:29:07.860: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 05/12/23 13:29:07.883
  STEP: Check that daemon pods images are updated. @ 05/12/23 13:29:07.892
  May 12 13:29:07.895: INFO: Wrong image for pod: daemon-set-ltvz2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 12 13:29:07.895: INFO: Wrong image for pod: daemon-set-nkrjp. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 12 13:29:07.901: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:07.901: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 13:29:08.452535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:08.904: INFO: Wrong image for pod: daemon-set-ltvz2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 12 13:29:08.906: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:08.906: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 13:29:09.453248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:09.912: INFO: Wrong image for pod: daemon-set-ltvz2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 12 13:29:09.926: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:09.926: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 13:29:10.454081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:10.905: INFO: Wrong image for pod: daemon-set-ltvz2. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  May 12 13:29:10.906: INFO: Pod daemon-set-qgtmq is not available
  May 12 13:29:10.911: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:10.911: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 13:29:11.454713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:11.921: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:11.922: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 13:29:12.455376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:12.903: INFO: Pod daemon-set-xnwvk is not available
  May 12 13:29:12.905: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:12.906: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Check that daemon pods are still running on every node of the cluster. @ 05/12/23 13:29:12.906
  May 12 13:29:12.908: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:12.908: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:29:12.910: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 13:29:12.910: INFO: Node onekube-ip-172-16-100-7 is running 0 daemon pod, expected 1
  E0512 13:29:13.456661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:13.920: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:29:13.920: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:29:13.922: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 13:29:13.923: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/23 13:29:13.928
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5941, will wait for the garbage collector to delete the pods @ 05/12/23 13:29:13.928
  May 12 13:29:13.984: INFO: Deleting DaemonSet.extensions daemon-set took: 3.866401ms
  May 12 13:29:14.084: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.272612ms
  E0512 13:29:14.457319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:15.457475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:16.387: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:29:16.387: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 12 13:29:16.389: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"98735"},"items":null}

  May 12 13:29:16.390: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"98735"},"items":null}

  May 12 13:29:16.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5941" for this suite. @ 05/12/23 13:29:16.397
• [10.612 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 05/12/23 13:29:16.406
  May 12 13:29:16.406: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename init-container @ 05/12/23 13:29:16.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:16.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:16.424
  STEP: creating the pod @ 05/12/23 13:29:16.431
  May 12 13:29:16.432: INFO: PodSpec: initContainers in spec.initContainers
  E0512 13:29:16.458052      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:17.458427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:18.458750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:19.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1295" for this suite. @ 05/12/23 13:29:19.378
• [2.978 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 05/12/23 13:29:19.39
  May 12 13:29:19.390: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:29:19.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:19.429
  E0512 13:29:19.502316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:19.502
  STEP: Creating configMap with name cm-test-opt-del-b1aac58f-c3e5-495c-8562-49354fa80642 @ 05/12/23 13:29:19.53
  STEP: Creating configMap with name cm-test-opt-upd-55b95671-d577-4f60-9fbd-7906da2dd4ca @ 05/12/23 13:29:19.534
  STEP: Creating the pod @ 05/12/23 13:29:19.536
  E0512 13:29:20.502604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:21.502978      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-b1aac58f-c3e5-495c-8562-49354fa80642 @ 05/12/23 13:29:21.579
  STEP: Updating configmap cm-test-opt-upd-55b95671-d577-4f60-9fbd-7906da2dd4ca @ 05/12/23 13:29:21.582
  STEP: Creating configMap with name cm-test-opt-create-5fcf0b05-c920-4a57-b617-710cdd8f2417 @ 05/12/23 13:29:21.588
  STEP: waiting to observe update in volume @ 05/12/23 13:29:21.593
  E0512 13:29:22.503114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:23.503783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:23.610: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2625" for this suite. @ 05/12/23 13:29:23.613
• [4.228 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 05/12/23 13:29:23.618
  May 12 13:29:23.618: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 13:29:23.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:23.632
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:23.634
  STEP: Creating a ResourceQuota @ 05/12/23 13:29:23.636
  STEP: Getting a ResourceQuota @ 05/12/23 13:29:23.638
  STEP: Listing all ResourceQuotas with LabelSelector @ 05/12/23 13:29:23.64
  STEP: Patching the ResourceQuota @ 05/12/23 13:29:23.643
  STEP: Deleting a Collection of ResourceQuotas @ 05/12/23 13:29:23.646
  STEP: Verifying the deleted ResourceQuota @ 05/12/23 13:29:23.652
  May 12 13:29:23.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3541" for this suite. @ 05/12/23 13:29:23.656
• [0.041 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 05/12/23 13:29:23.661
  May 12 13:29:23.661: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename namespaces @ 05/12/23 13:29:23.662
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:23.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:23.676
  STEP: Read namespace status @ 05/12/23 13:29:23.678
  May 12 13:29:23.680: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 05/12/23 13:29:23.68
  May 12 13:29:23.683: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 05/12/23 13:29:23.684
  May 12 13:29:23.690: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  May 12 13:29:23.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-2809" for this suite. @ 05/12/23 13:29:23.694
• [0.037 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 05/12/23 13:29:23.699
  May 12 13:29:23.699: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename proxy @ 05/12/23 13:29:23.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:23.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:23.714
  May 12 13:29:23.715: INFO: Creating pod...
  E0512 13:29:24.504090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:25.504723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:25.731: INFO: Creating service...
  May 12 13:29:25.765: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/pods/agnhost/proxy?method=DELETE
  May 12 13:29:25.778: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 12 13:29:25.778: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/pods/agnhost/proxy?method=OPTIONS
  May 12 13:29:25.780: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 12 13:29:25.780: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/pods/agnhost/proxy?method=PATCH
  May 12 13:29:25.783: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 12 13:29:25.783: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/pods/agnhost/proxy?method=POST
  May 12 13:29:25.787: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 12 13:29:25.788: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/pods/agnhost/proxy?method=PUT
  May 12 13:29:25.790: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 12 13:29:25.791: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/services/e2e-proxy-test-service/proxy?method=DELETE
  May 12 13:29:25.797: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  May 12 13:29:25.797: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/services/e2e-proxy-test-service/proxy?method=OPTIONS
  May 12 13:29:25.802: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  May 12 13:29:25.802: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/services/e2e-proxy-test-service/proxy?method=PATCH
  May 12 13:29:25.805: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  May 12 13:29:25.805: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/services/e2e-proxy-test-service/proxy?method=POST
  May 12 13:29:25.807: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  May 12 13:29:25.807: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/services/e2e-proxy-test-service/proxy?method=PUT
  May 12 13:29:25.809: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  May 12 13:29:25.810: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/pods/agnhost/proxy?method=GET
  May 12 13:29:25.811: INFO: http.Client request:GET StatusCode:301
  May 12 13:29:25.811: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/services/e2e-proxy-test-service/proxy?method=GET
  May 12 13:29:25.812: INFO: http.Client request:GET StatusCode:301
  May 12 13:29:25.813: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/pods/agnhost/proxy?method=HEAD
  May 12 13:29:25.815: INFO: http.Client request:HEAD StatusCode:301
  May 12 13:29:25.815: INFO: Starting http.Client for https://10.43.0.1:443/api/v1/namespaces/proxy-2604/services/e2e-proxy-test-service/proxy?method=HEAD
  May 12 13:29:25.817: INFO: http.Client request:HEAD StatusCode:301
  May 12 13:29:25.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-2604" for this suite. @ 05/12/23 13:29:25.819
• [2.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 05/12/23 13:29:25.829
  May 12 13:29:25.829: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 13:29:25.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:25.842
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:25.844
  STEP: Creating a ResourceQuota with terminating scope @ 05/12/23 13:29:25.845
  STEP: Ensuring ResourceQuota status is calculated @ 05/12/23 13:29:25.85
  E0512 13:29:26.504658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:27.505298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 05/12/23 13:29:27.86
  STEP: Ensuring ResourceQuota status is calculated @ 05/12/23 13:29:27.881
  E0512 13:29:28.505562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:29.506008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 05/12/23 13:29:29.885
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 05/12/23 13:29:29.901
  E0512 13:29:30.506710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:31.507396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 05/12/23 13:29:31.904
  E0512 13:29:32.507389      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:33.507736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/12/23 13:29:33.912
  STEP: Ensuring resource quota status released the pod usage @ 05/12/23 13:29:33.945
  E0512 13:29:34.507910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:35.508448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 05/12/23 13:29:35.951
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 05/12/23 13:29:35.977
  E0512 13:29:36.508662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:37.508935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 05/12/23 13:29:37.986
  E0512 13:29:38.509689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:39.509456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 05/12/23 13:29:39.99
  STEP: Ensuring resource quota status released the pod usage @ 05/12/23 13:29:40.002
  E0512 13:29:40.509830      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:41.510550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:42.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1182" for this suite. @ 05/12/23 13:29:42.047
• [16.227 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 05/12/23 13:29:42.059
  May 12 13:29:42.059: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename statefulset @ 05/12/23 13:29:42.06
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:42.128
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:42.136
  STEP: Creating service test in namespace statefulset-2810 @ 05/12/23 13:29:42.159
  STEP: Looking for a node to schedule stateful set and pod @ 05/12/23 13:29:42.177
  STEP: Creating pod with conflicting port in namespace statefulset-2810 @ 05/12/23 13:29:42.203
  STEP: Waiting until pod test-pod will start running in namespace statefulset-2810 @ 05/12/23 13:29:42.218
  E0512 13:29:42.511458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:43.511705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-2810 @ 05/12/23 13:29:44.248
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2810 @ 05/12/23 13:29:44.263
  May 12 13:29:44.291: INFO: Observed stateful pod in namespace: statefulset-2810, name: ss-0, uid: 9c76e452-0e3a-40b9-a4e3-7c462ab9a98b, status phase: Pending. Waiting for statefulset controller to delete.
  May 12 13:29:44.300: INFO: Observed stateful pod in namespace: statefulset-2810, name: ss-0, uid: 9c76e452-0e3a-40b9-a4e3-7c462ab9a98b, status phase: Failed. Waiting for statefulset controller to delete.
  May 12 13:29:44.306: INFO: Observed stateful pod in namespace: statefulset-2810, name: ss-0, uid: 9c76e452-0e3a-40b9-a4e3-7c462ab9a98b, status phase: Failed. Waiting for statefulset controller to delete.
  May 12 13:29:44.308: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2810
  STEP: Removing pod with conflicting port in namespace statefulset-2810 @ 05/12/23 13:29:44.308
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2810 and will be in running state @ 05/12/23 13:29:44.328
  E0512 13:29:44.512343      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:45.512117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:46.338: INFO: Deleting all statefulset in ns statefulset-2810
  May 12 13:29:46.340: INFO: Scaling statefulset ss to 0
  E0512 13:29:46.512815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:47.520090      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:48.520986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:49.521596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:50.522265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:51.522983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:52.522318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:53.523102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:54.523301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:55.523584      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:29:56.363: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 13:29:56.364: INFO: Deleting statefulset ss
  May 12 13:29:56.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2810" for this suite. @ 05/12/23 13:29:56.378
• [14.323 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 05/12/23 13:29:56.383
  May 12 13:29:56.383: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 13:29:56.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:29:56.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:29:56.405
  STEP: Creating secret with name secret-test-265eecad-08cc-4b67-b443-966e777ddbff @ 05/12/23 13:29:56.423
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:29:56.427
  E0512 13:29:56.524534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:57.525003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:58.524991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:29:59.525630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:30:00.449
  May 12 13:30:00.453: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-secrets-a26da0ac-442e-48ce-aa6d-761f49e334c8 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:30:00.461
  May 12 13:30:00.479: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5455" for this suite. @ 05/12/23 13:30:00.482
  STEP: Destroying namespace "secret-namespace-285" for this suite. @ 05/12/23 13:30:00.489
• [4.119 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 05/12/23 13:30:00.502
  May 12 13:30:00.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:30:00.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:30:00.521
  E0512 13:30:00.525772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:30:00.533
  STEP: Creating configMap with name projected-configmap-test-volume-beadf8f5-eaa5-4287-834d-fd3c07f95882 @ 05/12/23 13:30:00.534
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:30:00.537
  E0512 13:30:01.526269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:02.526697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:03.527313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:04.538733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:30:04.561
  May 12 13:30:04.563: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-configmaps-c27e73da-9cda-4247-a245-8a186686791c container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:30:04.568
  May 12 13:30:04.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8204" for this suite. @ 05/12/23 13:30:04.583
• [4.085 seconds]
------------------------------
S
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 05/12/23 13:30:04.588
  May 12 13:30:04.588: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename subpath @ 05/12/23 13:30:04.589
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:30:04.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:30:04.604
  STEP: Setting up data @ 05/12/23 13:30:04.606
  STEP: Creating pod pod-subpath-test-downwardapi-rfld @ 05/12/23 13:30:04.614
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/23 13:30:04.614
  E0512 13:30:05.544206      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:06.548097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:07.548621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:08.549136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:09.550066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:10.550427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:11.550309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:12.550577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:13.551363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:14.552452      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:15.553237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:16.553797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:17.555790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:18.555901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:19.555912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:20.556984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:21.557319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:22.557697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:23.558461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:24.559002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:25.558759      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:26.559007      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:27.559709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:28.560839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:30:28.7
  May 12 13:30:28.706: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-subpath-test-downwardapi-rfld container test-container-subpath-downwardapi-rfld: <nil>
  STEP: delete the pod @ 05/12/23 13:30:28.721
  STEP: Deleting pod pod-subpath-test-downwardapi-rfld @ 05/12/23 13:30:28.74
  May 12 13:30:28.740: INFO: Deleting pod "pod-subpath-test-downwardapi-rfld" in namespace "subpath-8901"
  May 12 13:30:28.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8901" for this suite. @ 05/12/23 13:30:28.745
• [24.162 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 05/12/23 13:30:28.75
  May 12 13:30:28.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:30:28.751
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:30:28.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:30:28.764
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/12/23 13:30:28.766
  May 12 13:30:28.766: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-5283 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  May 12 13:30:28.823: INFO: stderr: ""
  May 12 13:30:28.823: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 05/12/23 13:30:28.824
  May 12 13:30:28.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-5283 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  May 12 13:30:28.885: INFO: stderr: ""
  May 12 13:30:28.885: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/12/23 13:30:28.885
  May 12 13:30:28.887: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-5283 delete pods e2e-test-httpd-pod'
  E0512 13:30:29.561774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:30.562443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:30:30.682: INFO: stderr: ""
  May 12 13:30:30.682: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 12 13:30:30.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5283" for this suite. @ 05/12/23 13:30:30.686
• [1.940 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 05/12/23 13:30:30.692
  May 12 13:30:30.692: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename dns @ 05/12/23 13:30:30.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:30:30.704
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:30:30.707
  STEP: Creating a test externalName service @ 05/12/23 13:30:30.708
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-930.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local; sleep 1; done
   @ 05/12/23 13:30:30.713
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-930.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-930.svc.cluster.local; sleep 1; done
   @ 05/12/23 13:30:30.713
  STEP: creating a pod to probe DNS @ 05/12/23 13:30:30.713
  STEP: submitting the pod to kubernetes @ 05/12/23 13:30:30.713
  E0512 13:30:31.563250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:32.563446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/12/23 13:30:32.737
  STEP: looking for the results for each expected name from probers @ 05/12/23 13:30:32.745
  May 12 13:30:32.771: INFO: DNS probes using dns-test-b5e7404a-23e6-478d-a479-d3f8e94541ca succeeded

  STEP: changing the externalName to bar.example.com @ 05/12/23 13:30:32.771
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-930.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local; sleep 1; done
   @ 05/12/23 13:30:32.781
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-930.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-930.svc.cluster.local; sleep 1; done
   @ 05/12/23 13:30:32.781
  STEP: creating a second pod to probe DNS @ 05/12/23 13:30:32.781
  STEP: submitting the pod to kubernetes @ 05/12/23 13:30:32.781
  E0512 13:30:33.563483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:34.564874      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/12/23 13:30:34.803
  STEP: looking for the results for each expected name from probers @ 05/12/23 13:30:34.805
  May 12 13:30:34.810: INFO: File wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:34.812: INFO: File jessie_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:34.813: INFO: Lookups using dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 failed for: [wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local jessie_udp@dns-test-service-3.dns-930.svc.cluster.local]

  E0512 13:30:35.565680      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:36.566676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:37.567476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:38.568296      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:39.568228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:30:39.822: INFO: File wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:39.828: INFO: File jessie_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:39.829: INFO: Lookups using dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 failed for: [wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local jessie_udp@dns-test-service-3.dns-930.svc.cluster.local]

  E0512 13:30:40.568306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:41.569396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:42.569313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:43.569743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:44.570609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:30:44.825: INFO: File wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:44.838: INFO: File jessie_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:44.839: INFO: Lookups using dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 failed for: [wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local jessie_udp@dns-test-service-3.dns-930.svc.cluster.local]

  E0512 13:30:45.570950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:46.571368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:47.571533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:48.574360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:49.573881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:30:49.827: INFO: File wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:49.837: INFO: File jessie_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:49.837: INFO: Lookups using dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 failed for: [wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local jessie_udp@dns-test-service-3.dns-930.svc.cluster.local]

  E0512 13:30:50.574049      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:51.574203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:52.574556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:53.574555      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:54.574725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:30:54.825: INFO: File wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:54.836: INFO: File jessie_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:54.836: INFO: Lookups using dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 failed for: [wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local jessie_udp@dns-test-service-3.dns-930.svc.cluster.local]

  E0512 13:30:55.575548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:56.576608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:57.576610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:58.576739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:30:59.576864      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:30:59.816: INFO: File wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:59.818: INFO: File jessie_udp@dns-test-service-3.dns-930.svc.cluster.local from pod  dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  May 12 13:30:59.818: INFO: Lookups using dns-930/dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 failed for: [wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local jessie_udp@dns-test-service-3.dns-930.svc.cluster.local]

  E0512 13:31:00.577222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:01.577240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:02.577450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:03.577690      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:04.578447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:31:04.835: INFO: DNS probes using dns-test-2eada760-e22d-45ba-bd05-e20cde1d3be5 succeeded

  STEP: changing the service to type=ClusterIP @ 05/12/23 13:31:04.835
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-930.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-930.svc.cluster.local; sleep 1; done
   @ 05/12/23 13:31:04.871
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-930.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-930.svc.cluster.local; sleep 1; done
   @ 05/12/23 13:31:04.871
  STEP: creating a third pod to probe DNS @ 05/12/23 13:31:04.871
  STEP: submitting the pod to kubernetes @ 05/12/23 13:31:04.879
  E0512 13:31:05.579286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:06.582795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/12/23 13:31:06.908
  STEP: looking for the results for each expected name from probers @ 05/12/23 13:31:06.928
  May 12 13:31:06.945: INFO: DNS probes using dns-test-94d384e3-7869-461f-ba6f-8c08e4739ec7 succeeded

  May 12 13:31:06.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 13:31:06.96
  STEP: deleting the pod @ 05/12/23 13:31:06.98
  STEP: deleting the pod @ 05/12/23 13:31:06.995
  STEP: deleting the test externalName service @ 05/12/23 13:31:07.011
  STEP: Destroying namespace "dns-930" for this suite. @ 05/12/23 13:31:07.034
• [36.355 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 05/12/23 13:31:07.049
  May 12 13:31:07.049: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:31:07.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:31:07.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:31:07.065
  STEP: Creating configMap with name projected-configmap-test-volume-map-c22e05a0-e74b-4055-9e50-7c8c110d77ae @ 05/12/23 13:31:07.067
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:31:07.069
  E0512 13:31:07.580558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:08.580786      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:09.581105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:10.581694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:31:11.104
  May 12 13:31:11.110: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-configmaps-0fb5f348-0486-45ec-81bc-c3d49340a20b container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:31:11.122
  May 12 13:31:11.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1486" for this suite. @ 05/12/23 13:31:11.142
• [4.096 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 05/12/23 13:31:11.147
  May 12 13:31:11.147: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:31:11.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:31:11.16
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:31:11.161
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 13:31:11.163
  E0512 13:31:11.582554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:12.582919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:13.583304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:14.584179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:31:15.189
  May 12 13:31:15.190: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-af713177-a153-4097-af4e-928962c3653f container client-container: <nil>
  STEP: delete the pod @ 05/12/23 13:31:15.194
  May 12 13:31:15.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-80" for this suite. @ 05/12/23 13:31:15.206
• [4.062 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 05/12/23 13:31:15.21
  May 12 13:31:15.210: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:31:15.211
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:31:15.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:31:15.229
  STEP: validating api versions @ 05/12/23 13:31:15.231
  May 12 13:31:15.231: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-121 api-versions'
  May 12 13:31:15.276: INFO: stderr: ""
  May 12 13:31:15.276: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ncrd.projectcalico.org/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nhelm.cattle.io/v1\nk3s.cattle.io/v1\nlonghorn.io/v1beta1\nlonghorn.io/v1beta2\nmetallb.io/v1alpha1\nmetallb.io/v1beta1\nmetallb.io/v1beta2\nmetrics.k8s.io/v1beta1\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nsnapshot.storage.k8s.io/v1\nsnapshot.storage.k8s.io/v1beta1\nstorage.k8s.io/v1\ntraefik.containo.us/v1alpha1\ntraefik.io/v1alpha1\nv1\n"
  May 12 13:31:15.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-121" for this suite. @ 05/12/23 13:31:15.278
• [0.075 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 05/12/23 13:31:15.285
  May 12 13:31:15.285: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 13:31:15.285
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:31:15.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:31:15.298
  STEP: creating the pod @ 05/12/23 13:31:15.3
  STEP: setting up watch @ 05/12/23 13:31:15.3
  STEP: submitting the pod to kubernetes @ 05/12/23 13:31:15.402
  STEP: verifying the pod is in kubernetes @ 05/12/23 13:31:15.431
  STEP: verifying pod creation was observed @ 05/12/23 13:31:15.44
  E0512 13:31:15.585271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:16.585428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:17.586618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:18.586724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/12/23 13:31:19.486
  STEP: verifying pod deletion was observed @ 05/12/23 13:31:19.518
  E0512 13:31:19.586889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:20.587982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:31:21.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1581" for this suite. @ 05/12/23 13:31:21.037
• [5.761 seconds]
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 05/12/23 13:31:21.046
  May 12 13:31:21.046: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 05/12/23 13:31:21.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:31:21.057
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:31:21.059
  STEP: Setting up the test @ 05/12/23 13:31:21.061
  STEP: Creating hostNetwork=false pod @ 05/12/23 13:31:21.061
  E0512 13:31:21.588170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:22.588361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 05/12/23 13:31:23.079
  E0512 13:31:23.589353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:24.590556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 05/12/23 13:31:25.098
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 05/12/23 13:31:25.098
  May 12 13:31:25.098: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.098: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.099: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.099: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 12 13:31:25.171: INFO: Exec stderr: ""
  May 12 13:31:25.172: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.172: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.172: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.173: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 12 13:31:25.230: INFO: Exec stderr: ""
  May 12 13:31:25.231: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.231: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.232: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.232: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 12 13:31:25.289: INFO: Exec stderr: ""
  May 12 13:31:25.289: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.290: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.290: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 12 13:31:25.348: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 05/12/23 13:31:25.349
  May 12 13:31:25.349: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.349: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.350: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.350: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 12 13:31:25.404: INFO: Exec stderr: ""
  May 12 13:31:25.404: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.404: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.405: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.405: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  May 12 13:31:25.461: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 05/12/23 13:31:25.461
  May 12 13:31:25.461: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.461: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.462: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.462: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 12 13:31:25.521: INFO: Exec stderr: ""
  May 12 13:31:25.521: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.521: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.522: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.522: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  May 12 13:31:25.577: INFO: Exec stderr: ""
  May 12 13:31:25.577: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.577: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.577: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.577: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0512 13:31:25.590365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:31:25.632: INFO: Exec stderr: ""
  May 12 13:31:25.633: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-31 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:31:25.633: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:31:25.633: INFO: ExecWithOptions: Clientset creation
  May 12 13:31:25.633: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-31/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  May 12 13:31:25.700: INFO: Exec stderr: ""
  May 12 13:31:25.700: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-31" for this suite. @ 05/12/23 13:31:25.704
• [4.663 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 05/12/23 13:31:25.71
  May 12 13:31:25.710: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename statefulset @ 05/12/23 13:31:25.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:31:25.725
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:31:25.727
  STEP: Creating service test in namespace statefulset-4181 @ 05/12/23 13:31:25.728
  STEP: Creating a new StatefulSet @ 05/12/23 13:31:25.737
  May 12 13:31:25.746: INFO: Found 0 stateful pods, waiting for 3
  E0512 13:31:26.591415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:27.591390      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:28.591677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:29.591891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:30.591967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:31.592244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:32.592368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:33.593589      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:34.593178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:35.593296      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:31:35.749: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 12 13:31:35.749: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 12 13:31:35.749: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 05/12/23 13:31:35.755
  May 12 13:31:35.770: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 05/12/23 13:31:35.77
  E0512 13:31:36.594528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:37.595028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:38.595782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:39.595793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:40.596115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:41.596377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:42.596672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:43.596997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:44.597359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:45.597639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 05/12/23 13:31:45.781
  STEP: Performing a canary update @ 05/12/23 13:31:45.781
  May 12 13:31:45.797: INFO: Updating stateful set ss2
  May 12 13:31:45.811: INFO: Waiting for Pod statefulset-4181/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0512 13:31:46.598309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:47.598095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:48.598502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:49.598593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:50.598924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:51.599513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:52.600001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:53.600117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:54.600202      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:55.600939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 05/12/23 13:31:55.817
  May 12 13:31:55.859: INFO: Found 2 stateful pods, waiting for 3
  E0512 13:31:56.602078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:57.602620      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:58.603669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:31:59.604804      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:00.604172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:01.604240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:02.604436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:03.604746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:04.604872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:05.605253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:32:05.870: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  May 12 13:32:05.870: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  May 12 13:32:05.871: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 05/12/23 13:32:05.874
  May 12 13:32:05.890: INFO: Updating stateful set ss2
  May 12 13:32:05.901: INFO: Waiting for Pod statefulset-4181/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0512 13:32:06.606239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:07.606308      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:08.606495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:09.606967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:10.607474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:11.608629      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:12.609244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:13.609377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:14.609820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:15.610457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:32:15.928: INFO: Updating stateful set ss2
  May 12 13:32:15.942: INFO: Waiting for StatefulSet statefulset-4181/ss2 to complete update
  May 12 13:32:15.942: INFO: Waiting for Pod statefulset-4181/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0512 13:32:16.610701      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:17.610895      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:18.611881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:19.612326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:20.612466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:21.613505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:22.614625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:23.615013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:24.616117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:25.616725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:32:25.950: INFO: Deleting all statefulset in ns statefulset-4181
  May 12 13:32:25.952: INFO: Scaling statefulset ss2 to 0
  E0512 13:32:26.617668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:27.617650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:28.617749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:29.618048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:30.618171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:31.619101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:32.619424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:33.622769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:34.623271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:35.623525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:32:35.965: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 13:32:35.966: INFO: Deleting statefulset ss2
  May 12 13:32:35.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4181" for this suite. @ 05/12/23 13:32:35.983
• [70.277 seconds]
------------------------------
S
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 05/12/23 13:32:35.988
  May 12 13:32:35.988: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 13:32:35.989
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:32:36.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:32:36.005
  May 12 13:32:36.007: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: creating the pod @ 05/12/23 13:32:36.007
  STEP: submitting the pod to kubernetes @ 05/12/23 13:32:36.007
  E0512 13:32:36.624396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:37.624474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:32:38.098: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9201" for this suite. @ 05/12/23 13:32:38.101
• [2.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 05/12/23 13:32:38.112
  May 12 13:32:38.112: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:32:38.113
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:32:38.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:32:38.13
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 13:32:38.131
  E0512 13:32:38.625427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:39.625576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:40.627064      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:41.627992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:32:42.227
  May 12 13:32:42.230: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-36c19615-ec83-4e8c-a7a6-8457fb2d7618 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 13:32:42.233
  May 12 13:32:42.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5807" for this suite. @ 05/12/23 13:32:42.252
• [4.147 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 05/12/23 13:32:42.261
  May 12 13:32:42.261: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-pred @ 05/12/23 13:32:42.262
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:32:42.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:32:42.278
  May 12 13:32:42.280: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 12 13:32:42.284: INFO: Waiting for terminating namespaces to be deleted...
  May 12 13:32:42.285: INFO: 
  Logging pods the apiserver thinks is on node onekube-ip-172-16-100-5 before test
  May 12 13:32:42.296: INFO: helm-install-one-longhorn-nv79t from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.296: INFO: 	Container helm ready: false, restart count 0
  May 12 13:32:42.297: INFO: helm-install-one-metallb-6p76s from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.297: INFO: 	Container helm ready: false, restart count 0
  May 12 13:32:42.297: INFO: helm-install-one-traefik-qs2vx from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.297: INFO: 	Container helm ready: false, restart count 0
  May 12 13:32:42.297: INFO: helm-install-rke2-metrics-server-vb7g9 from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.297: INFO: 	Container helm ready: false, restart count 0
  May 12 13:32:42.298: INFO: helm-install-rke2-snapshot-controller-5cb8c from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.298: INFO: 	Container helm ready: false, restart count 1
  May 12 13:32:42.298: INFO: helm-install-rke2-snapshot-controller-crd-x6qww from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.298: INFO: 	Container helm ready: false, restart count 0
  May 12 13:32:42.298: INFO: helm-install-rke2-snapshot-validation-webhook-5ft26 from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.298: INFO: 	Container helm ready: false, restart count 0
  May 12 13:32:42.299: INFO: kube-proxy-onekube-ip-172-16-100-5 from kube-system started at 2023-05-12 10:27:24 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.299: INFO: 	Container kube-proxy ready: true, restart count 0
  May 12 13:32:42.299: INFO: rke2-canal-qjzfl from kube-system started at 2023-05-12 10:27:25 +0000 UTC (2 container statuses recorded)
  May 12 13:32:42.299: INFO: 	Container calico-node ready: true, restart count 0
  May 12 13:32:42.299: INFO: 	Container kube-flannel ready: true, restart count 0
  May 12 13:32:42.299: INFO: rke2-coredns-rke2-coredns-5896cccb79-95wmb from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.299: INFO: 	Container coredns ready: true, restart count 0
  May 12 13:32:42.300: INFO: rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-hg8jj from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.300: INFO: 	Container autoscaler ready: true, restart count 0
  May 12 13:32:42.300: INFO: rke2-metrics-server-6d45f6cb4d-h7dx4 from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.300: INFO: 	Container metrics-server ready: true, restart count 0
  May 12 13:32:42.300: INFO: rke2-snapshot-controller-7bf6d7bf5f-sg6ft from kube-system started at 2023-05-12 10:28:35 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.300: INFO: 	Container rke2-snapshot-controller ready: true, restart count 0
  May 12 13:32:42.300: INFO: rke2-snapshot-validation-webhook-b65d46c9f-8bqxq from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.300: INFO: 	Container rke2-snapshot-validation-webhook ready: true, restart count 0
  May 12 13:32:42.300: INFO: csi-attacher-79bf77b7d8-vr2c8 from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.301: INFO: 	Container csi-attacher ready: true, restart count 0
  May 12 13:32:42.301: INFO: csi-provisioner-566959ff99-rj4wp from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.301: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 12 13:32:42.301: INFO: csi-provisioner-566959ff99-srfx5 from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.301: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 12 13:32:42.301: INFO: csi-resizer-769c8fc86-4mp8g from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.301: INFO: 	Container csi-resizer ready: true, restart count 0
  May 12 13:32:42.301: INFO: csi-resizer-769c8fc86-5lvwx from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.301: INFO: 	Container csi-resizer ready: true, restart count 0
  May 12 13:32:42.302: INFO: csi-snapshotter-5677bd8f7f-2pqth from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.302: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 12 13:32:42.302: INFO: csi-snapshotter-5677bd8f7f-9dlsf from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.302: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 12 13:32:42.302: INFO: engine-image-ei-7fa7c208-kkhx7 from longhorn-system started at 2023-05-12 10:28:57 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.302: INFO: 	Container engine-image-ei-7fa7c208 ready: true, restart count 0
  May 12 13:32:42.302: INFO: instance-manager-e-1d0a2d1f86ac8de0e3ad611c8e7e88f7 from longhorn-system started at 2023-05-12 10:28:57 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.302: INFO: 	Container engine-manager ready: true, restart count 0
  May 12 13:32:42.303: INFO: longhorn-admission-webhook-9944c8788-5vs75 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.303: INFO: 	Container longhorn-admission-webhook ready: true, restart count 0
  May 12 13:32:42.303: INFO: longhorn-admission-webhook-9944c8788-n5t49 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.303: INFO: 	Container longhorn-admission-webhook ready: true, restart count 0
  May 12 13:32:42.303: INFO: longhorn-conversion-webhook-6c88c48f86-rwmr7 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.303: INFO: 	Container longhorn-conversion-webhook ready: true, restart count 0
  May 12 13:32:42.303: INFO: longhorn-conversion-webhook-6c88c48f86-x7rnk from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.303: INFO: 	Container longhorn-conversion-webhook ready: true, restart count 0
  May 12 13:32:42.304: INFO: longhorn-csi-plugin-8mtl4 from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (3 container statuses recorded)
  May 12 13:32:42.304: INFO: 	Container longhorn-csi-plugin ready: true, restart count 0
  May 12 13:32:42.304: INFO: 	Container longhorn-liveness-probe ready: true, restart count 0
  May 12 13:32:42.304: INFO: 	Container node-driver-registrar ready: true, restart count 0
  May 12 13:32:42.304: INFO: longhorn-manager-lsknf from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.304: INFO: 	Container longhorn-manager ready: true, restart count 0
  May 12 13:32:42.304: INFO: longhorn-recovery-backend-568bcc58fc-5d6pn from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.304: INFO: 	Container longhorn-recovery-backend ready: true, restart count 0
  May 12 13:32:42.304: INFO: longhorn-recovery-backend-568bcc58fc-lklhz from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.305: INFO: 	Container longhorn-recovery-backend ready: true, restart count 0
  May 12 13:32:42.305: INFO: one-metallb-controller-7965f57b86-zvww2 from metallb-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.305: INFO: 	Container controller ready: true, restart count 0
  May 12 13:32:42.305: INFO: one-metallb-speaker-tr56f from metallb-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.305: INFO: 	Container speaker ready: true, restart count 0
  May 12 13:32:42.305: INFO: sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-tc4w8 from sonobuoy started at 2023-05-12 12:31:55 +0000 UTC (2 container statuses recorded)
  May 12 13:32:42.305: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 12 13:32:42.305: INFO: 	Container systemd-logs ready: true, restart count 0
  May 12 13:32:42.306: INFO: one-traefik-66db8f599c-7j8zw from traefik-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.306: INFO: 	Container one-traefik ready: true, restart count 0
  May 12 13:32:42.306: INFO: 
  Logging pods the apiserver thinks is on node onekube-ip-172-16-100-7 before test
  May 12 13:32:42.313: INFO: kube-proxy-onekube-ip-172-16-100-7 from kube-system started at 2023-05-12 10:34:21 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container kube-proxy ready: true, restart count 0
  May 12 13:32:42.313: INFO: rke2-canal-58rgz from kube-system started at 2023-05-12 10:34:22 +0000 UTC (2 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container calico-node ready: true, restart count 0
  May 12 13:32:42.313: INFO: 	Container kube-flannel ready: true, restart count 0
  May 12 13:32:42.313: INFO: engine-image-ei-7fa7c208-kbxr9 from longhorn-system started at 2023-05-12 13:21:00 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container engine-image-ei-7fa7c208 ready: true, restart count 0
  May 12 13:32:42.313: INFO: instance-manager-e-bb03c767b3d98d645b664cf2a3258c7f from longhorn-system started at 2023-05-12 13:20:57 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container engine-manager ready: true, restart count 0
  May 12 13:32:42.313: INFO: longhorn-csi-plugin-dnp4n from longhorn-system started at 2023-05-12 13:20:55 +0000 UTC (3 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container longhorn-csi-plugin ready: true, restart count 0
  May 12 13:32:42.313: INFO: 	Container longhorn-liveness-probe ready: true, restart count 0
  May 12 13:32:42.313: INFO: 	Container node-driver-registrar ready: true, restart count 0
  May 12 13:32:42.313: INFO: longhorn-manager-zg9ps from longhorn-system started at 2023-05-12 13:20:55 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container longhorn-manager ready: true, restart count 0
  May 12 13:32:42.313: INFO: one-metallb-speaker-h8f7g from metallb-system started at 2023-05-12 13:20:55 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container speaker ready: true, restart count 0
  May 12 13:32:42.313: INFO: pod-exec-websocket-bfae4230-312a-4f75-963f-b57d59dde55e from pods-9201 started at 2023-05-12 13:32:36 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container main ready: true, restart count 0
  May 12 13:32:42.313: INFO: sonobuoy from sonobuoy started at 2023-05-12 12:31:54 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 12 13:32:42.313: INFO: sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-8q9cx from sonobuoy started at 2023-05-12 12:31:55 +0000 UTC (2 container statuses recorded)
  May 12 13:32:42.313: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 12 13:32:42.313: INFO: 	Container systemd-logs ready: true, restart count 0
  May 12 13:32:42.313: INFO: one-traefik-66db8f599c-zbk58 from traefik-system started at 2023-05-12 13:21:02 +0000 UTC (1 container statuses recorded)
  May 12 13:32:42.314: INFO: 	Container one-traefik ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 05/12/23 13:32:42.314
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.175e68a38cc160c1], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: true}, 1 node(s) had untolerated taint {node.longhorn.io/create-default-disk: true}, 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/4 nodes are available: 4 Preemption is not helpful for scheduling..] @ 05/12/23 13:32:42.337
  E0512 13:32:42.628100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:32:43.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5338" for this suite. @ 05/12/23 13:32:43.341
• [1.083 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 05/12/23 13:32:43.347
  May 12 13:32:43.347: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 05/12/23 13:32:43.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:32:43.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:32:43.362
  STEP: creating a target pod @ 05/12/23 13:32:43.363
  E0512 13:32:43.628561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:44.629498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 05/12/23 13:32:45.382
  E0512 13:32:45.630458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:46.631457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 05/12/23 13:32:47.414
  May 12 13:32:47.415: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-5276 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:32:47.415: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:32:47.416: INFO: ExecWithOptions: Clientset creation
  May 12 13:32:47.416: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/ephemeral-containers-test-5276/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  May 12 13:32:47.488: INFO: Exec stderr: ""
  May 12 13:32:47.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-5276" for this suite. @ 05/12/23 13:32:47.494
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 05/12/23 13:32:47.503
  May 12 13:32:47.503: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/23 13:32:47.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:32:47.514
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:32:47.516
  May 12 13:32:47.520: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5643" for this suite. @ 05/12/23 13:32:47.523
• [0.022 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 05/12/23 13:32:47.53
  May 12 13:32:47.530: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 13:32:47.531
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:32:47.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:32:47.546
  STEP: Creating secret with name secret-test-001e080f-2bf0-4407-a40d-70e2e8c2c70a @ 05/12/23 13:32:47.548
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:32:47.551
  E0512 13:32:47.632207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:48.632284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:49.633047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:50.635292      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:32:51.58
  May 12 13:32:51.587: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-secrets-64f57c5b-5e3a-4698-89a3-5cac7966ab26 container secret-env-test: <nil>
  STEP: delete the pod @ 05/12/23 13:32:51.607
  May 12 13:32:51.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8009" for this suite. @ 05/12/23 13:32:51.631
  E0512 13:32:51.634430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.107 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 05/12/23 13:32:51.644
  May 12 13:32:51.644: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 13:32:51.645
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:32:51.657
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:32:51.658
  STEP: Creating secret with name secret-test-map-14d63014-798f-4a08-8984-863c6c9337d2 @ 05/12/23 13:32:51.66
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:32:51.665
  E0512 13:32:52.634480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:53.634712      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:54.635543      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:55.636836      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:32:55.694
  May 12 13:32:55.703: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-secrets-56ab7640-ab3f-435e-9bb0-9b2a11533f31 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:32:55.72
  May 12 13:32:55.736: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4244" for this suite. @ 05/12/23 13:32:55.741
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 05/12/23 13:32:55.75
  May 12 13:32:55.750: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename subpath @ 05/12/23 13:32:55.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:32:55.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:32:55.764
  STEP: Setting up data @ 05/12/23 13:32:55.766
  STEP: Creating pod pod-subpath-test-secret-d2cq @ 05/12/23 13:32:55.773
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/23 13:32:55.773
  E0512 13:32:56.637494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:57.637839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:58.638097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:32:59.638545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:00.638761      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:01.638692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:02.638920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:03.639688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:04.640601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:05.640766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:06.641011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:07.641370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:08.641695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:09.641813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:10.641854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:11.643205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:12.643462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:13.644839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:14.644730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:15.645385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:16.645950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:17.646433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:18.646361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:19.646990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:33:19.884
  May 12 13:33:19.894: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-subpath-test-secret-d2cq container test-container-subpath-secret-d2cq: <nil>
  STEP: delete the pod @ 05/12/23 13:33:19.909
  STEP: Deleting pod pod-subpath-test-secret-d2cq @ 05/12/23 13:33:19.926
  May 12 13:33:19.926: INFO: Deleting pod "pod-subpath-test-secret-d2cq" in namespace "subpath-2500"
  May 12 13:33:19.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-2500" for this suite. @ 05/12/23 13:33:19.931
• [24.186 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 05/12/23 13:33:19.936
  May 12 13:33:19.936: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename gc @ 05/12/23 13:33:19.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:33:19.945
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:33:19.946
  STEP: create the rc1 @ 05/12/23 13:33:20.008
  STEP: create the rc2 @ 05/12/23 13:33:20.017
  E0512 13:33:20.647336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:21.649161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:22.649265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:23.649289      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:24.649365      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:25.649463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 05/12/23 13:33:26.027
  STEP: delete the rc simpletest-rc-to-be-deleted @ 05/12/23 13:33:26.434
  STEP: wait for the rc to be deleted @ 05/12/23 13:33:26.46
  E0512 13:33:26.649856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:27.649991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:28.650172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:29.650340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:30.650383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:33:31.488: INFO: 69 pods remaining
  May 12 13:33:31.488: INFO: 69 pods has nil DeletionTimestamp
  May 12 13:33:31.488: INFO: 
  E0512 13:33:31.650736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:32.650984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:33.651230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:34.651367      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:35.651477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/23 13:33:36.478
  May 12 13:33:36.543: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 12 13:33:36.548: INFO: Deleting pod "simpletest-rc-to-be-deleted-27st5" in namespace "gc-3648"
  May 12 13:33:36.576: INFO: Deleting pod "simpletest-rc-to-be-deleted-2g58w" in namespace "gc-3648"
  May 12 13:33:36.591: INFO: Deleting pod "simpletest-rc-to-be-deleted-2m26r" in namespace "gc-3648"
  May 12 13:33:36.607: INFO: Deleting pod "simpletest-rc-to-be-deleted-44r6s" in namespace "gc-3648"
  May 12 13:33:36.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h898" in namespace "gc-3648"
  May 12 13:33:36.635: INFO: Deleting pod "simpletest-rc-to-be-deleted-4j7fb" in namespace "gc-3648"
  May 12 13:33:36.649: INFO: Deleting pod "simpletest-rc-to-be-deleted-4jv2x" in namespace "gc-3648"
  E0512 13:33:36.651657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:33:36.661: INFO: Deleting pod "simpletest-rc-to-be-deleted-4kgdn" in namespace "gc-3648"
  May 12 13:33:36.672: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wvgx" in namespace "gc-3648"
  May 12 13:33:36.683: INFO: Deleting pod "simpletest-rc-to-be-deleted-4wvjm" in namespace "gc-3648"
  May 12 13:33:36.692: INFO: Deleting pod "simpletest-rc-to-be-deleted-5gh8f" in namespace "gc-3648"
  May 12 13:33:36.701: INFO: Deleting pod "simpletest-rc-to-be-deleted-5jzdk" in namespace "gc-3648"
  May 12 13:33:36.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-5m5fr" in namespace "gc-3648"
  May 12 13:33:36.730: INFO: Deleting pod "simpletest-rc-to-be-deleted-5xmc8" in namespace "gc-3648"
  May 12 13:33:36.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-6b256" in namespace "gc-3648"
  May 12 13:33:36.751: INFO: Deleting pod "simpletest-rc-to-be-deleted-6pb2c" in namespace "gc-3648"
  May 12 13:33:36.767: INFO: Deleting pod "simpletest-rc-to-be-deleted-6wq7f" in namespace "gc-3648"
  May 12 13:33:36.778: INFO: Deleting pod "simpletest-rc-to-be-deleted-74k49" in namespace "gc-3648"
  May 12 13:33:36.789: INFO: Deleting pod "simpletest-rc-to-be-deleted-77p5b" in namespace "gc-3648"
  May 12 13:33:36.799: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kffm" in namespace "gc-3648"
  May 12 13:33:36.808: INFO: Deleting pod "simpletest-rc-to-be-deleted-7q2j2" in namespace "gc-3648"
  May 12 13:33:36.819: INFO: Deleting pod "simpletest-rc-to-be-deleted-7wsr5" in namespace "gc-3648"
  May 12 13:33:36.833: INFO: Deleting pod "simpletest-rc-to-be-deleted-8cf9g" in namespace "gc-3648"
  May 12 13:33:36.843: INFO: Deleting pod "simpletest-rc-to-be-deleted-946hh" in namespace "gc-3648"
  May 12 13:33:36.853: INFO: Deleting pod "simpletest-rc-to-be-deleted-94jsz" in namespace "gc-3648"
  May 12 13:33:36.864: INFO: Deleting pod "simpletest-rc-to-be-deleted-96b5z" in namespace "gc-3648"
  May 12 13:33:36.876: INFO: Deleting pod "simpletest-rc-to-be-deleted-96kxn" in namespace "gc-3648"
  May 12 13:33:36.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-97svz" in namespace "gc-3648"
  May 12 13:33:36.898: INFO: Deleting pod "simpletest-rc-to-be-deleted-9cjph" in namespace "gc-3648"
  May 12 13:33:36.907: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7449" in namespace "gc-3648"
  May 12 13:33:36.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-ccmr7" in namespace "gc-3648"
  May 12 13:33:36.938: INFO: Deleting pod "simpletest-rc-to-be-deleted-chfvn" in namespace "gc-3648"
  May 12 13:33:36.949: INFO: Deleting pod "simpletest-rc-to-be-deleted-d2fmx" in namespace "gc-3648"
  May 12 13:33:36.960: INFO: Deleting pod "simpletest-rc-to-be-deleted-dlpdw" in namespace "gc-3648"
  May 12 13:33:36.972: INFO: Deleting pod "simpletest-rc-to-be-deleted-dmbpb" in namespace "gc-3648"
  May 12 13:33:36.981: INFO: Deleting pod "simpletest-rc-to-be-deleted-ds9tx" in namespace "gc-3648"
  May 12 13:33:36.993: INFO: Deleting pod "simpletest-rc-to-be-deleted-fh7vb" in namespace "gc-3648"
  May 12 13:33:37.003: INFO: Deleting pod "simpletest-rc-to-be-deleted-fld2x" in namespace "gc-3648"
  May 12 13:33:37.015: INFO: Deleting pod "simpletest-rc-to-be-deleted-fv8wz" in namespace "gc-3648"
  May 12 13:33:37.025: INFO: Deleting pod "simpletest-rc-to-be-deleted-fwtn5" in namespace "gc-3648"
  May 12 13:33:37.036: INFO: Deleting pod "simpletest-rc-to-be-deleted-g72p5" in namespace "gc-3648"
  May 12 13:33:37.045: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfgb9" in namespace "gc-3648"
  May 12 13:33:37.057: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcjwt" in namespace "gc-3648"
  May 12 13:33:37.067: INFO: Deleting pod "simpletest-rc-to-be-deleted-k67xb" in namespace "gc-3648"
  May 12 13:33:37.078: INFO: Deleting pod "simpletest-rc-to-be-deleted-khp9m" in namespace "gc-3648"
  May 12 13:33:37.088: INFO: Deleting pod "simpletest-rc-to-be-deleted-l48d5" in namespace "gc-3648"
  May 12 13:33:37.104: INFO: Deleting pod "simpletest-rc-to-be-deleted-lhmlh" in namespace "gc-3648"
  May 12 13:33:37.125: INFO: Deleting pod "simpletest-rc-to-be-deleted-lv9ww" in namespace "gc-3648"
  May 12 13:33:37.141: INFO: Deleting pod "simpletest-rc-to-be-deleted-mmjqb" in namespace "gc-3648"
  May 12 13:33:37.153: INFO: Deleting pod "simpletest-rc-to-be-deleted-mshdz" in namespace "gc-3648"
  May 12 13:33:37.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3648" for this suite. @ 05/12/23 13:33:37.173
• [17.243 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 05/12/23 13:33:37.181
  May 12 13:33:37.181: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/23 13:33:37.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:33:37.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:33:37.219
  May 12 13:33:37.239: INFO: Waiting up to 1m0s for all nodes to be ready
  E0512 13:33:37.653373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:38.653812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:39.654558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:40.654658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:41.655528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:42.656310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:43.656419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:44.656670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:45.657551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:46.657685      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:47.658513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:48.658753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:49.659466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:50.660434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:51.661377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:52.661610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:53.662197      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:54.662603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:55.663446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:56.663799      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:57.664418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:58.665449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:33:59.665696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:00.666410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:01.666464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:02.666594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:03.667181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:04.667377      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:05.668360      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:06.669539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:07.670826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:08.671263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:09.671431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:10.672502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:11.672683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:12.674777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:13.675363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:14.676067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:15.676563      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:16.677505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:17.678474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:18.678862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:19.679383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:20.680411      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:21.681165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:22.681383      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:23.681370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:24.681766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:25.682358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:26.682827      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:27.683992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:28.685100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:29.685740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:30.686357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:31.687631      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:32.688267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:33.689076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:34.689670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:35.689687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:36.691067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:34:37.291: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/12/23 13:34:37.314
  May 12 13:34:37.314: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/12/23 13:34:37.315
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:34:37.333
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:34:37.336
  STEP: Finding an available node @ 05/12/23 13:34:37.337
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/12/23 13:34:37.337
  E0512 13:34:37.692188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:38.692544      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/12/23 13:34:39.354
  May 12 13:34:39.363: INFO: found a healthy node: onekube-ip-172-16-100-7
  E0512 13:34:39.693339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:40.693204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:41.693318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:42.693720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:43.694256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:44.694695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:34:45.411: INFO: pods created so far: [1 1 1]
  May 12 13:34:45.411: INFO: length of pods created so far: 3
  E0512 13:34:45.695105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:46.695710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:34:47.420: INFO: pods created so far: [2 2 1]
  E0512 13:34:47.696506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:48.696653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:49.697520      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:50.697335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:51.697720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:52.698459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:53.699153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:34:54.421: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:34:54.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-6138" for this suite. @ 05/12/23 13:34:54.487
  STEP: Destroying namespace "sched-preemption-8665" for this suite. @ 05/12/23 13:34:54.491
• [77.314 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 05/12/23 13:34:54.502
  May 12 13:34:54.502: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:34:54.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:34:54.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:34:54.515
  STEP: Creating configMap with name configmap-test-volume-fafa673f-baec-46c7-93b6-137a6765c9b6 @ 05/12/23 13:34:54.516
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:34:54.519
  E0512 13:34:54.699310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:55.699561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:56.700545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:57.701193      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:34:58.549
  May 12 13:34:58.557: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-1d6fe026-7c81-4ba7-8e88-047365d99079 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:34:58.59
  May 12 13:34:58.607: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8454" for this suite. @ 05/12/23 13:34:58.61
• [4.113 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 05/12/23 13:34:58.616
  May 12 13:34:58.616: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename job @ 05/12/23 13:34:58.617
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:34:58.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:34:58.633
  STEP: Creating a job @ 05/12/23 13:34:58.635
  STEP: Ensuring job reaches completions @ 05/12/23 13:34:58.642
  E0512 13:34:58.701841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:34:59.701994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:00.703241      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:01.703746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:02.705281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:03.705199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:04.705928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:05.706385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:06.706839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:07.707291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:35:08.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9074" for this suite. @ 05/12/23 13:35:08.664
• [10.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 05/12/23 13:35:08.688
  May 12 13:35:08.688: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/23 13:35:08.689
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:35:08.707
  E0512 13:35:08.707366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:35:08.709
  E0512 13:35:09.707769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:10.708880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:35:10.722: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5779" for this suite. @ 05/12/23 13:35:10.727
• [2.045 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 05/12/23 13:35:10.734
  May 12 13:35:10.734: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename runtimeclass @ 05/12/23 13:35:10.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:35:10.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:35:10.748
  STEP: getting /apis @ 05/12/23 13:35:10.75
  STEP: getting /apis/node.k8s.io @ 05/12/23 13:35:10.755
  STEP: getting /apis/node.k8s.io/v1 @ 05/12/23 13:35:10.756
  STEP: creating @ 05/12/23 13:35:10.757
  STEP: watching @ 05/12/23 13:35:10.766
  May 12 13:35:10.766: INFO: starting watch
  STEP: getting @ 05/12/23 13:35:10.771
  STEP: listing @ 05/12/23 13:35:10.772
  STEP: patching @ 05/12/23 13:35:10.774
  STEP: updating @ 05/12/23 13:35:10.777
  May 12 13:35:10.780: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 05/12/23 13:35:10.78
  STEP: deleting a collection @ 05/12/23 13:35:10.787
  May 12 13:35:10.793: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8862" for this suite. @ 05/12/23 13:35:10.796
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 05/12/23 13:35:10.806
  May 12 13:35:10.806: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:35:10.807
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:35:10.821
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:35:10.824
  STEP: creating service in namespace services-691 @ 05/12/23 13:35:10.825
  STEP: creating service affinity-nodeport in namespace services-691 @ 05/12/23 13:35:10.825
  STEP: creating replication controller affinity-nodeport in namespace services-691 @ 05/12/23 13:35:10.839
  I0512 13:35:10.849220      20 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-691, replica count: 3
  E0512 13:35:11.710014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:12.710047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:13.719005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0512 13:35:13.901020      20 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 13:35:13.911: INFO: Creating new exec pod
  E0512 13:35:14.714849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:15.716138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:16.716082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:35:16.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-691 exec execpod-affinitymd2qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  May 12 13:35:17.158: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  May 12 13:35:17.158: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:35:17.158: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-691 exec execpod-affinitymd2qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.255.220 80'
  May 12 13:35:17.333: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.43.255.220 80\nConnection to 10.43.255.220 80 port [tcp/http] succeeded!\n"
  May 12 13:35:17.333: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:35:17.333: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-691 exec execpod-affinitymd2qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.5 31653'
  May 12 13:35:17.461: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.100.5 31653\nConnection to 172.16.100.5 31653 port [tcp/*] succeeded!\n"
  May 12 13:35:17.461: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:35:17.461: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-691 exec execpod-affinitymd2qz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.7 31653'
  May 12 13:35:17.571: INFO: stderr: "+ + nc -v -t -w 2 172.16.100.7 31653\necho hostName\nConnection to 172.16.100.7 31653 port [tcp/*] succeeded!\n"
  May 12 13:35:17.571: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:35:17.571: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-691 exec execpod-affinitymd2qz -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://172.16.100.5:31653/ ; done'
  E0512 13:35:17.716083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:35:17.743: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n+ echo\n+ curl -q -s --connect-timeout 2 http://172.16.100.5:31653/\n"
  May 12 13:35:17.743: INFO: stdout: "\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h\naffinity-nodeport-bn54h"
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.743: INFO: Received response from host: affinity-nodeport-bn54h
  May 12 13:35:17.744: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:35:17.746: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-691, will wait for the garbage collector to delete the pods @ 05/12/23 13:35:17.757
  May 12 13:35:17.815: INFO: Deleting ReplicationController affinity-nodeport took: 4.110311ms
  May 12 13:35:17.915: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.81697ms
  E0512 13:35:18.716847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:19.716814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-691" for this suite. @ 05/12/23 13:35:20.336
• [9.535 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 05/12/23 13:35:20.342
  May 12 13:35:20.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename podtemplate @ 05/12/23 13:35:20.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:35:20.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:35:20.356
  STEP: Create set of pod templates @ 05/12/23 13:35:20.358
  May 12 13:35:20.361: INFO: created test-podtemplate-1
  May 12 13:35:20.364: INFO: created test-podtemplate-2
  May 12 13:35:20.367: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 05/12/23 13:35:20.367
  STEP: delete collection of pod templates @ 05/12/23 13:35:20.368
  May 12 13:35:20.368: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 05/12/23 13:35:20.374
  May 12 13:35:20.374: INFO: requesting list of pod templates to confirm quantity
  May 12 13:35:20.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6424" for this suite. @ 05/12/23 13:35:20.378
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 05/12/23 13:35:20.382
  May 12 13:35:20.382: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-preemption @ 05/12/23 13:35:20.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:35:20.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:35:20.396
  May 12 13:35:20.405: INFO: Waiting up to 1m0s for all nodes to be ready
  E0512 13:35:20.717690      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:21.718268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:22.718602      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:23.718618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:24.719374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:25.721568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:26.722600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:27.730164      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:28.731069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:29.731240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:30.731352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:31.732627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:32.732925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:33.733062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:34.733301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:35.734595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:36.735441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:37.735697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:38.736961      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:39.737153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:40.737152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:41.738701      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:42.740098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:43.740443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:44.740521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:45.740727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:46.741069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:47.741892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:48.742697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:49.742806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:50.743743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:51.744399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:52.744806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:53.744948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:54.745511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:55.746062      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:56.747355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:57.747104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:58.747445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:35:59.747413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:00.749714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:01.749484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:02.750307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:03.750572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:04.753161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:05.752761      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:06.753686      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:07.754849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:08.755595      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:09.756008      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:10.756104      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:11.757051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:12.758203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:13.758754      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:14.759744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:15.760334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:16.760841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:17.760807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:18.761433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:19.762116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:36:20.464: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 05/12/23 13:36:20.466
  May 12 13:36:20.466: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-preemption-path @ 05/12/23 13:36:20.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:36:20.477
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:36:20.48
  May 12 13:36:20.489: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  May 12 13:36:20.491: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  May 12 13:36:20.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:36:20.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-625" for this suite. @ 05/12/23 13:36:20.531
  STEP: Destroying namespace "sched-preemption-4576" for this suite. @ 05/12/23 13:36:20.536
• [60.159 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 05/12/23 13:36:20.542
  May 12 13:36:20.542: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 13:36:20.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:36:20.558
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:36:20.56
  STEP: Creating secret with name s-test-opt-del-ec1ef8fa-f22f-461e-970d-681b39ef4a6f @ 05/12/23 13:36:20.564
  STEP: Creating secret with name s-test-opt-upd-b1276f9a-4ecd-4e46-9d28-4ae1361450fa @ 05/12/23 13:36:20.568
  STEP: Creating the pod @ 05/12/23 13:36:20.572
  E0512 13:36:20.762386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:21.763438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-ec1ef8fa-f22f-461e-970d-681b39ef4a6f @ 05/12/23 13:36:22.63
  STEP: Updating secret s-test-opt-upd-b1276f9a-4ecd-4e46-9d28-4ae1361450fa @ 05/12/23 13:36:22.64
  STEP: Creating secret with name s-test-opt-create-16ed6be8-2d1a-4bdd-81a2-916dae4ce55b @ 05/12/23 13:36:22.647
  STEP: waiting to observe update in volume @ 05/12/23 13:36:22.652
  E0512 13:36:22.763815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:23.764151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:24.764666      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:25.767176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:36:26.682: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3334" for this suite. @ 05/12/23 13:36:26.684
• [6.147 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 05/12/23 13:36:26.691
  May 12 13:36:26.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 13:36:26.692
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:36:26.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:36:26.707
  STEP: Creating projection with secret that has name secret-emptykey-test-2eb80a09-763a-4435-ab4d-8e0381652b10 @ 05/12/23 13:36:26.709
  May 12 13:36:26.710: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-393" for this suite. @ 05/12/23 13:36:26.712
• [0.025 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 05/12/23 13:36:26.718
  May 12 13:36:26.718: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename watch @ 05/12/23 13:36:26.719
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:36:26.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:36:26.734
  STEP: creating a new configmap @ 05/12/23 13:36:26.735
  STEP: modifying the configmap once @ 05/12/23 13:36:26.739
  STEP: modifying the configmap a second time @ 05/12/23 13:36:26.744
  STEP: deleting the configmap @ 05/12/23 13:36:26.749
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 05/12/23 13:36:26.753
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 05/12/23 13:36:26.754
  May 12 13:36:26.755: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9773  5d03c6dc-845c-49a1-a21f-f579446de568 104724 0 2023-05-12 13:36:26 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-12 13:36:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:36:26.755: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-9773  5d03c6dc-845c-49a1-a21f-f579446de568 104725 0 2023-05-12 13:36:26 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-05-12 13:36:26 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:36:26.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9773" for this suite. @ 05/12/23 13:36:26.758
• [0.043 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 05/12/23 13:36:26.764
  May 12 13:36:26.764: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename cronjob @ 05/12/23 13:36:26.765
  E0512 13:36:26.768216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:36:26.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:36:26.78
  STEP: Creating a ReplaceConcurrent cronjob @ 05/12/23 13:36:26.782
  STEP: Ensuring a job is scheduled @ 05/12/23 13:36:26.786
  E0512 13:36:27.768054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:28.768987      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:29.769283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:30.769443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:31.773840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:32.774873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:33.775739      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:34.776311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:35.776303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:36.776605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:37.777215      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:38.777866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:39.778047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:40.778871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:41.778922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:42.779277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:43.785041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:44.785001      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:45.787649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:46.788581      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:47.788057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:48.788881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:49.789570      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:50.790795      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:51.790039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:52.791238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:53.791689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:54.792960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:55.792966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:56.793579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:57.793532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:58.793742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:36:59.793883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 05/12/23 13:37:00.789
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 05/12/23 13:37:00.79
  STEP: Ensuring the job is replaced with a new one @ 05/12/23 13:37:00.791
  E0512 13:37:00.794254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:01.795582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:02.802995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:03.803849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:04.804457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:05.804323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:06.804501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:07.805964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:08.805807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:09.806133      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:10.806237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:11.806903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:12.807825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:13.808061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:14.809126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:15.809933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:16.810126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:17.810270      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:18.811019      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:19.815511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:20.816516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:21.816812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:22.817801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:23.818299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:24.818404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:25.818431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:26.819299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:27.820095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:28.820714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:29.821054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:30.821881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:31.822326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:32.823242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:33.823536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:34.823768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:35.824437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:36.825401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:37.825602      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:38.825794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:39.826338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:40.826512      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:41.827648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:42.828606      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:43.828597      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:44.829421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:45.829664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:46.830916      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:47.831415      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:48.831659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:49.831932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:50.832751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:51.834297      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:52.834676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:53.835474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:54.837089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:55.838199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:56.840364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:57.840170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:58.841269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:37:59.841352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 05/12/23 13:38:00.796
  May 12 13:38:00.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-7309" for this suite. @ 05/12/23 13:38:00.825
• [94.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS  E0512 13:38:00.841188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 05/12/23 13:38:00.847
  May 12 13:38:00.847: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:38:00.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:00.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:00.876
  STEP: Creating configMap with name projected-configmap-test-volume-map-8777158e-4081-450c-8e7d-0fa7828e3894 @ 05/12/23 13:38:00.878
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:38:00.882
  E0512 13:38:01.847147      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:02.848141      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:03.848323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:04.848413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:38:04.911
  May 12 13:38:04.913: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-configmaps-43957774-8e00-4b5c-86bb-8ec6ac1f3b1f container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:38:04.924
  May 12 13:38:04.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4302" for this suite. @ 05/12/23 13:38:04.941
• [4.098 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 05/12/23 13:38:04.948
  May 12 13:38:04.948: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename init-container @ 05/12/23 13:38:04.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:04.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:04.963
  STEP: creating the pod @ 05/12/23 13:38:04.965
  May 12 13:38:04.965: INFO: PodSpec: initContainers in spec.initContainers
  E0512 13:38:05.849108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:06.849923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:07.850947      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:08.851096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:09.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6813" for this suite. @ 05/12/23 13:38:09.13
• [4.186 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 05/12/23 13:38:09.136
  May 12 13:38:09.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:38:09.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:09.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:09.15
  STEP: create deployment with httpd image @ 05/12/23 13:38:09.151
  May 12 13:38:09.151: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8927 create -f -'
  E0512 13:38:09.851779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:10.066: INFO: stderr: ""
  May 12 13:38:10.066: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 05/12/23 13:38:10.066
  May 12 13:38:10.066: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8927 diff -f -'
  May 12 13:38:10.414: INFO: rc: 1
  May 12 13:38:10.414: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8927 delete -f -'
  May 12 13:38:10.470: INFO: stderr: ""
  May 12 13:38:10.470: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  May 12 13:38:10.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8927" for this suite. @ 05/12/23 13:38:10.475
• [1.345 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 05/12/23 13:38:10.483
  May 12 13:38:10.483: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename daemonsets @ 05/12/23 13:38:10.483
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:10.496
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:10.5
  STEP: Creating simple DaemonSet "daemon-set" @ 05/12/23 13:38:10.514
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/23 13:38:10.519
  May 12 13:38:10.524: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:38:10.524: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:38:10.527: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:38:10.527: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:38:10.851918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:11.554: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:38:11.555: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:38:11.559: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:38:11.559: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:38:11.852862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:12.531: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:38:12.531: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:38:12.534: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 13:38:12.534: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 05/12/23 13:38:12.536
  STEP: DeleteCollection of the DaemonSets @ 05/12/23 13:38:12.54
  STEP: Verify that ReplicaSets have been deleted @ 05/12/23 13:38:12.547
  May 12 13:38:12.559: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"105488"},"items":null}

  May 12 13:38:12.569: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"105489"},"items":[{"metadata":{"name":"daemon-set-m88qx","generateName":"daemon-set-","namespace":"daemonsets-8439","uid":"2a1a7ba6-9d67-40c6-bab4-a68cb6bc8d30","resourceVersion":"105477","creationTimestamp":"2023-05-12T13:38:10Z","labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"42f9678e520fe40fd7680c58ecee8112e312225c8796a5a3b9327273aef4d9ee","cni.projectcalico.org/podIP":"10.42.2.214/32","cni.projectcalico.org/podIPs":"10.42.2.214/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0632c322-91b3-4e7c-9b7b-8448bfc85168","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-12T13:38:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-12T13:38:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0632c322-91b3-4e7c-9b7b-8448bfc85168\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-12T13:38:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.214\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-stw8g","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-stw8g","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"onekube-ip-172-16-100-5","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["onekube-ip-172-16-100-5"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-12T13:38:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-12T13:38:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-12T13:38:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-12T13:38:10Z"}],"hostIP":"172.16.100.5","podIP":"10.42.2.214","podIPs":[{"ip":"10.42.2.214"}],"startTime":"2023-05-12T13:38:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-12T13:38:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://e45f0da354b42727f09b5f652f006b7fef1350bd6e85ec2b58048814c5fb5397","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-xjbf5","generateName":"daemon-set-","namespace":"daemonsets-8439","uid":"2bd1bf32-7379-4cf0-8153-8ea1b2a66f69","resourceVersion":"105489","creationTimestamp":"2023-05-12T13:38:10Z","deletionTimestamp":"2023-05-12T13:38:42Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"annotations":{"cni.projectcalico.org/containerID":"47b146c8c87967ef593cc198db1b7a1c31243ecee4fb5439032cc1399021267e","cni.projectcalico.org/podIP":"10.42.3.180/32","cni.projectcalico.org/podIPs":"10.42.3.180/32"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"0632c322-91b3-4e7c-9b7b-8448bfc85168","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"calico","operation":"Update","apiVersion":"v1","time":"2023-05-12T13:38:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}},"subresource":"status"},{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-05-12T13:38:10Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"0632c322-91b3-4e7c-9b7b-8448bfc85168\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-05-12T13:38:11Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.180\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-s756p","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-s756p","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"onekube-ip-172-16-100-7","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["onekube-ip-172-16-100-7"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-12T13:38:10Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-12T13:38:11Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-12T13:38:11Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-05-12T13:38:10Z"}],"hostIP":"172.16.100.7","podIP":"10.42.3.180","podIPs":[{"ip":"10.42.3.180"}],"startTime":"2023-05-12T13:38:10Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-05-12T13:38:11Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"containerd://92fc54d36c0cb3063eeed9e5809a2d005929bee89a69d42fbdb11a717fd44ee0","started":true}],"qosClass":"BestEffort"}}]}

  May 12 13:38:12.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8439" for this suite. @ 05/12/23 13:38:12.585
• [2.106 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 05/12/23 13:38:12.59
  May 12 13:38:12.590: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:38:12.591
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:12.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:12.604
  STEP: Creating secret with name projected-secret-test-ed54fa2e-4b43-4021-a070-4e9c2ed7a7e6 @ 05/12/23 13:38:12.606
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:38:12.609
  E0512 13:38:12.853765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:13.854794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:14.855544      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:15.855519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:16.856596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:17.856822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:38:18.66
  May 12 13:38:18.664: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-secrets-72424ec0-1c91-4a4d-a8e6-7c32fe54389b container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:38:18.671
  May 12 13:38:18.694: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3167" for this suite. @ 05/12/23 13:38:18.697
• [6.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 05/12/23 13:38:18.705
  May 12 13:38:18.706: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename deployment @ 05/12/23 13:38:18.707
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:18.727
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:18.729
  STEP: creating a Deployment @ 05/12/23 13:38:18.733
  STEP: waiting for Deployment to be created @ 05/12/23 13:38:18.736
  STEP: waiting for all Replicas to be Ready @ 05/12/23 13:38:18.737
  May 12 13:38:18.739: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 12 13:38:18.739: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 12 13:38:18.750: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 12 13:38:18.750: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 12 13:38:18.765: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 12 13:38:18.765: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 12 13:38:18.789: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  May 12 13:38:18.789: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0512 13:38:18.857740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:19.858510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:19.971: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  May 12 13:38:19.971: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  E0512 13:38:20.859227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:21.859857      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:22.020: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 05/12/23 13:38:22.02
  W0512 13:38:22.027120      20 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  May 12 13:38:22.030: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 05/12/23 13:38:22.034
  May 12 13:38:22.047: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0
  May 12 13:38:22.047: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0
  May 12 13:38:22.047: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0
  May 12 13:38:22.047: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 0
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:22.070: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:22.188: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:22.212: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:22.274: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:22.274: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  E0512 13:38:22.860782      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:23.026: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:23.031: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:23.056: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  STEP: listing Deployments @ 05/12/23 13:38:23.056
  May 12 13:38:23.059: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 05/12/23 13:38:23.059
  May 12 13:38:23.071: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 05/12/23 13:38:23.071
  May 12 13:38:23.089: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 12 13:38:23.090: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 12 13:38:23.100: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 12 13:38:23.115: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 12 13:38:23.121: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  May 12 13:38:23.126: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0512 13:38:23.861655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:23.968: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  May 12 13:38:24.020: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  May 12 13:38:24.059: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0512 13:38:24.862889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:25.002: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 05/12/23 13:38:25.028
  STEP: fetching the DeploymentStatus @ 05/12/23 13:38:25.033
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 1
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 3
  May 12 13:38:25.036: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 2
  May 12 13:38:25.037: INFO: observed Deployment test-deployment in namespace deployment-4821 with ReadyReplicas 3
  STEP: deleting the Deployment @ 05/12/23 13:38:25.037
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.044: INFO: observed event type MODIFIED
  May 12 13:38:25.049: INFO: Log out all the ReplicaSets if there is no deployment created
  May 12 13:38:25.052: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-4821  91b5cffe-3af4-4d80-8ac2-778ce76bf44a 105685 3 2023-05-12 13:38:18 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 9c4c2e1f-8bcf-4ee6-ba33-3c3bde76e1ce 0xc003db7217 0xc003db7218}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:38:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c4c2e1f-8bcf-4ee6-ba33-3c3bde76e1ce\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:38:23 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003db72a0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 12 13:38:25.054: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-4821  6e3b5c6d-d868-4147-ac48-f70d1869d1d1 105780 4 2023-05-12 13:38:22 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 9c4c2e1f-8bcf-4ee6-ba33-3c3bde76e1ce 0xc003db7307 0xc003db7308}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c4c2e1f-8bcf-4ee6-ba33-3c3bde76e1ce\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:38:25 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003db7390 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  May 12 13:38:25.056: INFO: pod: "test-deployment-5b5dcbcd95-r986x":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-r986x test-deployment-5b5dcbcd95- deployment-4821  38daeadb-8863-45bb-a268-a7c1c14186bc 105776 0 2023-05-12 13:38:23 +0000 UTC 2023-05-12 13:38:25 +0000 UTC 0xc003db7988 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:a78368e73d5f61ee95ad156c5e9fecc36b8cde13fa76325e30e507d5895dc3bc cni.projectcalico.org/podIP:10.42.2.216/32 cni.projectcalico.org/podIPs:10.42.2.216/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 6e3b5c6d-d868-4147-ac48-f70d1869d1d1 0xc003db79d7 0xc003db79d8}] [] [{calico Update v1 2023-05-12 13:38:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-12 13:38:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3b5c6d-d868-4147-ac48-f70d1869d1d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:38:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.216\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-slxvf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-slxvf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:10.42.2.216,StartTime:2023-05-12 13:38:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:38:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://01550825694471380cd14208008a5b04148e57e6955fa36d85a66badde46041e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.216,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 12 13:38:25.057: INFO: pod: "test-deployment-5b5dcbcd95-sckbp":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-sckbp test-deployment-5b5dcbcd95- deployment-4821  fdb0f745-7aed-485b-a785-b6ba3723a867 105742 0 2023-05-12 13:38:22 +0000 UTC 2023-05-12 13:38:25 +0000 UTC 0xc003db7bb0 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[cni.projectcalico.org/containerID:87c2bd86c301f378460c2669d59dd72ae3ae12bd33bfccb70569d126eb2fd8dd cni.projectcalico.org/podIP:10.42.3.183/32 cni.projectcalico.org/podIPs:10.42.3.183/32] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 6e3b5c6d-d868-4147-ac48-f70d1869d1d1 0xc003db7c07 0xc003db7c08}] [] [{calico Update v1 2023-05-12 13:38:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-12 13:38:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"6e3b5c6d-d868-4147-ac48-f70d1869d1d1\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:38:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.183\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z9drp,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z9drp,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.183,StartTime:2023-05-12 13:38:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:38:22 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:containerd://944f21c83da9c72f209e30fc2252c2585afd43c18a8e6993c37d4fd559ef852e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.183,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 12 13:38:25.057: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-4821  43c8d4d6-6928-4478-ae2f-a541a0f7b20e 105771 2 2023-05-12 13:38:23 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 9c4c2e1f-8bcf-4ee6-ba33-3c3bde76e1ce 0xc003db73f7 0xc003db73f8}] [] [{kube-controller-manager Update apps/v1 2023-05-12 13:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9c4c2e1f-8bcf-4ee6-ba33-3c3bde76e1ce\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-05-12 13:38:24 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003db7480 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  May 12 13:38:25.065: INFO: pod: "test-deployment-6fc78d85c6-26tvh":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-26tvh test-deployment-6fc78d85c6- deployment-4821  0812e29a-47fb-4fad-8168-35023a883607 105770 0 2023-05-12 13:38:24 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:9b6417b8b85ce83fc0747615928063b0463182b52ac58476f78db2169c39fb59 cni.projectcalico.org/podIP:10.42.2.217/32 cni.projectcalico.org/podIPs:10.42.2.217/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 43c8d4d6-6928-4478-ae2f-a541a0f7b20e 0xc005418897 0xc005418898}] [] [{calico Update v1 2023-05-12 13:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-12 13:38:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43c8d4d6-6928-4478-ae2f-a541a0f7b20e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:38:24 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.2.217\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mg5d6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mg5d6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-5,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:24 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.5,PodIP:10.42.2.217,StartTime:2023-05-12 13:38:24 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:38:24 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://84d459b9e2a7b3be89d4720e2c4371211e2f2678ce4cde253cb58e4485551292,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.2.217,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 12 13:38:25.065: INFO: pod: "test-deployment-6fc78d85c6-lwsgg":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-lwsgg test-deployment-6fc78d85c6- deployment-4821  211aeae2-61aa-4214-a850-0d275ac2f780 105735 0 2023-05-12 13:38:23 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[cni.projectcalico.org/containerID:f105165aa54d7e9ece812f0e3fea13d046025fca5bfd5d5ae80e196fcc5252b6 cni.projectcalico.org/podIP:10.42.3.184/32 cni.projectcalico.org/podIPs:10.42.3.184/32] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 43c8d4d6-6928-4478-ae2f-a541a0f7b20e 0xc005418ab7 0xc005418ab8}] [] [{calico Update v1 2023-05-12 13:38:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:cni.projectcalico.org/containerID":{},"f:cni.projectcalico.org/podIP":{},"f:cni.projectcalico.org/podIPs":{}}}} status} {kube-controller-manager Update v1 2023-05-12 13:38:23 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"43c8d4d6-6928-4478-ae2f-a541a0f7b20e\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-05-12 13:38:23 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.42.3.184\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-grphn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-grphn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:onekube-ip-172-16-100-7,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:23 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-05-12 13:38:23 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:172.16.100.7,PodIP:10.42.3.184,StartTime:2023-05-12 13:38:23 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-05-12 13:38:23 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:containerd://37d408f61bf0637bd24b64a652c24544b57a1e30af2e8eb5668c54a2c5d1aedf,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.42.3.184,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  May 12 13:38:25.066: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4821" for this suite. @ 05/12/23 13:38:25.079
• [6.393 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 05/12/23 13:38:25.099
  May 12 13:38:25.099: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/23 13:38:25.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:25.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:25.134
  May 12 13:38:25.151: INFO: created pod pod-service-account-defaultsa
  May 12 13:38:25.151: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  May 12 13:38:25.163: INFO: created pod pod-service-account-mountsa
  May 12 13:38:25.163: INFO: pod pod-service-account-mountsa service account token volume mount: true
  May 12 13:38:25.170: INFO: created pod pod-service-account-nomountsa
  May 12 13:38:25.170: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  May 12 13:38:25.176: INFO: created pod pod-service-account-defaultsa-mountspec
  May 12 13:38:25.176: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  May 12 13:38:25.183: INFO: created pod pod-service-account-mountsa-mountspec
  May 12 13:38:25.183: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  May 12 13:38:25.190: INFO: created pod pod-service-account-nomountsa-mountspec
  May 12 13:38:25.190: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  May 12 13:38:25.204: INFO: created pod pod-service-account-defaultsa-nomountspec
  May 12 13:38:25.210: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  May 12 13:38:25.217: INFO: created pod pod-service-account-mountsa-nomountspec
  May 12 13:38:25.217: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  May 12 13:38:25.225: INFO: created pod pod-service-account-nomountsa-nomountspec
  May 12 13:38:25.225: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  May 12 13:38:25.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-6817" for this suite. @ 05/12/23 13:38:25.234
• [0.143 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 05/12/23 13:38:25.247
  May 12 13:38:25.247: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:38:25.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:25.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:25.287
  STEP: creating a replication controller @ 05/12/23 13:38:25.289
  May 12 13:38:25.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 create -f -'
  May 12 13:38:25.675: INFO: stderr: ""
  May 12 13:38:25.675: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 05/12/23 13:38:25.675
  May 12 13:38:25.676: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 12 13:38:25.746: INFO: stderr: ""
  May 12 13:38:25.746: INFO: stdout: "update-demo-nautilus-2c7dc update-demo-nautilus-568jv "
  May 12 13:38:25.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get pods update-demo-nautilus-2c7dc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:38:25.799: INFO: stderr: ""
  May 12 13:38:25.799: INFO: stdout: ""
  May 12 13:38:25.799: INFO: update-demo-nautilus-2c7dc is created but not running
  E0512 13:38:25.863372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:26.864744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:27.865463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:28.865588      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:29.866018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:30.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  May 12 13:38:30.847: INFO: stderr: ""
  May 12 13:38:30.847: INFO: stdout: "update-demo-nautilus-2c7dc update-demo-nautilus-568jv "
  May 12 13:38:30.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get pods update-demo-nautilus-2c7dc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  E0512 13:38:30.866659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:30.910: INFO: stderr: ""
  May 12 13:38:30.910: INFO: stdout: "true"
  May 12 13:38:30.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get pods update-demo-nautilus-2c7dc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 12 13:38:30.974: INFO: stderr: ""
  May 12 13:38:30.974: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 12 13:38:30.974: INFO: validating pod update-demo-nautilus-2c7dc
  May 12 13:38:30.979: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 12 13:38:30.980: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 12 13:38:30.980: INFO: update-demo-nautilus-2c7dc is verified up and running
  May 12 13:38:30.980: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get pods update-demo-nautilus-568jv -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  May 12 13:38:31.027: INFO: stderr: ""
  May 12 13:38:31.027: INFO: stdout: "true"
  May 12 13:38:31.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get pods update-demo-nautilus-568jv -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  May 12 13:38:31.084: INFO: stderr: ""
  May 12 13:38:31.084: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  May 12 13:38:31.084: INFO: validating pod update-demo-nautilus-568jv
  May 12 13:38:31.090: INFO: got data: {
    "image": "nautilus.jpg"
  }

  May 12 13:38:31.090: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  May 12 13:38:31.090: INFO: update-demo-nautilus-568jv is verified up and running
  STEP: using delete to clean up resources @ 05/12/23 13:38:31.09
  May 12 13:38:31.090: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 delete --grace-period=0 --force -f -'
  May 12 13:38:31.183: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  May 12 13:38:31.183: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  May 12 13:38:31.183: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get rc,svc -l name=update-demo --no-headers'
  May 12 13:38:31.277: INFO: stderr: "No resources found in kubectl-8844 namespace.\n"
  May 12 13:38:31.277: INFO: stdout: ""
  May 12 13:38:31.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8844 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  May 12 13:38:31.335: INFO: stderr: ""
  May 12 13:38:31.335: INFO: stdout: ""
  May 12 13:38:31.335: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8844" for this suite. @ 05/12/23 13:38:31.339
• [6.100 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 05/12/23 13:38:31.348
  May 12 13:38:31.348: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:38:31.351
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:31.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:31.365
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-1291 @ 05/12/23 13:38:31.366
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 05/12/23 13:38:31.377
  STEP: creating service externalsvc in namespace services-1291 @ 05/12/23 13:38:31.377
  STEP: creating replication controller externalsvc in namespace services-1291 @ 05/12/23 13:38:31.387
  I0512 13:38:31.395544      20 runners.go:194] Created replication controller with name: externalsvc, namespace: services-1291, replica count: 2
  E0512 13:38:31.868957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:32.869295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:33.870016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0512 13:38:34.447581      20 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 05/12/23 13:38:34.453
  May 12 13:38:34.471: INFO: Creating new exec pod
  E0512 13:38:34.870633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:35.871373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:36.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-1291 exec execpod4lq22 -- /bin/sh -x -c nslookup clusterip-service.services-1291.svc.cluster.local'
  May 12 13:38:36.696: INFO: stderr: "+ nslookup clusterip-service.services-1291.svc.cluster.local\n"
  May 12 13:38:36.696: INFO: stdout: "Server:\t\t10.43.0.10\nAddress:\t10.43.0.10#53\n\nclusterip-service.services-1291.svc.cluster.local\tcanonical name = externalsvc.services-1291.svc.cluster.local.\nName:\texternalsvc.services-1291.svc.cluster.local\nAddress: 10.43.30.233\n\n"
  May 12 13:38:36.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-1291, will wait for the garbage collector to delete the pods @ 05/12/23 13:38:36.699
  May 12 13:38:36.754: INFO: Deleting ReplicationController externalsvc took: 3.376346ms
  May 12 13:38:36.854: INFO: Terminating ReplicationController externalsvc pods took: 100.27323ms
  E0512 13:38:36.872386      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:37.873249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:38.874059      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:39.076: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-1291" for this suite. @ 05/12/23 13:38:39.085
• [7.746 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 05/12/23 13:38:39.101
  May 12 13:38:39.101: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename disruption @ 05/12/23 13:38:39.101
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:39.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:39.121
  STEP: Creating a pdb that targets all three pods in a test replica set @ 05/12/23 13:38:39.123
  STEP: Waiting for the pdb to be processed @ 05/12/23 13:38:39.127
  STEP: First trying to evict a pod which shouldn't be evictable @ 05/12/23 13:38:39.138
  STEP: Waiting for all pods to be running @ 05/12/23 13:38:39.139
  May 12 13:38:39.140: INFO: pods: 0 < 3
  E0512 13:38:39.874208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:40.874388      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 05/12/23 13:38:41.143
  STEP: Updating the pdb to allow a pod to be evicted @ 05/12/23 13:38:41.148
  STEP: Waiting for the pdb to be processed @ 05/12/23 13:38:41.155
  E0512 13:38:41.875845      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:42.876303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/12/23 13:38:43.165
  STEP: Waiting for all pods to be running @ 05/12/23 13:38:43.165
  STEP: Waiting for the pdb to observed all healthy pods @ 05/12/23 13:38:43.176
  STEP: Patching the pdb to disallow a pod to be evicted @ 05/12/23 13:38:43.214
  STEP: Waiting for the pdb to be processed @ 05/12/23 13:38:43.246
  E0512 13:38:43.876425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:44.879952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 05/12/23 13:38:45.256
  STEP: locating a running pod @ 05/12/23 13:38:45.265
  STEP: Deleting the pdb to allow a pod to be evicted @ 05/12/23 13:38:45.287
  STEP: Waiting for the pdb to be deleted @ 05/12/23 13:38:45.293
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 05/12/23 13:38:45.298
  STEP: Waiting for all pods to be running @ 05/12/23 13:38:45.298
  May 12 13:38:45.342: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-6765" for this suite. @ 05/12/23 13:38:45.366
• [6.317 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 05/12/23 13:38:45.418
  May 12 13:38:45.418: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename job @ 05/12/23 13:38:45.454
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:45.485
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:45.487
  STEP: Creating Indexed job @ 05/12/23 13:38:45.49
  STEP: Ensuring job reaches completions @ 05/12/23 13:38:45.494
  E0512 13:38:45.878983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:46.880142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:47.880778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:48.880811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:49.880943      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:50.881161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:51.881307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:52.881831      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 05/12/23 13:38:53.503
  May 12 13:38:53.506: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-952" for this suite. @ 05/12/23 13:38:53.509
• [8.096 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 05/12/23 13:38:53.517
  May 12 13:38:53.517: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:38:53.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:53.531
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:53.532
  STEP: Creating configMap with name cm-test-opt-del-e476ef4d-35f9-4da7-aeee-e810134c38a4 @ 05/12/23 13:38:53.535
  STEP: Creating configMap with name cm-test-opt-upd-0984182a-58f8-4c7f-ab0d-bf262cb7870d @ 05/12/23 13:38:53.538
  STEP: Creating the pod @ 05/12/23 13:38:53.541
  E0512 13:38:53.881659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:54.883073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-e476ef4d-35f9-4da7-aeee-e810134c38a4 @ 05/12/23 13:38:55.582
  STEP: Updating configmap cm-test-opt-upd-0984182a-58f8-4c7f-ab0d-bf262cb7870d @ 05/12/23 13:38:55.59
  STEP: Creating configMap with name cm-test-opt-create-889413e9-386a-4909-b159-46cf39a5e9b2 @ 05/12/23 13:38:55.599
  STEP: waiting to observe update in volume @ 05/12/23 13:38:55.604
  E0512 13:38:55.882355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:56.894438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:57.883647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:38:58.883954      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:38:59.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4702" for this suite. @ 05/12/23 13:38:59.66
• [6.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 05/12/23 13:38:59.673
  May 12 13:38:59.673: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename events @ 05/12/23 13:38:59.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:59.694
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:59.696
  STEP: creating a test event @ 05/12/23 13:38:59.697
  STEP: listing events in all namespaces @ 05/12/23 13:38:59.701
  STEP: listing events in test namespace @ 05/12/23 13:38:59.706
  STEP: listing events with field selection filtering on source @ 05/12/23 13:38:59.707
  STEP: listing events with field selection filtering on reportingController @ 05/12/23 13:38:59.708
  STEP: getting the test event @ 05/12/23 13:38:59.709
  STEP: patching the test event @ 05/12/23 13:38:59.71
  STEP: getting the test event @ 05/12/23 13:38:59.713
  STEP: updating the test event @ 05/12/23 13:38:59.714
  STEP: getting the test event @ 05/12/23 13:38:59.717
  STEP: deleting the test event @ 05/12/23 13:38:59.718
  STEP: listing events in all namespaces @ 05/12/23 13:38:59.72
  STEP: listing events in test namespace @ 05/12/23 13:38:59.725
  May 12 13:38:59.726: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4696" for this suite. @ 05/12/23 13:38:59.728
• [0.058 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 05/12/23 13:38:59.736
  May 12 13:38:59.736: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-runtime @ 05/12/23 13:38:59.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:38:59.748
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:38:59.75
  STEP: create the container @ 05/12/23 13:38:59.751
  W0512 13:38:59.756697      20 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 05/12/23 13:38:59.757
  E0512 13:38:59.884974      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:00.885082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:01.886560      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 05/12/23 13:39:02.78
  STEP: the container should be terminated @ 05/12/23 13:39:02.787
  STEP: the termination message should be set @ 05/12/23 13:39:02.787
  May 12 13:39:02.787: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 05/12/23 13:39:02.787
  May 12 13:39:02.806: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-750" for this suite. @ 05/12/23 13:39:02.809
• [3.078 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 05/12/23 13:39:02.814
  May 12 13:39:02.814: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename cronjob @ 05/12/23 13:39:02.815
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:39:02.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:39:02.829
  STEP: Creating a cronjob @ 05/12/23 13:39:02.83
  STEP: Ensuring more than one job is running at a time @ 05/12/23 13:39:02.833
  E0512 13:39:02.886838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:03.886539      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:04.886697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:05.886915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:06.887640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:07.887684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:08.887880      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:09.888274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:10.889205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:11.890336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:12.890971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:13.895116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:14.895604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:15.895403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:16.896783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:17.897098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:18.897882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:19.897972      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:20.898788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:21.899353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:22.900221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:23.900282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:24.900513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:25.900636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:26.902078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:27.901765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:28.902249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:29.902693      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:30.903338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:31.904566      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:32.904855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:33.905462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:34.906101      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:35.906425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:36.907288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:37.907370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:38.907711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:39.907910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:40.908180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:41.908305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:42.908399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:43.908489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:44.909462      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:45.910185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:46.911173      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:47.912692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:48.912633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:49.913034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:50.913933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:51.914630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:52.915564      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:53.918338      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:54.919370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:55.919625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:56.920613      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:57.921229      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:58.922096      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:39:59.922835      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:00.923071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:01.923844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:02.924337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:03.924878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:04.925860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:05.926006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:06.926306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:07.927044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:08.926884      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:09.930568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:10.931054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:11.931492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:12.932728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:13.933423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:14.933601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:15.933819      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:16.942457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:17.942584      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:18.942924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:19.943406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:20.944054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:21.944630      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:22.945554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:23.946416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:24.947227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:25.947253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:26.948071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:27.948379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:28.949014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:29.950436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:30.951520      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:31.952863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:32.953205      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:33.954088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:34.954696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:35.955146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:36.956028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:37.956436      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:38.957398      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:39.957834      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:40.958856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:41.959274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:42.959713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:43.960027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:44.960917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:45.960998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:46.961915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:47.962631      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:48.962946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:49.963375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:50.963376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:51.963793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:52.964419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:53.964622      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:54.964677      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:55.966293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:56.966716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:57.966660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:58.967399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:40:59.967772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 05/12/23 13:41:00.836
  STEP: Removing cronjob @ 05/12/23 13:41:00.837
  May 12 13:41:00.841: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4537" for this suite. @ 05/12/23 13:41:00.844
• [118.042 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 05/12/23 13:41:00.867
  May 12 13:41:00.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/23 13:41:00.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:41:00.908
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:41:00.909
  STEP: Waiting for pod completion @ 05/12/23 13:41:00.916
  E0512 13:41:00.968174      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:01.968871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:02.969621      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:03.969492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:04.938: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-1356" for this suite. @ 05/12/23 13:41:04.941
• [4.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 05/12/23 13:41:04.95
  May 12 13:41:04.950: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename security-context @ 05/12/23 13:41:04.951
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:41:04.965
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:41:04.967
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/12/23 13:41:04.969
  E0512 13:41:04.970018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:05.971072      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:06.971499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:07.971481      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:08.972725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:41:08.995
  May 12 13:41:08.999: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod security-context-fc9983c4-8cf7-4309-ab31-bf4035a985ba container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:41:09.016
  May 12 13:41:09.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-5278" for this suite. @ 05/12/23 13:41:09.032
• [4.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 05/12/23 13:41:09.039
  May 12 13:41:09.040: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/23 13:41:09.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:41:09.051
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:41:09.053
  STEP: fetching the /apis discovery document @ 05/12/23 13:41:09.055
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 05/12/23 13:41:09.056
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 05/12/23 13:41:09.056
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 05/12/23 13:41:09.056
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 05/12/23 13:41:09.056
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 05/12/23 13:41:09.057
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 05/12/23 13:41:09.057
  May 12 13:41:09.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5076" for this suite. @ 05/12/23 13:41:09.06
• [0.024 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 05/12/23 13:41:09.065
  May 12 13:41:09.065: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:41:09.065
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:41:09.076
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:41:09.077
  STEP: creating service endpoint-test2 in namespace services-8485 @ 05/12/23 13:41:09.078
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8485 to expose endpoints map[] @ 05/12/23 13:41:09.088
  May 12 13:41:09.098: INFO: successfully validated that service endpoint-test2 in namespace services-8485 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-8485 @ 05/12/23 13:41:09.098
  E0512 13:41:09.972774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:10.973120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8485 to expose endpoints map[pod1:[80]] @ 05/12/23 13:41:11.13
  May 12 13:41:11.138: INFO: successfully validated that service endpoint-test2 in namespace services-8485 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 05/12/23 13:41:11.138
  May 12 13:41:11.138: INFO: Creating new exec pod
  E0512 13:41:11.975135      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:12.975872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:13.976865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:14.169: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-8485 exec execpodtzq4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 12 13:41:14.415: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 12 13:41:14.415: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:41:14.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-8485 exec execpodtzq4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.212.52 80'
  May 12 13:41:14.563: INFO: stderr: "+ nc -v -t -w 2 10.43.212.52 80\n+ echo hostName\nConnection to 10.43.212.52 80 port [tcp/http] succeeded!\n"
  May 12 13:41:14.563: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-8485 @ 05/12/23 13:41:14.563
  E0512 13:41:14.976198      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:15.977094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8485 to expose endpoints map[pod1:[80] pod2:[80]] @ 05/12/23 13:41:16.593
  May 12 13:41:16.614: INFO: successfully validated that service endpoint-test2 in namespace services-8485 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 05/12/23 13:41:16.614
  E0512 13:41:16.977349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:17.614: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-8485 exec execpodtzq4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  May 12 13:41:17.765: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 12 13:41:17.765: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:41:17.765: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-8485 exec execpodtzq4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.212.52 80'
  May 12 13:41:17.879: INFO: stderr: "+ nc -v -t -w 2 10.43.212.52 80\n+ echo hostName\nConnection to 10.43.212.52 80 port [tcp/http] succeeded!\n"
  May 12 13:41:17.879: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-8485 @ 05/12/23 13:41:17.879
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8485 to expose endpoints map[pod2:[80]] @ 05/12/23 13:41:17.894
  May 12 13:41:17.910: INFO: successfully validated that service endpoint-test2 in namespace services-8485 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 05/12/23 13:41:17.91
  E0512 13:41:17.977743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:18.911: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-8485 exec execpodtzq4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0512 13:41:18.978031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:19.027: INFO: stderr: "+ nc -v -t -w 2 endpoint-test2 80\n+ echo hostName\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  May 12 13:41:19.027: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  May 12 13:41:19.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-8485 exec execpodtzq4b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.212.52 80'
  May 12 13:41:19.147: INFO: stderr: "+ nc -v -t -w 2 10.43.212.52 80\n+ echo hostName\nConnection to 10.43.212.52 80 port [tcp/http] succeeded!\n"
  May 12 13:41:19.147: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-8485 @ 05/12/23 13:41:19.147
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-8485 to expose endpoints map[] @ 05/12/23 13:41:19.186
  May 12 13:41:19.194: INFO: successfully validated that service endpoint-test2 in namespace services-8485 exposes endpoints map[]
  May 12 13:41:19.194: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8485" for this suite. @ 05/12/23 13:41:19.215
• [10.155 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 05/12/23 13:41:19.221
  May 12 13:41:19.221: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename controllerrevisions @ 05/12/23 13:41:19.222
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:41:19.237
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:41:19.239
  STEP: Creating DaemonSet "e2e-l4lhd-daemon-set" @ 05/12/23 13:41:19.263
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/23 13:41:19.267
  May 12 13:41:19.271: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:19.272: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:19.274: INFO: Number of nodes with available pods controlled by daemonset e2e-l4lhd-daemon-set: 0
  May 12 13:41:19.274: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:41:19.978307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:20.278: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:20.278: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:20.280: INFO: Number of nodes with available pods controlled by daemonset e2e-l4lhd-daemon-set: 0
  May 12 13:41:20.280: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:41:20.979330      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:21.280: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:21.280: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:21.282: INFO: Number of nodes with available pods controlled by daemonset e2e-l4lhd-daemon-set: 2
  May 12 13:41:21.282: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-l4lhd-daemon-set
  STEP: Confirm DaemonSet "e2e-l4lhd-daemon-set" successfully created with "daemonset-name=e2e-l4lhd-daemon-set" label @ 05/12/23 13:41:21.283
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-l4lhd-daemon-set" @ 05/12/23 13:41:21.285
  May 12 13:41:21.287: INFO: Located ControllerRevision: "e2e-l4lhd-daemon-set-6bf8844b49"
  STEP: Patching ControllerRevision "e2e-l4lhd-daemon-set-6bf8844b49" @ 05/12/23 13:41:21.288
  May 12 13:41:21.291: INFO: e2e-l4lhd-daemon-set-6bf8844b49 has been patched
  STEP: Create a new ControllerRevision @ 05/12/23 13:41:21.291
  May 12 13:41:21.295: INFO: Created ControllerRevision: e2e-l4lhd-daemon-set-68688d9697
  STEP: Confirm that there are two ControllerRevisions @ 05/12/23 13:41:21.295
  May 12 13:41:21.295: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 12 13:41:21.297: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-l4lhd-daemon-set-6bf8844b49" @ 05/12/23 13:41:21.297
  STEP: Confirm that there is only one ControllerRevision @ 05/12/23 13:41:21.299
  May 12 13:41:21.299: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 12 13:41:21.301: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-l4lhd-daemon-set-68688d9697" @ 05/12/23 13:41:21.302
  May 12 13:41:21.306: INFO: e2e-l4lhd-daemon-set-68688d9697 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 05/12/23 13:41:21.306
  W0512 13:41:21.312439      20 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 05/12/23 13:41:21.312
  May 12 13:41:21.312: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0512 13:41:21.980034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:22.315: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 12 13:41:22.320: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-l4lhd-daemon-set-68688d9697=updated" @ 05/12/23 13:41:22.32
  STEP: Confirm that there is only one ControllerRevision @ 05/12/23 13:41:22.328
  May 12 13:41:22.328: INFO: Requesting list of ControllerRevisions to confirm quantity
  May 12 13:41:22.333: INFO: Found 1 ControllerRevisions
  May 12 13:41:22.337: INFO: ControllerRevision "e2e-l4lhd-daemon-set-7c559697d4" has revision 3
  STEP: Deleting DaemonSet "e2e-l4lhd-daemon-set" @ 05/12/23 13:41:22.34
  STEP: deleting DaemonSet.extensions e2e-l4lhd-daemon-set in namespace controllerrevisions-4740, will wait for the garbage collector to delete the pods @ 05/12/23 13:41:22.34
  May 12 13:41:22.398: INFO: Deleting DaemonSet.extensions e2e-l4lhd-daemon-set took: 4.472562ms
  May 12 13:41:22.500: INFO: Terminating DaemonSet.extensions e2e-l4lhd-daemon-set pods took: 101.297034ms
  E0512 13:41:22.980352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:23.903: INFO: Number of nodes with available pods controlled by daemonset e2e-l4lhd-daemon-set: 0
  May 12 13:41:23.903: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-l4lhd-daemon-set
  May 12 13:41:23.904: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"107644"},"items":null}

  May 12 13:41:23.905: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"107644"},"items":null}

  May 12 13:41:23.909: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-4740" for this suite. @ 05/12/23 13:41:23.911
• [4.694 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 05/12/23 13:41:23.916
  May 12 13:41:23.916: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename daemonsets @ 05/12/23 13:41:23.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:41:23.927
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:41:23.929
  STEP: Creating simple DaemonSet "daemon-set" @ 05/12/23 13:41:23.939
  STEP: Check that daemon pods launch on every node of the cluster. @ 05/12/23 13:41:23.942
  May 12 13:41:23.946: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:23.946: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:23.949: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:41:23.949: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:41:23.981354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:24.954: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:24.954: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:24.956: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 13:41:24.957: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 05/12/23 13:41:24.959
  May 12 13:41:24.972: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:24.972: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:24.975: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 13:41:24.976: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:41:24.981540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:25.985863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:25.992: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:25.999: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:26.007: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 13:41:26.007: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 13:41:26.980: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:26.981: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:26.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 13:41:26.985: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:41:26.985339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:27.985844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:27.991: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:27.992: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:27.999: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  May 12 13:41:27.999: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  May 12 13:41:28.979: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:41:28.979: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:41:28.981: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 13:41:28.981: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/23 13:41:28.984
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-2688, will wait for the garbage collector to delete the pods @ 05/12/23 13:41:28.985
  E0512 13:41:28.985873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:29.044: INFO: Deleting DaemonSet.extensions daemon-set took: 5.757789ms
  May 12 13:41:29.144: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.552287ms
  E0512 13:41:29.986670      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:30.987400      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:41:31.954: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:41:31.955: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 12 13:41:31.966: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"107789"},"items":null}

  May 12 13:41:31.970: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"107789"},"items":null}

  May 12 13:41:31.983: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0512 13:41:31.987451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "daemonsets-2688" for this suite. @ 05/12/23 13:41:31.987
• [8.078 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 05/12/23 13:41:31.994
  May 12 13:41:31.994: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename field-validation @ 05/12/23 13:41:31.995
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:41:32.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:41:32.007
  STEP: apply creating a deployment @ 05/12/23 13:41:32.065
  May 12 13:41:32.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2792" for this suite. @ 05/12/23 13:41:32.109
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 05/12/23 13:41:32.117
  May 12 13:41:32.117: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename init-container @ 05/12/23 13:41:32.117
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:41:32.135
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:41:32.136
  STEP: creating the pod @ 05/12/23 13:41:32.137
  May 12 13:41:32.137: INFO: PodSpec: initContainers in spec.initContainers
  E0512 13:41:32.988020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:33.988531      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:34.989342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:35.990580      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:36.991821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:37.991980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:38.992053      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:39.992418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:40.992926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:41.993032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:42.993557      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:43.993918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:44.994035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:45.994313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:46.994580      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:47.994987      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:48.995423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:49.996153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:50.996824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:51.997583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:52.997758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:53.998176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:54.998294      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:55.998994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:56.999804      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:58.000222      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:41:59.000567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:00.000640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:01.000717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:02.000854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:03.001103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:04.001233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:05.004733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:06.004932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:07.005152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:08.005473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:09.006195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:10.007027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:11.007818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:12.008825      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:13.009213      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:14.009598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:15.010710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:16.010600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:17.011986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:42:17.086: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-9169d748-cc6e-4c50-b441-6c4202beb8d0", GenerateName:"", Namespace:"init-container-1293", SelfLink:"", UID:"b13bb0a7-3d3c-4b65-9247-804817d03526", ResourceVersion:"108109", Generation:0, CreationTimestamp:time.Date(2023, time.May, 12, 13, 41, 32, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"137691507"}, Annotations:map[string]string{"cni.projectcalico.org/containerID":"6b6a7daeac488afb71d4447e8aca4e19ed95d9253d5a6669572159b1114fafd7", "cni.projectcalico.org/podIP":"10.42.3.208/32", "cni.projectcalico.org/podIPs":"10.42.3.208/32"}, OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"calico", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 12, 13, 41, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005656900), Subresource:"status"}, v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 12, 13, 41, 32, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005656930), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.May, 12, 13, 42, 17, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc005656960), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-526dk", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc00527ab40), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-526dk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-526dk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-526dk", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc004cb8e58), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"onekube-ip-172-16-100-7", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc0003019d0), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004cb8ed0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc004cb8ef0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc004cb8ef8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc004cb8efc), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc004ada9b0), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 12, 13, 41, 32, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 12, 13, 41, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 12, 13, 41, 32, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.May, 12, 13, 41, 32, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"172.16.100.7", PodIP:"10.42.3.208", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.42.3.208"}}, StartTime:time.Date(2023, time.May, 12, 13, 41, 32, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000301c70)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000301ce0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"containerd://2cbf45b9d53d76df6c08ffa251215726c63204903e45d9dcc35a1454a8875220", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00527abc0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc00527aba0), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc004cb8f7f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  May 12 13:42:17.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-1293" for this suite. @ 05/12/23 13:42:17.09
• [44.977 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 05/12/23 13:42:17.094
  May 12 13:42:17.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:42:17.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:42:17.114
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:42:17.116
  STEP: Setting up server cert @ 05/12/23 13:42:17.138
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:42:17.473
  STEP: Deploying the webhook pod @ 05/12/23 13:42:17.477
  STEP: Wait for the deployment to be ready @ 05/12/23 13:42:17.489
  May 12 13:42:17.502: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 13:42:18.012406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:19.012721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 13:42:19.525
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:42:19.546
  E0512 13:42:20.013465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:42:20.546: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  May 12 13:42:20.554: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 13:42:21.013937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 05/12/23 13:42:21.088
  STEP: Creating a custom resource that should be denied by the webhook @ 05/12/23 13:42:21.128
  E0512 13:42:22.014073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:23.021616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 05/12/23 13:42:23.145
  STEP: Updating the custom resource with disallowed data should be denied @ 05/12/23 13:42:23.149
  STEP: Deleting the custom resource should be denied @ 05/12/23 13:42:23.153
  STEP: Remove the offending key and value from the custom resource data @ 05/12/23 13:42:23.156
  STEP: Deleting the updated custom resource should be successful @ 05/12/23 13:42:23.16
  May 12 13:42:23.164: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7665" for this suite. @ 05/12/23 13:42:23.753
  STEP: Destroying namespace "webhook-markers-9845" for this suite. @ 05/12/23 13:42:23.781
• [6.702 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 05/12/23 13:42:23.797
  May 12 13:42:23.797: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:42:23.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:42:23.809
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:42:23.811
  STEP: creating an Endpoint @ 05/12/23 13:42:23.814
  STEP: waiting for available Endpoint @ 05/12/23 13:42:23.817
  STEP: listing all Endpoints @ 05/12/23 13:42:23.818
  STEP: updating the Endpoint @ 05/12/23 13:42:23.819
  STEP: fetching the Endpoint @ 05/12/23 13:42:23.823
  STEP: patching the Endpoint @ 05/12/23 13:42:23.824
  STEP: fetching the Endpoint @ 05/12/23 13:42:23.827
  STEP: deleting the Endpoint by Collection @ 05/12/23 13:42:23.828
  STEP: waiting for Endpoint deletion @ 05/12/23 13:42:23.831
  STEP: fetching the Endpoint @ 05/12/23 13:42:23.832
  May 12 13:42:23.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9681" for this suite. @ 05/12/23 13:42:23.836
• [0.043 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 05/12/23 13:42:23.841
  May 12 13:42:23.841: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 13:42:23.842
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:42:23.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:42:23.854
  STEP: Creating pod liveness-1067a6a2-7a4f-4e73-a6ce-76b8ac33b7fa in namespace container-probe-3037 @ 05/12/23 13:42:23.855
  E0512 13:42:24.038071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:25.022679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:42:25.868: INFO: Started pod liveness-1067a6a2-7a4f-4e73-a6ce-76b8ac33b7fa in namespace container-probe-3037
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/23 13:42:25.868
  May 12 13:42:25.869: INFO: Initial restart count of pod liveness-1067a6a2-7a4f-4e73-a6ce-76b8ac33b7fa is 0
  E0512 13:42:26.023117      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:27.023809      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:28.024717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:29.024651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:30.025559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:31.026045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:32.026567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:33.027119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:34.027778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:35.029297      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:36.029025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:37.029644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:38.029925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:39.030622      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:40.030679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:41.031315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:42.032304      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:43.032832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:44.033741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:45.034591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:42:45.930: INFO: Restart count of pod container-probe-3037/liveness-1067a6a2-7a4f-4e73-a6ce-76b8ac33b7fa is now 1 (20.060884567s elapsed)
  May 12 13:42:45.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 13:42:45.94
  STEP: Destroying namespace "container-probe-3037" for this suite. @ 05/12/23 13:42:45.958
• [22.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 05/12/23 13:42:45.974
  May 12 13:42:45.974: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename var-expansion @ 05/12/23 13:42:45.975
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:42:45.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:42:45.991
  E0512 13:42:46.035530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:47.036488      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:42:48.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:42:48.021: INFO: Deleting pod "var-expansion-a3e36e8f-c7d3-47ab-beb1-7fd3691201fa" in namespace "var-expansion-1912"
  May 12 13:42:48.032: INFO: Wait up to 5m0s for pod "var-expansion-a3e36e8f-c7d3-47ab-beb1-7fd3691201fa" to be fully deleted
  E0512 13:42:48.036039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:49.036977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:50.037091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:51.037254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:52.037322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-1912" for this suite. @ 05/12/23 13:42:52.047
• [6.078 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 05/12/23 13:42:52.053
  May 12 13:42:52.053: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:42:52.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:42:52.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:42:52.071
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/12/23 13:42:52.073
  E0512 13:42:53.038573      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:54.039395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:55.040511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:56.040508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:42:56.098
  May 12 13:42:56.100: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-d4801b31-04b9-4314-863d-45c5a5f6e978 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:42:56.113
  May 12 13:42:56.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4867" for this suite. @ 05/12/23 13:42:56.131
• [4.083 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 05/12/23 13:42:56.136
  May 12 13:42:56.136: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 13:42:56.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:42:56.159
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:42:56.16
  STEP: creating the pod @ 05/12/23 13:42:56.162
  STEP: submitting the pod to kubernetes @ 05/12/23 13:42:56.162
  STEP: verifying QOS class is set on the pod @ 05/12/23 13:42:56.167
  May 12 13:42:56.175: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-1195" for this suite. @ 05/12/23 13:42:56.178
• [0.047 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 05/12/23 13:42:56.184
  May 12 13:42:56.184: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/23 13:42:56.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:42:56.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:42:56.199
  E0512 13:42:57.041511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:42:58.041243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:42:58.229: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9974" for this suite. @ 05/12/23 13:42:58.232
• [2.052 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 05/12/23 13:42:58.237
  May 12 13:42:58.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename containers @ 05/12/23 13:42:58.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:42:58.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:42:58.298
  E0512 13:42:59.041860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:00.042930      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:00.317: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7195" for this suite. @ 05/12/23 13:43:00.32
• [2.089 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 05/12/23 13:43:00.326
  May 12 13:43:00.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-pred @ 05/12/23 13:43:00.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:00.337
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:00.34
  May 12 13:43:00.341: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 12 13:43:00.345: INFO: Waiting for terminating namespaces to be deleted...
  May 12 13:43:00.346: INFO: 
  Logging pods the apiserver thinks is on node onekube-ip-172-16-100-5 before test
  May 12 13:43:00.357: INFO: client-containers-c69bbece-2822-4df3-a01f-e8112142c66d from containers-7195 started at 2023-05-12 13:42:58 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container agnhost-container ready: true, restart count 0
  May 12 13:43:00.357: INFO: helm-install-one-longhorn-nv79t from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container helm ready: false, restart count 0
  May 12 13:43:00.357: INFO: helm-install-one-metallb-6p76s from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container helm ready: false, restart count 0
  May 12 13:43:00.357: INFO: helm-install-one-traefik-qs2vx from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container helm ready: false, restart count 0
  May 12 13:43:00.357: INFO: helm-install-rke2-metrics-server-vb7g9 from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container helm ready: false, restart count 0
  May 12 13:43:00.357: INFO: helm-install-rke2-snapshot-controller-5cb8c from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container helm ready: false, restart count 1
  May 12 13:43:00.357: INFO: helm-install-rke2-snapshot-controller-crd-x6qww from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container helm ready: false, restart count 0
  May 12 13:43:00.357: INFO: helm-install-rke2-snapshot-validation-webhook-5ft26 from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container helm ready: false, restart count 0
  May 12 13:43:00.357: INFO: kube-proxy-onekube-ip-172-16-100-5 from kube-system started at 2023-05-12 10:27:24 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container kube-proxy ready: true, restart count 0
  May 12 13:43:00.357: INFO: rke2-canal-qjzfl from kube-system started at 2023-05-12 10:27:25 +0000 UTC (2 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container calico-node ready: true, restart count 0
  May 12 13:43:00.357: INFO: 	Container kube-flannel ready: true, restart count 0
  May 12 13:43:00.357: INFO: rke2-coredns-rke2-coredns-5896cccb79-95wmb from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container coredns ready: true, restart count 0
  May 12 13:43:00.357: INFO: rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-hg8jj from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container autoscaler ready: true, restart count 0
  May 12 13:43:00.357: INFO: rke2-metrics-server-6d45f6cb4d-h7dx4 from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container metrics-server ready: true, restart count 0
  May 12 13:43:00.357: INFO: rke2-snapshot-controller-7bf6d7bf5f-sg6ft from kube-system started at 2023-05-12 10:28:35 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container rke2-snapshot-controller ready: true, restart count 0
  May 12 13:43:00.357: INFO: rke2-snapshot-validation-webhook-b65d46c9f-8bqxq from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container rke2-snapshot-validation-webhook ready: true, restart count 0
  May 12 13:43:00.357: INFO: busybox-readonly-fsb5de5038-5faf-46a5-bae1-27bea9af0ff4 from kubelet-test-9974 started at 2023-05-12 13:42:56 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container busybox-readonly-fsb5de5038-5faf-46a5-bae1-27bea9af0ff4 ready: true, restart count 0
  May 12 13:43:00.357: INFO: csi-attacher-79bf77b7d8-vr2c8 from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container csi-attacher ready: true, restart count 0
  May 12 13:43:00.357: INFO: csi-provisioner-566959ff99-rj4wp from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 12 13:43:00.357: INFO: csi-provisioner-566959ff99-srfx5 from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 12 13:43:00.357: INFO: csi-resizer-769c8fc86-4mp8g from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.357: INFO: 	Container csi-resizer ready: true, restart count 0
  May 12 13:43:00.358: INFO: csi-resizer-769c8fc86-5lvwx from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container csi-resizer ready: true, restart count 0
  May 12 13:43:00.358: INFO: csi-snapshotter-5677bd8f7f-2pqth from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 12 13:43:00.358: INFO: csi-snapshotter-5677bd8f7f-9dlsf from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 12 13:43:00.358: INFO: engine-image-ei-7fa7c208-kkhx7 from longhorn-system started at 2023-05-12 10:28:57 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container engine-image-ei-7fa7c208 ready: true, restart count 0
  May 12 13:43:00.358: INFO: instance-manager-e-1d0a2d1f86ac8de0e3ad611c8e7e88f7 from longhorn-system started at 2023-05-12 10:28:57 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container engine-manager ready: true, restart count 0
  May 12 13:43:00.358: INFO: longhorn-admission-webhook-9944c8788-5vs75 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container longhorn-admission-webhook ready: true, restart count 0
  May 12 13:43:00.358: INFO: longhorn-admission-webhook-9944c8788-n5t49 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container longhorn-admission-webhook ready: true, restart count 0
  May 12 13:43:00.358: INFO: longhorn-conversion-webhook-6c88c48f86-rwmr7 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container longhorn-conversion-webhook ready: true, restart count 0
  May 12 13:43:00.358: INFO: longhorn-conversion-webhook-6c88c48f86-x7rnk from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container longhorn-conversion-webhook ready: true, restart count 0
  May 12 13:43:00.358: INFO: longhorn-csi-plugin-8mtl4 from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (3 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container longhorn-csi-plugin ready: true, restart count 0
  May 12 13:43:00.358: INFO: 	Container longhorn-liveness-probe ready: true, restart count 0
  May 12 13:43:00.358: INFO: 	Container node-driver-registrar ready: true, restart count 0
  May 12 13:43:00.358: INFO: longhorn-manager-lsknf from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container longhorn-manager ready: true, restart count 0
  May 12 13:43:00.358: INFO: longhorn-recovery-backend-568bcc58fc-5d6pn from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container longhorn-recovery-backend ready: true, restart count 0
  May 12 13:43:00.358: INFO: longhorn-recovery-backend-568bcc58fc-lklhz from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container longhorn-recovery-backend ready: true, restart count 0
  May 12 13:43:00.358: INFO: one-metallb-controller-7965f57b86-zvww2 from metallb-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container controller ready: true, restart count 0
  May 12 13:43:00.358: INFO: one-metallb-speaker-tr56f from metallb-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container speaker ready: true, restart count 0
  May 12 13:43:00.358: INFO: sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-tc4w8 from sonobuoy started at 2023-05-12 12:31:55 +0000 UTC (2 container statuses recorded)
  May 12 13:43:00.358: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 12 13:43:00.359: INFO: 	Container systemd-logs ready: true, restart count 0
  May 12 13:43:00.359: INFO: one-traefik-66db8f599c-7j8zw from traefik-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.359: INFO: 	Container one-traefik ready: true, restart count 0
  May 12 13:43:00.359: INFO: 
  Logging pods the apiserver thinks is on node onekube-ip-172-16-100-7 before test
  May 12 13:43:00.368: INFO: kube-proxy-onekube-ip-172-16-100-7 from kube-system started at 2023-05-12 10:34:21 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.368: INFO: 	Container kube-proxy ready: true, restart count 0
  May 12 13:43:00.368: INFO: rke2-canal-58rgz from kube-system started at 2023-05-12 10:34:22 +0000 UTC (2 container statuses recorded)
  May 12 13:43:00.368: INFO: 	Container calico-node ready: true, restart count 0
  May 12 13:43:00.368: INFO: 	Container kube-flannel ready: true, restart count 0
  May 12 13:43:00.368: INFO: engine-image-ei-7fa7c208-kbxr9 from longhorn-system started at 2023-05-12 13:21:00 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.368: INFO: 	Container engine-image-ei-7fa7c208 ready: true, restart count 0
  May 12 13:43:00.369: INFO: instance-manager-e-bb03c767b3d98d645b664cf2a3258c7f from longhorn-system started at 2023-05-12 13:20:57 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.369: INFO: 	Container engine-manager ready: true, restart count 0
  May 12 13:43:00.369: INFO: longhorn-csi-plugin-dnp4n from longhorn-system started at 2023-05-12 13:20:55 +0000 UTC (3 container statuses recorded)
  May 12 13:43:00.369: INFO: 	Container longhorn-csi-plugin ready: true, restart count 0
  May 12 13:43:00.369: INFO: 	Container longhorn-liveness-probe ready: true, restart count 0
  May 12 13:43:00.369: INFO: 	Container node-driver-registrar ready: true, restart count 0
  May 12 13:43:00.369: INFO: longhorn-manager-zg9ps from longhorn-system started at 2023-05-12 13:20:55 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.369: INFO: 	Container longhorn-manager ready: true, restart count 0
  May 12 13:43:00.369: INFO: one-metallb-speaker-h8f7g from metallb-system started at 2023-05-12 13:20:55 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.369: INFO: 	Container speaker ready: true, restart count 0
  May 12 13:43:00.369: INFO: pod-qos-class-a01404e8-d3f7-4e19-b0b7-87d406495441 from pods-1195 started at 2023-05-12 13:42:56 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.369: INFO: 	Container agnhost ready: false, restart count 0
  May 12 13:43:00.369: INFO: sonobuoy from sonobuoy started at 2023-05-12 12:31:54 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.369: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 12 13:43:00.369: INFO: sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-8q9cx from sonobuoy started at 2023-05-12 12:31:55 +0000 UTC (2 container statuses recorded)
  May 12 13:43:00.369: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 12 13:43:00.369: INFO: 	Container systemd-logs ready: true, restart count 0
  May 12 13:43:00.369: INFO: one-traefik-66db8f599c-zbk58 from traefik-system started at 2023-05-12 13:21:02 +0000 UTC (1 container statuses recorded)
  May 12 13:43:00.369: INFO: 	Container one-traefik ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 05/12/23 13:43:00.369
  E0512 13:43:01.043877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:02.044960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 05/12/23 13:43:02.381
  STEP: Trying to apply a random label on the found node. @ 05/12/23 13:43:02.391
  STEP: verifying the node has the label kubernetes.io/e2e-8e02414b-30c1-4543-82ca-13a2a518dd3e 42 @ 05/12/23 13:43:02.416
  STEP: Trying to relaunch the pod, now with labels. @ 05/12/23 13:43:02.419
  E0512 13:43:03.045556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:04.046233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-8e02414b-30c1-4543-82ca-13a2a518dd3e off the node onekube-ip-172-16-100-5 @ 05/12/23 13:43:04.445
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-8e02414b-30c1-4543-82ca-13a2a518dd3e @ 05/12/23 13:43:04.474
  May 12 13:43:04.477: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9692" for this suite. @ 05/12/23 13:43:04.479
• [4.156 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 05/12/23 13:43:04.484
  May 12 13:43:04.484: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 13:43:04.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:04.498
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:04.5
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 13:43:04.502
  E0512 13:43:05.046872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:06.047100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:07.049186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:08.049967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:43:08.525
  May 12 13:43:08.531: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-f862c59a-2380-4116-a71c-243895af9936 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 13:43:08.541
  May 12 13:43:08.561: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5894" for this suite. @ 05/12/23 13:43:08.564
• [4.086 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 05/12/23 13:43:08.571
  May 12 13:43:08.571: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename gc @ 05/12/23 13:43:08.573
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:08.59
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:08.593
  STEP: create the deployment @ 05/12/23 13:43:08.594
  W0512 13:43:08.603521      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/12/23 13:43:08.603
  STEP: delete the deployment @ 05/12/23 13:43:08.719
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 05/12/23 13:43:08.723
  E0512 13:43:09.050239      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/23 13:43:09.232
  May 12 13:43:09.292: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 12 13:43:09.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-574" for this suite. @ 05/12/23 13:43:09.299
• [0.730 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 05/12/23 13:43:09.302
  May 12 13:43:09.302: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubelet-test @ 05/12/23 13:43:09.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:09.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:09.319
  E0512 13:43:10.051181      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:11.051933      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:11.334: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8780" for this suite. @ 05/12/23 13:43:11.337
• [2.038 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 05/12/23 13:43:11.342
  May 12 13:43:11.342: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename var-expansion @ 05/12/23 13:43:11.343
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:11.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:11.356
  E0512 13:43:12.051633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:13.052501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:13.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:43:13.391: INFO: Deleting pod "var-expansion-2b220ce2-c900-4183-8d38-04109e2e8ec2" in namespace "var-expansion-8431"
  May 12 13:43:13.410: INFO: Wait up to 5m0s for pod "var-expansion-2b220ce2-c900-4183-8d38-04109e2e8ec2" to be fully deleted
  E0512 13:43:14.052594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:15.052709      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-8431" for this suite. @ 05/12/23 13:43:15.423
• [4.110 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 05/12/23 13:43:15.457
  May 12 13:43:15.457: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:43:15.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:15.478
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:15.48
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3198 @ 05/12/23 13:43:15.483
  STEP: changing the ExternalName service to type=NodePort @ 05/12/23 13:43:15.488
  STEP: creating replication controller externalname-service in namespace services-3198 @ 05/12/23 13:43:15.504
  I0512 13:43:15.513300      20 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3198, replica count: 2
  E0512 13:43:16.053663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:17.059740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:18.056502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0512 13:43:18.564620      20 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 13:43:18.564: INFO: Creating new exec pod
  E0512 13:43:19.056565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:20.056696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:21.064243      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:21.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-3198 exec execpod77lvr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  May 12 13:43:21.832: INFO: stderr: "+ nc -v -t -w 2 externalname-service 80\n+ echo hostName\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  May 12 13:43:21.832: INFO: stdout: "externalname-service-tnc87"
  May 12 13:43:21.832: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-3198 exec execpod77lvr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.43.56.96 80'
  May 12 13:43:21.937: INFO: stderr: "+ nc -v -t -w 2 10.43.56.96 80\n+ echo hostName\nConnection to 10.43.56.96 80 port [tcp/http] succeeded!\n"
  May 12 13:43:21.937: INFO: stdout: "externalname-service-dbdrd"
  May 12 13:43:21.937: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-3198 exec execpod77lvr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.5 32212'
  May 12 13:43:22.047: INFO: stderr: "+ nc -v -t -w 2 172.16.100.5 32212\n+ Connection to 172.16.100.5 32212 port [tcp/*] succeeded!\necho hostName\n"
  May 12 13:43:22.047: INFO: stdout: "externalname-service-dbdrd"
  May 12 13:43:22.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-3198 exec execpod77lvr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.7 32212'
  E0512 13:43:22.057865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:22.203: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.100.7 32212\nConnection to 172.16.100.7 32212 port [tcp/*] succeeded!\n"
  May 12 13:43:22.203: INFO: stdout: ""
  E0512 13:43:23.058805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:23.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=services-3198 exec execpod77lvr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 172.16.100.7 32212'
  May 12 13:43:23.357: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 172.16.100.7 32212\nConnection to 172.16.100.7 32212 port [tcp/*] succeeded!\n"
  May 12 13:43:23.357: INFO: stdout: "externalname-service-tnc87"
  May 12 13:43:23.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 13:43:23.360: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-3198" for this suite. @ 05/12/23 13:43:23.383
• [7.933 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 05/12/23 13:43:23.394
  May 12 13:43:23.394: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 13:43:23.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:23.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:23.417
  STEP: Counting existing ResourceQuota @ 05/12/23 13:43:23.418
  E0512 13:43:24.059138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:25.059476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:26.059761      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:27.062002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:28.062991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/12/23 13:43:28.427
  STEP: Ensuring resource quota status is calculated @ 05/12/23 13:43:28.439
  E0512 13:43:29.063393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:30.064401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 05/12/23 13:43:30.45
  STEP: Ensuring resource quota status captures replication controller creation @ 05/12/23 13:43:30.462
  E0512 13:43:31.065165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:32.065310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 05/12/23 13:43:32.467
  STEP: Ensuring resource quota status released usage @ 05/12/23 13:43:32.47
  E0512 13:43:33.066020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:34.066616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:34.480: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8302" for this suite. @ 05/12/23 13:43:34.499
• [11.116 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 05/12/23 13:43:34.515
  May 12 13:43:34.515: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename podtemplate @ 05/12/23 13:43:34.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:34.533
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:34.535
  STEP: Create a pod template @ 05/12/23 13:43:34.537
  STEP: Replace a pod template @ 05/12/23 13:43:34.539
  May 12 13:43:34.544: INFO: Found updated podtemplate annotation: "true"

  May 12 13:43:34.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-1296" for this suite. @ 05/12/23 13:43:34.546
• [0.033 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 05/12/23 13:43:34.549
  May 12 13:43:34.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename ingressclass @ 05/12/23 13:43:34.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:34.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:34.561
  STEP: getting /apis @ 05/12/23 13:43:34.563
  STEP: getting /apis/networking.k8s.io @ 05/12/23 13:43:34.566
  STEP: getting /apis/networking.k8s.iov1 @ 05/12/23 13:43:34.567
  STEP: creating @ 05/12/23 13:43:34.567
  STEP: getting @ 05/12/23 13:43:34.575
  STEP: listing @ 05/12/23 13:43:34.576
  STEP: watching @ 05/12/23 13:43:34.577
  May 12 13:43:34.577: INFO: starting watch
  STEP: patching @ 05/12/23 13:43:34.578
  STEP: updating @ 05/12/23 13:43:34.581
  May 12 13:43:34.583: INFO: waiting for watch events with expected annotations
  May 12 13:43:34.583: INFO: saw patched and updated annotations
  STEP: deleting @ 05/12/23 13:43:34.584
  STEP: deleting a collection @ 05/12/23 13:43:34.588
  May 12 13:43:34.593: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-5819" for this suite. @ 05/12/23 13:43:34.595
• [0.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 05/12/23 13:43:34.602
  May 12 13:43:34.602: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename field-validation @ 05/12/23 13:43:34.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:34.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:34.616
  STEP: apply creating a deployment @ 05/12/23 13:43:34.617
  May 12 13:43:34.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2466" for this suite. @ 05/12/23 13:43:34.625
• [0.026 seconds]
------------------------------
SSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 05/12/23 13:43:34.629
  May 12 13:43:34.629: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:43:34.629
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:34.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:34.646
  STEP: creating Agnhost RC @ 05/12/23 13:43:34.647
  May 12 13:43:34.647: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8726 create -f -'
  May 12 13:43:35.028: INFO: stderr: ""
  May 12 13:43:35.028: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 05/12/23 13:43:35.029
  E0512 13:43:35.068485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:36.038: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 13:43:36.038: INFO: Found 0 / 1
  E0512 13:43:36.068837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:37.038: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 13:43:37.038: INFO: Found 1 / 1
  May 12 13:43:37.039: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 05/12/23 13:43:37.039
  May 12 13:43:37.047: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 13:43:37.047: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 12 13:43:37.047: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-8726 patch pod agnhost-primary-sx8r5 -p {"metadata":{"annotations":{"x":"y"}}}'
  E0512 13:43:37.069454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:37.167: INFO: stderr: ""
  May 12 13:43:37.167: INFO: stdout: "pod/agnhost-primary-sx8r5 patched\n"
  STEP: checking annotations @ 05/12/23 13:43:37.167
  May 12 13:43:37.169: INFO: Selector matched 1 pods for map[app:agnhost]
  May 12 13:43:37.169: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  May 12 13:43:37.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8726" for this suite. @ 05/12/23 13:43:37.172
• [2.549 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 05/12/23 13:43:37.179
  May 12 13:43:37.179: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename dns @ 05/12/23 13:43:37.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:43:37.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:43:37.245
  STEP: Creating a test headless service @ 05/12/23 13:43:37.247
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-889 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-889;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-889 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-889;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-889.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-889.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-889.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-889.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-889.svc;check="$$(dig +notcp +noall +answer +search 126.225.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.225.126_udp@PTR;check="$$(dig +tcp +noall +answer +search 126.225.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.225.126_tcp@PTR;sleep 1; done
   @ 05/12/23 13:43:37.273
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-889 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-889;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-889 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-889;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-889.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-889.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-889.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-889.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-889.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-889.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-889.svc;check="$$(dig +notcp +noall +answer +search 126.225.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.225.126_udp@PTR;check="$$(dig +tcp +noall +answer +search 126.225.43.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.43.225.126_tcp@PTR;sleep 1; done
   @ 05/12/23 13:43:37.278
  STEP: creating a pod to probe DNS @ 05/12/23 13:43:37.282
  STEP: submitting the pod to kubernetes @ 05/12/23 13:43:37.282
  E0512 13:43:38.069655      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:39.069820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 05/12/23 13:43:39.31
  STEP: looking for the results for each expected name from probers @ 05/12/23 13:43:39.312
  May 12 13:43:39.316: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.319: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.321: INFO: Unable to read wheezy_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.323: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.328: INFO: Unable to read wheezy_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.329: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.331: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.334: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.344: INFO: Unable to read jessie_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.346: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.348: INFO: Unable to read jessie_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.350: INFO: Unable to read jessie_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.352: INFO: Unable to read jessie_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.354: INFO: Unable to read jessie_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.357: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.359: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:39.377: INFO: Lookups using dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-889 wheezy_tcp@dns-test-service.dns-889 wheezy_udp@dns-test-service.dns-889.svc wheezy_tcp@dns-test-service.dns-889.svc wheezy_udp@_http._tcp.dns-test-service.dns-889.svc wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-889 jessie_tcp@dns-test-service.dns-889 jessie_udp@dns-test-service.dns-889.svc jessie_tcp@dns-test-service.dns-889.svc jessie_udp@_http._tcp.dns-test-service.dns-889.svc jessie_tcp@_http._tcp.dns-test-service.dns-889.svc]

  E0512 13:43:40.070303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:41.070604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:42.071673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:43.071701      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:44.072139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:44.386: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.388: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.389: INFO: Unable to read wheezy_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.391: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.392: INFO: Unable to read wheezy_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.393: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.395: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.396: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.403: INFO: Unable to read jessie_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.405: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.406: INFO: Unable to read jessie_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.407: INFO: Unable to read jessie_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.409: INFO: Unable to read jessie_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.411: INFO: Unable to read jessie_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.412: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.413: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:44.419: INFO: Lookups using dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-889 wheezy_tcp@dns-test-service.dns-889 wheezy_udp@dns-test-service.dns-889.svc wheezy_tcp@dns-test-service.dns-889.svc wheezy_udp@_http._tcp.dns-test-service.dns-889.svc wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-889 jessie_tcp@dns-test-service.dns-889 jessie_udp@dns-test-service.dns-889.svc jessie_tcp@dns-test-service.dns-889.svc jessie_udp@_http._tcp.dns-test-service.dns-889.svc jessie_tcp@_http._tcp.dns-test-service.dns-889.svc]

  E0512 13:43:45.072664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:46.072707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:47.073227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:48.073224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:49.073322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:49.382: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.384: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.386: INFO: Unable to read wheezy_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.388: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.390: INFO: Unable to read wheezy_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.391: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.393: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.396: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.418: INFO: Unable to read jessie_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.420: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.421: INFO: Unable to read jessie_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.424: INFO: Unable to read jessie_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.426: INFO: Unable to read jessie_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.428: INFO: Unable to read jessie_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.430: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.432: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:49.439: INFO: Lookups using dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-889 wheezy_tcp@dns-test-service.dns-889 wheezy_udp@dns-test-service.dns-889.svc wheezy_tcp@dns-test-service.dns-889.svc wheezy_udp@_http._tcp.dns-test-service.dns-889.svc wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-889 jessie_tcp@dns-test-service.dns-889 jessie_udp@dns-test-service.dns-889.svc jessie_tcp@dns-test-service.dns-889.svc jessie_udp@_http._tcp.dns-test-service.dns-889.svc jessie_tcp@_http._tcp.dns-test-service.dns-889.svc]

  E0512 13:43:50.074450      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:51.074735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:52.075650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:53.077639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:54.077747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:54.389: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.402: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.412: INFO: Unable to read wheezy_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.416: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.418: INFO: Unable to read wheezy_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.420: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.423: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.424: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.434: INFO: Unable to read jessie_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.436: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.438: INFO: Unable to read jessie_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.440: INFO: Unable to read jessie_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.442: INFO: Unable to read jessie_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.444: INFO: Unable to read jessie_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.445: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.447: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:54.455: INFO: Lookups using dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-889 wheezy_tcp@dns-test-service.dns-889 wheezy_udp@dns-test-service.dns-889.svc wheezy_tcp@dns-test-service.dns-889.svc wheezy_udp@_http._tcp.dns-test-service.dns-889.svc wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-889 jessie_tcp@dns-test-service.dns-889 jessie_udp@dns-test-service.dns-889.svc jessie_tcp@dns-test-service.dns-889.svc jessie_udp@_http._tcp.dns-test-service.dns-889.svc jessie_tcp@_http._tcp.dns-test-service.dns-889.svc]

  E0512 13:43:55.078537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:56.079440      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:57.080342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:58.080262      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:43:59.080098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:43:59.379: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.381: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.383: INFO: Unable to read wheezy_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.384: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.386: INFO: Unable to read wheezy_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.391: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.394: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.396: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.403: INFO: Unable to read jessie_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.405: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.407: INFO: Unable to read jessie_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.410: INFO: Unable to read jessie_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.412: INFO: Unable to read jessie_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.413: INFO: Unable to read jessie_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.415: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.416: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:43:59.422: INFO: Lookups using dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-889 wheezy_tcp@dns-test-service.dns-889 wheezy_udp@dns-test-service.dns-889.svc wheezy_tcp@dns-test-service.dns-889.svc wheezy_udp@_http._tcp.dns-test-service.dns-889.svc wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-889 jessie_tcp@dns-test-service.dns-889 jessie_udp@dns-test-service.dns-889.svc jessie_tcp@dns-test-service.dns-889.svc jessie_udp@_http._tcp.dns-test-service.dns-889.svc jessie_tcp@_http._tcp.dns-test-service.dns-889.svc]

  E0512 13:44:00.081532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:01.082221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:02.083844      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:03.084115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:04.084199      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:44:04.380: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.382: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.385: INFO: Unable to read wheezy_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.386: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.388: INFO: Unable to read wheezy_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.389: INFO: Unable to read wheezy_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.391: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.392: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.401: INFO: Unable to read jessie_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.403: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.405: INFO: Unable to read jessie_udp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.407: INFO: Unable to read jessie_tcp@dns-test-service.dns-889 from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.409: INFO: Unable to read jessie_udp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.412: INFO: Unable to read jessie_tcp@dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.414: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.416: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-889.svc from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:04.424: INFO: Lookups using dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-889 wheezy_tcp@dns-test-service.dns-889 wheezy_udp@dns-test-service.dns-889.svc wheezy_tcp@dns-test-service.dns-889.svc wheezy_udp@_http._tcp.dns-test-service.dns-889.svc wheezy_tcp@_http._tcp.dns-test-service.dns-889.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-889 jessie_tcp@dns-test-service.dns-889 jessie_udp@dns-test-service.dns-889.svc jessie_tcp@dns-test-service.dns-889.svc jessie_udp@_http._tcp.dns-test-service.dns-889.svc jessie_tcp@_http._tcp.dns-test-service.dns-889.svc]

  E0512 13:44:05.085138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:06.085393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:07.086475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:08.087333      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:09.088107      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:44:09.383: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc: the server could not find the requested resource (get pods dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc)
  May 12 13:44:09.422: INFO: Lookups using dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc failed for: [wheezy_udp@dns-test-service]

  E0512 13:44:10.088951      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:11.088750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:12.089837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:13.090317      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:14.090740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:44:14.456: INFO: DNS probes using dns-889/dns-test-9c3812ca-9301-4552-9ed7-57b8d97911cc succeeded

  May 12 13:44:14.456: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 13:44:14.458
  STEP: deleting the test service @ 05/12/23 13:44:14.481
  STEP: deleting the test headless service @ 05/12/23 13:44:14.51
  STEP: Destroying namespace "dns-889" for this suite. @ 05/12/23 13:44:14.525
• [37.354 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 05/12/23 13:44:14.537
  May 12 13:44:14.537: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename podtemplate @ 05/12/23 13:44:14.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:44:14.554
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:44:14.555
  May 12 13:44:14.568: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3901" for this suite. @ 05/12/23 13:44:14.57
• [0.037 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 05/12/23 13:44:14.575
  May 12 13:44:14.575: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/23 13:44:14.576
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:44:14.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:44:14.589
  E0512 13:44:15.094217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:16.092780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 05/12/23 13:44:16.614
  May 12 13:44:16.615: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5938 pod-service-account-7d120477-df2f-4a59-8c9e-7c52a14a6485 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 05/12/23 13:44:16.758
  May 12 13:44:16.758: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5938 pod-service-account-7d120477-df2f-4a59-8c9e-7c52a14a6485 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 05/12/23 13:44:16.862
  May 12 13:44:16.862: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-5938 pod-service-account-7d120477-df2f-4a59-8c9e-7c52a14a6485 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  May 12 13:44:16.993: INFO: Got root ca configmap in namespace "svcaccounts-5938"
  May 12 13:44:16.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5938" for this suite. @ 05/12/23 13:44:16.998
• [2.432 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 05/12/23 13:44:17.008
  May 12 13:44:17.008: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:44:17.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:44:17.021
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:44:17.023
  STEP: Creating configMap with name projected-configmap-test-volume-5ba7842a-907a-4bc0-ac3c-5b113836b4a6 @ 05/12/23 13:44:17.024
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:44:17.027
  E0512 13:44:17.092971      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:18.093411      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:19.094274      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:20.094625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:44:21.062
  May 12 13:44:21.069: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-projected-configmaps-ac666781-8db0-477a-b8a6-d3c389b204a4 container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:44:21.088
  E0512 13:44:21.095232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:44:21.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4272" for this suite. @ 05/12/23 13:44:21.117
• [4.113 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 05/12/23 13:44:21.121
  May 12 13:44:21.121: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:44:21.122
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:44:21.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:44:21.138
  STEP: Creating configMap with name configmap-test-volume-map-fe491a4b-c47d-4131-8dc5-7d861ca32dec @ 05/12/23 13:44:21.139
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:44:21.142
  E0512 13:44:22.095111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:23.095200      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:24.095331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:25.095421      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:44:25.155
  May 12 13:44:25.157: INFO: Trying to get logs from node onekube-ip-172-16-100-5 pod pod-configmaps-bf6fe5ab-7a4c-4b3b-aebf-818344f8e030 container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 13:44:25.165
  May 12 13:44:25.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8480" for this suite. @ 05/12/23 13:44:25.183
• [4.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 05/12/23 13:44:25.193
  May 12 13:44:25.193: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 13:44:25.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:44:25.207
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:44:25.209
  STEP: Creating pod liveness-6d097cd5-0a40-46f4-acb2-e60bdaffd2c0 in namespace container-probe-772 @ 05/12/23 13:44:25.211
  E0512 13:44:26.096148      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:27.096601      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:44:27.224: INFO: Started pod liveness-6d097cd5-0a40-46f4-acb2-e60bdaffd2c0 in namespace container-probe-772
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/23 13:44:27.224
  May 12 13:44:27.225: INFO: Initial restart count of pod liveness-6d097cd5-0a40-46f4-acb2-e60bdaffd2c0 is 0
  E0512 13:44:28.096726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:29.096766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:30.097124      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:31.098309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:32.125507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:33.101408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:34.101720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:35.101747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:36.102516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:37.103074      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:38.103114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:39.103250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:40.104063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:41.104407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:42.104440      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:43.104863      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:44.105017      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:45.105878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:46.106430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:47.106556      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:44:47.296: INFO: Restart count of pod container-probe-772/liveness-6d097cd5-0a40-46f4-acb2-e60bdaffd2c0 is now 1 (20.071138752s elapsed)
  E0512 13:44:48.106733      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:49.107044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:50.108283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:51.108638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:52.108842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:53.108994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:54.109538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:55.109533      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:56.109965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:57.110331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:58.110871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:44:59.111045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:00.112538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:01.112879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:02.113323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:03.113376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:04.113691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:05.114363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:06.114862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:07.115603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:45:07.374: INFO: Restart count of pod container-probe-772/liveness-6d097cd5-0a40-46f4-acb2-e60bdaffd2c0 is now 2 (40.149245755s elapsed)
  E0512 13:45:08.116086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:09.116438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:10.117219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:11.117950      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:12.118309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:13.118550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:14.119987      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:15.119026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:16.119617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:17.120128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:18.121050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:19.121363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:20.122755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:21.123904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:22.124610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:23.124920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:24.125706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:25.126254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:26.126642      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:27.126616      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:45:27.435: INFO: Restart count of pod container-probe-772/liveness-6d097cd5-0a40-46f4-acb2-e60bdaffd2c0 is now 3 (1m0.209774531s elapsed)
  E0512 13:45:28.126956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:29.126798      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:30.128536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:31.129253      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:32.132689      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:33.133780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:34.134026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:35.134099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:36.134439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:37.136569      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:38.137082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:39.137242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:40.137576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:41.138335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:42.144702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:43.144687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:44.145779      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:45.146257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:46.148178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:47.148179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:45:47.501: INFO: Restart count of pod container-probe-772/liveness-6d097cd5-0a40-46f4-acb2-e60bdaffd2c0 is now 4 (1m20.275913527s elapsed)
  E0512 13:45:48.149178      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:49.149724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:50.151132      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:51.152749      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:52.151513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:53.151909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:54.152553      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:55.154300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:56.156976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:57.157240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:58.158288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:45:59.157827      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:00.158745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:01.159113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:02.159532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:03.159643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:04.166021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:05.160373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:06.161576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:07.161711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:08.163228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:09.163153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:10.164622      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:11.164928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:12.170439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:13.171486      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:14.171873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:15.171988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:16.172121      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:17.172279      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:18.172924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:19.172865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:20.173318      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:21.174027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:22.174269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:23.174794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:24.175744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:25.175840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:26.176593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:27.177403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:28.178398      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:29.178463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:30.179565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:31.179807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:32.180853      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:33.181170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:34.181088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:35.181535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:36.198144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:37.183351      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:38.184339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:39.184757      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:40.185303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:41.186625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:42.188298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:43.188405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:44.188591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:45.189427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:46.189775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:47.190461      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:48.190583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:49.190718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:50.191179      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:51.191483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:52.194293      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:53.194657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:54.195596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:55.195939      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:46:55.770: INFO: Restart count of pod container-probe-772/liveness-6d097cd5-0a40-46f4-acb2-e60bdaffd2c0 is now 5 (2m28.544563766s elapsed)
  May 12 13:46:55.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 13:46:55.776
  STEP: Destroying namespace "container-probe-772" for this suite. @ 05/12/23 13:46:55.791
• [150.604 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 05/12/23 13:46:55.798
  May 12 13:46:55.798: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 13:46:55.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:46:55.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:46:55.813
  STEP: creating a collection of services @ 05/12/23 13:46:55.815
  May 12 13:46:55.815: INFO: Creating e2e-svc-a-4p2sr
  May 12 13:46:55.824: INFO: Creating e2e-svc-b-6cq8g
  May 12 13:46:55.835: INFO: Creating e2e-svc-c-qxh2w
  STEP: deleting service collection @ 05/12/23 13:46:55.845
  May 12 13:46:55.876: INFO: Collection of services has been deleted
  May 12 13:46:55.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6678" for this suite. @ 05/12/23 13:46:55.878
• [0.088 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 05/12/23 13:46:55.89
  May 12 13:46:55.890: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 13:46:55.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:46:55.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:46:55.905
  STEP: Creating pod busybox-8becafe8-f543-4e81-9349-793499e8890f in namespace container-probe-3848 @ 05/12/23 13:46:55.906
  E0512 13:46:56.197334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:57.197014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:46:57.927: INFO: Started pod busybox-8becafe8-f543-4e81-9349-793499e8890f in namespace container-probe-3848
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/23 13:46:57.928
  May 12 13:46:57.934: INFO: Initial restart count of pod busybox-8becafe8-f543-4e81-9349-793499e8890f is 0
  E0512 13:46:58.198122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:46:59.198953      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:00.200091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:01.199711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:02.202327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:03.206245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:04.206824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:05.207455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:06.207990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:07.208084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:08.208448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:09.208956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:10.209702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:11.209888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:12.210267      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:13.211339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:14.212048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:15.212780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:16.213208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:17.215572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:18.214982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:19.215476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:20.215603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:21.216505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:22.216932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:23.217303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:24.217466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:25.217889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:26.218855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:27.219261      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:28.220144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:29.220494      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:30.221155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:31.220716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:32.221646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:33.222076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:34.222924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:35.223406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:36.224208      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:37.224578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:38.224929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:39.224934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:40.226615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:41.227299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:42.227344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:43.228704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:44.229091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:45.229748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:46.230139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:47.231298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:48.232483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:49.232768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:50.233965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:51.234263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:52.234781      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:53.234818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:54.235550      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:55.236035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:56.236824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:57.236833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:58.238118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:47:59.238936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:00.246257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:01.246704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:02.250209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:03.247346      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:04.250437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:05.251359      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:06.251889      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:07.252624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:08.253911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:09.255790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:10.255949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:11.255973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:12.258559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:13.258923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:14.259579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:15.266770      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:16.266715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:17.267755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:18.268084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:19.268534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:20.269673      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:21.270456      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:22.270367      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:23.270921      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:24.271986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:25.271941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:26.272961      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:27.273607      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:28.274726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:29.275012      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:30.275238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:31.275626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:32.276306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:33.276536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:34.279028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:35.276851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:36.276969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:37.277234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:38.277777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:39.282477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:40.283155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:41.283796      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:42.284726      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:43.284871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:44.285100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:45.286028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:46.285592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:47.285905      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:48.286866      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:49.287196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:50.287730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:51.287537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:52.287648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:53.288159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:54.288755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:55.289823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:56.290125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:57.290329      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:58.290380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:48:59.290412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:00.291236      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:01.291507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:02.291559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:03.292472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:04.292952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:05.293848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:06.293151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:07.293862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:08.294808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:09.295615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:10.295565      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:11.295956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:12.296828      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:13.297227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:14.297516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:15.297493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:16.297731      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:17.297815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:18.298957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:19.299893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:20.300671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:21.301493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:22.301736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:23.302091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:24.303180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:25.303525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:26.303608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:27.305256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:28.303936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:29.304281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:30.304283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:31.305352      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:32.305480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:33.306321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:34.306574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:35.306920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:36.307081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:37.307649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:38.308150      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:39.309075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:40.309362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:41.309557      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:42.309678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:43.310196      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:44.310624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:45.313119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:46.313454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:47.313823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:48.314495      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:49.314769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:50.315627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:51.315848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:52.315881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:53.316061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:54.316841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:55.317242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:56.317502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:57.317934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:58.318281      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:49:59.318661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:00.323430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:01.323522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:02.324285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:03.324423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:04.324768      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:05.324995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:06.325073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:07.325977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:08.325919      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:09.326711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:10.327324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:11.327648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:12.327910      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:13.329002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:14.333446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:15.334321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:16.334311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:17.334540      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:18.335128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:19.335647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:20.336257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:21.336721      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:22.336793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:23.337188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:24.338025      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:25.337964      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:26.337938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:27.338802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:28.339508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:29.339674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:30.340524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:31.341678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:32.341777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:33.342131      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:34.342684      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:35.343244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:36.343428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:37.343529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:38.344082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:39.347454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:40.347184      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:41.348373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:42.348361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:43.349009      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:44.354552      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:45.355118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:46.354577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:47.354691      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:48.355244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:49.356723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:50.357475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:51.358694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:52.359108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:53.360387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:54.360397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:55.360926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:56.361619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:57.362598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:50:58.363399      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:50:58.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 13:50:58.847
  STEP: Destroying namespace "container-probe-3848" for this suite. @ 05/12/23 13:50:58.861
• [242.978 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 05/12/23 13:50:58.872
  May 12 13:50:58.872: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 13:50:58.873
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:50:58.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:50:58.887
  May 12 13:50:58.889: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-2777 version'
  May 12 13:50:58.930: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  May 12 13:50:58.930: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-14T13:21:19Z\", GoVersion:\"go1.20.3\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.1+rke2r1\", GitCommit:\"4c9411232e10168d7b050c49a1b59f6df9d7ea4b\", GitTreeState:\"clean\", BuildDate:\"2023-04-18T00:13:25Z\", GoVersion:\"go1.20.3 X:boringcrypto\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  May 12 13:50:58.930: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2777" for this suite. @ 05/12/23 13:50:58.934
• [0.067 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 05/12/23 13:50:58.939
  May 12 13:50:58.939: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 13:50:58.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:50:58.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:50:58.954
  STEP: Creating pod test-webserver-e4b252f5-ae21-48a6-8a69-01a6464ea2a5 in namespace container-probe-75 @ 05/12/23 13:50:58.955
  E0512 13:50:59.364020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:00.364648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:51:00.968: INFO: Started pod test-webserver-e4b252f5-ae21-48a6-8a69-01a6464ea2a5 in namespace container-probe-75
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/23 13:51:00.968
  May 12 13:51:00.970: INFO: Initial restart count of pod test-webserver-e4b252f5-ae21-48a6-8a69-01a6464ea2a5 is 0
  E0512 13:51:01.364922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:02.365827      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:03.365548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:04.365772      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:05.366246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:06.366561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:07.367425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:08.367857      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:09.368682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:10.368807      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:11.369378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:12.369553      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:13.369967      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:14.370398      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:15.371120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:16.370737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:17.370952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:18.371368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:19.372223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:20.373427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:21.373534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:22.373476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:23.374720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:24.375210      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:25.375801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:26.376765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:27.377143      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:28.377575      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:29.378437      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:30.378811      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:31.379647      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:32.379760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:33.380940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:34.381683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:35.381932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:36.382541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:37.383513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:38.383652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:39.384789      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:40.386815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:41.387765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:42.387872      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:43.387989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:44.388996      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:45.389865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:46.389305      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:47.389651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:48.390054      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:49.390559      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:50.390713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:51.390990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:52.391136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:53.391505      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:54.392404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:55.391918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:56.393497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:57.393916      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:58.395011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:51:59.396232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:00.396901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:01.397646      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:02.397966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:03.398966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:04.399642      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:05.403255      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:06.403223      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:07.403650      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:08.404846      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:09.405126      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:10.405862      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:11.406355      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:12.406251      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:13.413422      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:14.413460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:15.413578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:16.413839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:17.414259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:18.415634      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:19.415787      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:20.416224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:21.416332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:22.416669      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:23.419378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:24.419674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:25.420459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:26.420416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:27.421288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:28.421780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:29.422307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:30.422598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:31.423706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:32.423679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:33.424605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:34.424758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:35.424831      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:36.425905      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:37.426157      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:38.426706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:39.427395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:40.428111      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:41.428622      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:42.428627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:43.429747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:44.429873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:45.430485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:46.430472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:47.430912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:48.431460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:49.431810      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:50.432695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:51.433527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:52.434480      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:53.434758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:54.435078      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:55.435233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:56.435447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:57.435665      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:58.436095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:52:59.437250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:00.437511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:01.437775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:02.437861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:03.438865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:04.438792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:05.440909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:06.440080      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:07.440350      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:08.441981      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:09.442536      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:10.442463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:11.443518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:12.443992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:13.445112      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:14.445099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:15.445640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:16.445881      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:17.446339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:18.446769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:19.446979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:20.447142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:21.448191      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:22.448643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:23.448990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:24.449478      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:25.449968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:26.449976      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:27.450419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:28.450593      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:29.450783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:30.451823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:31.453568      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:32.452653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:33.452991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:34.454011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:35.454688      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:36.455706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:37.455851      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:38.456093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:39.456282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:40.456707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:41.457750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:42.458753      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:43.459115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:44.459136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:45.459423      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:46.460438      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:47.460720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:48.461926      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:49.462605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:50.463018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:51.463357      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:52.463538      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:53.470406      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:54.470472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:55.470736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:56.471035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:57.471105      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:58.472882      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:53:59.473180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:00.473263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:01.473510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:02.473982      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:03.474325      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:04.475076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:05.475824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:06.475711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:07.476499      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:08.477510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:09.478625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:10.478169      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:11.478301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:12.478384      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:13.479085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:14.482903      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:15.483042      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:16.484417      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:17.485039      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:18.485444      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:19.485465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:20.485878      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:21.486454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:22.486446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:23.486607      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:24.487420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:25.487918      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:26.488299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:27.488822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:28.489468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:29.489662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:30.489885      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:31.491443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:32.490627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:33.491792      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:34.492026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:35.492014      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:36.492159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:37.492403      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:38.492873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:39.494471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:40.495142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:41.495430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:42.495502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:43.495935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:44.496934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:45.497482      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:46.497968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:47.498944      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:48.499016      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:49.499577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:50.499724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:51.500334      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:52.500574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:53.500961      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:54.501321      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:55.501715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:56.502044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:57.502261      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:58.502296      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:54:59.502592      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:00.502952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:01.504056      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:01.743: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 13:55:01.752
  STEP: Destroying namespace "container-probe-75" for this suite. @ 05/12/23 13:55:01.767
• [242.835 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 05/12/23 13:55:01.775
  May 12 13:55:01.775: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename statefulset @ 05/12/23 13:55:01.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:01.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:01.789
  STEP: Creating service test in namespace statefulset-6812 @ 05/12/23 13:55:01.791
  STEP: Creating statefulset ss in namespace statefulset-6812 @ 05/12/23 13:55:01.801
  May 12 13:55:01.814: INFO: Found 0 stateful pods, waiting for 1
  E0512 13:55:02.504832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:03.505264      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:04.505441      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:05.506266      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:06.506892      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:07.506442      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:08.506886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:09.507465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:10.507805      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:11.508501      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:11.824: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 05/12/23 13:55:11.841
  STEP: Getting /status @ 05/12/23 13:55:11.857
  May 12 13:55:11.862: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 05/12/23 13:55:11.862
  May 12 13:55:11.871: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 05/12/23 13:55:11.871
  May 12 13:55:11.872: INFO: Observed &StatefulSet event: ADDED
  May 12 13:55:11.873: INFO: Found Statefulset ss in namespace statefulset-6812 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 12 13:55:11.873: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 05/12/23 13:55:11.873
  May 12 13:55:11.873: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  May 12 13:55:11.878: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 05/12/23 13:55:11.878
  May 12 13:55:11.879: INFO: Observed &StatefulSet event: ADDED
  May 12 13:55:11.879: INFO: Observed Statefulset ss in namespace statefulset-6812 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  May 12 13:55:11.879: INFO: Observed &StatefulSet event: MODIFIED
  May 12 13:55:11.879: INFO: Deleting all statefulset in ns statefulset-6812
  May 12 13:55:11.881: INFO: Scaling statefulset ss to 0
  E0512 13:55:12.509032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:13.509817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:14.509865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:15.509743      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:16.510266      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:17.510723      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:18.510986      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:19.511102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:20.511485      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:21.512145      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:21.892: INFO: Waiting for statefulset status.replicas updated to 0
  May 12 13:55:21.895: INFO: Deleting statefulset ss
  May 12 13:55:21.917: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6812" for this suite. @ 05/12/23 13:55:21.924
• [20.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 05/12/23 13:55:21.938
  May 12 13:55:21.938: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename conformance-tests @ 05/12/23 13:55:21.939
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:21.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:21.954
  STEP: Getting node addresses @ 05/12/23 13:55:21.956
  May 12 13:55:21.956: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  May 12 13:55:21.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-3413" for this suite. @ 05/12/23 13:55:21.962
• [0.028 seconds]
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 05/12/23 13:55:21.967
  May 12 13:55:21.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 13:55:21.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:21.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:21.981
  STEP: Creating secret with name secret-test-map-ab1cb52f-e5d4-40b1-90dc-5a5fd7ebfa7c @ 05/12/23 13:55:21.982
  STEP: Creating a pod to test consume secrets @ 05/12/23 13:55:21.984
  E0512 13:55:22.512790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:23.513372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:24.524998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:25.520093      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:55:26.017
  May 12 13:55:26.030: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-secrets-9f0fcbbc-a397-493c-a724-af4669563dd7 container secret-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:55:26.054
  May 12 13:55:26.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3491" for this suite. @ 05/12/23 13:55:26.07
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 05/12/23 13:55:26.077
  May 12 13:55:26.077: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename svc-latency @ 05/12/23 13:55:26.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:26.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:26.091
  May 12 13:55:26.092: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-1300 @ 05/12/23 13:55:26.093
  I0512 13:55:26.096770      20 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-1300, replica count: 1
  E0512 13:55:26.519869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0512 13:55:27.150513      20 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  May 12 13:55:27.263: INFO: Created: latency-svc-c7vw9
  May 12 13:55:27.282: INFO: Got endpoints: latency-svc-c7vw9 [31.184186ms]
  May 12 13:55:27.321: INFO: Created: latency-svc-bc2t7
  May 12 13:55:27.324: INFO: Got endpoints: latency-svc-bc2t7 [37.3853ms]
  May 12 13:55:27.388: INFO: Created: latency-svc-2ksql
  May 12 13:55:27.424: INFO: Created: latency-svc-dhdvs
  May 12 13:55:27.425: INFO: Created: latency-svc-nh9x8
  May 12 13:55:27.425: INFO: Created: latency-svc-5s695
  May 12 13:55:27.434: INFO: Got endpoints: latency-svc-2ksql [144.383903ms]
  May 12 13:55:27.470: INFO: Created: latency-svc-hzdb7
  May 12 13:55:27.470: INFO: Created: latency-svc-5fzph
  May 12 13:55:27.491: INFO: Got endpoints: latency-svc-5fzph [201.118766ms]
  May 12 13:55:27.471: INFO: Created: latency-svc-dmkbj
  May 12 13:55:27.492: INFO: Got endpoints: latency-svc-dmkbj [202.182846ms]
  May 12 13:55:27.471: INFO: Created: latency-svc-m268v
  May 12 13:55:27.493: INFO: Got endpoints: latency-svc-m268v [203.961403ms]
  May 12 13:55:27.471: INFO: Created: latency-svc-d8c5j
  May 12 13:55:27.494: INFO: Got endpoints: latency-svc-d8c5j [203.986233ms]
  May 12 13:55:27.472: INFO: Created: latency-svc-sczrh
  May 12 13:55:27.494: INFO: Got endpoints: latency-svc-sczrh [205.345104ms]
  May 12 13:55:27.472: INFO: Created: latency-svc-c58vr
  May 12 13:55:27.495: INFO: Got endpoints: latency-svc-c58vr [205.794735ms]
  May 12 13:55:27.472: INFO: Created: latency-svc-xbl44
  May 12 13:55:27.496: INFO: Got endpoints: latency-svc-xbl44 [207.551344ms]
  May 12 13:55:27.472: INFO: Created: latency-svc-5knd2
  May 12 13:55:27.497: INFO: Got endpoints: latency-svc-5knd2 [209.932295ms]
  May 12 13:55:27.473: INFO: Created: latency-svc-rbpjz
  May 12 13:55:27.498: INFO: Got endpoints: latency-svc-rbpjz [211.311566ms]
  May 12 13:55:27.473: INFO: Created: latency-svc-j7spf
  May 12 13:55:27.499: INFO: Got endpoints: latency-svc-j7spf [212.5729ms]
  May 12 13:55:27.484: INFO: Created: latency-svc-n7vp8
  May 12 13:55:27.501: INFO: Got endpoints: latency-svc-n7vp8 [67.011345ms]
  May 12 13:55:27.487: INFO: Got endpoints: latency-svc-hzdb7 [160.819305ms]
  May 12 13:55:27.487: INFO: Got endpoints: latency-svc-5s695 [198.077855ms]
  May 12 13:55:27.487: INFO: Got endpoints: latency-svc-dhdvs [197.786175ms]
  May 12 13:55:27.487: INFO: Got endpoints: latency-svc-nh9x8 [197.68277ms]
  May 12 13:55:27.510: INFO: Created: latency-svc-cwplw
  May 12 13:55:27.511: INFO: Got endpoints: latency-svc-cwplw [19.665213ms]
  E0512 13:55:27.519949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:27.585: INFO: Created: latency-svc-xqftl
  May 12 13:55:27.586: INFO: Created: latency-svc-4znxl
  May 12 13:55:27.609: INFO: Created: latency-svc-x2ggt
  May 12 13:55:27.586: INFO: Created: latency-svc-hmz6s
  May 12 13:55:27.586: INFO: Created: latency-svc-gvfrc
  May 12 13:55:27.586: INFO: Created: latency-svc-qqkvh
  May 12 13:55:27.608: INFO: Created: latency-svc-gqmft
  May 12 13:55:27.608: INFO: Created: latency-svc-t226r
  May 12 13:55:27.608: INFO: Created: latency-svc-kgbbq
  May 12 13:55:27.609: INFO: Created: latency-svc-84gp7
  May 12 13:55:27.609: INFO: Created: latency-svc-ffvpl
  May 12 13:55:27.609: INFO: Created: latency-svc-qz7pn
  May 12 13:55:27.609: INFO: Created: latency-svc-sq8vx
  May 12 13:55:27.609: INFO: Created: latency-svc-v6frc
  May 12 13:55:27.610: INFO: Created: latency-svc-fnxvb
  May 12 13:55:27.610: INFO: Got endpoints: latency-svc-xqftl [107.671963ms]
  May 12 13:55:27.611: INFO: Got endpoints: latency-svc-4znxl [103.032166ms]
  May 12 13:55:27.612: INFO: Got endpoints: latency-svc-x2ggt [100.797419ms]
  May 12 13:55:27.612: INFO: Got endpoints: latency-svc-hmz6s [104.192059ms]
  May 12 13:55:27.612: INFO: Got endpoints: latency-svc-gvfrc [117.450993ms]
  May 12 13:55:27.612: INFO: Got endpoints: latency-svc-qqkvh [112.426723ms]
  May 12 13:55:27.613: INFO: Got endpoints: latency-svc-gqmft [115.852769ms]
  May 12 13:55:27.613: INFO: Got endpoints: latency-svc-t226r [116.541481ms]
  May 12 13:55:27.613: INFO: Got endpoints: latency-svc-kgbbq [117.261549ms]
  May 12 13:55:27.613: INFO: Got endpoints: latency-svc-84gp7 [121.237305ms]
  May 12 13:55:27.613: INFO: Got endpoints: latency-svc-ffvpl [106.163435ms]
  May 12 13:55:27.614: INFO: Got endpoints: latency-svc-qz7pn [119.953801ms]
  May 12 13:55:27.614: INFO: Got endpoints: latency-svc-sq8vx [120.682829ms]
  May 12 13:55:27.614: INFO: Got endpoints: latency-svc-v6frc [104.261064ms]
  May 12 13:55:27.614: INFO: Got endpoints: latency-svc-fnxvb [115.443393ms]
  May 12 13:55:27.622: INFO: Created: latency-svc-f8k7q
  May 12 13:55:27.631: INFO: Got endpoints: latency-svc-f8k7q [16.158986ms]
  May 12 13:55:27.638: INFO: Created: latency-svc-tbltd
  May 12 13:55:27.638: INFO: Created: latency-svc-thqjn
  May 12 13:55:27.647: INFO: Got endpoints: latency-svc-tbltd [32.157142ms]
  May 12 13:55:27.652: INFO: Got endpoints: latency-svc-thqjn [35.771437ms]
  May 12 13:55:27.654: INFO: Created: latency-svc-w8g9s
  May 12 13:55:27.660: INFO: Created: latency-svc-lq4mf
  May 12 13:55:27.662: INFO: Created: latency-svc-mdxfd
  May 12 13:55:27.669: INFO: Created: latency-svc-pnxqm
  May 12 13:55:27.674: INFO: Created: latency-svc-n957l
  May 12 13:55:27.678: INFO: Created: latency-svc-kb6fz
  May 12 13:55:27.685: INFO: Created: latency-svc-8lhhs
  May 12 13:55:27.690: INFO: Created: latency-svc-vm6r6
  May 12 13:55:27.696: INFO: Got endpoints: latency-svc-w8g9s [80.263264ms]
  May 12 13:55:27.697: INFO: Created: latency-svc-jtt6z
  May 12 13:55:27.701: INFO: Created: latency-svc-v48d7
  May 12 13:55:27.705: INFO: Created: latency-svc-gr8w8
  May 12 13:55:27.708: INFO: Created: latency-svc-m245q
  May 12 13:55:27.713: INFO: Created: latency-svc-f2529
  May 12 13:55:27.718: INFO: Created: latency-svc-89bzq
  May 12 13:55:27.722: INFO: Created: latency-svc-g44rs
  May 12 13:55:27.725: INFO: Created: latency-svc-dsz9v
  May 12 13:55:27.740: INFO: Got endpoints: latency-svc-lq4mf [123.987235ms]
  May 12 13:55:27.750: INFO: Created: latency-svc-tdnpb
  May 12 13:55:27.798: INFO: Got endpoints: latency-svc-mdxfd [181.099276ms]
  May 12 13:55:27.849: INFO: Created: latency-svc-2k47k
  May 12 13:55:27.875: INFO: Got endpoints: latency-svc-pnxqm [257.908491ms]
  May 12 13:55:27.902: INFO: Created: latency-svc-zhs56
  May 12 13:55:27.910: INFO: Got endpoints: latency-svc-n957l [292.147611ms]
  May 12 13:55:27.945: INFO: Got endpoints: latency-svc-kb6fz [317.813119ms]
  May 12 13:55:27.970: INFO: Created: latency-svc-2nb7k
  May 12 13:55:27.971: INFO: Created: latency-svc-5cbgr
  May 12 13:55:27.992: INFO: Got endpoints: latency-svc-8lhhs [364.624619ms]
  May 12 13:55:28.000: INFO: Created: latency-svc-xcclc
  May 12 13:55:28.052: INFO: Got endpoints: latency-svc-vm6r6 [423.805254ms]
  May 12 13:55:28.069: INFO: Created: latency-svc-9zkqh
  May 12 13:55:28.106: INFO: Got endpoints: latency-svc-jtt6z [477.995867ms]
  May 12 13:55:28.120: INFO: Created: latency-svc-bvjms
  May 12 13:55:28.144: INFO: Got endpoints: latency-svc-v48d7 [515.654932ms]
  May 12 13:55:28.157: INFO: Created: latency-svc-7wcql
  May 12 13:55:28.192: INFO: Got endpoints: latency-svc-gr8w8 [563.618084ms]
  May 12 13:55:28.200: INFO: Created: latency-svc-vmkwm
  May 12 13:55:28.242: INFO: Got endpoints: latency-svc-m245q [617.235462ms]
  May 12 13:55:28.249: INFO: Created: latency-svc-q2c9x
  May 12 13:55:28.291: INFO: Got endpoints: latency-svc-f2529 [659.842654ms]
  May 12 13:55:28.304: INFO: Created: latency-svc-hfp4r
  May 12 13:55:28.344: INFO: Got endpoints: latency-svc-89bzq [696.966583ms]
  May 12 13:55:28.350: INFO: Created: latency-svc-n585j
  May 12 13:55:28.401: INFO: Got endpoints: latency-svc-g44rs [748.639343ms]
  May 12 13:55:28.424: INFO: Created: latency-svc-9v2ln
  May 12 13:55:28.445: INFO: Got endpoints: latency-svc-dsz9v [748.110023ms]
  May 12 13:55:28.455: INFO: Created: latency-svc-4l7mw
  May 12 13:55:28.512: INFO: Got endpoints: latency-svc-tdnpb [771.768964ms]
  E0512 13:55:28.520048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:28.529: INFO: Created: latency-svc-j6tts
  May 12 13:55:28.542: INFO: Got endpoints: latency-svc-2k47k [741.931074ms]
  May 12 13:55:28.552: INFO: Created: latency-svc-vwpgs
  May 12 13:55:28.590: INFO: Got endpoints: latency-svc-zhs56 [714.231935ms]
  May 12 13:55:28.598: INFO: Created: latency-svc-wrmrm
  May 12 13:55:28.663: INFO: Got endpoints: latency-svc-2nb7k [742.490312ms]
  May 12 13:55:28.679: INFO: Created: latency-svc-vm56c
  May 12 13:55:28.697: INFO: Got endpoints: latency-svc-5cbgr [751.149009ms]
  May 12 13:55:28.713: INFO: Created: latency-svc-7666w
  May 12 13:55:28.744: INFO: Got endpoints: latency-svc-xcclc [750.851162ms]
  May 12 13:55:28.752: INFO: Created: latency-svc-4h69v
  May 12 13:55:28.804: INFO: Got endpoints: latency-svc-9zkqh [750.109031ms]
  May 12 13:55:28.819: INFO: Created: latency-svc-cz4ck
  May 12 13:55:28.843: INFO: Got endpoints: latency-svc-bvjms [735.774982ms]
  May 12 13:55:28.852: INFO: Created: latency-svc-79pbs
  May 12 13:55:28.892: INFO: Got endpoints: latency-svc-7wcql [747.809481ms]
  May 12 13:55:28.900: INFO: Created: latency-svc-xxs8q
  May 12 13:55:28.965: INFO: Got endpoints: latency-svc-vmkwm [772.499965ms]
  May 12 13:55:28.980: INFO: Created: latency-svc-588hw
  May 12 13:55:28.994: INFO: Got endpoints: latency-svc-q2c9x [751.919273ms]
  May 12 13:55:29.002: INFO: Created: latency-svc-55xhk
  May 12 13:55:29.042: INFO: Got endpoints: latency-svc-hfp4r [750.993914ms]
  May 12 13:55:29.054: INFO: Created: latency-svc-gr74r
  May 12 13:55:29.093: INFO: Got endpoints: latency-svc-n585j [748.052542ms]
  May 12 13:55:29.100: INFO: Created: latency-svc-9gdfn
  May 12 13:55:29.142: INFO: Got endpoints: latency-svc-9v2ln [741.14599ms]
  May 12 13:55:29.151: INFO: Created: latency-svc-26krm
  May 12 13:55:29.220: INFO: Got endpoints: latency-svc-4l7mw [774.901456ms]
  May 12 13:55:29.229: INFO: Created: latency-svc-hbj8c
  May 12 13:55:29.242: INFO: Got endpoints: latency-svc-j6tts [730.134856ms]
  May 12 13:55:29.249: INFO: Created: latency-svc-zpv6t
  May 12 13:55:29.294: INFO: Got endpoints: latency-svc-vwpgs [748.756256ms]
  May 12 13:55:29.305: INFO: Created: latency-svc-99bzf
  May 12 13:55:29.340: INFO: Got endpoints: latency-svc-wrmrm [749.313251ms]
  May 12 13:55:29.349: INFO: Created: latency-svc-v88hj
  May 12 13:55:29.390: INFO: Got endpoints: latency-svc-vm56c [726.672752ms]
  May 12 13:55:29.400: INFO: Created: latency-svc-pkwhh
  May 12 13:55:29.459: INFO: Got endpoints: latency-svc-7666w [760.372816ms]
  May 12 13:55:29.477: INFO: Created: latency-svc-7hl7h
  May 12 13:55:29.492: INFO: Got endpoints: latency-svc-4h69v [748.296311ms]
  May 12 13:55:29.501: INFO: Created: latency-svc-zzs4s
  E0512 13:55:29.520860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:29.543: INFO: Got endpoints: latency-svc-cz4ck [738.334794ms]
  May 12 13:55:29.550: INFO: Created: latency-svc-q9zrq
  May 12 13:55:29.612: INFO: Got endpoints: latency-svc-79pbs [768.977739ms]
  May 12 13:55:29.631: INFO: Created: latency-svc-jmn6s
  May 12 13:55:29.644: INFO: Got endpoints: latency-svc-xxs8q [751.678935ms]
  May 12 13:55:29.651: INFO: Created: latency-svc-jkrfv
  May 12 13:55:29.693: INFO: Got endpoints: latency-svc-588hw [727.312359ms]
  May 12 13:55:29.702: INFO: Created: latency-svc-vp57t
  May 12 13:55:29.744: INFO: Got endpoints: latency-svc-55xhk [749.08642ms]
  May 12 13:55:29.752: INFO: Created: latency-svc-7479c
  May 12 13:55:29.812: INFO: Got endpoints: latency-svc-gr74r [767.831262ms]
  May 12 13:55:29.824: INFO: Created: latency-svc-7p8cd
  May 12 13:55:29.843: INFO: Got endpoints: latency-svc-9gdfn [750.542832ms]
  May 12 13:55:29.852: INFO: Created: latency-svc-w24j2
  May 12 13:55:29.894: INFO: Got endpoints: latency-svc-26krm [751.987244ms]
  May 12 13:55:29.904: INFO: Created: latency-svc-x75qc
  May 12 13:55:29.943: INFO: Got endpoints: latency-svc-hbj8c [722.288451ms]
  May 12 13:55:29.951: INFO: Created: latency-svc-g9l2p
  May 12 13:55:29.993: INFO: Got endpoints: latency-svc-zpv6t [750.309512ms]
  May 12 13:55:30.002: INFO: Created: latency-svc-9mzwp
  May 12 13:55:30.042: INFO: Got endpoints: latency-svc-99bzf [748.459639ms]
  May 12 13:55:30.051: INFO: Created: latency-svc-42jkt
  May 12 13:55:30.094: INFO: Got endpoints: latency-svc-v88hj [753.498957ms]
  May 12 13:55:30.102: INFO: Created: latency-svc-shrkl
  May 12 13:55:30.143: INFO: Got endpoints: latency-svc-pkwhh [752.414582ms]
  May 12 13:55:30.151: INFO: Created: latency-svc-8phr8
  May 12 13:55:30.193: INFO: Got endpoints: latency-svc-7hl7h [733.499902ms]
  May 12 13:55:30.200: INFO: Created: latency-svc-cjwzc
  May 12 13:55:30.244: INFO: Got endpoints: latency-svc-zzs4s [751.682681ms]
  May 12 13:55:30.252: INFO: Created: latency-svc-pdt49
  May 12 13:55:30.293: INFO: Got endpoints: latency-svc-q9zrq [749.076042ms]
  May 12 13:55:30.306: INFO: Created: latency-svc-k6vwv
  May 12 13:55:30.343: INFO: Got endpoints: latency-svc-jmn6s [728.897774ms]
  May 12 13:55:30.351: INFO: Created: latency-svc-2nvkj
  May 12 13:55:30.393: INFO: Got endpoints: latency-svc-jkrfv [749.085593ms]
  May 12 13:55:30.401: INFO: Created: latency-svc-88l74
  May 12 13:55:30.442: INFO: Got endpoints: latency-svc-vp57t [748.918056ms]
  May 12 13:55:30.450: INFO: Created: latency-svc-xljf6
  May 12 13:55:30.492: INFO: Got endpoints: latency-svc-7479c [748.370724ms]
  May 12 13:55:30.501: INFO: Created: latency-svc-2qx7f
  E0512 13:55:30.521541      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:30.543: INFO: Got endpoints: latency-svc-7p8cd [730.754712ms]
  May 12 13:55:30.554: INFO: Created: latency-svc-jd7m8
  May 12 13:55:30.597: INFO: Got endpoints: latency-svc-w24j2 [753.609061ms]
  May 12 13:55:30.609: INFO: Created: latency-svc-lcz49
  May 12 13:55:30.640: INFO: Got endpoints: latency-svc-x75qc [745.955375ms]
  May 12 13:55:30.651: INFO: Created: latency-svc-k62rx
  May 12 13:55:30.692: INFO: Got endpoints: latency-svc-g9l2p [749.175057ms]
  May 12 13:55:30.698: INFO: Created: latency-svc-blsh4
  May 12 13:55:30.743: INFO: Got endpoints: latency-svc-9mzwp [749.831805ms]
  May 12 13:55:30.749: INFO: Created: latency-svc-9bcpn
  May 12 13:55:30.793: INFO: Got endpoints: latency-svc-42jkt [750.241857ms]
  May 12 13:55:30.800: INFO: Created: latency-svc-9t9gv
  May 12 13:55:30.845: INFO: Got endpoints: latency-svc-shrkl [750.627306ms]
  May 12 13:55:30.851: INFO: Created: latency-svc-5mg9s
  May 12 13:55:30.893: INFO: Got endpoints: latency-svc-8phr8 [749.542209ms]
  May 12 13:55:30.902: INFO: Created: latency-svc-tmgnz
  May 12 13:55:30.943: INFO: Got endpoints: latency-svc-cjwzc [749.782254ms]
  May 12 13:55:30.952: INFO: Created: latency-svc-w2wms
  May 12 13:55:30.992: INFO: Got endpoints: latency-svc-pdt49 [747.59961ms]
  May 12 13:55:31.000: INFO: Created: latency-svc-n5xgw
  May 12 13:55:31.044: INFO: Got endpoints: latency-svc-k6vwv [750.992261ms]
  May 12 13:55:31.052: INFO: Created: latency-svc-wnc5k
  May 12 13:55:31.096: INFO: Got endpoints: latency-svc-2nvkj [752.075742ms]
  May 12 13:55:31.106: INFO: Created: latency-svc-26n9z
  May 12 13:55:31.142: INFO: Got endpoints: latency-svc-88l74 [748.562056ms]
  May 12 13:55:31.150: INFO: Created: latency-svc-tjngs
  May 12 13:55:31.196: INFO: Got endpoints: latency-svc-xljf6 [754.272412ms]
  May 12 13:55:31.203: INFO: Created: latency-svc-jxtjg
  May 12 13:55:31.242: INFO: Got endpoints: latency-svc-2qx7f [749.938605ms]
  May 12 13:55:31.250: INFO: Created: latency-svc-4jdh4
  May 12 13:55:31.295: INFO: Got endpoints: latency-svc-jd7m8 [752.077216ms]
  May 12 13:55:31.328: INFO: Created: latency-svc-bc2wq
  May 12 13:55:31.341: INFO: Got endpoints: latency-svc-lcz49 [743.791446ms]
  May 12 13:55:31.356: INFO: Created: latency-svc-57chm
  May 12 13:55:31.392: INFO: Got endpoints: latency-svc-k62rx [751.54849ms]
  May 12 13:55:31.400: INFO: Created: latency-svc-n7g4b
  May 12 13:55:31.442: INFO: Got endpoints: latency-svc-blsh4 [750.486013ms]
  May 12 13:55:31.449: INFO: Created: latency-svc-qgbnj
  May 12 13:55:31.494: INFO: Got endpoints: latency-svc-9bcpn [751.12749ms]
  May 12 13:55:31.501: INFO: Created: latency-svc-cqldv
  E0512 13:55:31.521745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:31.544: INFO: Got endpoints: latency-svc-9t9gv [751.741976ms]
  May 12 13:55:31.552: INFO: Created: latency-svc-l49r6
  May 12 13:55:31.593: INFO: Got endpoints: latency-svc-5mg9s [748.715633ms]
  May 12 13:55:31.602: INFO: Created: latency-svc-7qbzc
  May 12 13:55:31.643: INFO: Got endpoints: latency-svc-tmgnz [750.38048ms]
  May 12 13:55:31.651: INFO: Created: latency-svc-h87gl
  May 12 13:55:31.694: INFO: Got endpoints: latency-svc-w2wms [751.065193ms]
  May 12 13:55:31.703: INFO: Created: latency-svc-976wj
  May 12 13:55:31.740: INFO: Got endpoints: latency-svc-n5xgw [746.443854ms]
  May 12 13:55:31.752: INFO: Created: latency-svc-99t94
  May 12 13:55:31.793: INFO: Got endpoints: latency-svc-wnc5k [748.717287ms]
  May 12 13:55:31.802: INFO: Created: latency-svc-pkppj
  May 12 13:55:31.842: INFO: Got endpoints: latency-svc-26n9z [746.75007ms]
  May 12 13:55:31.852: INFO: Created: latency-svc-zfc4w
  May 12 13:55:31.893: INFO: Got endpoints: latency-svc-tjngs [751.126869ms]
  May 12 13:55:31.901: INFO: Created: latency-svc-bkhcr
  May 12 13:55:31.944: INFO: Got endpoints: latency-svc-jxtjg [747.917314ms]
  May 12 13:55:31.953: INFO: Created: latency-svc-2pcvh
  May 12 13:55:31.991: INFO: Got endpoints: latency-svc-4jdh4 [748.567598ms]
  May 12 13:55:32.001: INFO: Created: latency-svc-lczz2
  May 12 13:55:32.045: INFO: Got endpoints: latency-svc-bc2wq [749.437973ms]
  May 12 13:55:32.055: INFO: Created: latency-svc-lqvmk
  May 12 13:55:32.094: INFO: Got endpoints: latency-svc-57chm [752.75612ms]
  May 12 13:55:32.108: INFO: Created: latency-svc-vwkdn
  May 12 13:55:32.151: INFO: Got endpoints: latency-svc-n7g4b [759.578776ms]
  May 12 13:55:32.169: INFO: Created: latency-svc-pjm2d
  May 12 13:55:32.206: INFO: Got endpoints: latency-svc-qgbnj [763.468342ms]
  May 12 13:55:32.220: INFO: Created: latency-svc-kj62w
  May 12 13:55:32.244: INFO: Got endpoints: latency-svc-cqldv [750.534184ms]
  May 12 13:55:32.256: INFO: Created: latency-svc-64wmz
  May 12 13:55:32.291: INFO: Got endpoints: latency-svc-l49r6 [746.173261ms]
  May 12 13:55:32.311: INFO: Created: latency-svc-qc4ct
  May 12 13:55:32.346: INFO: Got endpoints: latency-svc-7qbzc [752.475541ms]
  May 12 13:55:32.357: INFO: Created: latency-svc-ss5sf
  May 12 13:55:32.395: INFO: Got endpoints: latency-svc-h87gl [751.404023ms]
  May 12 13:55:32.404: INFO: Created: latency-svc-znvgk
  May 12 13:55:32.441: INFO: Got endpoints: latency-svc-976wj [746.267552ms]
  May 12 13:55:32.452: INFO: Created: latency-svc-2vj9r
  May 12 13:55:32.493: INFO: Got endpoints: latency-svc-99t94 [752.520327ms]
  May 12 13:55:32.500: INFO: Created: latency-svc-95lts
  E0512 13:55:32.522472      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:32.542: INFO: Got endpoints: latency-svc-pkppj [749.465854ms]
  May 12 13:55:32.552: INFO: Created: latency-svc-wgjtd
  May 12 13:55:32.592: INFO: Got endpoints: latency-svc-zfc4w [749.636954ms]
  May 12 13:55:32.599: INFO: Created: latency-svc-9dwbq
  May 12 13:55:32.641: INFO: Got endpoints: latency-svc-bkhcr [748.040872ms]
  May 12 13:55:32.651: INFO: Created: latency-svc-h79mx
  May 12 13:55:32.694: INFO: Got endpoints: latency-svc-2pcvh [749.473639ms]
  May 12 13:55:32.701: INFO: Created: latency-svc-bw5dc
  May 12 13:55:32.758: INFO: Got endpoints: latency-svc-lczz2 [767.236647ms]
  May 12 13:55:32.775: INFO: Created: latency-svc-7wwlc
  May 12 13:55:32.794: INFO: Got endpoints: latency-svc-lqvmk [748.969981ms]
  May 12 13:55:32.805: INFO: Created: latency-svc-jsz9x
  May 12 13:55:32.868: INFO: Got endpoints: latency-svc-vwkdn [773.860372ms]
  May 12 13:55:32.883: INFO: Created: latency-svc-8tfsz
  May 12 13:55:32.893: INFO: Got endpoints: latency-svc-pjm2d [741.636044ms]
  May 12 13:55:32.901: INFO: Created: latency-svc-b68bv
  May 12 13:55:32.942: INFO: Got endpoints: latency-svc-kj62w [736.041625ms]
  May 12 13:55:32.950: INFO: Created: latency-svc-gpfzl
  May 12 13:55:32.993: INFO: Got endpoints: latency-svc-64wmz [748.689998ms]
  May 12 13:55:33.002: INFO: Created: latency-svc-2d9dp
  May 12 13:55:33.051: INFO: Got endpoints: latency-svc-qc4ct [755.835396ms]
  May 12 13:55:33.075: INFO: Created: latency-svc-h2kl7
  May 12 13:55:33.092: INFO: Got endpoints: latency-svc-ss5sf [745.343748ms]
  May 12 13:55:33.102: INFO: Created: latency-svc-grrh9
  May 12 13:55:33.143: INFO: Got endpoints: latency-svc-znvgk [747.650465ms]
  May 12 13:55:33.151: INFO: Created: latency-svc-48qr9
  May 12 13:55:33.202: INFO: Got endpoints: latency-svc-2vj9r [761.254506ms]
  May 12 13:55:33.212: INFO: Created: latency-svc-gpztb
  May 12 13:55:33.243: INFO: Got endpoints: latency-svc-95lts [749.999618ms]
  May 12 13:55:33.250: INFO: Created: latency-svc-98x9t
  May 12 13:55:33.294: INFO: Got endpoints: latency-svc-wgjtd [751.57688ms]
  May 12 13:55:33.304: INFO: Created: latency-svc-zc94x
  May 12 13:55:33.342: INFO: Got endpoints: latency-svc-9dwbq [750.232084ms]
  May 12 13:55:33.350: INFO: Created: latency-svc-cfpnp
  May 12 13:55:33.394: INFO: Got endpoints: latency-svc-h79mx [753.03188ms]
  May 12 13:55:33.402: INFO: Created: latency-svc-kt4c6
  May 12 13:55:33.445: INFO: Got endpoints: latency-svc-bw5dc [750.947928ms]
  May 12 13:55:33.453: INFO: Created: latency-svc-qt2zr
  May 12 13:55:33.490: INFO: Got endpoints: latency-svc-7wwlc [728.912202ms]
  May 12 13:55:33.502: INFO: Created: latency-svc-g59dm
  E0512 13:55:33.522549      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:33.544: INFO: Got endpoints: latency-svc-jsz9x [750.305942ms]
  May 12 13:55:33.552: INFO: Created: latency-svc-95j7w
  May 12 13:55:33.611: INFO: Got endpoints: latency-svc-8tfsz [743.67503ms]
  May 12 13:55:33.629: INFO: Created: latency-svc-bbp7z
  May 12 13:55:33.646: INFO: Got endpoints: latency-svc-b68bv [752.402205ms]
  May 12 13:55:33.653: INFO: Created: latency-svc-kfmnx
  May 12 13:55:33.692: INFO: Got endpoints: latency-svc-gpfzl [750.193322ms]
  May 12 13:55:33.701: INFO: Created: latency-svc-hqh8w
  May 12 13:55:33.756: INFO: Got endpoints: latency-svc-2d9dp [763.06111ms]
  May 12 13:55:33.775: INFO: Created: latency-svc-q6zcr
  May 12 13:55:33.792: INFO: Got endpoints: latency-svc-h2kl7 [739.425621ms]
  May 12 13:55:33.801: INFO: Created: latency-svc-g5gpj
  May 12 13:55:33.841: INFO: Got endpoints: latency-svc-grrh9 [748.710929ms]
  May 12 13:55:33.851: INFO: Created: latency-svc-x5k4s
  May 12 13:55:33.909: INFO: Got endpoints: latency-svc-48qr9 [765.842763ms]
  May 12 13:55:33.926: INFO: Created: latency-svc-5jkrq
  May 12 13:55:33.944: INFO: Got endpoints: latency-svc-gpztb [740.677344ms]
  May 12 13:55:33.952: INFO: Created: latency-svc-7gfr5
  May 12 13:55:33.991: INFO: Got endpoints: latency-svc-98x9t [747.955865ms]
  May 12 13:55:33.999: INFO: Created: latency-svc-fczdq
  May 12 13:55:34.067: INFO: Got endpoints: latency-svc-zc94x [772.649484ms]
  May 12 13:55:34.082: INFO: Created: latency-svc-5n6k5
  May 12 13:55:34.093: INFO: Got endpoints: latency-svc-cfpnp [750.808463ms]
  May 12 13:55:34.101: INFO: Created: latency-svc-69cbw
  May 12 13:55:34.143: INFO: Got endpoints: latency-svc-kt4c6 [748.212358ms]
  May 12 13:55:34.150: INFO: Created: latency-svc-xwbhj
  May 12 13:55:34.207: INFO: Got endpoints: latency-svc-qt2zr [761.915547ms]
  May 12 13:55:34.218: INFO: Created: latency-svc-62rzs
  May 12 13:55:34.243: INFO: Got endpoints: latency-svc-g59dm [750.85567ms]
  May 12 13:55:34.252: INFO: Created: latency-svc-c6795
  May 12 13:55:34.301: INFO: Got endpoints: latency-svc-95j7w [756.59602ms]
  May 12 13:55:34.329: INFO: Created: latency-svc-j22b6
  May 12 13:55:34.346: INFO: Got endpoints: latency-svc-bbp7z [734.652798ms]
  May 12 13:55:34.355: INFO: Created: latency-svc-v8ffz
  May 12 13:55:34.393: INFO: Got endpoints: latency-svc-kfmnx [747.693191ms]
  May 12 13:55:34.401: INFO: Created: latency-svc-pcb7q
  May 12 13:55:34.442: INFO: Got endpoints: latency-svc-hqh8w [749.492178ms]
  May 12 13:55:34.451: INFO: Created: latency-svc-6qcwn
  May 12 13:55:34.513: INFO: Got endpoints: latency-svc-q6zcr [756.224597ms]
  E0512 13:55:34.524392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:34.532: INFO: Created: latency-svc-wx2q8
  May 12 13:55:34.542: INFO: Got endpoints: latency-svc-g5gpj [749.873716ms]
  May 12 13:55:34.551: INFO: Created: latency-svc-gq75p
  May 12 13:55:34.613: INFO: Got endpoints: latency-svc-x5k4s [771.829215ms]
  May 12 13:55:34.637: INFO: Created: latency-svc-hhrp5
  May 12 13:55:34.646: INFO: Got endpoints: latency-svc-5jkrq [737.450102ms]
  May 12 13:55:34.658: INFO: Created: latency-svc-xbdfw
  May 12 13:55:34.708: INFO: Got endpoints: latency-svc-7gfr5 [763.930672ms]
  May 12 13:55:34.724: INFO: Created: latency-svc-94xz2
  May 12 13:55:34.753: INFO: Got endpoints: latency-svc-fczdq [761.994946ms]
  May 12 13:55:34.765: INFO: Created: latency-svc-zzcf4
  May 12 13:55:34.795: INFO: Got endpoints: latency-svc-5n6k5 [727.811001ms]
  May 12 13:55:34.804: INFO: Created: latency-svc-8pdn7
  May 12 13:55:34.845: INFO: Got endpoints: latency-svc-69cbw [751.43513ms]
  May 12 13:55:34.855: INFO: Created: latency-svc-7bzv9
  May 12 13:55:34.893: INFO: Got endpoints: latency-svc-xwbhj [750.089317ms]
  May 12 13:55:34.902: INFO: Created: latency-svc-wgg6c
  May 12 13:55:34.944: INFO: Got endpoints: latency-svc-62rzs [736.939179ms]
  May 12 13:55:34.953: INFO: Created: latency-svc-9686r
  May 12 13:55:35.010: INFO: Got endpoints: latency-svc-c6795 [766.91572ms]
  May 12 13:55:35.029: INFO: Created: latency-svc-92f9j
  May 12 13:55:35.044: INFO: Got endpoints: latency-svc-j22b6 [731.304897ms]
  May 12 13:55:35.066: INFO: Created: latency-svc-hkzf7
  May 12 13:55:35.106: INFO: Got endpoints: latency-svc-v8ffz [759.670251ms]
  May 12 13:55:35.130: INFO: Created: latency-svc-hw5gq
  May 12 13:55:35.146: INFO: Got endpoints: latency-svc-pcb7q [752.42701ms]
  May 12 13:55:35.201: INFO: Got endpoints: latency-svc-6qcwn [758.873713ms]
  May 12 13:55:35.246: INFO: Got endpoints: latency-svc-wx2q8 [732.201908ms]
  May 12 13:55:35.294: INFO: Got endpoints: latency-svc-gq75p [751.860662ms]
  May 12 13:55:35.343: INFO: Got endpoints: latency-svc-hhrp5 [729.031989ms]
  May 12 13:55:35.403: INFO: Got endpoints: latency-svc-xbdfw [756.935839ms]
  May 12 13:55:35.461: INFO: Got endpoints: latency-svc-94xz2 [752.472868ms]
  May 12 13:55:35.509: INFO: Got endpoints: latency-svc-zzcf4 [755.864739ms]
  E0512 13:55:35.525434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:35.553: INFO: Got endpoints: latency-svc-8pdn7 [758.037044ms]
  May 12 13:55:35.597: INFO: Got endpoints: latency-svc-7bzv9 [751.792771ms]
  May 12 13:55:35.655: INFO: Got endpoints: latency-svc-wgg6c [762.115363ms]
  May 12 13:55:35.708: INFO: Got endpoints: latency-svc-9686r [764.384725ms]
  May 12 13:55:35.747: INFO: Got endpoints: latency-svc-92f9j [736.499134ms]
  May 12 13:55:35.793: INFO: Got endpoints: latency-svc-hkzf7 [748.629192ms]
  May 12 13:55:35.845: INFO: Got endpoints: latency-svc-hw5gq [738.641679ms]
  May 12 13:55:35.845: INFO: Latencies: [16.158986ms 19.665213ms 32.157142ms 35.771437ms 37.3853ms 67.011345ms 80.263264ms 100.797419ms 103.032166ms 104.192059ms 104.261064ms 106.163435ms 107.671963ms 112.426723ms 115.443393ms 115.852769ms 116.541481ms 117.261549ms 117.450993ms 119.953801ms 120.682829ms 121.237305ms 123.987235ms 144.383903ms 160.819305ms 181.099276ms 197.68277ms 197.786175ms 198.077855ms 201.118766ms 202.182846ms 203.961403ms 203.986233ms 205.345104ms 205.794735ms 207.551344ms 209.932295ms 211.311566ms 212.5729ms 257.908491ms 292.147611ms 317.813119ms 364.624619ms 423.805254ms 477.995867ms 515.654932ms 563.618084ms 617.235462ms 659.842654ms 696.966583ms 714.231935ms 722.288451ms 726.672752ms 727.312359ms 727.811001ms 728.897774ms 728.912202ms 729.031989ms 730.134856ms 730.754712ms 731.304897ms 732.201908ms 733.499902ms 734.652798ms 735.774982ms 736.041625ms 736.499134ms 736.939179ms 737.450102ms 738.334794ms 738.641679ms 739.425621ms 740.677344ms 741.14599ms 741.636044ms 741.931074ms 742.490312ms 743.67503ms 743.791446ms 745.343748ms 745.955375ms 746.173261ms 746.267552ms 746.443854ms 746.75007ms 747.59961ms 747.650465ms 747.693191ms 747.809481ms 747.917314ms 747.955865ms 748.040872ms 748.052542ms 748.110023ms 748.212358ms 748.296311ms 748.370724ms 748.459639ms 748.562056ms 748.567598ms 748.629192ms 748.639343ms 748.689998ms 748.710929ms 748.715633ms 748.717287ms 748.756256ms 748.918056ms 748.969981ms 749.076042ms 749.085593ms 749.08642ms 749.175057ms 749.313251ms 749.437973ms 749.465854ms 749.473639ms 749.492178ms 749.542209ms 749.636954ms 749.782254ms 749.831805ms 749.873716ms 749.938605ms 749.999618ms 750.089317ms 750.109031ms 750.193322ms 750.232084ms 750.241857ms 750.305942ms 750.309512ms 750.38048ms 750.486013ms 750.534184ms 750.542832ms 750.627306ms 750.808463ms 750.851162ms 750.85567ms 750.947928ms 750.992261ms 750.993914ms 751.065193ms 751.126869ms 751.12749ms 751.149009ms 751.404023ms 751.43513ms 751.54849ms 751.57688ms 751.678935ms 751.682681ms 751.741976ms 751.792771ms 751.860662ms 751.919273ms 751.987244ms 752.075742ms 752.077216ms 752.402205ms 752.414582ms 752.42701ms 752.472868ms 752.475541ms 752.520327ms 752.75612ms 753.03188ms 753.498957ms 753.609061ms 754.272412ms 755.835396ms 755.864739ms 756.224597ms 756.59602ms 756.935839ms 758.037044ms 758.873713ms 759.578776ms 759.670251ms 760.372816ms 761.254506ms 761.915547ms 761.994946ms 762.115363ms 763.06111ms 763.468342ms 763.930672ms 764.384725ms 765.842763ms 766.91572ms 767.236647ms 767.831262ms 768.977739ms 771.768964ms 771.829215ms 772.499965ms 772.649484ms 773.860372ms 774.901456ms]
  May 12 13:55:35.845: INFO: 50 %ile: 748.629192ms
  May 12 13:55:35.845: INFO: 90 %ile: 760.372816ms
  May 12 13:55:35.845: INFO: 99 %ile: 773.860372ms
  May 12 13:55:35.845: INFO: Total sample count: 200
  May 12 13:55:35.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-1300" for this suite. @ 05/12/23 13:55:35.85
• [9.778 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 05/12/23 13:55:35.855
  May 12 13:55:35.855: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename configmap @ 05/12/23 13:55:35.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:35.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:35.871
  STEP: Creating configMap with name configmap-test-volume-a9031643-47e1-4237-a7fb-fc389536f2bb @ 05/12/23 13:55:35.872
  STEP: Creating a pod to test consume configMaps @ 05/12/23 13:55:35.876
  E0512 13:55:36.525832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:37.526455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:38.527735      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:39.528156      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:55:39.911
  May 12 13:55:39.925: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-configmaps-abc36694-6cee-4b39-ae93-0243ab33a302 container configmap-volume-test: <nil>
  STEP: delete the pod @ 05/12/23 13:55:39.94
  May 12 13:55:39.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8650" for this suite. @ 05/12/23 13:55:39.956
• [4.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 05/12/23 13:55:39.966
  May 12 13:55:39.966: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:55:39.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:39.978
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:39.979
  STEP: Creating Pod @ 05/12/23 13:55:39.981
  E0512 13:55:40.528718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:41.529070      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 05/12/23 13:55:42.002
  May 12 13:55:42.002: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-7970 PodName:pod-sharedvolume-61f6481c-7c55-4e74-bea6-bf1b805ccbd0 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:55:42.002: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:55:42.004: INFO: ExecWithOptions: Clientset creation
  May 12 13:55:42.004: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/emptydir-7970/pods/pod-sharedvolume-61f6481c-7c55-4e74-bea6-bf1b805ccbd0/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  May 12 13:55:42.092: INFO: Exec stderr: ""
  May 12 13:55:42.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7970" for this suite. @ 05/12/23 13:55:42.106
• [2.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 05/12/23 13:55:42.126
  May 12 13:55:42.126: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 13:55:42.128
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:42.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:42.201
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 13:55:42.217
  E0512 13:55:42.529303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:43.529382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:44.530615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:45.530562      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:55:46.246
  May 12 13:55:46.250: INFO: Trying to get logs from node onekube-ip-172-16-100-5 pod downwardapi-volume-d31cc0e3-e2ac-4630-b9c7-21b4bb1612dc container client-container: <nil>
  STEP: delete the pod @ 05/12/23 13:55:46.267
  May 12 13:55:46.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1579" for this suite. @ 05/12/23 13:55:46.283
• [4.162 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 05/12/23 13:55:46.289
  May 12 13:55:46.289: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:55:46.29
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:46.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:46.303
  STEP: Setting up server cert @ 05/12/23 13:55:46.32
  E0512 13:55:46.531047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:55:46.823
  STEP: Deploying the webhook pod @ 05/12/23 13:55:46.826
  STEP: Wait for the deployment to be ready @ 05/12/23 13:55:46.839
  May 12 13:55:46.842: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0512 13:55:47.532102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:48.533115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 13:55:48.846
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:55:48.857
  E0512 13:55:49.534854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:49.859: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  E0512 13:55:50.537301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:50.858: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 05/12/23 13:55:50.861
  STEP: create a pod that should be updated by the webhook @ 05/12/23 13:55:50.874
  May 12 13:55:50.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8423" for this suite. @ 05/12/23 13:55:50.958
  STEP: Destroying namespace "webhook-markers-4672" for this suite. @ 05/12/23 13:55:50.962
• [4.677 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 05/12/23 13:55:50.967
  May 12 13:55:50.967: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename var-expansion @ 05/12/23 13:55:50.968
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:55:50.98
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:55:50.982
  STEP: creating the pod @ 05/12/23 13:55:50.984
  STEP: waiting for pod running @ 05/12/23 13:55:50.989
  E0512 13:55:51.534962      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:52.535868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 05/12/23 13:55:53.013
  May 12 13:55:53.023: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-4625 PodName:var-expansion-28d88b5b-0ac8-4f1d-9da1-16a28cd662ac ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:55:53.023: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:55:53.025: INFO: ExecWithOptions: Clientset creation
  May 12 13:55:53.026: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-4625/pods/var-expansion-28d88b5b-0ac8-4f1d-9da1-16a28cd662ac/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 05/12/23 13:55:53.113
  May 12 13:55:53.115: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-4625 PodName:var-expansion-28d88b5b-0ac8-4f1d-9da1-16a28cd662ac ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 13:55:53.115: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 13:55:53.116: INFO: ExecWithOptions: Clientset creation
  May 12 13:55:53.116: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/var-expansion-4625/pods/var-expansion-28d88b5b-0ac8-4f1d-9da1-16a28cd662ac/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 05/12/23 13:55:53.176
  E0512 13:55:53.536993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:55:53.699: INFO: Successfully updated pod "var-expansion-28d88b5b-0ac8-4f1d-9da1-16a28cd662ac"
  STEP: waiting for annotated pod running @ 05/12/23 13:55:53.699
  STEP: deleting the pod gracefully @ 05/12/23 13:55:53.704
  May 12 13:55:53.704: INFO: Deleting pod "var-expansion-28d88b5b-0ac8-4f1d-9da1-16a28cd662ac" in namespace "var-expansion-4625"
  May 12 13:55:53.712: INFO: Wait up to 5m0s for pod "var-expansion-28d88b5b-0ac8-4f1d-9da1-16a28cd662ac" to be fully deleted
  E0512 13:55:54.536923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:55.537537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:56.537660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:57.538409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:58.538683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:55:59.539401      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:00.540425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:01.540605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:02.540704      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:03.540764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:04.540891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:05.541780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:06.542671      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:07.543108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:08.544120      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:09.544993      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:10.546114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:11.546483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:12.546980      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:13.547781      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:14.547957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:15.547979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:16.549324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:17.549430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:18.549741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:19.550328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:20.551750      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:21.560435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:22.560968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:23.561618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:24.561968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:25.562301      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:26.563257      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:27.563315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:56:27.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4625" for this suite. @ 05/12/23 13:56:27.879
• [36.924 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 05/12/23 13:56:27.906
  May 12 13:56:27.906: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename init-container @ 05/12/23 13:56:27.907
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:56:27.921
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:56:27.923
  STEP: creating the pod @ 05/12/23 13:56:27.924
  May 12 13:56:27.924: INFO: PodSpec: initContainers in spec.initContainers
  E0512 13:56:28.563502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:29.563561      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:30.564513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:31.565182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:56:31.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-5405" for this suite. @ 05/12/23 13:56:31.586
• [3.686 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 05/12/23 13:56:31.595
  May 12 13:56:31.595: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename gc @ 05/12/23 13:56:31.596
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:56:31.611
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:56:31.613
  STEP: create the rc @ 05/12/23 13:56:31.615
  W0512 13:56:31.620247      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0512 13:56:32.565945      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:33.566668      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:34.566331      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:35.566393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:36.567248      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/12/23 13:56:36.624
  STEP: wait for all pods to be garbage collected @ 05/12/23 13:56:36.628
  E0512 13:56:37.567370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:38.567652      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:39.571585      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:40.578548      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:41.578656      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/23 13:56:41.646
  May 12 13:56:41.735: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 12 13:56:41.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8378" for this suite. @ 05/12/23 13:56:41.742
• [10.157 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 05/12/23 13:56:41.754
  May 12 13:56:41.754: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename watch @ 05/12/23 13:56:41.755
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:56:41.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:56:41.77
  STEP: creating a watch on configmaps with a certain label @ 05/12/23 13:56:41.771
  STEP: creating a new configmap @ 05/12/23 13:56:41.773
  STEP: modifying the configmap once @ 05/12/23 13:56:41.776
  STEP: changing the label value of the configmap @ 05/12/23 13:56:41.782
  STEP: Expecting to observe a delete notification for the watched object @ 05/12/23 13:56:41.786
  May 12 13:56:41.786: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3971  5c3eecab-558b-402d-ac48-e99d6011091e 115606 0 2023-05-12 13:56:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-12 13:56:41 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:56:41.786: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3971  5c3eecab-558b-402d-ac48-e99d6011091e 115607 0 2023-05-12 13:56:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-12 13:56:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:56:41.787: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3971  5c3eecab-558b-402d-ac48-e99d6011091e 115608 0 2023-05-12 13:56:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-12 13:56:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 05/12/23 13:56:41.788
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 05/12/23 13:56:41.792
  E0512 13:56:42.579155      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:43.579764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:44.580326      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:45.581371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:46.581307      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:47.582018      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:48.581965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:49.582493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:50.582934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:51.583466      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 05/12/23 13:56:51.793
  STEP: modifying the configmap a third time @ 05/12/23 13:56:51.821
  STEP: deleting the configmap @ 05/12/23 13:56:51.843
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 05/12/23 13:56:51.847
  May 12 13:56:51.847: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3971  5c3eecab-558b-402d-ac48-e99d6011091e 115674 0 2023-05-12 13:56:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-12 13:56:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:56:51.848: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3971  5c3eecab-558b-402d-ac48-e99d6011091e 115676 0 2023-05-12 13:56:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-12 13:56:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:56:51.848: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-3971  5c3eecab-558b-402d-ac48-e99d6011091e 115679 0 2023-05-12 13:56:41 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-05-12 13:56:51 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  May 12 13:56:51.848: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3971" for this suite. @ 05/12/23 13:56:51.851
• [10.101 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 05/12/23 13:56:51.856
  May 12 13:56:51.856: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename replication-controller @ 05/12/23 13:56:51.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:56:51.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:56:51.871
  STEP: Given a ReplicationController is created @ 05/12/23 13:56:51.873
  STEP: When the matched label of one of its pods change @ 05/12/23 13:56:51.876
  May 12 13:56:51.879: INFO: Pod name pod-release: Found 0 pods out of 1
  E0512 13:56:52.583852      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:53.584808      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:54.584873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:55.585797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:56.586869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:56:56.886: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 05/12/23 13:56:56.912
  E0512 13:56:57.586959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:56:57.919: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3487" for this suite. @ 05/12/23 13:56:57.923
• [6.071 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 05/12/23 13:56:57.931
  May 12 13:56:57.931: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir-wrapper @ 05/12/23 13:56:57.932
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:56:57.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:56:57.945
  E0512 13:56:58.587729      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:56:59.588021      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:00.589369      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:01.590341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:01.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 05/12/23 13:57:01.998
  STEP: Cleaning up the configmap @ 05/12/23 13:57:02.008
  STEP: Cleaning up the pod @ 05/12/23 13:57:02.014
  STEP: Destroying namespace "emptydir-wrapper-8805" for this suite. @ 05/12/23 13:57:02.027
• [4.102 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 05/12/23 13:57:02.035
  May 12 13:57:02.035: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 13:57:02.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:57:02.048
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:57:02.049
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 05/12/23 13:57:02.051
  E0512 13:57:02.590748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:03.590934      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:04.591625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:05.592530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:57:06.073
  May 12 13:57:06.075: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-6e6d4d3a-dbed-430b-ade6-a4812ff9d3e6 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 13:57:06.078
  May 12 13:57:06.087: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4758" for this suite. @ 05/12/23 13:57:06.09
• [4.058 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 05/12/23 13:57:06.094
  May 12 13:57:06.094: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 13:57:06.095
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:57:06.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:57:06.107
  STEP: Setting up server cert @ 05/12/23 13:57:06.124
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 13:57:06.388
  STEP: Deploying the webhook pod @ 05/12/23 13:57:06.392
  STEP: Wait for the deployment to be ready @ 05/12/23 13:57:06.403
  May 12 13:57:06.411: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 13:57:06.592901      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:07.594095      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:08.415: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.May, 12, 13, 57, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 57, 6, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.May, 12, 13, 57, 6, 0, time.Local), LastTransitionTime:time.Date(2023, time.May, 12, 13, 57, 6, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0512 13:57:08.593644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:09.593820      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 13:57:10.416
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 13:57:10.426
  E0512 13:57:10.594252      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:11.426: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 05/12/23 13:57:11.494
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/23 13:57:11.533
  STEP: Deleting the collection of validation webhooks @ 05/12/23 13:57:11.59
  E0512 13:57:11.594788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 05/12/23 13:57:11.611
  May 12 13:57:11.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1292" for this suite. @ 05/12/23 13:57:11.659
  STEP: Destroying namespace "webhook-markers-3451" for this suite. @ 05/12/23 13:57:11.67
• [5.584 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 05/12/23 13:57:11.68
  May 12 13:57:11.680: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 13:57:11.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:57:11.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:57:11.7
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 13:57:11.702
  E0512 13:57:12.595703      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:13.595195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:14.595427      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:15.595856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:57:15.737
  May 12 13:57:15.745: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-1716d1e9-a899-4e4d-9f4a-89802cb4dc43 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 13:57:15.763
  May 12 13:57:15.782: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3384" for this suite. @ 05/12/23 13:57:15.785
• [4.110 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 05/12/23 13:57:15.799
  May 12 13:57:15.799: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename subpath @ 05/12/23 13:57:15.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:57:15.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:57:15.866
  STEP: Setting up data @ 05/12/23 13:57:15.868
  STEP: Creating pod pod-subpath-test-configmap-58sm @ 05/12/23 13:57:15.875
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/23 13:57:15.876
  E0512 13:57:16.596319      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:17.596103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:18.596279      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:19.596833      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:20.598098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:21.598991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:22.598614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:23.599059      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:24.599609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:25.599714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:26.600956      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:27.601446      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:28.602614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:29.602803      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:30.603838      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:31.605044      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:32.605342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:33.606542      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:34.607619      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:35.607638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:36.608171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:37.609075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 13:57:37.978
  May 12 13:57:37.988: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-subpath-test-configmap-58sm container test-container-subpath-configmap-58sm: <nil>
  STEP: delete the pod @ 05/12/23 13:57:38.012
  STEP: Deleting pod pod-subpath-test-configmap-58sm @ 05/12/23 13:57:38.03
  May 12 13:57:38.030: INFO: Deleting pod "pod-subpath-test-configmap-58sm" in namespace "subpath-9065"
  May 12 13:57:38.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9065" for this suite. @ 05/12/23 13:57:38.034
• [22.239 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 05/12/23 13:57:38.041
  May 12 13:57:38.041: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename daemonsets @ 05/12/23 13:57:38.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:57:38.054
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:57:38.055
  May 12 13:57:38.068: INFO: Create a RollingUpdate DaemonSet
  May 12 13:57:38.072: INFO: Check that daemon pods launch on every node of the cluster
  May 12 13:57:38.076: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:57:38.076: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:57:38.077: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:57:38.077: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:57:38.609037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:39.081: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:57:39.081: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:57:39.082: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:57:39.082: INFO: Node onekube-ip-172-16-100-5 is running 0 daemon pod, expected 1
  E0512 13:57:39.610075      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:40.092: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:57:40.092: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  May 12 13:57:40.101: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  May 12 13:57:40.101: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  May 12 13:57:40.102: INFO: Update the DaemonSet to trigger a rollout
  May 12 13:57:40.117: INFO: Updating DaemonSet daemon-set
  E0512 13:57:40.610217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:41.610478      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:42.610599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:43.129: INFO: Roll back the DaemonSet before rollout is complete
  May 12 13:57:43.135: INFO: Updating DaemonSet daemon-set
  May 12 13:57:43.137: INFO: Make sure DaemonSet rollback is complete
  May 12 13:57:43.141: INFO: Wrong image for pod: daemon-set-snx45. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  May 12 13:57:43.141: INFO: Pod daemon-set-snx45 is not available
  May 12 13:57:43.145: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:57:43.145: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 13:57:43.611578      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:44.155: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:57:44.155: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 13:57:44.611854      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:45.152: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:57:45.152: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  E0512 13:57:45.612366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:46.149: INFO: Pod daemon-set-85qmx is not available
  May 12 13:57:46.151: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-4 with taints [{Key:CriticalAddonsOnly Value:true Effect:NoExecute TimeAdded:<nil>}], skip checking this node
  May 12 13:57:46.151: INFO: DaemonSet pods can't tolerate node onekube-ip-172-16-100-6 with taints [{Key:node.longhorn.io/create-default-disk Value:true Effect:NoSchedule TimeAdded:<nil>}], skip checking this node
  STEP: Deleting DaemonSet "daemon-set" @ 05/12/23 13:57:46.155
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8999, will wait for the garbage collector to delete the pods @ 05/12/23 13:57:46.155
  May 12 13:57:46.211: INFO: Deleting DaemonSet.extensions daemon-set took: 4.857965ms
  May 12 13:57:46.312: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.618305ms
  E0512 13:57:46.612848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:47.613478      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:47.615: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  May 12 13:57:47.615: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  May 12 13:57:47.617: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"116304"},"items":null}

  May 12 13:57:47.618: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"116304"},"items":null}

  May 12 13:57:47.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8999" for this suite. @ 05/12/23 13:57:47.625
• [9.588 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 05/12/23 13:57:47.63
  May 12 13:57:47.631: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/12/23 13:57:47.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:57:47.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:57:47.662
  STEP: create the container to handle the HTTPGet hook request. @ 05/12/23 13:57:47.665
  E0512 13:57:48.613705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:49.614047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/12/23 13:57:49.683
  E0512 13:57:50.618537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:51.615151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 05/12/23 13:57:51.71
  E0512 13:57:52.615215      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:53.616310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 05/12/23 13:57:53.727
  May 12 13:57:53.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-8638" for this suite. @ 05/12/23 13:57:53.742
• [6.115 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 05/12/23 13:57:53.747
  May 12 13:57:53.747: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/12/23 13:57:53.747
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:57:53.763
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:57:53.764
  STEP: create the container to handle the HTTPGet hook request. @ 05/12/23 13:57:53.769
  E0512 13:57:54.616503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:55.616790      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/12/23 13:57:55.781
  E0512 13:57:56.617032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:57.617071      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/12/23 13:57:57.792
  STEP: delete the pod with lifecycle hook @ 05/12/23 13:57:57.795
  E0512 13:57:58.617577      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:57:59.619083      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:57:59.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-6315" for this suite. @ 05/12/23 13:57:59.813
• [6.075 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 05/12/23 13:57:59.83
  May 12 13:57:59.830: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 13:57:59.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:57:59.845
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:57:59.847
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 05/12/23 13:57:59.849
  May 12 13:57:59.850: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 13:58:00.619391      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:01.620419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:02.620707      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:03.621362      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:04.621841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:05.627041      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 05/12/23 13:58:06.44
  May 12 13:58:06.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 13:58:06.622163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:07.623605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:58:07.824: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 13:58:08.626299      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:09.629113      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:10.629207      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:11.629913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:12.630183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:13.630936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:58:13.993: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7466" for this suite. @ 05/12/23 13:58:13.997
• [14.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 05/12/23 13:58:14.003
  May 12 13:58:14.003: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 13:58:14.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:58:14.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:58:14.018
  STEP: Create a pod @ 05/12/23 13:58:14.02
  E0512 13:58:14.631487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:15.631602      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/12/23 13:58:16.029
  May 12 13:58:16.035: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  May 12 13:58:16.035: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5017" for this suite. @ 05/12/23 13:58:16.042
• [2.043 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 05/12/23 13:58:16.047
  May 12 13:58:16.047: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 13:58:16.048
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 13:58:16.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 13:58:16.063
  STEP: Creating pod liveness-873e896f-1c60-4caf-a951-97761e9ad682 in namespace container-probe-2486 @ 05/12/23 13:58:16.064
  E0512 13:58:16.632375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:17.632758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 13:58:18.081: INFO: Started pod liveness-873e896f-1c60-4caf-a951-97761e9ad682 in namespace container-probe-2486
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/23 13:58:18.081
  May 12 13:58:18.091: INFO: Initial restart count of pod liveness-873e896f-1c60-4caf-a951-97761e9ad682 is 0
  E0512 13:58:18.633979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:19.634286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:20.635276      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:21.638320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:22.639336      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:23.640047      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:24.640408      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:25.640995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:26.642382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:27.643098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:28.644088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:29.644032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:30.645412      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:31.649534      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:32.649143      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:33.649913      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:34.650741      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:35.651254      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:36.651973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:37.651611      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:38.652163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:39.652249      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:40.653250      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:41.654265      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:42.654396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:43.654929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:44.656061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:45.656389      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:46.657382      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:47.657506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:48.658385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:49.658815      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:50.659771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:51.661306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:52.662458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:53.663073      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:54.663459      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:55.664586      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:56.665425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:57.665610      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:58.665609      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:58:59.666002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:00.666094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:01.666217      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:02.666516      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:03.667183      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:04.668404      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:05.670269      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:06.670263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:07.670286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:08.670648      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:09.671264      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:10.672180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:11.672784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:12.673416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:13.673949      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:14.674273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:15.674502      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:16.675479      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:17.676263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:18.677247      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:19.677822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:20.678784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:21.679003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:22.679409      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:23.679823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:24.680636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:25.681177      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:26.681418      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:27.681887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:28.682291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:29.682463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:30.683115      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:31.683924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:32.684443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:33.684855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:34.685802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:35.686033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:36.688063      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:37.687715      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:38.687984      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:39.688238      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:40.688419      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:41.688639      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:42.689020      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:43.689201      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:44.689977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:45.690682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:46.691263      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:47.694324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:48.694977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:49.695471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:50.696870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:51.697434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:52.697865      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:53.698649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:54.699057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:55.699497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:56.700076      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:57.700067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:58.701100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 13:59:59.701045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:00.701503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:01.702284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:02.703433      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:03.704278      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:04.705046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:05.715086      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:06.719204      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:07.718489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:08.719138      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:09.719140      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:10.719363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:11.721189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:12.721513      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:13.721861      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:14.722842      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:15.723163      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:16.723028      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:17.723474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:18.724169      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:19.724385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:20.725530      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:21.726394      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:22.727000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:23.727439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:24.728023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:25.728153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:26.728929      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:27.729431      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:28.730123      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:29.730567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:30.731672      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:31.736873      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:32.737045      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:33.738027      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:34.738927      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:35.739617      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:36.740339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:37.741066      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:38.742081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:39.743682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:40.742467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:41.742717      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:42.743224      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:43.743458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:44.743718      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:45.744783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:46.745486      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:47.746032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:48.746850      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:49.747228      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:50.747244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:51.748618      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:52.749448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:53.750660      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:54.751658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:55.752091      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:56.753347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:57.753498      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:58.754371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:00:59.755170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:00.756153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:01.757240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:02.757476      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:03.758473      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:04.759608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:05.759958      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:06.760952      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:07.760936      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:08.761189      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:09.761453      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:10.761674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:11.761813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:12.762871      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:13.763519      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:14.763891      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:15.764425      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:16.765535      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:17.766002      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:18.766449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:19.766900      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:20.768108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:21.768341      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:22.768605      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:23.769902      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:24.770448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:25.770760      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:26.771615      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:27.771909      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:28.772052      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:29.773077      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:30.773662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:31.774751      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:32.775725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:33.776623      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:34.777923      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:35.778736      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:36.779710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:37.780151      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:38.780705      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:39.781015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:40.781373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:41.782407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:42.783087      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:43.784298      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:44.785385      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:45.785455      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:46.786532      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:47.787185      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:48.788130      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:49.788594      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:50.789160      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:51.789946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:52.790077      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:53.790165      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:54.790687      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:55.790702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:56.792094      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:57.792303      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:58.793286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:01:59.793182      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:00.793584      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:01.794506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:02.794858      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:03.794994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:04.795840      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:05.795961      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:06.796136      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:07.797596      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:08.797959      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:09.798794      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:10.798740      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:11.799766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:12.800129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:13.801374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:14.802793      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:15.801847      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:16.803032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:17.804059      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:18.804089      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:02:18.892: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 14:02:18.895
  STEP: Destroying namespace "container-probe-2486" for this suite. @ 05/12/23 14:02:18.905
• [242.863 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 05/12/23 14:02:18.911
  May 12 14:02:18.911: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 14:02:18.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:02:18.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:02:18.925
  STEP: creating a Service @ 05/12/23 14:02:18.928
  STEP: watching for the Service to be added @ 05/12/23 14:02:18.942
  May 12 14:02:18.946: INFO: Found Service test-service-5n7d7 in namespace services-6921 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  May 12 14:02:18.946: INFO: Service test-service-5n7d7 created
  STEP: Getting /status @ 05/12/23 14:02:18.946
  May 12 14:02:18.948: INFO: Service test-service-5n7d7 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 05/12/23 14:02:18.948
  STEP: watching for the Service to be patched @ 05/12/23 14:02:18.957
  May 12 14:02:18.962: INFO: observed Service test-service-5n7d7 in namespace services-6921 with annotations: map[] & LoadBalancer: {[]}
  May 12 14:02:18.963: INFO: Found Service test-service-5n7d7 in namespace services-6921 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  May 12 14:02:18.963: INFO: Service test-service-5n7d7 has service status patched
  STEP: updating the ServiceStatus @ 05/12/23 14:02:18.963
  May 12 14:02:18.969: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 05/12/23 14:02:18.969
  May 12 14:02:18.973: INFO: Observed Service test-service-5n7d7 in namespace services-6921 with annotations: map[] & Conditions: {[]}
  May 12 14:02:18.973: INFO: Observed event: &Service{ObjectMeta:{test-service-5n7d7  services-6921  023e7fca-c5dc-4346-a90d-e7f010b2e6d1 117980 0 2023-05-12 14:02:18 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-12 14:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-12 14:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.236.141,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.236.141],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  May 12 14:02:18.973: INFO: Observed event: &Service{ObjectMeta:{test-service-5n7d7  services-6921  023e7fca-c5dc-4346-a90d-e7f010b2e6d1 117981 0 2023-05-12 14:02:18 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-05-12 14:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-05-12 14:02:18 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.43.236.141,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.43.236.141],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{},},Conditions:[]Condition{},},}
  May 12 14:02:18.973: INFO: Found Service test-service-5n7d7 in namespace services-6921 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  May 12 14:02:18.973: INFO: Service test-service-5n7d7 has service status updated
  STEP: patching the service @ 05/12/23 14:02:18.973
  STEP: watching for the Service to be patched @ 05/12/23 14:02:18.98
  May 12 14:02:18.982: INFO: observed Service test-service-5n7d7 in namespace services-6921 with labels: map[test-service-static:true]
  May 12 14:02:18.982: INFO: observed Service test-service-5n7d7 in namespace services-6921 with labels: map[test-service-static:true]
  May 12 14:02:18.982: INFO: observed Service test-service-5n7d7 in namespace services-6921 with labels: map[test-service-static:true]
  May 12 14:02:18.982: INFO: observed Service test-service-5n7d7 in namespace services-6921 with labels: map[test-service-static:true]
  May 12 14:02:18.982: INFO: Found Service test-service-5n7d7 in namespace services-6921 with labels: map[test-service:patched test-service-static:true]
  May 12 14:02:18.983: INFO: Service test-service-5n7d7 patched
  STEP: deleting the service @ 05/12/23 14:02:18.983
  STEP: watching for the Service to be deleted @ 05/12/23 14:02:18.991
  May 12 14:02:18.993: INFO: Observed event: ADDED
  May 12 14:02:18.993: INFO: Observed event: MODIFIED
  May 12 14:02:18.993: INFO: Observed event: MODIFIED
  May 12 14:02:18.993: INFO: Observed event: MODIFIED
  May 12 14:02:18.993: INFO: Observed event: MODIFIED
  May 12 14:02:18.993: INFO: Found Service test-service-5n7d7 in namespace services-6921 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  May 12 14:02:18.994: INFO: Service test-service-5n7d7 deleted
  May 12 14:02:18.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6921" for this suite. @ 05/12/23 14:02:18.996
• [0.090 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 05/12/23 14:02:19.004
  May 12 14:02:19.004: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 05/12/23 14:02:19.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:02:19.015
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:02:19.016
  STEP: create the container to handle the HTTPGet hook request. @ 05/12/23 14:02:19.02
  E0512 14:02:19.804300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:20.804402      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 05/12/23 14:02:21.051
  E0512 14:02:21.805497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:22.806043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 05/12/23 14:02:23.08
  STEP: delete the pod with lifecycle hook @ 05/12/23 14:02:23.09
  E0512 14:02:23.806868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:24.807558      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:02:25.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9499" for this suite. @ 05/12/23 14:02:25.118
• [6.119 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 05/12/23 14:02:25.13
  May 12 14:02:25.131: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename security-context @ 05/12/23 14:02:25.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:02:25.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:02:25.15
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 05/12/23 14:02:25.152
  E0512 14:02:25.807988      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:26.808395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:27.808839      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:28.809227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:02:29.179
  May 12 14:02:29.189: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod security-context-678bfa56-f9c1-4519-a6d4-dcc3c7eb52c0 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 14:02:29.214
  May 12 14:02:29.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-8992" for this suite. @ 05/12/23 14:02:29.232
• [4.105 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 05/12/23 14:02:29.237
  May 12 14:02:29.237: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 14:02:29.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:02:29.248
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:02:29.251
  STEP: set up a multi version CRD @ 05/12/23 14:02:29.252
  May 12 14:02:29.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:02:29.809813      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:30.810716      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:31.811690      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 05/12/23 14:02:32.763
  STEP: check the unserved version gets removed @ 05/12/23 14:02:32.776
  E0512 14:02:32.812202      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:33.813220      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 05/12/23 14:02:34.063
  E0512 14:02:34.813787      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:35.813849      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:36.814327      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:02:36.899: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7261" for this suite. @ 05/12/23 14:02:36.903
• [7.670 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 05/12/23 14:02:36.908
  May 12 14:02:36.908: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sysctl @ 05/12/23 14:02:36.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:02:36.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:02:36.928
  STEP: Creating a pod with one valid and two invalid sysctls @ 05/12/23 14:02:36.929
  May 12 14:02:36.932: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6112" for this suite. @ 05/12/23 14:02:36.934
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 05/12/23 14:02:36.942
  May 12 14:02:36.942: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename job @ 05/12/23 14:02:36.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:02:36.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:02:36.957
  STEP: Creating a job @ 05/12/23 14:02:36.958
  STEP: Ensuring active pods == parallelism @ 05/12/23 14:02:36.962
  E0512 14:02:37.814765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:38.816168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Orphaning one of the Job's Pods @ 05/12/23 14:02:38.973
  May 12 14:02:39.538: INFO: Successfully updated pod "adopt-release-d6cn8"
  STEP: Checking that the Job readopts the Pod @ 05/12/23 14:02:39.538
  E0512 14:02:39.816470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:40.816822      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing the labels from the Job's Pod @ 05/12/23 14:02:41.547
  E0512 14:02:41.817537      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:02:42.058: INFO: Successfully updated pod "adopt-release-d6cn8"
  STEP: Checking that the Job releases the Pod @ 05/12/23 14:02:42.058
  E0512 14:02:42.817492      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:43.817890      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:02:44.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2110" for this suite. @ 05/12/23 14:02:44.093
• [7.162 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 05/12/23 14:02:44.109
  May 12 14:02:44.109: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pod-network-test @ 05/12/23 14:02:44.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:02:44.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:02:44.136
  STEP: Performing setup for networking test in namespace pod-network-test-8747 @ 05/12/23 14:02:44.137
  STEP: creating a selector @ 05/12/23 14:02:44.137
  STEP: Creating the service pods in kubernetes @ 05/12/23 14:02:44.138
  May 12 14:02:44.138: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0512 14:02:44.818932      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:45.819302      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:46.820644      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:47.821366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:48.822051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:49.822366      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:50.822755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:51.823467      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:52.823745      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:53.824088      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:54.825226      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:55.825407      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/12/23 14:02:56.229
  E0512 14:02:56.825604      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:57.825778      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:02:58.271: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 12 14:02:58.271: INFO: Breadth first check of 10.42.2.244 on host 172.16.100.5...
  May 12 14:02:58.276: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.250:9080/dial?request=hostname&protocol=udp&host=10.42.2.244&port=8081&tries=1'] Namespace:pod-network-test-8747 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 14:02:58.277: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 14:02:58.279: INFO: ExecWithOptions: Clientset creation
  May 12 14:02:58.280: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-8747/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.2.244%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 12 14:02:58.368: INFO: Waiting for responses: map[]
  May 12 14:02:58.369: INFO: reached 10.42.2.244 after 0/1 tries
  May 12 14:02:58.369: INFO: Breadth first check of 10.42.3.249 on host 172.16.100.7...
  May 12 14:02:58.370: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.42.3.250:9080/dial?request=hostname&protocol=udp&host=10.42.3.249&port=8081&tries=1'] Namespace:pod-network-test-8747 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 14:02:58.370: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 14:02:58.371: INFO: ExecWithOptions: Clientset creation
  May 12 14:02:58.371: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-8747/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.42.3.250%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.42.3.249%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  May 12 14:02:58.459: INFO: Waiting for responses: map[]
  May 12 14:02:58.460: INFO: reached 10.42.3.249 after 0/1 tries
  May 12 14:02:58.461: INFO: Going to retry 0 out of 2 pods....
  May 12 14:02:58.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8747" for this suite. @ 05/12/23 14:02:58.464
• [14.360 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 05/12/23 14:02:58.471
  May 12 14:02:58.471: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/23 14:02:58.471
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:02:58.49
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:02:58.492
  May 12 14:02:58.493: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:02:58.825841      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:02:59.826050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:00.826225      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:01.827353      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:02.826904      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:03.827758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:04.828428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:03:05.263: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-193" for this suite. @ 05/12/23 14:03:05.266
• [6.799 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 05/12/23 14:03:05.271
  May 12 14:03:05.271: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename namespaces @ 05/12/23 14:03:05.272
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:03:05.285
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:03:05.287
  STEP: Updating Namespace "namespaces-7979" @ 05/12/23 14:03:05.288
  May 12 14:03:05.293: INFO: Namespace "namespaces-7979" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"9ddd5405-7c85-47bc-86b2-34c9975c020e", "kubernetes.io/metadata.name":"namespaces-7979", "namespaces-7979":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  May 12 14:03:05.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7979" for this suite. @ 05/12/23 14:03:05.296
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 05/12/23 14:03:05.305
  May 12 14:03:05.305: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename cronjob @ 05/12/23 14:03:05.306
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:03:05.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:03:05.32
  STEP: Creating a cronjob @ 05/12/23 14:03:05.321
  STEP: creating @ 05/12/23 14:03:05.321
  STEP: getting @ 05/12/23 14:03:05.324
  STEP: listing @ 05/12/23 14:03:05.326
  STEP: watching @ 05/12/23 14:03:05.327
  May 12 14:03:05.327: INFO: starting watch
  STEP: cluster-wide listing @ 05/12/23 14:03:05.328
  STEP: cluster-wide watching @ 05/12/23 14:03:05.329
  May 12 14:03:05.329: INFO: starting watch
  STEP: patching @ 05/12/23 14:03:05.33
  STEP: updating @ 05/12/23 14:03:05.336
  May 12 14:03:05.342: INFO: waiting for watch events with expected annotations
  May 12 14:03:05.342: INFO: saw patched and updated annotations
  STEP: patching /status @ 05/12/23 14:03:05.342
  STEP: updating /status @ 05/12/23 14:03:05.347
  STEP: get /status @ 05/12/23 14:03:05.35
  STEP: deleting @ 05/12/23 14:03:05.351
  STEP: deleting a collection @ 05/12/23 14:03:05.359
  May 12 14:03:05.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4633" for this suite. @ 05/12/23 14:03:05.366
• [0.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 05/12/23 14:03:05.371
  May 12 14:03:05.371: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 14:03:05.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:03:05.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:03:05.383
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 14:03:05.384
  E0512 14:03:05.828546      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:06.830623      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:07.831545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:08.835128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:03:09.396
  May 12 14:03:09.397: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-3886e3f3-7be2-4f11-a7ff-09b7cd9be726 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 14:03:09.401
  May 12 14:03:09.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7206" for this suite. @ 05/12/23 14:03:09.415
• [4.055 seconds]
------------------------------
SSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 05/12/23 14:03:09.426
  May 12 14:03:09.426: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 14:03:09.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:03:09.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:03:09.441
  STEP: Creating pod test-grpc-bd62da2e-53e1-4ede-af71-6f04efaa1991 in namespace container-probe-8043 @ 05/12/23 14:03:09.442
  E0512 14:03:09.832172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:10.832288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:03:11.459: INFO: Started pod test-grpc-bd62da2e-53e1-4ede-af71-6f04efaa1991 in namespace container-probe-8043
  STEP: checking the pod's current state and verifying that restartCount is present @ 05/12/23 14:03:11.46
  May 12 14:03:11.470: INFO: Initial restart count of pod test-grpc-bd62da2e-53e1-4ede-af71-6f04efaa1991 is 0
  E0512 14:03:11.833574      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:12.833614      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:13.835144      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:14.835758      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:15.836484      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:16.837251      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:17.837626      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:18.838774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:19.839571      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:20.839738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:21.840119      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:22.840449      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:23.841445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:24.840925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:25.840998      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:26.842284      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:27.843079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:28.843969      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:29.845375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:30.845373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:31.845633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:32.845850      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:33.846032      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:34.846510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:35.846545      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:36.847633      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:37.848392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:38.848061      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:39.848643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:40.848694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:41.849159      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:42.849323      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:43.850451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:44.850724      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:45.851468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:46.852697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:47.853579      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:48.853653      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:49.854335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:50.854682      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:51.855332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:52.855737      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:53.855925      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:54.858035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:55.857896      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:56.858405      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:57.858800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:58.859149      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:03:59.859783      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:00.860079      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:01.860966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:02.861050      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:03.862259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:04.862780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:05.862777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:06.862848      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:07.863920      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:08.863957      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:09.864643      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:10.864368      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:11.864968      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:12.865374      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:13.866010      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:14.866153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:15.866256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:16.867447      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:17.867245      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:18.867992      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:19.868989      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:20.869212      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:21.869728      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:22.870694      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:23.870392      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:24.870915      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:25.871599      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:26.871886      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:04:27.723: INFO: Restart count of pod container-probe-8043/test-grpc-bd62da2e-53e1-4ede-af71-6f04efaa1991 is now 1 (1m16.251059486s elapsed)
  May 12 14:04:27.723: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 05/12/23 14:04:27.736
  STEP: Destroying namespace "container-probe-8043" for this suite. @ 05/12/23 14:04:27.766
• [78.350 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 05/12/23 14:04:27.777
  May 12 14:04:27.777: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename gc @ 05/12/23 14:04:27.778
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:04:27.789
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:04:27.79
  STEP: create the deployment @ 05/12/23 14:04:27.792
  W0512 14:04:27.799669      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 05/12/23 14:04:27.801
  STEP: delete the deployment @ 05/12/23 14:04:27.81
  STEP: wait for all rs to be garbage collected @ 05/12/23 14:04:27.836
  STEP: expected 0 rs, got 1 rs @ 05/12/23 14:04:27.85
  STEP: expected 0 pods, got 2 pods @ 05/12/23 14:04:27.854
  E0512 14:04:27.872784      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/23 14:04:28.374
  May 12 14:04:28.429: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 12 14:04:28.431: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4711" for this suite. @ 05/12/23 14:04:28.434
• [0.660 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 05/12/23 14:04:28.44
  May 12 14:04:28.440: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename job @ 05/12/23 14:04:28.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:04:28.455
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:04:28.456
  STEP: Creating a job @ 05/12/23 14:04:28.458
  STEP: Ensure pods equal to parallelism count is attached to the job @ 05/12/23 14:04:28.46
  E0512 14:04:28.873515      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:29.874108      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 05/12/23 14:04:30.464
  STEP: updating /status @ 05/12/23 14:04:30.474
  STEP: get /status @ 05/12/23 14:04:30.482
  May 12 14:04:30.485: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-5396" for this suite. @ 05/12/23 14:04:30.491
• [2.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 05/12/23 14:04:30.5
  May 12 14:04:30.500: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename var-expansion @ 05/12/23 14:04:30.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:04:30.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:04:30.515
  STEP: Creating a pod to test substitution in volume subpath @ 05/12/23 14:04:30.516
  E0512 14:04:30.874720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:31.875686      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:32.876554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:33.876774      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:04:34.546
  May 12 14:04:34.554: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod var-expansion-13a19dfe-2b0e-4608-ab72-7fb6def79b23 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 14:04:34.568
  May 12 14:04:34.589: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1482" for this suite. @ 05/12/23 14:04:34.593
• [4.097 seconds]
------------------------------
SSS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 05/12/23 14:04:34.598
  May 12 14:04:34.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename endpointslice @ 05/12/23 14:04:34.6
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:04:34.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:04:34.612
  E0512 14:04:34.877814      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:35.877791      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:04:36.683: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-9553" for this suite. @ 05/12/23 14:04:36.686
• [2.091 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 05/12/23 14:04:36.69
  May 12 14:04:36.691: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename subpath @ 05/12/23 14:04:36.694
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:04:36.71
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:04:36.711
  STEP: Setting up data @ 05/12/23 14:04:36.713
  STEP: Creating pod pod-subpath-test-configmap-kj4d @ 05/12/23 14:04:36.721
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/23 14:04:36.721
  E0512 14:04:36.879137      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:37.879195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:38.879572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:39.880256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:40.880416      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:41.880797      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:42.881961      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:43.882894      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:44.883898      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:45.884448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:46.885361      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:47.886376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:48.886702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:49.887203      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:50.887483      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:51.887802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:52.888554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:53.888912      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:54.889152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:55.889679      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:56.890755      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:57.890990      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:58.891102      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:04:59.891332      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:05:00.805
  May 12 14:05:00.807: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-subpath-test-configmap-kj4d container test-container-subpath-configmap-kj4d: <nil>
  STEP: delete the pod @ 05/12/23 14:05:00.811
  STEP: Deleting pod pod-subpath-test-configmap-kj4d @ 05/12/23 14:05:00.822
  May 12 14:05:00.822: INFO: Deleting pod "pod-subpath-test-configmap-kj4d" in namespace "subpath-4356"
  May 12 14:05:00.824: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4356" for this suite. @ 05/12/23 14:05:00.826
• [24.141 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 05/12/23 14:05:00.832
  May 12 14:05:00.832: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 14:05:00.833
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:00.844
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:00.845
  STEP: Setting up server cert @ 05/12/23 14:05:00.867
  E0512 14:05:00.891517      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 14:05:01.192
  STEP: Deploying the webhook pod @ 05/12/23 14:05:01.196
  STEP: Wait for the deployment to be ready @ 05/12/23 14:05:01.207
  May 12 14:05:01.210: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0512 14:05:01.892320      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:02.892748      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 14:05:03.214
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 14:05:03.224
  E0512 14:05:03.894116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:05:04.225: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 05/12/23 14:05:04.226
  STEP: Creating a custom resource definition that should be denied by the webhook @ 05/12/23 14:05:04.24
  May 12 14:05:04.240: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 14:05:04.251: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1334" for this suite. @ 05/12/23 14:05:04.308
  STEP: Destroying namespace "webhook-markers-6316" for this suite. @ 05/12/23 14:05:04.313
• [3.486 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 05/12/23 14:05:04.319
  May 12 14:05:04.319: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 14:05:04.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:04.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:04.335
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 14:05:04.336
  E0512 14:05:04.894311      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:05.894340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:06.894337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:07.894664      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:05:08.361
  May 12 14:05:08.368: INFO: Trying to get logs from node onekube-ip-172-16-100-5 pod downwardapi-volume-ce01b7d6-6576-4599-a764-2de715a22477 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 14:05:08.404
  May 12 14:05:08.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3773" for this suite. @ 05/12/23 14:05:08.424
• [4.108 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 05/12/23 14:05:08.428
  May 12 14:05:08.428: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename subpath @ 05/12/23 14:05:08.429
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:08.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:08.443
  STEP: Setting up data @ 05/12/23 14:05:08.444
  STEP: Creating pod pod-subpath-test-projected-n6tz @ 05/12/23 14:05:08.452
  STEP: Creating a pod to test atomic-volume-subpath @ 05/12/23 14:05:08.452
  E0512 14:05:08.895348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:09.897043      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:10.897186      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:11.897966      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:12.899015      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:13.899429      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:14.899337      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:15.899370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:16.900868      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:17.901092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:18.901855      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:19.902285      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:20.903084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:21.903234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:22.903477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:23.905011      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:24.906474      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:25.905955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:26.906414      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:27.906678      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:28.906708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:29.907034      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:30.907180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:31.908175      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:05:32.545
  May 12 14:05:32.548: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-subpath-test-projected-n6tz container test-container-subpath-projected-n6tz: <nil>
  STEP: delete the pod @ 05/12/23 14:05:32.559
  STEP: Deleting pod pod-subpath-test-projected-n6tz @ 05/12/23 14:05:32.57
  May 12 14:05:32.570: INFO: Deleting pod "pod-subpath-test-projected-n6tz" in namespace "subpath-3759"
  May 12 14:05:32.572: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3759" for this suite. @ 05/12/23 14:05:32.574
• [24.149 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 05/12/23 14:05:32.578
  May 12 14:05:32.578: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 14:05:32.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:32.593
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:32.594
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 05/12/23 14:05:32.595
  E0512 14:05:32.908232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:33.909024      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:34.910313      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:35.910583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:05:36.621
  May 12 14:05:36.632: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-a264d561-3bf9-4a19-9c44-ec8890268e0a container test-container: <nil>
  STEP: delete the pod @ 05/12/23 14:05:36.651
  May 12 14:05:36.672: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9425" for this suite. @ 05/12/23 14:05:36.675
• [4.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 05/12/23 14:05:36.683
  May 12 14:05:36.683: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl-logs @ 05/12/23 14:05:36.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:36.744
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:36.747
  STEP: creating an pod @ 05/12/23 14:05:36.748
  May 12 14:05:36.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-logs-4960 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  May 12 14:05:36.812: INFO: stderr: ""
  May 12 14:05:36.812: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 05/12/23 14:05:36.813
  May 12 14:05:36.813: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  E0512 14:05:36.911625      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:37.912464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:05:38.823: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 05/12/23 14:05:38.823
  May 12 14:05:38.824: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-logs-4960 logs logs-generator logs-generator'
  May 12 14:05:38.903: INFO: stderr: ""
  May 12 14:05:38.903: INFO: stdout: "I0512 14:05:37.512855       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/9vl 275\nI0512 14:05:37.712927       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/clg 313\nI0512 14:05:37.913772       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/twgd 344\nI0512 14:05:38.113707       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/n2l 450\nI0512 14:05:38.313410       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/hq2 494\nI0512 14:05:38.513395       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/mc5n 421\nI0512 14:05:38.713909       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/ddnl 289\n"
  STEP: limiting log lines @ 05/12/23 14:05:38.903
  May 12 14:05:38.903: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-logs-4960 logs logs-generator logs-generator --tail=1'
  E0512 14:05:38.913172      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:05:38.956: INFO: stderr: ""
  May 12 14:05:38.956: INFO: stdout: "I0512 14:05:38.913552       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/2rc 363\n"
  May 12 14:05:38.956: INFO: got output "I0512 14:05:38.913552       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/2rc 363\n"
  STEP: limiting log bytes @ 05/12/23 14:05:38.956
  May 12 14:05:38.956: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-logs-4960 logs logs-generator logs-generator --limit-bytes=1'
  May 12 14:05:39.006: INFO: stderr: ""
  May 12 14:05:39.006: INFO: stdout: "I"
  May 12 14:05:39.006: INFO: got output "I"
  STEP: exposing timestamps @ 05/12/23 14:05:39.006
  May 12 14:05:39.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-logs-4960 logs logs-generator logs-generator --tail=1 --timestamps'
  May 12 14:05:39.079: INFO: stderr: ""
  May 12 14:05:39.079: INFO: stdout: "2023-05-12T14:05:38.913624168Z I0512 14:05:38.913552       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/2rc 363\n"
  May 12 14:05:39.079: INFO: got output "2023-05-12T14:05:38.913624168Z I0512 14:05:38.913552       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/2rc 363\n"
  STEP: restricting to a time range @ 05/12/23 14:05:39.079
  E0512 14:05:39.913451      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:40.913714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:05:41.580: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-logs-4960 logs logs-generator logs-generator --since=1s'
  May 12 14:05:41.652: INFO: stderr: ""
  May 12 14:05:41.652: INFO: stdout: "I0512 14:05:40.713211       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/sp2q 499\nI0512 14:05:40.913682       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/d2g 425\nI0512 14:05:41.113628       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/d4r 349\nI0512 14:05:41.313044       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/ffn 534\nI0512 14:05:41.513853       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/mmz 321\n"
  May 12 14:05:41.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-logs-4960 logs logs-generator logs-generator --since=24h'
  May 12 14:05:41.748: INFO: stderr: ""
  May 12 14:05:41.748: INFO: stdout: "I0512 14:05:37.512855       1 logs_generator.go:76] 0 POST /api/v1/namespaces/kube-system/pods/9vl 275\nI0512 14:05:37.712927       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/clg 313\nI0512 14:05:37.913772       1 logs_generator.go:76] 2 PUT /api/v1/namespaces/ns/pods/twgd 344\nI0512 14:05:38.113707       1 logs_generator.go:76] 3 POST /api/v1/namespaces/default/pods/n2l 450\nI0512 14:05:38.313410       1 logs_generator.go:76] 4 POST /api/v1/namespaces/ns/pods/hq2 494\nI0512 14:05:38.513395       1 logs_generator.go:76] 5 GET /api/v1/namespaces/ns/pods/mc5n 421\nI0512 14:05:38.713909       1 logs_generator.go:76] 6 GET /api/v1/namespaces/ns/pods/ddnl 289\nI0512 14:05:38.913552       1 logs_generator.go:76] 7 POST /api/v1/namespaces/kube-system/pods/2rc 363\nI0512 14:05:39.113161       1 logs_generator.go:76] 8 POST /api/v1/namespaces/ns/pods/qtcg 309\nI0512 14:05:39.313388       1 logs_generator.go:76] 9 GET /api/v1/namespaces/ns/pods/6v8 559\nI0512 14:05:39.513891       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/tks 218\nI0512 14:05:39.713297       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/tqc4 352\nI0512 14:05:39.912984       1 logs_generator.go:76] 12 PUT /api/v1/namespaces/kube-system/pods/c74 234\nI0512 14:05:40.113681       1 logs_generator.go:76] 13 PUT /api/v1/namespaces/kube-system/pods/wvk 583\nI0512 14:05:40.313214       1 logs_generator.go:76] 14 GET /api/v1/namespaces/default/pods/2wv 461\nI0512 14:05:40.513895       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/ns/pods/9mq 216\nI0512 14:05:40.713211       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/sp2q 499\nI0512 14:05:40.913682       1 logs_generator.go:76] 17 GET /api/v1/namespaces/kube-system/pods/d2g 425\nI0512 14:05:41.113628       1 logs_generator.go:76] 18 GET /api/v1/namespaces/default/pods/d4r 349\nI0512 14:05:41.313044       1 logs_generator.go:76] 19 POST /api/v1/namespaces/ns/pods/ffn 534\nI0512 14:05:41.513853       1 logs_generator.go:76] 20 PUT /api/v1/namespaces/ns/pods/mmz 321\nI0512 14:05:41.713292       1 logs_generator.go:76] 21 PUT /api/v1/namespaces/default/pods/wgw 445\n"
  May 12 14:05:41.748: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-logs-4960 delete pod logs-generator'
  E0512 14:05:41.913806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:05:42.510: INFO: stderr: ""
  May 12 14:05:42.510: INFO: stdout: "pod \"logs-generator\" deleted\n"
  May 12 14:05:42.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-4960" for this suite. @ 05/12/23 14:05:42.512
• [5.832 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 05/12/23 14:05:42.516
  May 12 14:05:42.516: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 14:05:42.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:42.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:42.53
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 14:05:42.531
  E0512 14:05:42.914031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:43.914706      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:44.915100      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:45.915734      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:05:46.551
  May 12 14:05:46.560: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-a7b2b3ac-ba1f-4c10-a145-eb56e1b78801 container client-container: <nil>
  STEP: delete the pod @ 05/12/23 14:05:46.569
  May 12 14:05:46.587: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2621" for this suite. @ 05/12/23 14:05:46.589
• [4.076 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 05/12/23 14:05:46.593
  May 12 14:05:46.593: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename field-validation @ 05/12/23 14:05:46.593
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:46.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:46.607
  May 12 14:05:46.609: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:05:46.916765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:47.917246      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:48.918103      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0512 14:05:49.265856      20 warnings.go:70] unknown field "alpha"
  W0512 14:05:49.267297      20 warnings.go:70] unknown field "beta"
  W0512 14:05:49.269262      20 warnings.go:70] unknown field "delta"
  W0512 14:05:49.270658      20 warnings.go:70] unknown field "epsilon"
  W0512 14:05:49.272182      20 warnings.go:70] unknown field "gamma"
  May 12 14:05:49.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6837" for this suite. @ 05/12/23 14:05:49.313
• [2.733 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 05/12/23 14:05:49.326
  May 12 14:05:49.326: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 14:05:49.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:49.346
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:49.347
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 05/12/23 14:05:49.348
  May 12 14:05:49.349: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-6816 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  May 12 14:05:49.405: INFO: stderr: ""
  May 12 14:05:49.405: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 05/12/23 14:05:49.405
  May 12 14:05:49.407: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-6816 delete pods e2e-test-httpd-pod'
  E0512 14:05:49.919266      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:50.919084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:05:51.591: INFO: stderr: ""
  May 12 14:05:51.591: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  May 12 14:05:51.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6816" for this suite. @ 05/12/23 14:05:51.593
• [2.272 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 05/12/23 14:05:51.599
  May 12 14:05:51.599: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename resourcequota @ 05/12/23 14:05:51.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:05:51.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:05:51.665
  STEP: Counting existing ResourceQuota @ 05/12/23 14:05:51.666
  E0512 14:05:51.920129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:52.920940      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:53.921085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:54.921234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:55.922082      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 05/12/23 14:05:56.669
  STEP: Ensuring resource quota status is calculated @ 05/12/23 14:05:56.672
  E0512 14:05:56.922527      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:57.924221      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 05/12/23 14:05:58.68
  STEP: Ensuring resource quota status captures replicaset creation @ 05/12/23 14:05:58.702
  E0512 14:05:58.924216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:05:59.924661      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 05/12/23 14:06:00.704
  STEP: Ensuring resource quota status released usage @ 05/12/23 14:06:00.708
  E0512 14:06:00.925547      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:01.925582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:06:02.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6589" for this suite. @ 05/12/23 14:06:02.714
• [11.121 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 05/12/23 14:06:02.721
  May 12 14:06:02.721: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename disruption @ 05/12/23 14:06:02.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:06:02.739
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:06:02.741
  STEP: creating the pdb @ 05/12/23 14:06:02.744
  STEP: Waiting for the pdb to be processed @ 05/12/23 14:06:02.755
  STEP: updating the pdb @ 05/12/23 14:06:02.762
  STEP: Waiting for the pdb to be processed @ 05/12/23 14:06:02.779
  STEP: patching the pdb @ 05/12/23 14:06:02.786
  STEP: Waiting for the pdb to be processed @ 05/12/23 14:06:02.796
  STEP: Waiting for the pdb to be deleted @ 05/12/23 14:06:02.809
  May 12 14:06:02.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5702" for this suite. @ 05/12/23 14:06:02.817
• [0.100 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 05/12/23 14:06:02.821
  May 12 14:06:02.821: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/23 14:06:02.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:06:02.839
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:06:02.84
  STEP: Creating a pod to test service account token:  @ 05/12/23 14:06:02.841
  E0512 14:06:02.925746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:03.926395      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:04.930435      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:05.931373      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:06:06.88
  May 12 14:06:06.887: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod test-pod-31540baa-90a9-4c0f-85bf-e21e8195235b container agnhost-container: <nil>
  STEP: delete the pod @ 05/12/23 14:06:06.895
  May 12 14:06:06.910: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9908" for this suite. @ 05/12/23 14:06:06.914
• [4.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 05/12/23 14:06:06.923
  May 12 14:06:06.923: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-webhook @ 05/12/23 14:06:06.925
  E0512 14:06:06.932612      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:06:06.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:06:06.988
  STEP: Setting up server cert @ 05/12/23 14:06:06.99
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 05/12/23 14:06:07.253
  STEP: Deploying the custom resource conversion webhook pod @ 05/12/23 14:06:07.256
  STEP: Wait for the deployment to be ready @ 05/12/23 14:06:07.27
  May 12 14:06:07.279: INFO: deployment "sample-crd-conversion-webhook-deployment" doesn't have the required revision set
  E0512 14:06:07.934775      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:08.935233      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 14:06:09.286
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 14:06:09.303
  E0512 14:06:09.935801      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:06:10.304: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  May 12 14:06:10.311: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:06:10.936315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:11.936413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:12.936576      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 05/12/23 14:06:12.967
  STEP: v2 custom resource should be converted @ 05/12/23 14:06:12.976
  May 12 14:06:12.980: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-6852" for this suite. @ 05/12/23 14:06:13.589
• [6.680 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 05/12/23 14:06:13.607
  May 12 14:06:13.607: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename kubectl @ 05/12/23 14:06:13.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:06:13.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:06:13.622
  STEP: starting the proxy server @ 05/12/23 14:06:13.624
  May 12 14:06:13.624: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=kubectl-437 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 05/12/23 14:06:13.661
  May 12 14:06:13.667: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-437" for this suite. @ 05/12/23 14:06:13.67
• [0.070 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 05/12/23 14:06:13.678
  May 12 14:06:13.678: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 14:06:13.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:06:13.69
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:06:13.691
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 05/12/23 14:06:13.693
  May 12 14:06:13.693: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:06:13.936528      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:14.939785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:06:15.130: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:06:15.941288      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:16.942306      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:17.941970      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:18.954266      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:19.955771      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:06:20.756: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6711" for this suite. @ 05/12/23 14:06:20.759
• [7.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 05/12/23 14:06:20.766
  May 12 14:06:20.766: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename webhook @ 05/12/23 14:06:20.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:06:20.781
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:06:20.782
  STEP: Setting up server cert @ 05/12/23 14:06:20.8
  E0512 14:06:20.955344      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 05/12/23 14:06:21.243
  STEP: Deploying the webhook pod @ 05/12/23 14:06:21.246
  STEP: Wait for the deployment to be ready @ 05/12/23 14:06:21.257
  May 12 14:06:21.278: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0512 14:06:21.955924      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:22.956869      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 05/12/23 14:06:23.294
  STEP: Verifying the service has paired with the endpoint @ 05/12/23 14:06:23.302
  E0512 14:06:23.956738      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:06:24.302: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 05/12/23 14:06:24.309
  STEP: create a namespace for the webhook @ 05/12/23 14:06:24.352
  STEP: create a configmap should be unconditionally rejected by the webhook @ 05/12/23 14:06:24.367
  May 12 14:06:24.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6592" for this suite. @ 05/12/23 14:06:24.438
  STEP: Destroying namespace "webhook-markers-1307" for this suite. @ 05/12/23 14:06:24.447
  STEP: Destroying namespace "fail-closed-namespace-5447" for this suite. @ 05/12/23 14:06:24.452
• [3.692 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 05/12/23 14:06:24.459
  May 12 14:06:24.459: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename secrets @ 05/12/23 14:06:24.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:06:24.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:06:24.474
  May 12 14:06:24.499: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7501" for this suite. @ 05/12/23 14:06:24.501
• [0.046 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 05/12/23 14:06:24.506
  May 12 14:06:24.506: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename job @ 05/12/23 14:06:24.507
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:06:24.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:06:24.52
  STEP: Creating a job @ 05/12/23 14:06:24.521
  STEP: Ensuring active pods == parallelism @ 05/12/23 14:06:24.525
  E0512 14:06:24.957506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:25.957580      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:26.958393      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:27.959122      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 05/12/23 14:06:28.532
  STEP: deleting Job.batch foo in namespace job-9411, will wait for the garbage collector to delete the pods @ 05/12/23 14:06:28.532
  May 12 14:06:28.600: INFO: Deleting Job.batch foo took: 11.878806ms
  May 12 14:06:28.703: INFO: Terminating Job.batch foo pods took: 102.176727ms
  E0512 14:06:28.960271      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:29.960746      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:30.960710      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:31.961583      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:32.961777      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:33.962244      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:34.963489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:35.964948      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:36.965460      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:37.966363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:38.967272      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:39.968340      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:40.968551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:41.969725      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:42.970857      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:43.971006      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:44.972979      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:45.973190      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:46.973240      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:47.974493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:48.974439      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:49.975192      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:50.976033      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:51.976960      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:52.977324      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:53.977428      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:54.978627      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:55.979491      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:56.980526      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:57.981092      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:58.981524      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:06:59.982358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 05/12/23 14:07:00.906
  May 12 14:07:00.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-9411" for this suite. @ 05/12/23 14:07:00.91
• [36.407 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 05/12/23 14:07:00.914
  May 12 14:07:00.914: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename sched-pred @ 05/12/23 14:07:00.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:07:00.928
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:07:00.93
  May 12 14:07:00.931: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  May 12 14:07:00.935: INFO: Waiting for terminating namespaces to be deleted...
  May 12 14:07:00.936: INFO: 
  Logging pods the apiserver thinks is on node onekube-ip-172-16-100-5 before test
  May 12 14:07:00.943: INFO: helm-install-one-longhorn-nv79t from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.944: INFO: 	Container helm ready: false, restart count 0
  May 12 14:07:00.944: INFO: helm-install-one-metallb-6p76s from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.944: INFO: 	Container helm ready: false, restart count 0
  May 12 14:07:00.944: INFO: helm-install-one-traefik-qs2vx from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.944: INFO: 	Container helm ready: false, restart count 0
  May 12 14:07:00.944: INFO: helm-install-rke2-metrics-server-vb7g9 from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.944: INFO: 	Container helm ready: false, restart count 0
  May 12 14:07:00.944: INFO: helm-install-rke2-snapshot-controller-5cb8c from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.944: INFO: 	Container helm ready: false, restart count 1
  May 12 14:07:00.944: INFO: helm-install-rke2-snapshot-controller-crd-x6qww from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.944: INFO: 	Container helm ready: false, restart count 0
  May 12 14:07:00.944: INFO: helm-install-rke2-snapshot-validation-webhook-5ft26 from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.944: INFO: 	Container helm ready: false, restart count 0
  May 12 14:07:00.944: INFO: kube-proxy-onekube-ip-172-16-100-5 from kube-system started at 2023-05-12 10:27:24 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.944: INFO: 	Container kube-proxy ready: true, restart count 0
  May 12 14:07:00.945: INFO: rke2-canal-qjzfl from kube-system started at 2023-05-12 10:27:25 +0000 UTC (2 container statuses recorded)
  May 12 14:07:00.945: INFO: 	Container calico-node ready: true, restart count 0
  May 12 14:07:00.945: INFO: 	Container kube-flannel ready: true, restart count 0
  May 12 14:07:00.945: INFO: rke2-coredns-rke2-coredns-5896cccb79-95wmb from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.945: INFO: 	Container coredns ready: true, restart count 0
  May 12 14:07:00.945: INFO: rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-hg8jj from kube-system started at 2023-05-12 10:28:11 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.945: INFO: 	Container autoscaler ready: true, restart count 0
  May 12 14:07:00.945: INFO: rke2-metrics-server-6d45f6cb4d-h7dx4 from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.945: INFO: 	Container metrics-server ready: true, restart count 0
  May 12 14:07:00.945: INFO: rke2-snapshot-controller-7bf6d7bf5f-sg6ft from kube-system started at 2023-05-12 10:28:35 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.945: INFO: 	Container rke2-snapshot-controller ready: true, restart count 0
  May 12 14:07:00.945: INFO: rke2-snapshot-validation-webhook-b65d46c9f-8bqxq from kube-system started at 2023-05-12 10:28:32 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.945: INFO: 	Container rke2-snapshot-validation-webhook ready: true, restart count 0
  May 12 14:07:00.945: INFO: csi-attacher-79bf77b7d8-vr2c8 from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.945: INFO: 	Container csi-attacher ready: true, restart count 0
  May 12 14:07:00.945: INFO: csi-provisioner-566959ff99-rj4wp from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.945: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 12 14:07:00.946: INFO: csi-provisioner-566959ff99-srfx5 from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.946: INFO: 	Container csi-provisioner ready: true, restart count 0
  May 12 14:07:00.946: INFO: csi-resizer-769c8fc86-4mp8g from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.946: INFO: 	Container csi-resizer ready: true, restart count 0
  May 12 14:07:00.946: INFO: csi-resizer-769c8fc86-5lvwx from longhorn-system started at 2023-05-12 10:29:35 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.946: INFO: 	Container csi-resizer ready: true, restart count 0
  May 12 14:07:00.946: INFO: csi-snapshotter-5677bd8f7f-2pqth from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.946: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 12 14:07:00.946: INFO: csi-snapshotter-5677bd8f7f-9dlsf from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.946: INFO: 	Container csi-snapshotter ready: true, restart count 0
  May 12 14:07:00.946: INFO: engine-image-ei-7fa7c208-kkhx7 from longhorn-system started at 2023-05-12 10:28:57 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.946: INFO: 	Container engine-image-ei-7fa7c208 ready: true, restart count 0
  May 12 14:07:00.946: INFO: instance-manager-e-1d0a2d1f86ac8de0e3ad611c8e7e88f7 from longhorn-system started at 2023-05-12 10:28:57 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.946: INFO: 	Container engine-manager ready: true, restart count 0
  May 12 14:07:00.946: INFO: longhorn-admission-webhook-9944c8788-5vs75 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.946: INFO: 	Container longhorn-admission-webhook ready: true, restart count 0
  May 12 14:07:00.946: INFO: longhorn-admission-webhook-9944c8788-n5t49 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.947: INFO: 	Container longhorn-admission-webhook ready: true, restart count 0
  May 12 14:07:00.947: INFO: longhorn-conversion-webhook-6c88c48f86-rwmr7 from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.947: INFO: 	Container longhorn-conversion-webhook ready: true, restart count 0
  May 12 14:07:00.947: INFO: longhorn-conversion-webhook-6c88c48f86-x7rnk from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.947: INFO: 	Container longhorn-conversion-webhook ready: true, restart count 0
  May 12 14:07:00.947: INFO: longhorn-csi-plugin-8mtl4 from longhorn-system started at 2023-05-12 10:29:36 +0000 UTC (3 container statuses recorded)
  May 12 14:07:00.947: INFO: 	Container longhorn-csi-plugin ready: true, restart count 0
  May 12 14:07:00.947: INFO: 	Container longhorn-liveness-probe ready: true, restart count 0
  May 12 14:07:00.947: INFO: 	Container node-driver-registrar ready: true, restart count 0
  May 12 14:07:00.947: INFO: longhorn-manager-lsknf from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.947: INFO: 	Container longhorn-manager ready: true, restart count 0
  May 12 14:07:00.947: INFO: longhorn-recovery-backend-568bcc58fc-5d6pn from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.947: INFO: 	Container longhorn-recovery-backend ready: true, restart count 0
  May 12 14:07:00.947: INFO: longhorn-recovery-backend-568bcc58fc-lklhz from longhorn-system started at 2023-05-12 10:28:33 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.947: INFO: 	Container longhorn-recovery-backend ready: true, restart count 0
  May 12 14:07:00.947: INFO: one-metallb-controller-7965f57b86-zvww2 from metallb-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.947: INFO: 	Container controller ready: true, restart count 0
  May 12 14:07:00.947: INFO: one-metallb-speaker-tr56f from metallb-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.948: INFO: 	Container speaker ready: true, restart count 0
  May 12 14:07:00.948: INFO: sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-tc4w8 from sonobuoy started at 2023-05-12 12:31:55 +0000 UTC (2 container statuses recorded)
  May 12 14:07:00.948: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 12 14:07:00.948: INFO: 	Container systemd-logs ready: true, restart count 0
  May 12 14:07:00.948: INFO: one-traefik-66db8f599c-7j8zw from traefik-system started at 2023-05-12 10:28:34 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.948: INFO: 	Container one-traefik ready: true, restart count 0
  May 12 14:07:00.948: INFO: 
  Logging pods the apiserver thinks is on node onekube-ip-172-16-100-7 before test
  May 12 14:07:00.956: INFO: kube-proxy-onekube-ip-172-16-100-7 from kube-system started at 2023-05-12 10:34:21 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.956: INFO: 	Container kube-proxy ready: true, restart count 0
  May 12 14:07:00.956: INFO: rke2-canal-58rgz from kube-system started at 2023-05-12 10:34:22 +0000 UTC (2 container statuses recorded)
  May 12 14:07:00.956: INFO: 	Container calico-node ready: true, restart count 0
  May 12 14:07:00.956: INFO: 	Container kube-flannel ready: true, restart count 0
  May 12 14:07:00.956: INFO: engine-image-ei-7fa7c208-kbxr9 from longhorn-system started at 2023-05-12 13:21:00 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.956: INFO: 	Container engine-image-ei-7fa7c208 ready: true, restart count 0
  May 12 14:07:00.956: INFO: instance-manager-e-bb03c767b3d98d645b664cf2a3258c7f from longhorn-system started at 2023-05-12 13:20:57 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.957: INFO: 	Container engine-manager ready: true, restart count 0
  May 12 14:07:00.957: INFO: longhorn-csi-plugin-dnp4n from longhorn-system started at 2023-05-12 13:20:55 +0000 UTC (3 container statuses recorded)
  May 12 14:07:00.957: INFO: 	Container longhorn-csi-plugin ready: true, restart count 0
  May 12 14:07:00.957: INFO: 	Container longhorn-liveness-probe ready: true, restart count 0
  May 12 14:07:00.957: INFO: 	Container node-driver-registrar ready: true, restart count 0
  May 12 14:07:00.957: INFO: longhorn-manager-zg9ps from longhorn-system started at 2023-05-12 13:20:55 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.957: INFO: 	Container longhorn-manager ready: true, restart count 0
  May 12 14:07:00.957: INFO: one-metallb-speaker-h8f7g from metallb-system started at 2023-05-12 13:20:55 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.957: INFO: 	Container speaker ready: true, restart count 0
  May 12 14:07:00.957: INFO: sonobuoy from sonobuoy started at 2023-05-12 12:31:54 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.957: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  May 12 14:07:00.957: INFO: sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-8q9cx from sonobuoy started at 2023-05-12 12:31:55 +0000 UTC (2 container statuses recorded)
  May 12 14:07:00.957: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  May 12 14:07:00.957: INFO: 	Container systemd-logs ready: true, restart count 0
  May 12 14:07:00.957: INFO: one-traefik-66db8f599c-zbk58 from traefik-system started at 2023-05-12 13:21:02 +0000 UTC (1 container statuses recorded)
  May 12 14:07:00.957: INFO: 	Container one-traefik ready: true, restart count 0
  E0512 14:07:00.982654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the node has the label node onekube-ip-172-16-100-5 @ 05/12/23 14:07:00.983
  STEP: verifying the node has the label node onekube-ip-172-16-100-7 @ 05/12/23 14:07:01.004
  May 12 14:07:01.020: INFO: Pod kube-proxy-onekube-ip-172-16-100-5 requesting resource cpu=250m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.020: INFO: Pod kube-proxy-onekube-ip-172-16-100-7 requesting resource cpu=250m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.020: INFO: Pod rke2-canal-58rgz requesting resource cpu=250m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.020: INFO: Pod rke2-canal-qjzfl requesting resource cpu=250m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.020: INFO: Pod rke2-coredns-rke2-coredns-5896cccb79-95wmb requesting resource cpu=100m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.020: INFO: Pod rke2-coredns-rke2-coredns-autoscaler-f6766cdc9-hg8jj requesting resource cpu=25m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.020: INFO: Pod rke2-metrics-server-6d45f6cb4d-h7dx4 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.020: INFO: Pod rke2-snapshot-controller-7bf6d7bf5f-sg6ft requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.020: INFO: Pod rke2-snapshot-validation-webhook-b65d46c9f-8bqxq requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod csi-attacher-79bf77b7d8-vr2c8 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod csi-provisioner-566959ff99-rj4wp requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod csi-provisioner-566959ff99-srfx5 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod csi-resizer-769c8fc86-4mp8g requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod csi-resizer-769c8fc86-5lvwx requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod csi-snapshotter-5677bd8f7f-2pqth requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod csi-snapshotter-5677bd8f7f-9dlsf requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod engine-image-ei-7fa7c208-kbxr9 requesting resource cpu=0m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.021: INFO: Pod engine-image-ei-7fa7c208-kkhx7 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod instance-manager-e-1d0a2d1f86ac8de0e3ad611c8e7e88f7 requesting resource cpu=240m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.021: INFO: Pod instance-manager-e-bb03c767b3d98d645b664cf2a3258c7f requesting resource cpu=240m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.022: INFO: Pod longhorn-admission-webhook-9944c8788-5vs75 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.022: INFO: Pod longhorn-admission-webhook-9944c8788-n5t49 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.022: INFO: Pod longhorn-conversion-webhook-6c88c48f86-rwmr7 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.022: INFO: Pod longhorn-conversion-webhook-6c88c48f86-x7rnk requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.022: INFO: Pod longhorn-csi-plugin-8mtl4 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.022: INFO: Pod longhorn-csi-plugin-dnp4n requesting resource cpu=0m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.022: INFO: Pod longhorn-manager-lsknf requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.022: INFO: Pod longhorn-manager-zg9ps requesting resource cpu=0m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.022: INFO: Pod longhorn-recovery-backend-568bcc58fc-5d6pn requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.022: INFO: Pod longhorn-recovery-backend-568bcc58fc-lklhz requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.022: INFO: Pod one-metallb-controller-7965f57b86-zvww2 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.023: INFO: Pod one-metallb-speaker-h8f7g requesting resource cpu=0m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.023: INFO: Pod one-metallb-speaker-tr56f requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.023: INFO: Pod sonobuoy requesting resource cpu=0m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.023: INFO: Pod sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-8q9cx requesting resource cpu=0m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.023: INFO: Pod sonobuoy-systemd-logs-daemon-set-1668315c3e674a59-tc4w8 requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.023: INFO: Pod one-traefik-66db8f599c-7j8zw requesting resource cpu=0m on Node onekube-ip-172-16-100-5
  May 12 14:07:01.023: INFO: Pod one-traefik-66db8f599c-zbk58 requesting resource cpu=0m on Node onekube-ip-172-16-100-7
  STEP: Starting Pods to consume most of the cluster CPU. @ 05/12/23 14:07:01.023
  May 12 14:07:01.023: INFO: Creating a pod which consumes cpu=882m on Node onekube-ip-172-16-100-7
  May 12 14:07:01.027: INFO: Creating a pod which consumes cpu=794m on Node onekube-ip-172-16-100-5
  E0512 14:07:01.982764      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:02.982905      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:03.983187      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:04.984081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 05/12/23 14:07:05.058
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2f3a142a-03ed-475b-b6d5-d7d961305b49.175e6a82e199c88e], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1228/filler-pod-2f3a142a-03ed-475b-b6d5-d7d961305b49 to onekube-ip-172-16-100-5] @ 05/12/23 14:07:05.065
  STEP: Considering event: 
  Type = [Warning], Name = [filler-pod-2f3a142a-03ed-475b-b6d5-d7d961305b49.175e6a832f6e3457], Reason = [FailedMount], Message = [MountVolume.SetUp failed for volume "kube-api-access-l8ddj" : failed to sync configmap cache: timed out waiting for the condition] @ 05/12/23 14:07:05.066
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2f3a142a-03ed-475b-b6d5-d7d961305b49.175e6a835e7bbb51], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/12/23 14:07:05.068
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2f3a142a-03ed-475b-b6d5-d7d961305b49.175e6a835f1fe561], Reason = [Created], Message = [Created container filler-pod-2f3a142a-03ed-475b-b6d5-d7d961305b49] @ 05/12/23 14:07:05.074
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-2f3a142a-03ed-475b-b6d5-d7d961305b49.175e6a8365c45ce5], Reason = [Started], Message = [Started container filler-pod-2f3a142a-03ed-475b-b6d5-d7d961305b49] @ 05/12/23 14:07:05.074
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e00770f9-68e3-4351-8c5d-a8a4bd686202.175e6a82e11a9d63], Reason = [Scheduled], Message = [Successfully assigned sched-pred-1228/filler-pod-e00770f9-68e3-4351-8c5d-a8a4bd686202 to onekube-ip-172-16-100-7] @ 05/12/23 14:07:05.074
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e00770f9-68e3-4351-8c5d-a8a4bd686202.175e6a8316a2f9a2], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 05/12/23 14:07:05.074
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e00770f9-68e3-4351-8c5d-a8a4bd686202.175e6a8317a214ec], Reason = [Created], Message = [Created container filler-pod-e00770f9-68e3-4351-8c5d-a8a4bd686202] @ 05/12/23 14:07:05.074
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-e00770f9-68e3-4351-8c5d-a8a4bd686202.175e6a831f0dfe3b], Reason = [Started], Message = [Started container filler-pod-e00770f9-68e3-4351-8c5d-a8a4bd686202] @ 05/12/23 14:07:05.082
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.175e6a83d20f0631], Reason = [FailedScheduling], Message = [0/4 nodes are available: 1 node(s) had untolerated taint {CriticalAddonsOnly: true}, 1 node(s) had untolerated taint {node.longhorn.io/create-default-disk: true}, 2 Insufficient cpu. preemption: 0/4 nodes are available: 2 No preemption victims found for incoming pod, 2 Preemption is not helpful for scheduling..] @ 05/12/23 14:07:05.086
  E0512 14:07:05.984506      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node onekube-ip-172-16-100-5 @ 05/12/23 14:07:06.081
  STEP: verifying the node doesn't have the label node @ 05/12/23 14:07:06.099
  STEP: removing the label node off the node onekube-ip-172-16-100-7 @ 05/12/23 14:07:06.103
  STEP: verifying the node doesn't have the label node @ 05/12/23 14:07:06.122
  May 12 14:07:06.124: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1228" for this suite. @ 05/12/23 14:07:06.131
• [5.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 05/12/23 14:07:06.155
  May 12 14:07:06.156: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename downward-api @ 05/12/23 14:07:06.158
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:07:06.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:07:06.181
  STEP: Creating a pod to test downward api env vars @ 05/12/23 14:07:06.183
  E0512 14:07:06.984676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:07.986310      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:08.986744      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:09.987762      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:07:10.218
  May 12 14:07:10.220: INFO: Trying to get logs from node onekube-ip-172-16-100-5 pod downward-api-813babfd-0904-4943-8f91-a034e484a791 container dapi-container: <nil>
  STEP: delete the pod @ 05/12/23 14:07:10.232
  May 12 14:07:10.244: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7148" for this suite. @ 05/12/23 14:07:10.247
• [4.098 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 05/12/23 14:07:10.253
  May 12 14:07:10.253: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename container-probe @ 05/12/23 14:07:10.253
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:07:10.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:07:10.266
  E0512 14:07:10.987941      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:11.988300      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:12.988888      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:13.989410      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:14.989349      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:15.989695      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:16.990060      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:17.990819      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:18.991378      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:19.991975      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:20.991525      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:21.992219      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:22.992640      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:23.992521      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:24.992806      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:25.993776      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:26.995727      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:27.996766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:28.998275      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:29.999046      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:30.998917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:31.999055      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:32.999463      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:33.999557      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:35.000309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:36.001030      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:37.001420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:38.001907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:39.002170      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:40.002487      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:41.003714      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:42.004035      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:43.004722      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:44.005139      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:45.004935      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:46.006037      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:47.006234      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:48.006582      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:49.007146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:50.006917      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:51.007322      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:52.008067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:53.009084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:54.009328      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:55.009713      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:56.009858      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:57.010826      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:58.010831      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:07:59.011697      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:00.011850      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:01.014282      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:02.015057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:03.014996      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:04.015946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:05.016657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:06.017118      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:07.017218      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:08.017493      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:09.017870      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:10.018067      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:08:10.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-6122" for this suite. @ 05/12/23 14:08:10.294
• [60.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 05/12/23 14:08:10.331
  May 12 14:08:10.331: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename csiinlinevolumes @ 05/12/23 14:08:10.335
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:08:10.348
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:08:10.353
  STEP: creating @ 05/12/23 14:08:10.356
  STEP: getting @ 05/12/23 14:08:10.366
  STEP: listing @ 05/12/23 14:08:10.368
  STEP: deleting @ 05/12/23 14:08:10.369
  May 12 14:08:10.382: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3647" for this suite. @ 05/12/23 14:08:10.385
• [0.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 05/12/23 14:08:10.395
  May 12 14:08:10.395: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename emptydir @ 05/12/23 14:08:10.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:08:10.404
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:08:10.407
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 05/12/23 14:08:10.409
  E0512 14:08:11.018468      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:12.018662      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:13.018907      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:14.019026      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:08:14.424
  May 12 14:08:14.426: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod pod-39aab665-d800-4770-b45e-a682e2690a60 container test-container: <nil>
  STEP: delete the pod @ 05/12/23 14:08:14.437
  May 12 14:08:14.457: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5102" for this suite. @ 05/12/23 14:08:14.459
• [4.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 05/12/23 14:08:14.467
  May 12 14:08:14.468: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename var-expansion @ 05/12/23 14:08:14.468
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:08:14.481
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:08:14.483
  STEP: creating the pod with failed condition @ 05/12/23 14:08:14.485
  E0512 14:08:15.019887      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:16.021567      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:17.023316      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:18.022658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:19.022818      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:20.022829      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:21.023477      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:22.024242      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:23.024860      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:24.026375      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:25.027180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:26.027077      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:27.028518      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:28.029098      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:29.029339      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:30.030457      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:31.030734      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:32.033180      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:33.034232      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:34.034893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:35.035711      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:36.035977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:37.036209      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:38.036503      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:39.038347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:40.039188      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:41.039591      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:42.040590      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:43.040832      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:44.040824      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:45.041994      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:46.045364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:47.046312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:48.046937      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:49.047081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:50.050663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:51.050511      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:52.051448      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:53.051788      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:54.051800      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:55.055214      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:56.055823      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:57.056013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:58.056879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:08:59.058358      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:00.058465      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:01.058657      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:02.058922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:03.059698      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:04.060005      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:05.060129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:06.061013      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:07.062424      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:08.063128      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:09.064051      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:10.064497      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:11.065911      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:12.067048      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:13.067142      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:14.067031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:15.068471      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:16.068946      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:17.069003      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:18.070314      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:19.070309      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:20.070603      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:21.071938      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:22.072928      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:23.073916      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:24.074380      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:25.074470      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:26.074765      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:27.074819      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:28.075291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:29.075676      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:30.077176      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:31.079517      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:32.086658      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:33.086973      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:34.087097      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:35.087883      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:36.087991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:37.089230      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:38.090510      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:39.090608      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:40.091084      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:41.091598      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:42.091659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:43.092683      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:44.093175      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:45.093742      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:46.094259      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:47.094264      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:48.094893      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:49.095458      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:50.095991      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:51.100335      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:52.113554      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:53.113780      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:54.113983      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:55.114031      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:56.114507      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:57.115636      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:58.116237      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:09:59.124443      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:00.129007      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:01.125256      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:02.126277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:03.127152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:04.127283      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:05.127663      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:06.129817      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:07.130125      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:08.130600      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:09.130856      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:10.131464      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:11.132227      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:12.132955      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:13.133396      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:14.133348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 05/12/23 14:10:14.492
  May 12 14:10:15.002: INFO: Successfully updated pod "var-expansion-dfec5b57-6595-4f04-8fa6-24f8b5f00a6e"
  STEP: waiting for pod running @ 05/12/23 14:10:15.005
  E0512 14:10:15.134649      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:16.134420      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 05/12/23 14:10:17.015
  May 12 14:10:17.015: INFO: Deleting pod "var-expansion-dfec5b57-6595-4f04-8fa6-24f8b5f00a6e" in namespace "var-expansion-182"
  May 12 14:10:17.044: INFO: Wait up to 5m0s for pod "var-expansion-dfec5b57-6595-4f04-8fa6-24f8b5f00a6e" to be fully deleted
  E0512 14:10:17.134273      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:18.134747      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:19.134879      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:20.135624      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:21.136413      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:22.136372      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:23.137730      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:24.138587      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:25.138508      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:26.138922      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:27.139171      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:28.139434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:29.139802      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:30.140821      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:31.141347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:32.141430      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:33.142057      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:34.142342      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:35.143081      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:36.143489      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:37.144085      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:38.144379      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:39.144475      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:40.146766      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:41.147315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:42.148161      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:43.148965      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:44.149708      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:45.150129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:46.150286      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:47.150312      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:48.150445      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:10:49.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-182" for this suite. @ 05/12/23 14:10:49.13
• [154.667 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 05/12/23 14:10:49.14
  May 12 14:10:49.140: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename gc @ 05/12/23 14:10:49.141
  E0512 14:10:49.156654      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:10:49.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:10:49.16
  STEP: create the rc @ 05/12/23 14:10:49.164
  W0512 14:10:49.166917      20 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0512 14:10:50.157168      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:51.157152      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:52.158116      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:53.158216      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:54.158347      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:10:55.158387      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 05/12/23 14:10:55.171
  STEP: wait for the rc to be deleted @ 05/12/23 14:10:55.175
  E0512 14:10:56.158702      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:10:56.184: INFO: 80 pods remaining
  May 12 14:10:56.184: INFO: 80 pods has nil DeletionTimestamp
  May 12 14:10:56.185: INFO: 
  E0512 14:10:57.158696      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:10:57.190: INFO: 72 pods remaining
  May 12 14:10:57.190: INFO: 72 pods has nil DeletionTimestamp
  May 12 14:10:57.190: INFO: 
  E0512 14:10:58.158812      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:10:58.192: INFO: 59 pods remaining
  May 12 14:10:58.192: INFO: 59 pods has nil DeletionTimestamp
  May 12 14:10:58.192: INFO: 
  E0512 14:10:59.159572      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:10:59.181: INFO: 40 pods remaining
  May 12 14:10:59.182: INFO: 40 pods has nil DeletionTimestamp
  May 12 14:10:59.182: INFO: 
  E0512 14:11:00.160143      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:00.185: INFO: 32 pods remaining
  May 12 14:11:00.185: INFO: 32 pods has nil DeletionTimestamp
  May 12 14:11:00.186: INFO: 
  E0512 14:11:01.161348      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:01.215: INFO: 19 pods remaining
  May 12 14:11:01.216: INFO: 19 pods has nil DeletionTimestamp
  May 12 14:11:01.216: INFO: 
  E0512 14:11:02.161651      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 05/12/23 14:11:02.207
  May 12 14:11:02.293: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  May 12 14:11:02.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5342" for this suite. @ 05/12/23 14:11:02.3
• [13.164 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 05/12/23 14:11:02.304
  May 12 14:11:02.304: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename field-validation @ 05/12/23 14:11:02.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:02.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:02.32
  May 12 14:11:02.321: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  W0512 14:11:02.322320      20 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc004adb6c0 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0512 14:11:03.162295      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:04.162364      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0512 14:11:04.881305      20 warnings.go:70] unknown field "alpha"
  W0512 14:11:04.881573      20 warnings.go:70] unknown field "beta"
  W0512 14:11:04.881710      20 warnings.go:70] unknown field "delta"
  W0512 14:11:04.881866      20 warnings.go:70] unknown field "epsilon"
  W0512 14:11:04.882029      20 warnings.go:70] unknown field "gamma"
  May 12 14:11:04.893: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8060" for this suite. @ 05/12/23 14:11:04.905
• [2.606 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 05/12/23 14:11:04.912
  May 12 14:11:04.912: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 14:11:04.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:04.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:04.931
  May 12 14:11:04.933: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:11:05.162523      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:06.162877      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/12/23 14:11:06.307
  May 12 14:11:06.307: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6252 --namespace=crd-publish-openapi-6252 create -f -'
  E0512 14:11:07.173291      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:07.277: INFO: stderr: ""
  May 12 14:11:07.277: INFO: stdout: "e2e-test-crd-publish-openapi-8673-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 12 14:11:07.277: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6252 --namespace=crd-publish-openapi-6252 delete e2e-test-crd-publish-openapi-8673-crds test-cr'
  May 12 14:11:07.390: INFO: stderr: ""
  May 12 14:11:07.390: INFO: stdout: "e2e-test-crd-publish-openapi-8673-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  May 12 14:11:07.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6252 --namespace=crd-publish-openapi-6252 apply -f -'
  May 12 14:11:07.828: INFO: stderr: ""
  May 12 14:11:07.828: INFO: stdout: "e2e-test-crd-publish-openapi-8673-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  May 12 14:11:07.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6252 --namespace=crd-publish-openapi-6252 delete e2e-test-crd-publish-openapi-8673-crds test-cr'
  May 12 14:11:07.962: INFO: stderr: ""
  May 12 14:11:07.962: INFO: stdout: "e2e-test-crd-publish-openapi-8673-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 05/12/23 14:11:07.962
  May 12 14:11:07.962: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6252 explain e2e-test-crd-publish-openapi-8673-crds'
  E0512 14:11:08.174000      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:08.463: INFO: stderr: ""
  May 12 14:11:08.463: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-8673-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0512 14:11:09.184354      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:09.797: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6252" for this suite. @ 05/12/23 14:11:09.802
• [4.895 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 05/12/23 14:11:09.808
  May 12 14:11:09.808: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pods @ 05/12/23 14:11:09.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:09.825
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:09.827
  STEP: creating pod @ 05/12/23 14:11:09.828
  E0512 14:11:10.185153      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:11.185277      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:11.857: INFO: Pod pod-hostip-10bcf48a-0a23-46f8-9edd-c78f3ad6de33 has hostIP: 172.16.100.7
  May 12 14:11:11.858: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3568" for this suite. @ 05/12/23 14:11:11.868
• [2.070 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 05/12/23 14:11:11.881
  May 12 14:11:11.881: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename custom-resource-definition @ 05/12/23 14:11:11.882
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:11.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:11.899
  May 12 14:11:11.900: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:11:12.187997      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:12.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7341" for this suite. @ 05/12/23 14:11:12.982
• [1.105 seconds]
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 05/12/23 14:11:12.986
  May 12 14:11:12.986: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename services @ 05/12/23 14:11:12.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:12.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:13
  STEP: fetching services @ 05/12/23 14:11:13.001
  May 12 14:11:13.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8513" for this suite. @ 05/12/23 14:11:13.005
• [0.024 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 05/12/23 14:11:13.01
  May 12 14:11:13.010: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename crd-publish-openapi @ 05/12/23 14:11:13.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:13.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:13.028
  May 12 14:11:13.030: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  E0512 14:11:13.202268      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:14.214266      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 05/12/23 14:11:15.078
  May 12 14:11:15.079: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6189 --namespace=crd-publish-openapi-6189 create -f -'
  E0512 14:11:15.214972      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:15.808: INFO: stderr: ""
  May 12 14:11:15.808: INFO: stdout: "e2e-test-crd-publish-openapi-6924-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 12 14:11:15.808: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6189 --namespace=crd-publish-openapi-6189 delete e2e-test-crd-publish-openapi-6924-crds test-cr'
  May 12 14:11:15.865: INFO: stderr: ""
  May 12 14:11:15.865: INFO: stdout: "e2e-test-crd-publish-openapi-6924-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  May 12 14:11:15.865: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6189 --namespace=crd-publish-openapi-6189 apply -f -'
  May 12 14:11:16.099: INFO: stderr: ""
  May 12 14:11:16.099: INFO: stdout: "e2e-test-crd-publish-openapi-6924-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  May 12 14:11:16.099: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6189 --namespace=crd-publish-openapi-6189 delete e2e-test-crd-publish-openapi-6924-crds test-cr'
  May 12 14:11:16.149: INFO: stderr: ""
  May 12 14:11:16.149: INFO: stdout: "e2e-test-crd-publish-openapi-6924-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 05/12/23 14:11:16.149
  May 12 14:11:16.149: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-2468520 --namespace=crd-publish-openapi-6189 explain e2e-test-crd-publish-openapi-6924-crds'
  E0512 14:11:16.215837      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:16.396: INFO: stderr: ""
  May 12 14:11:16.396: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-6924-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0512 14:11:17.216371      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:18.223659      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:18.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6189" for this suite. @ 05/12/23 14:11:18.334
• [5.327 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 05/12/23 14:11:18.34
  May 12 14:11:18.340: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename dns @ 05/12/23 14:11:18.341
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:18.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:18.356
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 05/12/23 14:11:18.359
  May 12 14:11:18.364: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-555  7708f6fb-22db-4666-9ed8-ee22f8735ea5 123890 0 2023-05-12 14:11:18 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-05-12 14:11:18 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5bjtj,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5bjtj,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0512 14:11:19.224315      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:20.224551      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 05/12/23 14:11:20.376
  May 12 14:11:20.377: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-555 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 14:11:20.377: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 14:11:20.377: INFO: ExecWithOptions: Clientset creation
  May 12 14:11:20.377: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-555/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 05/12/23 14:11:20.458
  May 12 14:11:20.458: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-555 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 14:11:20.458: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 14:11:20.458: INFO: ExecWithOptions: Clientset creation
  May 12 14:11:20.459: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/dns-555/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 12 14:11:20.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  May 12 14:11:20.532: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-555" for this suite. @ 05/12/23 14:11:20.541
• [2.208 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 05/12/23 14:11:20.549
  May 12 14:11:20.549: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename svcaccounts @ 05/12/23 14:11:20.549
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:20.565
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:20.567
  STEP: creating a ServiceAccount @ 05/12/23 14:11:20.569
  STEP: watching for the ServiceAccount to be added @ 05/12/23 14:11:20.575
  STEP: patching the ServiceAccount @ 05/12/23 14:11:20.577
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 05/12/23 14:11:20.581
  STEP: deleting the ServiceAccount @ 05/12/23 14:11:20.584
  May 12 14:11:20.594: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9151" for this suite. @ 05/12/23 14:11:20.597
• [0.052 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 05/12/23 14:11:20.603
  May 12 14:11:20.603: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename pod-network-test @ 05/12/23 14:11:20.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:20.615
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:20.616
  STEP: Performing setup for networking test in namespace pod-network-test-4770 @ 05/12/23 14:11:20.618
  STEP: creating a selector @ 05/12/23 14:11:20.618
  STEP: Creating the service pods in kubernetes @ 05/12/23 14:11:20.618
  May 12 14:11:20.618: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0512 14:11:21.225029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:22.225769      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:23.226069      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:24.226522      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:25.226674      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:26.226785      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:27.226787      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:28.227376      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:29.227434      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:30.228029      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:31.228529      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:32.228195      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:33.228906      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:34.229146      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:35.229397      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:36.229699      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:37.230114      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:38.230454      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:39.231023      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:40.230995      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:41.231977      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:42.232099      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 05/12/23 14:11:42.737
  E0512 14:11:43.232363      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:44.233129      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  May 12 14:11:44.785: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  May 12 14:11:44.786: INFO: Going to poll 10.42.2.70 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May 12 14:11:44.789: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.2.70:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4770 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 14:11:44.789: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 14:11:44.790: INFO: ExecWithOptions: Clientset creation
  May 12 14:11:44.791: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-4770/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.2.70%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 12 14:11:44.865: INFO: Found all 1 expected endpoints: [netserver-0]
  May 12 14:11:44.865: INFO: Going to poll 10.42.3.76 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  May 12 14:11:44.866: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.42.3.76:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-4770 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  May 12 14:11:44.867: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  May 12 14:11:44.867: INFO: ExecWithOptions: Clientset creation
  May 12 14:11:44.867: INFO: ExecWithOptions: execute(POST https://10.43.0.1:443/api/v1/namespaces/pod-network-test-4770/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.42.3.76%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  May 12 14:11:44.934: INFO: Found all 1 expected endpoints: [netserver-1]
  May 12 14:11:44.935: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-4770" for this suite. @ 05/12/23 14:11:44.938
• [24.340 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 05/12/23 14:11:44.943
  May 12 14:11:44.943: INFO: >>> kubeConfig: /tmp/kubeconfig-2468520
  STEP: Building a namespace api object, basename projected @ 05/12/23 14:11:44.944
  STEP: Waiting for a default service account to be provisioned in namespace @ 05/12/23 14:11:44.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 05/12/23 14:11:44.956
  STEP: Creating a pod to test downward API volume plugin @ 05/12/23 14:11:44.958
  E0512 14:11:45.233692      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:46.233720      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:47.234370      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0512 14:11:48.235638      20 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 05/12/23 14:11:48.975
  May 12 14:11:48.977: INFO: Trying to get logs from node onekube-ip-172-16-100-7 pod downwardapi-volume-246426ef-00b8-41ec-bcfe-4d5c6bdc0dac container client-container: <nil>
  STEP: delete the pod @ 05/12/23 14:11:48.986
  May 12 14:11:48.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1788" for this suite. @ 05/12/23 14:11:48.999
• [4.061 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  May 12 14:11:49.007: INFO: Running AfterSuite actions on node 1
  May 12 14:11:49.007: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.042 seconds]
------------------------------

Ran 378 of 7207 Specs in 5992.313 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h39m52.653986043s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

