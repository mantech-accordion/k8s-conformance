  I0804 19:47:36.384698      22 e2e.go:117] Starting e2e run "aa536a21-1f67-4537-80d5-a1f09aaa06c5" on Ginkgo node 1
  Aug  4 19:47:36.412: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1691178456 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Aug  4 19:47:36.564: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 19:47:36.565: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Aug  4 19:47:36.588: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Aug  4 19:47:36.591: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kindnet' (0 seconds elapsed)
  Aug  4 19:47:36.591: INFO: 2 / 2 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Aug  4 19:47:36.591: INFO: e2e test version: v1.27.3
  Aug  4 19:47:36.591: INFO: kube-apiserver version: v1.27.3
  Aug  4 19:47:36.592: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 19:47:36.595: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.030 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 08/04/23 19:47:36.855
  Aug  4 19:47:36.855: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename var-expansion @ 08/04/23 19:47:36.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:47:36.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:47:36.868
  STEP: Creating a pod to test substitution in volume subpath @ 08/04/23 19:47:36.87
  STEP: Saw pod success @ 08/04/23 19:47:40.885
  Aug  4 19:47:40.887: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-2d73c15a-0948-416d-8895-00de1082db6e container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 19:47:40.9
  Aug  4 19:47:40.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-7602" for this suite. @ 08/04/23 19:47:40.913
• [4.061 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 08/04/23 19:47:40.917
  Aug  4 19:47:40.917: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 19:47:40.918
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:47:40.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:47:40.929
  STEP: creating Agnhost RC @ 08/04/23 19:47:40.931
  Aug  4 19:47:40.931: INFO: namespace kubectl-7753
  Aug  4 19:47:40.931: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7753 create -f -'
  Aug  4 19:47:41.177: INFO: stderr: ""
  Aug  4 19:47:41.177: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/04/23 19:47:41.177
  Aug  4 19:47:42.180: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 19:47:42.180: INFO: Found 0 / 1
  Aug  4 19:47:43.181: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 19:47:43.181: INFO: Found 0 / 1
  Aug  4 19:47:44.180: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 19:47:44.180: INFO: Found 0 / 1
  Aug  4 19:47:45.182: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 19:47:45.182: INFO: Found 1 / 1
  Aug  4 19:47:45.182: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug  4 19:47:45.184: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 19:47:45.184: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug  4 19:47:45.184: INFO: wait on agnhost-primary startup in kubectl-7753 
  Aug  4 19:47:45.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7753 logs agnhost-primary-fhpwl agnhost-primary'
  Aug  4 19:47:45.247: INFO: stderr: ""
  Aug  4 19:47:45.247: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 08/04/23 19:47:45.247
  Aug  4 19:47:45.247: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7753 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Aug  4 19:47:45.315: INFO: stderr: ""
  Aug  4 19:47:45.315: INFO: stdout: "service/rm2 exposed\n"
  Aug  4 19:47:45.318: INFO: Service rm2 in namespace kubectl-7753 found.
  STEP: exposing service @ 08/04/23 19:47:47.323
  Aug  4 19:47:47.323: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7753 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Aug  4 19:47:47.391: INFO: stderr: ""
  Aug  4 19:47:47.391: INFO: stdout: "service/rm3 exposed\n"
  Aug  4 19:47:47.395: INFO: Service rm3 in namespace kubectl-7753 found.
  Aug  4 19:47:49.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7753" for this suite. @ 08/04/23 19:47:49.402
• [8.488 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 08/04/23 19:47:49.406
  Aug  4 19:47:49.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 19:47:49.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:47:49.415
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:47:49.417
  STEP: Creating configMap that has name configmap-test-emptyKey-bd6c7971-5099-4e70-8924-9704580b9134 @ 08/04/23 19:47:49.419
  Aug  4 19:47:49.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4548" for this suite. @ 08/04/23 19:47:49.422
• [0.019 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 08/04/23 19:47:49.427
  Aug  4 19:47:49.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 19:47:49.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:47:49.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:47:49.438
  STEP: Setting up server cert @ 08/04/23 19:47:49.451
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 19:47:49.84
  STEP: Deploying the webhook pod @ 08/04/23 19:47:49.845
  STEP: Wait for the deployment to be ready @ 08/04/23 19:47:49.853
  Aug  4 19:47:49.856: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 08/04/23 19:47:51.878
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 19:47:51.888
  Aug  4 19:47:52.888: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 08/04/23 19:47:52.89
  STEP: create a namespace for the webhook @ 08/04/23 19:47:52.903
  STEP: create a configmap should be unconditionally rejected by the webhook @ 08/04/23 19:47:52.911
  Aug  4 19:47:52.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9345" for this suite. @ 08/04/23 19:47:52.96
  STEP: Destroying namespace "webhook-markers-9782" for this suite. @ 08/04/23 19:47:52.964
  STEP: Destroying namespace "fail-closed-namespace-4961" for this suite. @ 08/04/23 19:47:52.968
• [3.544 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:166
  STEP: Creating a kubernetes client @ 08/04/23 19:47:52.972
  Aug  4 19:47:52.972: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename daemonsets @ 08/04/23 19:47:52.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:47:53.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:47:53.008
  STEP: Creating simple DaemonSet "daemon-set" @ 08/04/23 19:47:53.022
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/04/23 19:47:53.027
  Aug  4 19:47:53.030: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 19:47:53.030: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  Aug  4 19:47:54.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 19:47:54.035: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  Aug  4 19:47:55.036: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 19:47:55.036: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  Aug  4 19:47:56.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 19:47:56.037: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  Aug  4 19:47:57.037: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 19:47:57.037: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  Aug  4 19:47:58.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 19:47:58.035: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
  Aug  4 19:47:59.035: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 19:47:59.035: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 08/04/23 19:47:59.037
  Aug  4 19:47:59.049: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 19:47:59.049: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  Aug  4 19:48:00.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 19:48:00.054: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  Aug  4 19:48:01.053: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 19:48:01.053: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  Aug  4 19:48:02.054: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 19:48:02.054: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/04/23 19:48:02.056
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8600, will wait for the garbage collector to delete the pods @ 08/04/23 19:48:02.056
  Aug  4 19:48:02.112: INFO: Deleting DaemonSet.extensions daemon-set took: 4.311275ms
  Aug  4 19:48:02.213: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.695517ms
  Aug  4 19:48:04.016: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 19:48:04.016: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  4 19:48:04.019: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"850"},"items":null}

  Aug  4 19:48:04.021: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"850"},"items":null}

  Aug  4 19:48:04.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8600" for this suite. @ 08/04/23 19:48:04.028
• [11.060 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 08/04/23 19:48:04.032
  Aug  4 19:48:04.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename namespaces @ 08/04/23 19:48:04.032
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:04.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:04.044
  STEP: Creating a test namespace @ 08/04/23 19:48:04.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:04.054
  STEP: Creating a service in the namespace @ 08/04/23 19:48:04.056
  STEP: Deleting the namespace @ 08/04/23 19:48:04.064
  STEP: Waiting for the namespace to be removed. @ 08/04/23 19:48:04.071
  STEP: Recreating the namespace @ 08/04/23 19:48:10.073
  STEP: Verifying there is no service in the namespace @ 08/04/23 19:48:10.081
  Aug  4 19:48:10.084: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-9887" for this suite. @ 08/04/23 19:48:10.086
  STEP: Destroying namespace "nsdeletetest-3835" for this suite. @ 08/04/23 19:48:10.089
  Aug  4 19:48:10.091: INFO: Namespace nsdeletetest-3835 was already deleted
  STEP: Destroying namespace "nsdeletetest-5403" for this suite. @ 08/04/23 19:48:10.091
• [6.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 08/04/23 19:48:10.096
  Aug  4 19:48:10.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename namespaces @ 08/04/23 19:48:10.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:10.104
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:10.106
  STEP: Creating a test namespace @ 08/04/23 19:48:10.108
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:10.116
  STEP: Creating a pod in the namespace @ 08/04/23 19:48:10.118
  STEP: Waiting for the pod to have running status @ 08/04/23 19:48:10.124
  STEP: Deleting the namespace @ 08/04/23 19:48:12.129
  STEP: Waiting for the namespace to be removed. @ 08/04/23 19:48:12.135
  STEP: Recreating the namespace @ 08/04/23 19:48:23.138
  STEP: Verifying there are no pods in the namespace @ 08/04/23 19:48:23.147
  Aug  4 19:48:23.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6059" for this suite. @ 08/04/23 19:48:23.151
  STEP: Destroying namespace "nsdeletetest-1392" for this suite. @ 08/04/23 19:48:23.155
  Aug  4 19:48:23.157: INFO: Namespace nsdeletetest-1392 was already deleted
  STEP: Destroying namespace "nsdeletetest-9414" for this suite. @ 08/04/23 19:48:23.157
• [13.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 08/04/23 19:48:23.161
  Aug  4 19:48:23.161: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 19:48:23.162
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:23.169
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:23.171
  STEP: Creating configMap with name configmap-test-upd-6e9976e5-f7da-440f-adf0-bfefef90315b @ 08/04/23 19:48:23.175
  STEP: Creating the pod @ 08/04/23 19:48:23.179
  STEP: Waiting for pod with text data @ 08/04/23 19:48:25.189
  STEP: Waiting for pod with binary data @ 08/04/23 19:48:25.195
  Aug  4 19:48:25.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4211" for this suite. @ 08/04/23 19:48:25.202
• [2.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 08/04/23 19:48:25.206
  Aug  4 19:48:25.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename tables @ 08/04/23 19:48:25.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:25.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:25.217
  Aug  4 19:48:25.221: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-2060" for this suite. @ 08/04/23 19:48:25.223
• [0.020 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 08/04/23 19:48:25.226
  Aug  4 19:48:25.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 19:48:25.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:25.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:25.237
  STEP: Creating secret with name secret-test-51709006-4416-4737-b87a-b8b5d27a215e @ 08/04/23 19:48:25.239
  STEP: Creating a pod to test consume secrets @ 08/04/23 19:48:25.242
  STEP: Saw pod success @ 08/04/23 19:48:29.257
  Aug  4 19:48:29.259: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-e6f58c4e-57a8-4146-ac49-332d216ee57f container secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 19:48:29.264
  Aug  4 19:48:29.273: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-6422" for this suite. @ 08/04/23 19:48:29.275
• [4.052 seconds]
------------------------------
S
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 08/04/23 19:48:29.279
  Aug  4 19:48:29.279: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename endpointslice @ 08/04/23 19:48:29.279
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:29.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:29.29
  Aug  4 19:48:31.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8663" for this suite. @ 08/04/23 19:48:31.324
• [2.050 seconds]
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 08/04/23 19:48:31.329
  Aug  4 19:48:31.329: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 19:48:31.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:31.339
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:31.341
  STEP: Creating secret with name secret-test-af767154-5e91-4024-b0e1-bb43a27bb37e @ 08/04/23 19:48:31.344
  STEP: Creating a pod to test consume secrets @ 08/04/23 19:48:31.349
  STEP: Saw pod success @ 08/04/23 19:48:35.362
  Aug  4 19:48:35.364: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-bca8029b-0ff5-408d-b2b9-40ab4a7ac799 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 19:48:35.373
  Aug  4 19:48:35.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4121" for this suite. @ 08/04/23 19:48:35.386
• [4.062 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 08/04/23 19:48:35.391
  Aug  4 19:48:35.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 19:48:35.391
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:35.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:35.401
  Aug  4 19:48:35.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6844" for this suite. @ 08/04/23 19:48:35.426
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 08/04/23 19:48:35.43
  Aug  4 19:48:35.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl-logs @ 08/04/23 19:48:35.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:35.439
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:35.441
  STEP: creating an pod @ 08/04/23 19:48:35.443
  Aug  4 19:48:35.443: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-logs-3558 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Aug  4 19:48:35.506: INFO: stderr: ""
  Aug  4 19:48:35.506: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 08/04/23 19:48:35.506
  Aug  4 19:48:35.507: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Aug  4 19:48:37.513: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 08/04/23 19:48:37.513
  Aug  4 19:48:37.513: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-logs-3558 logs logs-generator logs-generator'
  Aug  4 19:48:37.572: INFO: stderr: ""
  Aug  4 19:48:37.572: INFO: stdout: "I0804 19:48:36.116626       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/h5w 566\nI0804 19:48:36.316806       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/z2g 516\nI0804 19:48:36.517324       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/h6x 309\nI0804 19:48:36.717723       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/5ns 450\nI0804 19:48:36.917060       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/27s 566\nI0804 19:48:37.117451       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/9rkr 472\nI0804 19:48:37.316782       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/spzx 505\nI0804 19:48:37.516940       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/kwgd 293\n"
  STEP: limiting log lines @ 08/04/23 19:48:37.572
  Aug  4 19:48:37.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-logs-3558 logs logs-generator logs-generator --tail=1'
  Aug  4 19:48:37.634: INFO: stderr: ""
  Aug  4 19:48:37.634: INFO: stdout: "I0804 19:48:37.516940       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/kwgd 293\n"
  Aug  4 19:48:37.634: INFO: got output "I0804 19:48:37.516940       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/kwgd 293\n"
  STEP: limiting log bytes @ 08/04/23 19:48:37.634
  Aug  4 19:48:37.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-logs-3558 logs logs-generator logs-generator --limit-bytes=1'
  Aug  4 19:48:37.693: INFO: stderr: ""
  Aug  4 19:48:37.693: INFO: stdout: "I"
  Aug  4 19:48:37.693: INFO: got output "I"
  STEP: exposing timestamps @ 08/04/23 19:48:37.693
  Aug  4 19:48:37.693: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-logs-3558 logs logs-generator logs-generator --tail=1 --timestamps'
  Aug  4 19:48:37.754: INFO: stderr: ""
  Aug  4 19:48:37.754: INFO: stdout: "2023-08-04T19:48:37.717467030Z I0804 19:48:37.717312       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/w6zm 534\n"
  Aug  4 19:48:37.754: INFO: got output "2023-08-04T19:48:37.717467030Z I0804 19:48:37.717312       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/w6zm 534\n"
  STEP: restricting to a time range @ 08/04/23 19:48:37.754
  Aug  4 19:48:40.255: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-logs-3558 logs logs-generator logs-generator --since=1s'
  Aug  4 19:48:40.317: INFO: stderr: ""
  Aug  4 19:48:40.317: INFO: stdout: "I0804 19:48:39.316837       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/rf8 505\nI0804 19:48:39.517176       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/tnbh 568\nI0804 19:48:39.717541       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/nbs6 436\nI0804 19:48:39.916841       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/pj7l 532\nI0804 19:48:40.117250       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/gc7 599\n"
  Aug  4 19:48:40.317: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-logs-3558 logs logs-generator logs-generator --since=24h'
  Aug  4 19:48:40.380: INFO: stderr: ""
  Aug  4 19:48:40.380: INFO: stdout: "I0804 19:48:36.116626       1 logs_generator.go:76] 0 GET /api/v1/namespaces/ns/pods/h5w 566\nI0804 19:48:36.316806       1 logs_generator.go:76] 1 POST /api/v1/namespaces/kube-system/pods/z2g 516\nI0804 19:48:36.517324       1 logs_generator.go:76] 2 GET /api/v1/namespaces/default/pods/h6x 309\nI0804 19:48:36.717723       1 logs_generator.go:76] 3 POST /api/v1/namespaces/ns/pods/5ns 450\nI0804 19:48:36.917060       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/ns/pods/27s 566\nI0804 19:48:37.117451       1 logs_generator.go:76] 5 POST /api/v1/namespaces/default/pods/9rkr 472\nI0804 19:48:37.316782       1 logs_generator.go:76] 6 POST /api/v1/namespaces/ns/pods/spzx 505\nI0804 19:48:37.516940       1 logs_generator.go:76] 7 GET /api/v1/namespaces/default/pods/kwgd 293\nI0804 19:48:37.717312       1 logs_generator.go:76] 8 PUT /api/v1/namespaces/ns/pods/w6zm 534\nI0804 19:48:37.917539       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/57j 458\nI0804 19:48:38.116829       1 logs_generator.go:76] 10 PUT /api/v1/namespaces/kube-system/pods/mmv 450\nI0804 19:48:38.317217       1 logs_generator.go:76] 11 POST /api/v1/namespaces/ns/pods/j2xw 518\nI0804 19:48:38.517549       1 logs_generator.go:76] 12 POST /api/v1/namespaces/ns/pods/xtl 231\nI0804 19:48:38.716832       1 logs_generator.go:76] 13 POST /api/v1/namespaces/kube-system/pods/q4vs 290\nI0804 19:48:38.917208       1 logs_generator.go:76] 14 PUT /api/v1/namespaces/default/pods/slbn 248\nI0804 19:48:39.117539       1 logs_generator.go:76] 15 POST /api/v1/namespaces/kube-system/pods/plg9 367\nI0804 19:48:39.316837       1 logs_generator.go:76] 16 PUT /api/v1/namespaces/kube-system/pods/rf8 505\nI0804 19:48:39.517176       1 logs_generator.go:76] 17 GET /api/v1/namespaces/default/pods/tnbh 568\nI0804 19:48:39.717541       1 logs_generator.go:76] 18 PUT /api/v1/namespaces/ns/pods/nbs6 436\nI0804 19:48:39.916841       1 logs_generator.go:76] 19 GET /api/v1/namespaces/default/pods/pj7l 532\nI0804 19:48:40.117250       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/gc7 599\nI0804 19:48:40.317619       1 logs_generator.go:76] 21 POST /api/v1/namespaces/kube-system/pods/bdc4 214\n"
  Aug  4 19:48:40.380: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-logs-3558 delete pod logs-generator'
  Aug  4 19:48:41.466: INFO: stderr: ""
  Aug  4 19:48:41.466: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Aug  4 19:48:41.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-3558" for this suite. @ 08/04/23 19:48:41.468
• [6.042 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 08/04/23 19:48:41.472
  Aug  4 19:48:41.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 19:48:41.473
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:41.482
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:41.484
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 19:48:41.487
  STEP: Saw pod success @ 08/04/23 19:48:45.501
  Aug  4 19:48:45.503: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-e4067370-70ea-4151-9bc9-25f1bfbeb8d1 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 19:48:45.508
  Aug  4 19:48:45.519: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9833" for this suite. @ 08/04/23 19:48:45.521
• [4.052 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 08/04/23 19:48:45.524
  Aug  4 19:48:45.524: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 19:48:45.525
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:45.534
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:45.536
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 08/04/23 19:48:45.538
  Aug  4 19:48:45.538: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 08/04/23 19:48:50.799
  Aug  4 19:48:50.800: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 19:48:52.027: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 19:48:57.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-2131" for this suite. @ 08/04/23 19:48:57.077
• [11.556 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 08/04/23 19:48:57.083
  Aug  4 19:48:57.083: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replicaset @ 08/04/23 19:48:57.084
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:48:57.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:48:57.095
  Aug  4 19:48:57.097: INFO: Creating ReplicaSet my-hostname-basic-c150ef3d-2872-4579-b90c-55f5eec5ed9a
  Aug  4 19:48:57.104: INFO: Pod name my-hostname-basic-c150ef3d-2872-4579-b90c-55f5eec5ed9a: Found 0 pods out of 1
  Aug  4 19:49:02.106: INFO: Pod name my-hostname-basic-c150ef3d-2872-4579-b90c-55f5eec5ed9a: Found 1 pods out of 1
  Aug  4 19:49:02.106: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-c150ef3d-2872-4579-b90c-55f5eec5ed9a" is running
  Aug  4 19:49:02.108: INFO: Pod "my-hostname-basic-c150ef3d-2872-4579-b90c-55f5eec5ed9a-wcqwz" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-04 19:48:57 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-04 19:48:58 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-04 19:48:58 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-04 19:48:57 +0000 UTC Reason: Message:}])
  Aug  4 19:49:02.108: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/04/23 19:49:02.108
  Aug  4 19:49:02.114: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5377" for this suite. @ 08/04/23 19:49:02.116
• [5.037 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 08/04/23 19:49:02.12
  Aug  4 19:49:02.120: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename job @ 08/04/23 19:49:02.121
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:49:02.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:49:02.132
  STEP: Creating a suspended job @ 08/04/23 19:49:02.135
  STEP: Patching the Job @ 08/04/23 19:49:02.141
  STEP: Watching for Job to be patched @ 08/04/23 19:49:02.148
  Aug  4 19:49:02.150: INFO: Event ADDED observed for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking:]
  Aug  4 19:49:02.150: INFO: Event MODIFIED found for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 08/04/23 19:49:02.15
  STEP: Watching for Job to be updated @ 08/04/23 19:49:02.179
  Aug  4 19:49:02.180: INFO: Event MODIFIED found for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  4 19:49:02.180: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 08/04/23 19:49:02.18
  Aug  4 19:49:02.182: INFO: Job: e2e-dthtv as labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv]
  STEP: Waiting for job to complete @ 08/04/23 19:49:02.182
  STEP: Delete a job collection with a labelselector @ 08/04/23 19:49:10.186
  STEP: Watching for Job to be deleted @ 08/04/23 19:49:10.191
  Aug  4 19:49:10.192: INFO: Event MODIFIED observed for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  4 19:49:10.192: INFO: Event MODIFIED observed for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  4 19:49:10.192: INFO: Event MODIFIED observed for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  4 19:49:10.192: INFO: Event MODIFIED observed for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  4 19:49:10.193: INFO: Event MODIFIED observed for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Aug  4 19:49:10.193: INFO: Event DELETED found for Job e2e-dthtv in namespace job-7721 with labels: map[e2e-dthtv:patched e2e-job-label:e2e-dthtv] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 08/04/23 19:49:10.193
  Aug  4 19:49:10.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7721" for this suite. @ 08/04/23 19:49:10.197
• [8.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 08/04/23 19:49:10.205
  Aug  4 19:49:10.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-pred @ 08/04/23 19:49:10.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:49:10.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:49:10.218
  Aug  4 19:49:10.220: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug  4 19:49:10.225: INFO: Waiting for terminating namespaces to be deleted...
  Aug  4 19:49:10.226: INFO: 
  Logging pods the apiserver thinks is on node k8sconformance before test
  Aug  4 19:49:10.229: INFO: coredns-5d78c9869d-zl979 from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container coredns ready: true, restart count 1
  Aug  4 19:49:10.230: INFO: etcd-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container etcd ready: true, restart count 0
  Aug  4 19:49:10.230: INFO: kindnet-5zl6d from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container kindnet-cni ready: true, restart count 0
  Aug  4 19:49:10.230: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug  4 19:49:10.230: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug  4 19:49:10.230: INFO: kube-proxy-mslwk from kube-system started at 2023-08-04 19:46:58 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug  4 19:49:10.230: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug  4 19:49:10.230: INFO: storage-provisioner from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container storage-provisioner ready: true, restart count 1
  Aug  4 19:49:10.230: INFO: sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-fdn76 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 19:49:10.230: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 19:49:10.230: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  4 19:49:10.230: INFO: 
  Logging pods the apiserver thinks is on node k8sconformance-m02 before test
  Aug  4 19:49:10.233: INFO: kindnet-2zd4g from kube-system started at 2023-08-04 19:47:12 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.233: INFO: 	Container kindnet-cni ready: true, restart count 0
  Aug  4 19:49:10.233: INFO: kube-proxy-nn2ml from kube-system started at 2023-08-04 19:47:12 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.233: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug  4 19:49:10.233: INFO: sonobuoy from sonobuoy started at 2023-08-04 19:47:19 +0000 UTC (1 container statuses recorded)
  Aug  4 19:49:10.233: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug  4 19:49:10.233: INFO: sonobuoy-e2e-job-e2b8ac8d10f74a23 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 19:49:10.233: INFO: 	Container e2e ready: true, restart count 0
  Aug  4 19:49:10.233: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 19:49:10.233: INFO: sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-plz82 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 19:49:10.233: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 19:49:10.233: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/04/23 19:49:10.233
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/04/23 19:49:12.245
  STEP: Trying to apply a random label on the found node. @ 08/04/23 19:49:12.257
  STEP: verifying the node has the label kubernetes.io/e2e-0b0451b5-9d19-4284-a343-599eaea5c736 95 @ 08/04/23 19:49:12.264
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 08/04/23 19:49:12.267
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.49.3 on the node which pod4 resides and expect not scheduled @ 08/04/23 19:49:14.278
  STEP: removing the label kubernetes.io/e2e-0b0451b5-9d19-4284-a343-599eaea5c736 off the node k8sconformance-m02 @ 08/04/23 19:54:14.287
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-0b0451b5-9d19-4284-a343-599eaea5c736 @ 08/04/23 19:54:14.295
  Aug  4 19:54:14.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-1799" for this suite. @ 08/04/23 19:54:14.3
• [304.099 seconds]
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 08/04/23 19:54:14.305
  Aug  4 19:54:14.305: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 19:54:14.305
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:54:14.314
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:54:14.316
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 19:54:14.318
  STEP: Saw pod success @ 08/04/23 19:54:18.332
  Aug  4 19:54:18.334: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-fbb5d308-0e1e-4e79-8731-0b9ebc387146 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 19:54:18.35
  Aug  4 19:54:18.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4951" for this suite. @ 08/04/23 19:54:18.365
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 08/04/23 19:54:18.37
  Aug  4 19:54:18.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 19:54:18.37
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:54:18.377
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:54:18.379
  STEP: Starting the proxy @ 08/04/23 19:54:18.381
  Aug  4 19:54:18.381: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-1446 proxy --unix-socket=/tmp/kubectl-proxy-unix2518496711/test'
  STEP: retrieving proxy /api/ output @ 08/04/23 19:54:18.424
  Aug  4 19:54:18.425: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1446" for this suite. @ 08/04/23 19:54:18.428
• [0.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 08/04/23 19:54:18.434
  Aug  4 19:54:18.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 19:54:18.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:54:18.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:54:18.444
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 08/04/23 19:54:18.446
  Aug  4 19:54:18.446: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 19:54:19.690: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 19:54:24.851: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7165" for this suite. @ 08/04/23 19:54:24.856
• [6.426 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 08/04/23 19:54:24.861
  Aug  4 19:54:24.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 19:54:24.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:54:24.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:54:24.871
  STEP: Creating resourceQuota "e2e-rq-status-qb9xd" @ 08/04/23 19:54:24.875
  Aug  4 19:54:24.879: INFO: Resource quota "e2e-rq-status-qb9xd" reports spec: hard cpu limit of 500m
  Aug  4 19:54:24.879: INFO: Resource quota "e2e-rq-status-qb9xd" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-qb9xd" /status @ 08/04/23 19:54:24.879
  STEP: Confirm /status for "e2e-rq-status-qb9xd" resourceQuota via watch @ 08/04/23 19:54:24.899
  Aug  4 19:54:24.900: INFO: observed resourceQuota "e2e-rq-status-qb9xd" in namespace "resourcequota-2748" with hard status: v1.ResourceList(nil)
  Aug  4 19:54:24.900: INFO: Found resourceQuota "e2e-rq-status-qb9xd" in namespace "resourcequota-2748" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug  4 19:54:24.900: INFO: ResourceQuota "e2e-rq-status-qb9xd" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 08/04/23 19:54:24.901
  Aug  4 19:54:24.906: INFO: Resource quota "e2e-rq-status-qb9xd" reports spec: hard cpu limit of 1
  Aug  4 19:54:24.906: INFO: Resource quota "e2e-rq-status-qb9xd" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-qb9xd" /status @ 08/04/23 19:54:24.906
  STEP: Confirm /status for "e2e-rq-status-qb9xd" resourceQuota via watch @ 08/04/23 19:54:24.91
  Aug  4 19:54:24.911: INFO: observed resourceQuota "e2e-rq-status-qb9xd" in namespace "resourcequota-2748" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Aug  4 19:54:24.912: INFO: Found resourceQuota "e2e-rq-status-qb9xd" in namespace "resourcequota-2748" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Aug  4 19:54:24.912: INFO: ResourceQuota "e2e-rq-status-qb9xd" /status was patched
  STEP: Get "e2e-rq-status-qb9xd" /status @ 08/04/23 19:54:24.912
  Aug  4 19:54:24.914: INFO: Resourcequota "e2e-rq-status-qb9xd" reports status: hard cpu of 1
  Aug  4 19:54:24.914: INFO: Resourcequota "e2e-rq-status-qb9xd" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-qb9xd" /status before checking Spec is unchanged @ 08/04/23 19:54:24.915
  Aug  4 19:54:24.918: INFO: Resourcequota "e2e-rq-status-qb9xd" reports status: hard cpu of 2
  Aug  4 19:54:24.918: INFO: Resourcequota "e2e-rq-status-qb9xd" reports status: hard memory of 2Gi
  Aug  4 19:54:24.919: INFO: Found resourceQuota "e2e-rq-status-qb9xd" in namespace "resourcequota-2748" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Aug  4 19:56:59.925: INFO: ResourceQuota "e2e-rq-status-qb9xd" Spec was unchanged and /status reset
  Aug  4 19:56:59.926: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2748" for this suite. @ 08/04/23 19:56:59.928
• [155.072 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 08/04/23 19:56:59.934
  Aug  4 19:56:59.934: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename job @ 08/04/23 19:56:59.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:56:59.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:56:59.944
  STEP: Creating a job @ 08/04/23 19:56:59.946
  STEP: Ensure pods equal to parallelism count is attached to the job @ 08/04/23 19:56:59.95
  STEP: patching /status @ 08/04/23 19:57:01.953
  STEP: updating /status @ 08/04/23 19:57:01.959
  STEP: get /status @ 08/04/23 19:57:01.983
  Aug  4 19:57:01.985: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4046" for this suite. @ 08/04/23 19:57:01.987
• [2.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 08/04/23 19:57:01.993
  Aug  4 19:57:01.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename taint-multiple-pods @ 08/04/23 19:57:01.994
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:57:02.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:57:02.005
  Aug  4 19:57:02.007: INFO: Waiting up to 1m0s for all nodes to be ready
  Aug  4 19:58:02.017: INFO: Waiting for terminating namespaces to be deleted...
  Aug  4 19:58:02.019: INFO: Starting informer...
  STEP: Starting pods... @ 08/04/23 19:58:02.019
  Aug  4 19:58:02.229: INFO: Pod1 is running on k8sconformance-m02. Tainting Node
  Aug  4 19:58:04.444: INFO: Pod2 is running on k8sconformance-m02. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/04/23 19:58:04.444
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/04/23 19:58:04.452
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 08/04/23 19:58:04.454
  Aug  4 19:58:09.942: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Aug  4 19:58:30.047: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Aug  4 19:58:30.047: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/04/23 19:58:30.057
  STEP: Destroying namespace "taint-multiple-pods-1267" for this suite. @ 08/04/23 19:58:30.059
• [88.069 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 08/04/23 19:58:30.063
  Aug  4 19:58:30.063: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename disruption @ 08/04/23 19:58:30.064
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:58:30.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:58:30.076
  STEP: Waiting for the pdb to be processed @ 08/04/23 19:58:30.084
  STEP: Updating PodDisruptionBudget status @ 08/04/23 19:58:32.088
  STEP: Waiting for all pods to be running @ 08/04/23 19:58:32.092
  Aug  4 19:58:32.094: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 08/04/23 19:58:34.097
  STEP: Waiting for the pdb to be processed @ 08/04/23 19:58:34.105
  STEP: Patching PodDisruptionBudget status @ 08/04/23 19:58:34.109
  STEP: Waiting for the pdb to be processed @ 08/04/23 19:58:34.114
  Aug  4 19:58:34.115: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-9017" for this suite. @ 08/04/23 19:58:34.117
• [4.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 08/04/23 19:58:34.123
  Aug  4 19:58:34.123: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename runtimeclass @ 08/04/23 19:58:34.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:58:34.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:58:34.132
  Aug  4 19:58:36.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-9576" for this suite. @ 08/04/23 19:58:36.152
• [2.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 08/04/23 19:58:36.156
  Aug  4 19:58:36.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename var-expansion @ 08/04/23 19:58:36.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:58:36.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:58:36.165
  Aug  4 19:58:38.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 19:58:38.178: INFO: Deleting pod "var-expansion-03c3c958-0005-4a49-a4cd-b455b7161f6a" in namespace "var-expansion-2567"
  Aug  4 19:58:38.183: INFO: Wait up to 5m0s for pod "var-expansion-03c3c958-0005-4a49-a4cd-b455b7161f6a" to be fully deleted
  STEP: Destroying namespace "var-expansion-2567" for this suite. @ 08/04/23 19:58:40.189
• [4.036 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 08/04/23 19:58:40.193
  Aug  4 19:58:40.193: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename endpointslice @ 08/04/23 19:58:40.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:58:40.202
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:58:40.204
  STEP: getting /apis @ 08/04/23 19:58:40.205
  STEP: getting /apis/discovery.k8s.io @ 08/04/23 19:58:40.208
  STEP: getting /apis/discovery.k8s.iov1 @ 08/04/23 19:58:40.209
  STEP: creating @ 08/04/23 19:58:40.21
  STEP: getting @ 08/04/23 19:58:40.22
  STEP: listing @ 08/04/23 19:58:40.223
  STEP: watching @ 08/04/23 19:58:40.224
  Aug  4 19:58:40.224: INFO: starting watch
  STEP: cluster-wide listing @ 08/04/23 19:58:40.225
  STEP: cluster-wide watching @ 08/04/23 19:58:40.226
  Aug  4 19:58:40.226: INFO: starting watch
  STEP: patching @ 08/04/23 19:58:40.227
  STEP: updating @ 08/04/23 19:58:40.23
  Aug  4 19:58:40.234: INFO: waiting for watch events with expected annotations
  Aug  4 19:58:40.234: INFO: saw patched and updated annotations
  STEP: deleting @ 08/04/23 19:58:40.234
  STEP: deleting a collection @ 08/04/23 19:58:40.241
  Aug  4 19:58:40.248: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8938" for this suite. @ 08/04/23 19:58:40.25
• [0.060 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 08/04/23 19:58:40.254
  Aug  4 19:58:40.254: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename gc @ 08/04/23 19:58:40.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:58:40.261
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:58:40.263
  STEP: create the rc @ 08/04/23 19:58:40.265
  W0804 19:58:40.268475      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 08/04/23 19:58:45.272
  STEP: wait for all pods to be garbage collected @ 08/04/23 19:58:45.277
  STEP: Gathering metrics @ 08/04/23 19:58:50.283
  Aug  4 19:58:50.362: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  4 19:58:50.362: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-6180" for this suite. @ 08/04/23 19:58:50.364
• [10.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 08/04/23 19:58:50.37
  Aug  4 19:58:50.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 19:58:50.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:58:50.38
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:58:50.382
  STEP: Creating configMap with name projected-configmap-test-volume-d43c4119-e493-469c-8872-5bf65c173914 @ 08/04/23 19:58:50.384
  STEP: Creating a pod to test consume configMaps @ 08/04/23 19:58:50.388
  STEP: Saw pod success @ 08/04/23 19:58:52.398
  Aug  4 19:58:52.399: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-bb1c0634-0993-4509-a9fd-d6bfbfa95022 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 19:58:52.411
  Aug  4 19:58:52.420: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3242" for this suite. @ 08/04/23 19:58:52.422
• [2.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 08/04/23 19:58:52.427
  Aug  4 19:58:52.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename cronjob @ 08/04/23 19:58:52.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 19:58:52.434
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 19:58:52.436
  STEP: Creating a suspended cronjob @ 08/04/23 19:58:52.437
  STEP: Ensuring no jobs are scheduled @ 08/04/23 19:58:52.442
  STEP: Ensuring no job exists by listing jobs explicitly @ 08/04/23 20:03:52.446
  STEP: Removing cronjob @ 08/04/23 20:03:52.448
  Aug  4 20:03:52.453: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4182" for this suite. @ 08/04/23 20:03:52.455
• [300.031 seconds]
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 08/04/23 20:03:52.458
  Aug  4 20:03:52.458: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename cronjob @ 08/04/23 20:03:52.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:03:52.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:03:52.468
  STEP: Creating a ReplaceConcurrent cronjob @ 08/04/23 20:03:52.47
  STEP: Ensuring a job is scheduled @ 08/04/23 20:03:52.475
  STEP: Ensuring exactly one is scheduled @ 08/04/23 20:04:00.479
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/04/23 20:04:00.481
  STEP: Ensuring the job is replaced with a new one @ 08/04/23 20:04:00.484
  STEP: Removing cronjob @ 08/04/23 20:05:00.486
  Aug  4 20:05:00.490: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3195" for this suite. @ 08/04/23 20:05:00.493
• [68.041 seconds]
------------------------------
SSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 08/04/23 20:05:00.499
  Aug  4 20:05:00.499: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pod-network-test @ 08/04/23 20:05:00.5
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:00.51
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:00.512
  STEP: Performing setup for networking test in namespace pod-network-test-6247 @ 08/04/23 20:05:00.514
  STEP: creating a selector @ 08/04/23 20:05:00.514
  STEP: Creating the service pods in kubernetes @ 08/04/23 20:05:00.514
  Aug  4 20:05:00.514: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 08/04/23 20:05:22.57
  Aug  4 20:05:24.581: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Aug  4 20:05:24.581: INFO: Breadth first check of 10.244.0.6 on host 192.168.49.2...
  Aug  4 20:05:24.583: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.7:9080/dial?request=hostname&protocol=http&host=10.244.0.6&port=8083&tries=1'] Namespace:pod-network-test-6247 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:05:24.583: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:05:24.583: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:05:24.583: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6247/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.7%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.0.6%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  4 20:05:24.654: INFO: Waiting for responses: map[]
  Aug  4 20:05:24.654: INFO: reached 10.244.0.6 after 0/1 tries
  Aug  4 20:05:24.654: INFO: Breadth first check of 10.244.1.32 on host 192.168.49.3...
  Aug  4 20:05:24.656: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.0.7:9080/dial?request=hostname&protocol=http&host=10.244.1.32&port=8083&tries=1'] Namespace:pod-network-test-6247 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:05:24.656: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:05:24.657: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:05:24.657: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-6247/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.0.7%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.244.1.32%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  4 20:05:24.709: INFO: Waiting for responses: map[]
  Aug  4 20:05:24.709: INFO: reached 10.244.1.32 after 0/1 tries
  Aug  4 20:05:24.709: INFO: Going to retry 0 out of 2 pods....
  Aug  4 20:05:24.709: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-6247" for this suite. @ 08/04/23 20:05:24.712
• [24.217 seconds]
------------------------------
SSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 08/04/23 20:05:24.716
  Aug  4 20:05:24.716: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:05:24.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:24.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:24.726
  STEP: Creating projection with secret that has name secret-emptykey-test-2e275280-5734-444e-9b83-afb7e9ec5a1b @ 08/04/23 20:05:24.728
  Aug  4 20:05:24.730: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7225" for this suite. @ 08/04/23 20:05:24.731
• [0.019 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 08/04/23 20:05:24.736
  Aug  4 20:05:24.736: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:05:24.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:24.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:24.747
  STEP: Creating secret with name projected-secret-test-86fd2922-8134-4cae-ac6f-32ea37c45f8d @ 08/04/23 20:05:24.748
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:05:24.752
  STEP: Saw pod success @ 08/04/23 20:05:28.766
  Aug  4 20:05:28.768: INFO: Trying to get logs from node k8sconformance pod pod-projected-secrets-cd58f145-9fd7-490f-b4be-cb114c59e92f container secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:05:28.782
  Aug  4 20:05:28.792: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9444" for this suite. @ 08/04/23 20:05:28.794
• [4.062 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:610
  STEP: Creating a kubernetes client @ 08/04/23 20:05:28.798
  Aug  4 20:05:28.798: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename field-validation @ 08/04/23 20:05:28.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:28.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:28.808
  Aug  4 20:05:28.810: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  W0804 20:05:31.340507      22 warnings.go:70] unknown field "alpha"
  W0804 20:05:31.340530      22 warnings.go:70] unknown field "beta"
  W0804 20:05:31.340534      22 warnings.go:70] unknown field "delta"
  W0804 20:05:31.340538      22 warnings.go:70] unknown field "epsilon"
  W0804 20:05:31.340541      22 warnings.go:70] unknown field "gamma"
  Aug  4 20:05:31.348: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9819" for this suite. @ 08/04/23 20:05:31.356
• [2.561 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 08/04/23 20:05:31.36
  Aug  4 20:05:31.360: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename job @ 08/04/23 20:05:31.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:31.368
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:31.37
  STEP: Creating a job @ 08/04/23 20:05:31.372
  STEP: Ensuring active pods == parallelism @ 08/04/23 20:05:31.375
  STEP: Orphaning one of the Job's Pods @ 08/04/23 20:05:33.378
  Aug  4 20:05:33.889: INFO: Successfully updated pod "adopt-release-8sspk"
  STEP: Checking that the Job readopts the Pod @ 08/04/23 20:05:33.889
  STEP: Removing the labels from the Job's Pod @ 08/04/23 20:05:35.897
  Aug  4 20:05:36.406: INFO: Successfully updated pod "adopt-release-8sspk"
  STEP: Checking that the Job releases the Pod @ 08/04/23 20:05:36.406
  Aug  4 20:05:38.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2863" for this suite. @ 08/04/23 20:05:38.414
• [7.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 08/04/23 20:05:38.421
  Aug  4 20:05:38.421: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:05:38.422
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:38.432
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:38.434
  STEP: Creating projection with secret that has name projected-secret-test-map-faded395-3236-401e-95a4-ef2f6b3925bf @ 08/04/23 20:05:38.436
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:05:38.439
  STEP: Saw pod success @ 08/04/23 20:05:42.452
  Aug  4 20:05:42.454: INFO: Trying to get logs from node k8sconformance pod pod-projected-secrets-99de068a-6698-4dc6-a3da-0336d8bfb637 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:05:42.459
  Aug  4 20:05:42.469: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2527" for this suite. @ 08/04/23 20:05:42.471
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 08/04/23 20:05:42.475
  Aug  4 20:05:42.475: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:05:42.475
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:42.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:42.485
  STEP: creating a replication controller @ 08/04/23 20:05:42.487
  Aug  4 20:05:42.487: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 create -f -'
  Aug  4 20:05:43.131: INFO: stderr: ""
  Aug  4 20:05:43.131: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/04/23 20:05:43.131
  Aug  4 20:05:43.131: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  4 20:05:43.197: INFO: stderr: ""
  Aug  4 20:05:43.197: INFO: stdout: "update-demo-nautilus-6zjxs update-demo-nautilus-lbrfq "
  Aug  4 20:05:43.197: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get pods update-demo-nautilus-6zjxs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:05:43.251: INFO: stderr: ""
  Aug  4 20:05:43.251: INFO: stdout: ""
  Aug  4 20:05:43.251: INFO: update-demo-nautilus-6zjxs is created but not running
  Aug  4 20:05:48.252: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  4 20:05:48.308: INFO: stderr: ""
  Aug  4 20:05:48.308: INFO: stdout: "update-demo-nautilus-6zjxs update-demo-nautilus-lbrfq "
  Aug  4 20:05:48.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get pods update-demo-nautilus-6zjxs -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:05:48.361: INFO: stderr: ""
  Aug  4 20:05:48.361: INFO: stdout: "true"
  Aug  4 20:05:48.361: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get pods update-demo-nautilus-6zjxs -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  4 20:05:48.418: INFO: stderr: ""
  Aug  4 20:05:48.418: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  4 20:05:48.418: INFO: validating pod update-demo-nautilus-6zjxs
  Aug  4 20:05:48.421: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  4 20:05:48.421: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  4 20:05:48.421: INFO: update-demo-nautilus-6zjxs is verified up and running
  Aug  4 20:05:48.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get pods update-demo-nautilus-lbrfq -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:05:48.477: INFO: stderr: ""
  Aug  4 20:05:48.477: INFO: stdout: "true"
  Aug  4 20:05:48.477: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get pods update-demo-nautilus-lbrfq -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  4 20:05:48.531: INFO: stderr: ""
  Aug  4 20:05:48.531: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  4 20:05:48.531: INFO: validating pod update-demo-nautilus-lbrfq
  Aug  4 20:05:48.534: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  4 20:05:48.534: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  4 20:05:48.534: INFO: update-demo-nautilus-lbrfq is verified up and running
  STEP: using delete to clean up resources @ 08/04/23 20:05:48.534
  Aug  4 20:05:48.534: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 delete --grace-period=0 --force -f -'
  Aug  4 20:05:48.589: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:05:48.589: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug  4 20:05:48.589: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get rc,svc -l name=update-demo --no-headers'
  Aug  4 20:05:48.651: INFO: stderr: "No resources found in kubectl-7491 namespace.\n"
  Aug  4 20:05:48.651: INFO: stdout: ""
  Aug  4 20:05:48.652: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7491 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug  4 20:05:48.719: INFO: stderr: ""
  Aug  4 20:05:48.719: INFO: stdout: ""
  Aug  4 20:05:48.719: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7491" for this suite. @ 08/04/23 20:05:48.722
• [6.251 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 08/04/23 20:05:48.726
  Aug  4 20:05:48.726: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename proxy @ 08/04/23 20:05:48.727
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:48.737
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:48.739
  STEP: starting an echo server on multiple ports @ 08/04/23 20:05:48.749
  STEP: creating replication controller proxy-service-c5fq2 in namespace proxy-3601 @ 08/04/23 20:05:48.749
  I0804 20:05:48.755121      22 runners.go:194] Created replication controller with name: proxy-service-c5fq2, namespace: proxy-3601, replica count: 1
  I0804 20:05:49.806346      22 runners.go:194] proxy-service-c5fq2 Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0804 20:05:50.806795      22 runners.go:194] proxy-service-c5fq2 Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 20:05:50.809: INFO: setup took 2.067917525s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 08/04/23 20:05:50.809
  Aug  4 20:05:50.814: INFO: (0) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 5.250154ms)
  Aug  4 20:05:50.814: INFO: (0) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 5.302287ms)
  Aug  4 20:05:50.814: INFO: (0) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 5.638379ms)
  Aug  4 20:05:50.814: INFO: (0) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 5.702507ms)
  Aug  4 20:05:50.815: INFO: (0) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 6.03413ms)
  Aug  4 20:05:50.815: INFO: (0) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 6.068527ms)
  Aug  4 20:05:50.815: INFO: (0) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 6.319993ms)
  Aug  4 20:05:50.816: INFO: (0) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 7.676411ms)
  Aug  4 20:05:50.817: INFO: (0) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 7.868036ms)
  Aug  4 20:05:50.817: INFO: (0) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 7.877425ms)
  Aug  4 20:05:50.817: INFO: (0) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 8.011237ms)
  Aug  4 20:05:50.819: INFO: (0) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 10.65556ms)
  Aug  4 20:05:50.819: INFO: (0) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 10.700345ms)
  Aug  4 20:05:50.820: INFO: (0) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 10.923673ms)
  Aug  4 20:05:50.820: INFO: (0) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 10.977507ms)
  Aug  4 20:05:50.820: INFO: (0) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 10.947911ms)
  Aug  4 20:05:50.824: INFO: (1) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 4.627181ms)
  Aug  4 20:05:50.824: INFO: (1) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 4.72419ms)
  Aug  4 20:05:50.824: INFO: (1) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.626388ms)
  Aug  4 20:05:50.824: INFO: (1) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 4.535573ms)
  Aug  4 20:05:50.824: INFO: (1) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.621568ms)
  Aug  4 20:05:50.824: INFO: (1) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.572472ms)
  Aug  4 20:05:50.824: INFO: (1) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 4.659594ms)
  Aug  4 20:05:50.825: INFO: (1) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.626008ms)
  Aug  4 20:05:50.825: INFO: (1) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 5.023598ms)
  Aug  4 20:05:50.825: INFO: (1) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 5.042046ms)
  Aug  4 20:05:50.825: INFO: (1) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 5.038297ms)
  Aug  4 20:05:50.825: INFO: (1) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 5.068912ms)
  Aug  4 20:05:50.825: INFO: (1) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 5.104038ms)
  Aug  4 20:05:50.825: INFO: (1) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 5.069068ms)
  Aug  4 20:05:50.826: INFO: (1) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 5.695674ms)
  Aug  4 20:05:50.826: INFO: (1) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 5.891039ms)
  Aug  4 20:05:50.828: INFO: (2) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 2.230802ms)
  Aug  4 20:05:50.828: INFO: (2) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 2.498769ms)
  Aug  4 20:05:50.829: INFO: (2) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 2.975686ms)
  Aug  4 20:05:50.829: INFO: (2) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 2.336884ms)
  Aug  4 20:05:50.829: INFO: (2) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 2.785387ms)
  Aug  4 20:05:50.829: INFO: (2) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 2.960653ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.233255ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 2.927911ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 3.149574ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 2.921574ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 3.560842ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 3.681626ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.371148ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 4.426378ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 3.552659ms)
  Aug  4 20:05:50.830: INFO: (2) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.25589ms)
  Aug  4 20:05:50.834: INFO: (3) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.356121ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.960667ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 3.996916ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 4.147975ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 4.324631ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.101247ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.98573ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 4.213845ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 4.15071ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 4.28793ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.988131ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.293107ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.089708ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 4.595867ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.733402ms)
  Aug  4 20:05:50.835: INFO: (3) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 4.805036ms)
  Aug  4 20:05:50.838: INFO: (4) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 2.87367ms)
  Aug  4 20:05:50.838: INFO: (4) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 3.025624ms)
  Aug  4 20:05:50.838: INFO: (4) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 2.959097ms)
  Aug  4 20:05:50.838: INFO: (4) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.048281ms)
  Aug  4 20:05:50.839: INFO: (4) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 2.994583ms)
  Aug  4 20:05:50.839: INFO: (4) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.01039ms)
  Aug  4 20:05:50.839: INFO: (4) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 3.10235ms)
  Aug  4 20:05:50.839: INFO: (4) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 3.16612ms)
  Aug  4 20:05:50.839: INFO: (4) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 3.228132ms)
  Aug  4 20:05:50.839: INFO: (4) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.386929ms)
  Aug  4 20:05:50.840: INFO: (4) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.26107ms)
  Aug  4 20:05:50.840: INFO: (4) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.559768ms)
  Aug  4 20:05:50.840: INFO: (4) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.704482ms)
  Aug  4 20:05:50.840: INFO: (4) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.585592ms)
  Aug  4 20:05:50.840: INFO: (4) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 4.639563ms)
  Aug  4 20:05:50.840: INFO: (4) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.633498ms)
  Aug  4 20:05:50.843: INFO: (5) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 2.79816ms)
  Aug  4 20:05:50.843: INFO: (5) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 3.01561ms)
  Aug  4 20:05:50.843: INFO: (5) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 3.13647ms)
  Aug  4 20:05:50.843: INFO: (5) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.015285ms)
  Aug  4 20:05:50.843: INFO: (5) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.045027ms)
  Aug  4 20:05:50.843: INFO: (5) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 3.183259ms)
  Aug  4 20:05:50.844: INFO: (5) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.299879ms)
  Aug  4 20:05:50.844: INFO: (5) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 3.325691ms)
  Aug  4 20:05:50.844: INFO: (5) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 3.414261ms)
  Aug  4 20:05:50.904: INFO: (5) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 63.688096ms)
  Aug  4 20:05:50.904: INFO: (5) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 63.969587ms)
  Aug  4 20:05:50.904: INFO: (5) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 64.041099ms)
  Aug  4 20:05:50.904: INFO: (5) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 64.041512ms)
  Aug  4 20:05:50.904: INFO: (5) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 64.0261ms)
  Aug  4 20:05:50.904: INFO: (5) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 64.140198ms)
  Aug  4 20:05:50.904: INFO: (5) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 64.095825ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 4.12542ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 4.103475ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.381948ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.517831ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.432024ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 4.442389ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.448292ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 4.457256ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 4.430052ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 4.5002ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 4.516925ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.571052ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.432543ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 4.455501ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.581757ms)
  Aug  4 20:05:50.909: INFO: (6) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.784699ms)
  Aug  4 20:05:50.912: INFO: (7) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 2.994149ms)
  Aug  4 20:05:50.912: INFO: (7) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.035406ms)
  Aug  4 20:05:50.913: INFO: (7) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 3.141255ms)
  Aug  4 20:05:50.913: INFO: (7) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 3.471316ms)
  Aug  4 20:05:50.913: INFO: (7) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 3.463771ms)
  Aug  4 20:05:50.913: INFO: (7) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 3.951462ms)
  Aug  4 20:05:50.913: INFO: (7) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.99355ms)
  Aug  4 20:05:50.913: INFO: (7) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 3.974579ms)
  Aug  4 20:05:50.913: INFO: (7) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 4.011417ms)
  Aug  4 20:05:50.914: INFO: (7) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 4.231769ms)
  Aug  4 20:05:50.914: INFO: (7) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.28185ms)
  Aug  4 20:05:50.914: INFO: (7) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.295653ms)
  Aug  4 20:05:50.914: INFO: (7) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 4.425038ms)
  Aug  4 20:05:50.914: INFO: (7) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.286492ms)
  Aug  4 20:05:50.914: INFO: (7) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.442536ms)
  Aug  4 20:05:50.914: INFO: (7) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 4.661016ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 2.754994ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 2.849585ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 2.883709ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.234402ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.215972ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.268288ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.16848ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 3.198317ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 3.306428ms)
  Aug  4 20:05:50.917: INFO: (8) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 3.340466ms)
  Aug  4 20:05:50.918: INFO: (8) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 4.199262ms)
  Aug  4 20:05:50.919: INFO: (8) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.506645ms)
  Aug  4 20:05:50.919: INFO: (8) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 4.375186ms)
  Aug  4 20:05:50.919: INFO: (8) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.367865ms)
  Aug  4 20:05:50.919: INFO: (8) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.384181ms)
  Aug  4 20:05:50.919: INFO: (8) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.425858ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.79088ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 4.857105ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.85313ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.891227ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.839845ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 4.941936ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 4.897098ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 4.970287ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.960508ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 5.00347ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 5.092411ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.985789ms)
  Aug  4 20:05:50.924: INFO: (9) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 5.441341ms)
  Aug  4 20:05:50.925: INFO: (9) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 5.845703ms)
  Aug  4 20:05:50.925: INFO: (9) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 5.876316ms)
  Aug  4 20:05:50.925: INFO: (9) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 5.873177ms)
  Aug  4 20:05:50.928: INFO: (10) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 3.565168ms)
  Aug  4 20:05:50.928: INFO: (10) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.542391ms)
  Aug  4 20:05:50.928: INFO: (10) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 3.561618ms)
  Aug  4 20:05:50.928: INFO: (10) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 3.565594ms)
  Aug  4 20:05:50.928: INFO: (10) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 3.648133ms)
  Aug  4 20:05:50.928: INFO: (10) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.711314ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.596825ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 3.907922ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 4.213904ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.38317ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.507682ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.417989ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.447445ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 4.386312ms)
  Aug  4 20:05:50.929: INFO: (10) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.54936ms)
  Aug  4 20:05:50.930: INFO: (10) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.794508ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 2.672574ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.131981ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.242845ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 3.269303ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.235799ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.264496ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.359752ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 3.430368ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 3.525193ms)
  Aug  4 20:05:50.933: INFO: (11) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 3.505294ms)
  Aug  4 20:05:50.934: INFO: (11) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 3.902597ms)
  Aug  4 20:05:50.934: INFO: (11) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 3.950975ms)
  Aug  4 20:05:50.934: INFO: (11) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.056163ms)
  Aug  4 20:05:50.934: INFO: (11) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 3.935893ms)
  Aug  4 20:05:50.934: INFO: (11) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.109663ms)
  Aug  4 20:05:50.934: INFO: (11) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.12927ms)
  Aug  4 20:05:50.937: INFO: (12) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 2.477374ms)
  Aug  4 20:05:50.937: INFO: (12) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 2.545385ms)
  Aug  4 20:05:50.937: INFO: (12) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 2.511409ms)
  Aug  4 20:05:50.937: INFO: (12) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 2.603026ms)
  Aug  4 20:05:50.938: INFO: (12) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.369784ms)
  Aug  4 20:05:50.938: INFO: (12) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.60203ms)
  Aug  4 20:05:50.938: INFO: (12) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 3.640231ms)
  Aug  4 20:05:50.938: INFO: (12) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 4.02377ms)
  Aug  4 20:05:50.938: INFO: (12) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.097832ms)
  Aug  4 20:05:50.938: INFO: (12) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.20615ms)
  Aug  4 20:05:50.940: INFO: (12) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 5.412456ms)
  Aug  4 20:05:50.940: INFO: (12) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 5.461499ms)
  Aug  4 20:05:50.940: INFO: (12) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 5.523593ms)
  Aug  4 20:05:50.940: INFO: (12) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 5.486837ms)
  Aug  4 20:05:50.940: INFO: (12) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 5.463968ms)
  Aug  4 20:05:50.940: INFO: (12) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 5.524859ms)
  Aug  4 20:05:50.943: INFO: (13) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.264817ms)
  Aug  4 20:05:50.943: INFO: (13) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.206033ms)
  Aug  4 20:05:50.943: INFO: (13) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 3.280515ms)
  Aug  4 20:05:50.943: INFO: (13) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 3.286832ms)
  Aug  4 20:05:50.943: INFO: (13) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.262642ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 8.739016ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 8.758041ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 8.866045ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 8.891881ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 8.816353ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 8.936883ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 9.148064ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 9.136657ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 9.121188ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 9.265973ms)
  Aug  4 20:05:50.949: INFO: (13) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 9.259645ms)
  Aug  4 20:05:51.004: INFO: (14) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 54.911157ms)
  Aug  4 20:05:51.004: INFO: (14) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 54.892154ms)
  Aug  4 20:05:51.004: INFO: (14) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 54.899238ms)
  Aug  4 20:05:51.004: INFO: (14) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 54.93709ms)
  Aug  4 20:05:51.004: INFO: (14) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 55.023472ms)
  Aug  4 20:05:51.004: INFO: (14) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 54.932263ms)
  Aug  4 20:05:51.005: INFO: (14) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 55.579255ms)
  Aug  4 20:05:51.005: INFO: (14) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 55.488164ms)
  Aug  4 20:05:51.005: INFO: (14) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 55.597452ms)
  Aug  4 20:05:51.005: INFO: (14) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 55.507626ms)
  Aug  4 20:05:51.005: INFO: (14) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 55.570155ms)
  Aug  4 20:05:51.005: INFO: (14) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 55.966501ms)
  Aug  4 20:05:51.008: INFO: (14) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 58.935586ms)
  Aug  4 20:05:51.008: INFO: (14) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 58.991096ms)
  Aug  4 20:05:51.009: INFO: (14) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 59.614358ms)
  Aug  4 20:05:51.009: INFO: (14) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 59.692207ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 3.500271ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 3.466174ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 3.407462ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 3.694493ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.765855ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.711792ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.729015ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 3.91171ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 4.464168ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 4.43296ms)
  Aug  4 20:05:51.014: INFO: (15) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.483982ms)
  Aug  4 20:05:51.014: INFO: (15) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 4.393602ms)
  Aug  4 20:05:51.013: INFO: (15) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.449646ms)
  Aug  4 20:05:51.014: INFO: (15) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.565784ms)
  Aug  4 20:05:51.014: INFO: (15) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.536501ms)
  Aug  4 20:05:51.014: INFO: (15) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.811389ms)
  Aug  4 20:05:51.018: INFO: (16) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 3.628282ms)
  Aug  4 20:05:51.018: INFO: (16) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 3.648276ms)
  Aug  4 20:05:51.018: INFO: (16) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 3.844021ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.500642ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 4.683682ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.587809ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.633844ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.661996ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 4.946771ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 5.026571ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 5.074106ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 5.031222ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 5.066664ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 5.028241ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 4.971835ms)
  Aug  4 20:05:51.019: INFO: (16) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 5.01826ms)
  Aug  4 20:05:51.022: INFO: (17) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 3.145334ms)
  Aug  4 20:05:51.022: INFO: (17) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.20741ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 3.717837ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 3.821058ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 3.807386ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 3.863116ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 3.79247ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 3.891892ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.8126ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 3.90372ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 4.131207ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 4.083706ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 4.207472ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 4.245276ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 4.219531ms)
  Aug  4 20:05:51.023: INFO: (17) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 4.302341ms)
  Aug  4 20:05:51.026: INFO: (18) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 2.575582ms)
  Aug  4 20:05:51.026: INFO: (18) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 2.714031ms)
  Aug  4 20:05:51.026: INFO: (18) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 2.654388ms)
  Aug  4 20:05:51.026: INFO: (18) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 2.665668ms)
  Aug  4 20:05:51.027: INFO: (18) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 3.181456ms)
  Aug  4 20:05:51.027: INFO: (18) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 3.199455ms)
  Aug  4 20:05:51.027: INFO: (18) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 3.150357ms)
  Aug  4 20:05:51.027: INFO: (18) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 3.201687ms)
  Aug  4 20:05:51.027: INFO: (18) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 3.171969ms)
  Aug  4 20:05:51.027: INFO: (18) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 3.280042ms)
  Aug  4 20:05:51.028: INFO: (18) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 4.669683ms)
  Aug  4 20:05:51.028: INFO: (18) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 4.741296ms)
  Aug  4 20:05:51.028: INFO: (18) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 4.714713ms)
  Aug  4 20:05:51.028: INFO: (18) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 4.761539ms)
  Aug  4 20:05:51.028: INFO: (18) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 4.826521ms)
  Aug  4 20:05:51.028: INFO: (18) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 4.854842ms)
  Aug  4 20:05:51.033: INFO: (19) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:443/proxy/tlsrewritem... (200; 4.166176ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm/proxy/rewriteme">test</a> (200; 5.145338ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">test<... (200; 5.100028ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/: <a href="/api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:1080/proxy/rewriteme">... (200; 5.514485ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname2/proxy/: bar (200; 5.609546ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname2/proxy/: tls qux (200; 5.500643ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname1/proxy/: foo (200; 5.508434ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 5.56872ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:160/proxy/: foo (200; 5.601669ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/services/proxy-service-c5fq2:portname1/proxy/: foo (200; 5.591219ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/services/https:proxy-service-c5fq2:tlsportname1/proxy/: tls baz (200; 5.664243ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 5.723894ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:462/proxy/: tls qux (200; 5.674007ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/https:proxy-service-c5fq2-6jwjm:460/proxy/: tls baz (200; 5.602116ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/pods/http:proxy-service-c5fq2-6jwjm:162/proxy/: bar (200; 5.598543ms)
  Aug  4 20:05:51.034: INFO: (19) /api/v1/namespaces/proxy-3601/services/http:proxy-service-c5fq2:portname2/proxy/: bar (200; 5.666621ms)
  Aug  4 20:05:51.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-c5fq2 in namespace proxy-3601, will wait for the garbage collector to delete the pods @ 08/04/23 20:05:51.036
  Aug  4 20:05:51.093: INFO: Deleting ReplicationController proxy-service-c5fq2 took: 4.448895ms
  Aug  4 20:05:51.193: INFO: Terminating ReplicationController proxy-service-c5fq2 pods took: 100.178674ms
  STEP: Destroying namespace "proxy-3601" for this suite. @ 08/04/23 20:05:53.994
• [5.281 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 08/04/23 20:05:54.007
  Aug  4 20:05:54.007: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:05:54.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:54.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:54.019
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/04/23 20:05:54.022
  Aug  4 20:05:54.022: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7689 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug  4 20:05:54.083: INFO: stderr: ""
  Aug  4 20:05:54.083: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 08/04/23 20:05:54.083
  Aug  4 20:05:54.083: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7689 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Aug  4 20:05:54.143: INFO: stderr: ""
  Aug  4 20:05:54.143: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/04/23 20:05:54.143
  Aug  4 20:05:54.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7689 delete pods e2e-test-httpd-pod'
  Aug  4 20:05:56.624: INFO: stderr: ""
  Aug  4 20:05:56.624: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug  4 20:05:56.624: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7689" for this suite. @ 08/04/23 20:05:56.626
• [2.622 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 08/04/23 20:05:56.631
  Aug  4 20:05:56.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename events @ 08/04/23 20:05:56.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:56.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:56.641
  STEP: Create set of events @ 08/04/23 20:05:56.643
  Aug  4 20:05:56.646: INFO: created test-event-1
  Aug  4 20:05:56.648: INFO: created test-event-2
  Aug  4 20:05:56.651: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 08/04/23 20:05:56.651
  STEP: delete collection of events @ 08/04/23 20:05:56.652
  Aug  4 20:05:56.653: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/04/23 20:05:56.664
  Aug  4 20:05:56.664: INFO: requesting list of events to confirm quantity
  Aug  4 20:05:56.665: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-7639" for this suite. @ 08/04/23 20:05:56.667
• [0.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 08/04/23 20:05:56.671
  Aug  4 20:05:56.671: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 20:05:56.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:05:56.679
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:05:56.681
  STEP: set up a multi version CRD @ 08/04/23 20:05:56.683
  Aug  4 20:05:56.683: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: rename a version @ 08/04/23 20:05:59.926
  STEP: check the new version name is served @ 08/04/23 20:05:59.937
  STEP: check the old version name is removed @ 08/04/23 20:06:00.648
  STEP: check the other version is not changed @ 08/04/23 20:06:01.309
  Aug  4 20:06:03.928: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8878" for this suite. @ 08/04/23 20:06:03.934
• [7.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 08/04/23 20:06:03.939
  Aug  4 20:06:03.939: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:06:03.94
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:06:03.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:06:03.952
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:06:03.954
  STEP: Saw pod success @ 08/04/23 20:06:07.969
  Aug  4 20:06:07.971: INFO: Trying to get logs from node k8sconformance pod downwardapi-volume-5e55565e-7edf-4826-ab36-43ef98a0320a container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:06:07.976
  Aug  4 20:06:07.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1329" for this suite. @ 08/04/23 20:06:07.989
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 08/04/23 20:06:07.993
  Aug  4 20:06:07.993: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:06:07.994
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:06:08.004
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:06:08.006
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/04/23 20:06:08.008
  STEP: Saw pod success @ 08/04/23 20:06:12.022
  Aug  4 20:06:12.024: INFO: Trying to get logs from node k8sconformance pod pod-5fb4d59f-4e35-4bd3-b032-095f598c2885 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:06:12.03
  Aug  4 20:06:12.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8255" for this suite. @ 08/04/23 20:06:12.046
• [4.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 08/04/23 20:06:12.051
  Aug  4 20:06:12.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replication-controller @ 08/04/23 20:06:12.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:06:12.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:06:12.06
  STEP: creating a ReplicationController @ 08/04/23 20:06:12.064
  STEP: waiting for RC to be added @ 08/04/23 20:06:12.067
  STEP: waiting for available Replicas @ 08/04/23 20:06:12.068
  STEP: patching ReplicationController @ 08/04/23 20:06:12.724
  STEP: waiting for RC to be modified @ 08/04/23 20:06:12.733
  STEP: patching ReplicationController status @ 08/04/23 20:06:12.733
  STEP: waiting for RC to be modified @ 08/04/23 20:06:12.737
  STEP: waiting for available Replicas @ 08/04/23 20:06:12.737
  STEP: fetching ReplicationController status @ 08/04/23 20:06:12.741
  STEP: patching ReplicationController scale @ 08/04/23 20:06:12.743
  STEP: waiting for RC to be modified @ 08/04/23 20:06:12.749
  STEP: waiting for ReplicationController's scale to be the max amount @ 08/04/23 20:06:12.749
  STEP: fetching ReplicationController; ensuring that it's patched @ 08/04/23 20:06:14.101
  STEP: updating ReplicationController status @ 08/04/23 20:06:14.102
  STEP: waiting for RC to be modified @ 08/04/23 20:06:14.107
  STEP: listing all ReplicationControllers @ 08/04/23 20:06:14.108
  STEP: checking that ReplicationController has expected values @ 08/04/23 20:06:14.109
  STEP: deleting ReplicationControllers by collection @ 08/04/23 20:06:14.109
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 08/04/23 20:06:14.115
  Aug  4 20:06:14.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0804 20:06:14.147237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-9475" for this suite. @ 08/04/23 20:06:14.149
• [2.101 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 08/04/23 20:06:14.154
  Aug  4 20:06:14.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename runtimeclass @ 08/04/23 20:06:14.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:06:14.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:06:14.166
  STEP: getting /apis @ 08/04/23 20:06:14.167
  STEP: getting /apis/node.k8s.io @ 08/04/23 20:06:14.17
  STEP: getting /apis/node.k8s.io/v1 @ 08/04/23 20:06:14.171
  STEP: creating @ 08/04/23 20:06:14.171
  STEP: watching @ 08/04/23 20:06:14.18
  Aug  4 20:06:14.180: INFO: starting watch
  STEP: getting @ 08/04/23 20:06:14.185
  STEP: listing @ 08/04/23 20:06:14.186
  STEP: patching @ 08/04/23 20:06:14.188
  STEP: updating @ 08/04/23 20:06:14.191
  Aug  4 20:06:14.194: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 08/04/23 20:06:14.194
  STEP: deleting a collection @ 08/04/23 20:06:14.199
  Aug  4 20:06:14.207: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5490" for this suite. @ 08/04/23 20:06:14.209
• [0.058 seconds]
------------------------------
S
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 08/04/23 20:06:14.212
  Aug  4 20:06:14.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename init-container @ 08/04/23 20:06:14.213
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:06:14.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:06:14.224
  STEP: creating the pod @ 08/04/23 20:06:14.226
  Aug  4 20:06:14.226: INFO: PodSpec: initContainers in spec.initContainers
  E0804 20:06:15.148057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:16.148217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:17.149090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:18.149193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:19.149431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:20.149977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:21.150265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:22.150666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:23.150921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:24.151227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:25.151489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:26.151655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:27.152346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:28.152573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:29.152800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:30.153721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:31.153966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:32.154568      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:33.154865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:34.155013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:35.155255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:36.156085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:37.156615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:38.156850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:39.157071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:40.157315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:41.157554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:42.157898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:43.158059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:44.158257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:45.158626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:46.158894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:47.159426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:48.159676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:49.160023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:50.160206      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:51.160407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:52.160688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:53.161808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:54.162024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:55.162315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:56.162512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:57.163155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:06:58.062: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-633642fa-850f-4755-8a58-013296ddebde", GenerateName:"", Namespace:"init-container-6046", SelfLink:"", UID:"36c4195d-3f88-40d9-937d-977e61ecad16", ResourceVersion:"3371", Generation:0, CreationTimestamp:time.Date(2023, time.August, 4, 20, 6, 14, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"226053606"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 4, 20, 6, 14, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001309b90), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.August, 4, 20, 6, 58, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc001309bc0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-7b586", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0047792c0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7b586", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7b586", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-7b586", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0035af160), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"k8sconformance-m02", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000497c00), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0035af1f0)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0035af210)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0035af218), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0035af21c), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc0047d5050), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 4, 20, 6, 14, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 4, 20, 6, 14, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 4, 20, 6, 14, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.August, 4, 20, 6, 14, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.49.3", PodIP:"10.244.1.39", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.244.1.39"}}, StartTime:time.Date(2023, time.August, 4, 20, 6, 14, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000497d50)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc000497f10)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"docker-pullable://registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"docker://9b7f0ceba1d5220f7e72cd5fdeb6c3df090559f20f9122546781f139a62e055c", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004779340), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc004779320), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0035af29f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Aug  4 20:06:58.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-6046" for this suite. @ 08/04/23 20:06:58.065
• [43.857 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 08/04/23 20:06:58.071
  Aug  4 20:06:58.071: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:06:58.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:06:58.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:06:58.084
  STEP: Creating configMap with name projected-configmap-test-volume-map-5fe110be-d51d-4511-b234-08066f301f2b @ 08/04/23 20:06:58.086
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:06:58.088
  E0804 20:06:58.163885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:06:59.163952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:00.164602      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:01.164886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:07:02.102
  Aug  4 20:07:02.104: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-ec76e152-cb9a-46c5-8f7c-8f24863f062b container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:07:02.117
  Aug  4 20:07:02.128: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1637" for this suite. @ 08/04/23 20:07:02.13
• [4.064 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 08/04/23 20:07:02.135
  Aug  4 20:07:02.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 08/04/23 20:07:02.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:07:02.145
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:07:02.147
  STEP: creating a target pod @ 08/04/23 20:07:02.148
  E0804 20:07:02.165985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:03.166790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding an ephemeral container @ 08/04/23 20:07:04.161
  E0804 20:07:04.167807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:05.168657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:06.168816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking pod container endpoints @ 08/04/23 20:07:06.177
  Aug  4 20:07:06.177: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-576 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:07:06.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:07:06.177: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:07:06.177: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/ephemeral-containers-test-576/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Aug  4 20:07:06.253: INFO: Exec stderr: ""
  Aug  4 20:07:06.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-576" for this suite. @ 08/04/23 20:07:06.262
• [4.130 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 08/04/23 20:07:06.266
  Aug  4 20:07:06.266: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename subpath @ 08/04/23 20:07:06.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:07:06.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:07:06.278
  STEP: Setting up data @ 08/04/23 20:07:06.28
  STEP: Creating pod pod-subpath-test-projected-bq2x @ 08/04/23 20:07:06.287
  STEP: Creating a pod to test atomic-volume-subpath @ 08/04/23 20:07:06.287
  E0804 20:07:07.169110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:08.169344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:09.169404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:10.169621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:11.169959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:12.170469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:13.170970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:14.170978      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:15.171615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:16.171847      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:17.172673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:18.172909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:19.173750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:20.173974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:21.174599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:22.174979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:23.176033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:24.176210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:25.176742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:26.176963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:27.177950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:28.178192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:29.178201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:30.178445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:07:30.329
  Aug  4 20:07:30.330: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-projected-bq2x container test-container-subpath-projected-bq2x: <nil>
  STEP: delete the pod @ 08/04/23 20:07:30.336
  STEP: Deleting pod pod-subpath-test-projected-bq2x @ 08/04/23 20:07:30.345
  Aug  4 20:07:30.345: INFO: Deleting pod "pod-subpath-test-projected-bq2x" in namespace "subpath-6055"
  Aug  4 20:07:30.346: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6055" for this suite. @ 08/04/23 20:07:30.348
• [24.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 08/04/23 20:07:30.352
  Aug  4 20:07:30.352: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 20:07:30.353
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:07:30.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:07:30.363
  STEP: Discovering how many secrets are in namespace by default @ 08/04/23 20:07:30.365
  E0804 20:07:31.179010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:32.179547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:33.180024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:34.180645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:35.181205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 08/04/23 20:07:35.367
  E0804 20:07:36.181935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:37.182765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:38.183410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:39.183528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:40.183967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/04/23 20:07:40.369
  STEP: Ensuring resource quota status is calculated @ 08/04/23 20:07:40.372
  E0804 20:07:41.184958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:42.185441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Secret @ 08/04/23 20:07:42.378
  STEP: Ensuring resource quota status captures secret creation @ 08/04/23 20:07:42.39
  E0804 20:07:43.185561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:44.185832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a secret @ 08/04/23 20:07:44.393
  STEP: Ensuring resource quota status released usage @ 08/04/23 20:07:44.397
  E0804 20:07:45.186086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:46.186323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:07:46.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-1895" for this suite. @ 08/04/23 20:07:46.403
• [16.054 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 08/04/23 20:07:46.406
  Aug  4 20:07:46.406: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-runtime @ 08/04/23 20:07:46.407
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:07:46.417
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:07:46.419
  STEP: create the container @ 08/04/23 20:07:46.421
  W0804 20:07:46.426316      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/04/23 20:07:46.426
  E0804 20:07:47.186965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:48.187064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:49.188058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/04/23 20:07:49.436
  STEP: the container should be terminated @ 08/04/23 20:07:49.438
  STEP: the termination message should be set @ 08/04/23 20:07:49.438
  Aug  4 20:07:49.438: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 08/04/23 20:07:49.438
  Aug  4 20:07:49.446: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-8707" for this suite. @ 08/04/23 20:07:49.45
• [3.047 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 08/04/23 20:07:49.454
  Aug  4 20:07:49.454: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename field-validation @ 08/04/23 20:07:49.455
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:07:49.463
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:07:49.465
  STEP: apply creating a deployment @ 08/04/23 20:07:49.467
  Aug  4 20:07:49.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5312" for this suite. @ 08/04/23 20:07:49.475
• [0.024 seconds]
------------------------------
SS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 08/04/23 20:07:49.478
  Aug  4 20:07:49.478: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pod-network-test @ 08/04/23 20:07:49.479
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:07:49.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:07:49.489
  STEP: Performing setup for networking test in namespace pod-network-test-1880 @ 08/04/23 20:07:49.491
  STEP: creating a selector @ 08/04/23 20:07:49.491
  STEP: Creating the service pods in kubernetes @ 08/04/23 20:07:49.491
  Aug  4 20:07:49.491: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0804 20:07:50.188989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:51.189366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:52.190433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:53.190657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:54.190942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:55.191168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:56.191745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:57.192323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:58.192703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:07:59.192906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:00.193019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:01.193223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:02.193905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:03.194179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:04.194956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:05.195199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:06.196052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:07.196591      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:08.197084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:09.197311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:10.198325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:11.198544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/04/23 20:08:11.544
  E0804 20:08:12.199031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:13.199265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:08:13.561: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Aug  4 20:08:13.561: INFO: Going to poll 10.244.0.15 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Aug  4 20:08:13.563: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.0.15:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1880 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:08:13.563: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:08:13.563: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:08:13.563: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1880/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.0.15%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug  4 20:08:13.629: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug  4 20:08:13.629: INFO: Going to poll 10.244.1.44 on port 8083 at least 0 times, with a maximum of 34 tries before failing
  Aug  4 20:08:13.631: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.244.1.44:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-1880 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:08:13.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:08:13.632: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:08:13.632: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-1880/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.244.1.44%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug  4 20:08:13.696: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug  4 20:08:13.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-1880" for this suite. @ 08/04/23 20:08:13.699
• [24.225 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 08/04/23 20:08:13.703
  Aug  4 20:08:13.704: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename svcaccounts @ 08/04/23 20:08:13.704
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:08:13.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:08:13.714
  Aug  4 20:08:13.725: INFO: created pod pod-service-account-defaultsa
  Aug  4 20:08:13.725: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Aug  4 20:08:13.728: INFO: created pod pod-service-account-mountsa
  Aug  4 20:08:13.728: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Aug  4 20:08:13.732: INFO: created pod pod-service-account-nomountsa
  Aug  4 20:08:13.732: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Aug  4 20:08:13.735: INFO: created pod pod-service-account-defaultsa-mountspec
  Aug  4 20:08:13.735: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Aug  4 20:08:13.741: INFO: created pod pod-service-account-mountsa-mountspec
  Aug  4 20:08:13.742: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Aug  4 20:08:13.749: INFO: created pod pod-service-account-nomountsa-mountspec
  Aug  4 20:08:13.749: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Aug  4 20:08:13.753: INFO: created pod pod-service-account-defaultsa-nomountspec
  Aug  4 20:08:13.753: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Aug  4 20:08:13.757: INFO: created pod pod-service-account-mountsa-nomountspec
  Aug  4 20:08:13.757: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Aug  4 20:08:13.762: INFO: created pod pod-service-account-nomountsa-nomountspec
  Aug  4 20:08:13.762: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Aug  4 20:08:13.762: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5359" for this suite. @ 08/04/23 20:08:13.764
• [0.070 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 08/04/23 20:08:13.773
  Aug  4 20:08:13.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:08:13.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:08:13.815
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:08:13.817
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:08:13.82
  E0804 20:08:14.200246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:15.200445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:08:15.831
  Aug  4 20:08:15.833: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-224ab169-5c40-4fef-9d2b-6769b8709582 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:08:15.839
  Aug  4 20:08:15.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7053" for this suite. @ 08/04/23 20:08:15.855
• [2.087 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 08/04/23 20:08:15.861
  Aug  4 20:08:15.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:08:15.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:08:15.869
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:08:15.871
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:08:15.873
  E0804 20:08:16.200549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:17.201420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:08:17.884
  Aug  4 20:08:17.886: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-5e41430b-c479-4243-a7ba-876d68f41d1c container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:08:17.892
  Aug  4 20:08:17.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3901" for this suite. @ 08/04/23 20:08:17.904
• [2.048 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 08/04/23 20:08:17.909
  Aug  4 20:08:17.909: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename taint-single-pod @ 08/04/23 20:08:17.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:08:17.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:08:17.919
  Aug  4 20:08:17.921: INFO: Waiting up to 1m0s for all nodes to be ready
  E0804 20:08:18.202322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:19.203612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:20.204601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:21.204638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:22.205166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:23.205395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:24.205885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:25.206083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:26.206445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:27.206979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:28.208138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:29.208678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:30.209202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:31.209435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:32.210336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:33.210403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:34.210991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:35.210992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:36.211971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:37.212660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:38.213195      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:39.213470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:40.214020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:41.214568      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:42.215342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:43.215697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:44.216037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:45.216270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:46.216432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:47.216535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:48.216583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:49.216785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:50.216936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:51.217803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:52.218443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:53.218680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:54.219464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:55.219701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:56.220054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:57.220869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:58.220944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:08:59.221125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:00.222230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:01.222467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:02.223508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:03.224040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:04.224325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:05.224569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:06.225442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:07.225957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:08.226557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:09.226773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:10.227805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:11.228026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:12.229126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:13.229362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:14.229521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:15.229706      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:16.230236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:17.230485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:09:17.933: INFO: Waiting for terminating namespaces to be deleted...
  Aug  4 20:09:17.934: INFO: Starting informer...
  STEP: Starting pod... @ 08/04/23 20:09:17.934
  Aug  4 20:09:18.146: INFO: Pod is running on k8sconformance-m02. Tainting Node
  STEP: Trying to apply a taint on the Node @ 08/04/23 20:09:18.146
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/04/23 20:09:18.154
  STEP: Waiting short time to make sure Pod is queued for deletion @ 08/04/23 20:09:18.156
  Aug  4 20:09:18.156: INFO: Pod wasn't evicted. Proceeding
  Aug  4 20:09:18.156: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 08/04/23 20:09:18.165
  STEP: Waiting some time to make sure that toleration time passed. @ 08/04/23 20:09:18.17
  E0804 20:09:18.231430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:19.232087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:20.232294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:21.232515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:22.232781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:23.233003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:24.233244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:25.233353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:26.233578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:27.233970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:28.234117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:29.234334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:30.234543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:31.234794      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:32.235241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:33.236031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:34.236218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:35.236488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:36.236735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:37.237258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:38.237487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:39.237686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:40.238245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:41.238468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:42.238775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:43.238959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:44.240029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:45.240247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:46.240376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:47.240973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:48.241215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:49.241973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:50.242169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:51.242388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:52.243023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:53.243153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:54.244049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:55.244270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:56.244437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:57.245232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:58.245462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:09:59.245560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:00.245761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:01.245837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:02.246386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:03.246563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:04.247010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:05.247266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:06.247595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:07.248101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:08.248315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:09.248520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:10.248701      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:11.248904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:12.249315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:13.250120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:14.250321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:15.250534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:16.250767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:17.251440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:18.252060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:19.252154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:20.252414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:21.252669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:22.253086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:23.253599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:24.253812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:25.254162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:26.254398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:27.254945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:28.255015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:29.255260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:30.255406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:31.256046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:32.256446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:10:33.170: INFO: Pod wasn't evicted. Test successful
  Aug  4 20:10:33.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-4283" for this suite. @ 08/04/23 20:10:33.173
• [135.267 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:286
  STEP: Creating a kubernetes client @ 08/04/23 20:10:33.177
  Aug  4 20:10:33.177: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename field-validation @ 08/04/23 20:10:33.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:10:33.186
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:10:33.188
  Aug  4 20:10:33.190: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:10:33.257535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:34.257646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:35.257727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:10:35.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4708" for this suite. @ 08/04/23 20:10:35.738
• [2.566 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 08/04/23 20:10:35.743
  Aug  4 20:10:35.743: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:10:35.744
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:10:35.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:10:35.753
  E0804 20:10:36.258445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:37.258958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:38.259669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:39.259900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:10:39.786
  Aug  4 20:10:39.787: INFO: Trying to get logs from node k8sconformance-m02 pod client-envvars-2dd1b604-53cb-484d-9458-e7c4f63c98e5 container env3cont: <nil>
  STEP: delete the pod @ 08/04/23 20:10:39.801
  Aug  4 20:10:39.812: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7375" for this suite. @ 08/04/23 20:10:39.814
• [4.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 08/04/23 20:10:39.82
  Aug  4 20:10:39.820: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename cronjob @ 08/04/23 20:10:39.821
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:10:39.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:10:39.835
  STEP: Creating a cronjob @ 08/04/23 20:10:39.837
  STEP: Ensuring more than one job is running at a time @ 08/04/23 20:10:39.841
  E0804 20:10:40.260366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:41.260855      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:42.261617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:43.261730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:44.262398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:45.262659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:46.262961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:47.263566      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:48.264019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:49.264264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:50.264964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:51.265203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:52.265867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:53.265991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:54.266316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:55.266520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:56.266745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:57.267033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:58.267632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:10:59.268084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:00.268403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:01.268598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:02.269246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:03.269473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:04.269528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:05.269749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:06.269841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:07.270497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:08.271548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:09.271800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:10.271970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:11.272192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:12.272429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:13.272577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:14.272979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:15.273182      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:16.273277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:17.273746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:18.274063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:19.274281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:20.274714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:21.274967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:22.275637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:23.276049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:24.276455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:25.276679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:26.276822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:27.277409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:28.278095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:29.278458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:30.278880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:31.279135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:32.279901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:33.280115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:34.280806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:35.281006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:36.281653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:37.282326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:38.283185      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:39.284034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:40.284603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:41.285291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:42.285856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:43.286010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:44.286774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:45.287027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:46.287391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:47.288195      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:48.288350      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:49.288568      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:50.288735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:51.289210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:52.289616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:53.289826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:54.290430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:55.290656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:56.290769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:57.291530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:58.292188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:11:59.292408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:00.293271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:01.293514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 08/04/23 20:12:01.844
  STEP: Removing cronjob @ 08/04/23 20:12:01.846
  Aug  4 20:12:01.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5129" for this suite. @ 08/04/23 20:12:01.851
• [82.035 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 08/04/23 20:12:01.856
  Aug  4 20:12:01.856: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 20:12:01.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:01.866
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:01.868
  STEP: Creating configMap configmap-2528/configmap-test-3799033e-2ed2-44c2-bcc2-f6845bd93404 @ 08/04/23 20:12:01.87
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:12:01.875
  E0804 20:12:02.294148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:03.294332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:04.294878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:05.295256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:12:05.889
  Aug  4 20:12:05.891: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-ca108ac5-b93c-41a2-a6de-823199044073 container env-test: <nil>
  STEP: delete the pod @ 08/04/23 20:12:05.898
  Aug  4 20:12:05.908: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2528" for this suite. @ 08/04/23 20:12:05.909
• [4.057 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 08/04/23 20:12:05.913
  Aug  4 20:12:05.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename limitrange @ 08/04/23 20:12:05.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:05.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:05.922
  STEP: Creating LimitRange "e2e-limitrange-zhzh6" in namespace "limitrange-4115" @ 08/04/23 20:12:05.924
  STEP: Creating another limitRange in another namespace @ 08/04/23 20:12:05.928
  Aug  4 20:12:05.935: INFO: Namespace "e2e-limitrange-zhzh6-1408" created
  Aug  4 20:12:05.935: INFO: Creating LimitRange "e2e-limitrange-zhzh6" in namespace "e2e-limitrange-zhzh6-1408"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-zhzh6" @ 08/04/23 20:12:05.939
  Aug  4 20:12:05.940: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-zhzh6" in "limitrange-4115" namespace @ 08/04/23 20:12:05.94
  Aug  4 20:12:05.944: INFO: LimitRange "e2e-limitrange-zhzh6" has been patched
  STEP: Delete LimitRange "e2e-limitrange-zhzh6" by Collection with labelSelector: "e2e-limitrange-zhzh6=patched" @ 08/04/23 20:12:05.944
  STEP: Confirm that the limitRange "e2e-limitrange-zhzh6" has been deleted @ 08/04/23 20:12:05.948
  Aug  4 20:12:05.948: INFO: Requesting list of LimitRange to confirm quantity
  Aug  4 20:12:05.949: INFO: Found 0 LimitRange with label "e2e-limitrange-zhzh6=patched"
  Aug  4 20:12:05.949: INFO: LimitRange "e2e-limitrange-zhzh6" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-zhzh6" @ 08/04/23 20:12:05.949
  Aug  4 20:12:05.951: INFO: Found 1 limitRange
  Aug  4 20:12:05.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-4115" for this suite. @ 08/04/23 20:12:05.953
  STEP: Destroying namespace "e2e-limitrange-zhzh6-1408" for this suite. @ 08/04/23 20:12:05.956
• [0.046 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 08/04/23 20:12:05.959
  Aug  4 20:12:05.959: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:12:05.96
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:05.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:05.969
  STEP: Creating secret with name secret-test-b68a8ac5-b682-43fe-972b-be98b947b349 @ 08/04/23 20:12:05.971
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:12:05.973
  E0804 20:12:06.296071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:07.296588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:08.296951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:09.297890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:12:09.986
  Aug  4 20:12:09.988: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-d09f6599-2cda-4878-a8cd-eb2a45a76023 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:12:09.994
  Aug  4 20:12:10.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7960" for this suite. @ 08/04/23 20:12:10.006
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 08/04/23 20:12:10.01
  Aug  4 20:12:10.010: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename proxy @ 08/04/23 20:12:10.011
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:10.02
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:10.022
  Aug  4 20:12:10.024: INFO: Creating pod...
  E0804 20:12:10.298742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:11.298979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:12:12.034: INFO: Creating service...
  Aug  4 20:12:12.042: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/pods/agnhost/proxy?method=DELETE
  Aug  4 20:12:12.046: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug  4 20:12:12.046: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/pods/agnhost/proxy?method=OPTIONS
  Aug  4 20:12:12.048: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug  4 20:12:12.048: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/pods/agnhost/proxy?method=PATCH
  Aug  4 20:12:12.050: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug  4 20:12:12.050: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/pods/agnhost/proxy?method=POST
  Aug  4 20:12:12.052: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug  4 20:12:12.052: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/pods/agnhost/proxy?method=PUT
  Aug  4 20:12:12.054: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug  4 20:12:12.054: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/services/e2e-proxy-test-service/proxy?method=DELETE
  Aug  4 20:12:12.060: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug  4 20:12:12.060: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Aug  4 20:12:12.062: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug  4 20:12:12.062: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/services/e2e-proxy-test-service/proxy?method=PATCH
  Aug  4 20:12:12.064: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug  4 20:12:12.064: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/services/e2e-proxy-test-service/proxy?method=POST
  Aug  4 20:12:12.066: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug  4 20:12:12.066: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/services/e2e-proxy-test-service/proxy?method=PUT
  Aug  4 20:12:12.068: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug  4 20:12:12.068: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/pods/agnhost/proxy?method=GET
  Aug  4 20:12:12.069: INFO: http.Client request:GET StatusCode:301
  Aug  4 20:12:12.069: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/services/e2e-proxy-test-service/proxy?method=GET
  Aug  4 20:12:12.071: INFO: http.Client request:GET StatusCode:301
  Aug  4 20:12:12.071: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/pods/agnhost/proxy?method=HEAD
  Aug  4 20:12:12.072: INFO: http.Client request:HEAD StatusCode:301
  Aug  4 20:12:12.072: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-9480/services/e2e-proxy-test-service/proxy?method=HEAD
  Aug  4 20:12:12.074: INFO: http.Client request:HEAD StatusCode:301
  Aug  4 20:12:12.074: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-9480" for this suite. @ 08/04/23 20:12:12.076
• [2.094 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 08/04/23 20:12:12.105
  Aug  4 20:12:12.105: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:12:12.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:12.115
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:12.117
  STEP: Creating configMap with name projected-configmap-test-volume-1bc1c03c-d953-4920-be69-e07112c5bb77 @ 08/04/23 20:12:12.119
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:12:12.121
  E0804 20:12:12.299990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:13.300248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:14.301022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:15.301255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:12:16.134
  Aug  4 20:12:16.135: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-11d09126-bdb6-4c92-b800-983e2583a6df container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:12:16.141
  Aug  4 20:12:16.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4549" for this suite. @ 08/04/23 20:12:16.152
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 08/04/23 20:12:16.159
  Aug  4 20:12:16.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:12:16.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:16.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:16.169
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/04/23 20:12:16.17
  E0804 20:12:16.302210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:17.302278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:18.303192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:19.304039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:12:20.183
  Aug  4 20:12:20.184: INFO: Trying to get logs from node k8sconformance-m02 pod pod-810b3ffe-dbcd-4eee-a9d0-d5ff905359fc container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:12:20.19
  Aug  4 20:12:20.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9669" for this suite. @ 08/04/23 20:12:20.202
• [4.045 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 08/04/23 20:12:20.205
  Aug  4 20:12:20.205: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename gc @ 08/04/23 20:12:20.205
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:20.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:20.213
  STEP: create the deployment @ 08/04/23 20:12:20.215
  W0804 20:12:20.219916      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/04/23 20:12:20.22
  E0804 20:12:20.304753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 08/04/23 20:12:20.727
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 08/04/23 20:12:20.732
  STEP: Gathering metrics @ 08/04/23 20:12:21.248
  E0804 20:12:21.304921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:12:21.340: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  4 20:12:21.340: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8350" for this suite. @ 08/04/23 20:12:21.342
• [1.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 08/04/23 20:12:21.349
  Aug  4 20:12:21.349: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sysctl @ 08/04/23 20:12:21.35
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:21.358
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:21.36
  STEP: Creating a pod with one valid and two invalid sysctls @ 08/04/23 20:12:21.361
  Aug  4 20:12:21.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-3929" for this suite. @ 08/04/23 20:12:21.366
• [0.020 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 08/04/23 20:12:21.37
  Aug  4 20:12:21.370: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename gc @ 08/04/23 20:12:21.371
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:12:21.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:12:21.38
  STEP: create the rc @ 08/04/23 20:12:21.384
  W0804 20:12:21.386985      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0804 20:12:22.306548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:23.305832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:24.305982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:25.306230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:26.307107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:27.308102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/04/23 20:12:27.406
  STEP: wait for the rc to be deleted @ 08/04/23 20:12:27.413
  E0804 20:12:28.308228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:29.309384      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:30.311529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:31.312669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:32.313312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 08/04/23 20:12:32.416
  E0804 20:12:33.314380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:34.315101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:35.315694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:36.315879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:37.316611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:38.316937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:39.317162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:40.317573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:41.318641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:42.318973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:43.320015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:44.320208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:45.320402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:46.321335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:47.322133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:48.322352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:49.322543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:50.322789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:51.323508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:52.324052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:53.324413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:54.324649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:55.324846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:56.325453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:57.325989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:58.326275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:12:59.326485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:00.326708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:01.327292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:02.328088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/04/23 20:13:02.427
  Aug  4 20:13:02.508: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  4 20:13:02.508: INFO: Deleting pod "simpletest.rc-2blx4" in namespace "gc-4184"
  Aug  4 20:13:02.518: INFO: Deleting pod "simpletest.rc-2jvws" in namespace "gc-4184"
  Aug  4 20:13:02.524: INFO: Deleting pod "simpletest.rc-2mw5s" in namespace "gc-4184"
  Aug  4 20:13:02.535: INFO: Deleting pod "simpletest.rc-44crq" in namespace "gc-4184"
  Aug  4 20:13:02.544: INFO: Deleting pod "simpletest.rc-45d5v" in namespace "gc-4184"
  Aug  4 20:13:02.558: INFO: Deleting pod "simpletest.rc-4mbbs" in namespace "gc-4184"
  Aug  4 20:13:02.609: INFO: Deleting pod "simpletest.rc-4pnss" in namespace "gc-4184"
  Aug  4 20:13:02.620: INFO: Deleting pod "simpletest.rc-4whbs" in namespace "gc-4184"
  Aug  4 20:13:02.636: INFO: Deleting pod "simpletest.rc-5l825" in namespace "gc-4184"
  Aug  4 20:13:02.648: INFO: Deleting pod "simpletest.rc-5pj9t" in namespace "gc-4184"
  Aug  4 20:13:02.664: INFO: Deleting pod "simpletest.rc-5wp7b" in namespace "gc-4184"
  Aug  4 20:13:02.718: INFO: Deleting pod "simpletest.rc-6cxf5" in namespace "gc-4184"
  Aug  4 20:13:02.733: INFO: Deleting pod "simpletest.rc-6zcq9" in namespace "gc-4184"
  Aug  4 20:13:02.746: INFO: Deleting pod "simpletest.rc-75ccq" in namespace "gc-4184"
  Aug  4 20:13:02.807: INFO: Deleting pod "simpletest.rc-7gc8m" in namespace "gc-4184"
  Aug  4 20:13:02.821: INFO: Deleting pod "simpletest.rc-7z9c6" in namespace "gc-4184"
  Aug  4 20:13:02.835: INFO: Deleting pod "simpletest.rc-8bm65" in namespace "gc-4184"
  Aug  4 20:13:02.845: INFO: Deleting pod "simpletest.rc-8l8zn" in namespace "gc-4184"
  Aug  4 20:13:02.918: INFO: Deleting pod "simpletest.rc-8q4t7" in namespace "gc-4184"
  Aug  4 20:13:02.934: INFO: Deleting pod "simpletest.rc-8vbn2" in namespace "gc-4184"
  Aug  4 20:13:03.005: INFO: Deleting pod "simpletest.rc-9bm9p" in namespace "gc-4184"
  Aug  4 20:13:03.027: INFO: Deleting pod "simpletest.rc-9ctf5" in namespace "gc-4184"
  Aug  4 20:13:03.042: INFO: Deleting pod "simpletest.rc-9hrvj" in namespace "gc-4184"
  Aug  4 20:13:03.118: INFO: Deleting pod "simpletest.rc-9mrbg" in namespace "gc-4184"
  Aug  4 20:13:03.214: INFO: Deleting pod "simpletest.rc-9q55j" in namespace "gc-4184"
  Aug  4 20:13:03.235: INFO: Deleting pod "simpletest.rc-9twvq" in namespace "gc-4184"
  Aug  4 20:13:03.316: INFO: Deleting pod "simpletest.rc-9vfzg" in namespace "gc-4184"
  E0804 20:13:03.328373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:03.336: INFO: Deleting pod "simpletest.rc-b5f2l" in namespace "gc-4184"
  Aug  4 20:13:03.408: INFO: Deleting pod "simpletest.rc-b5qpt" in namespace "gc-4184"
  Aug  4 20:13:03.424: INFO: Deleting pod "simpletest.rc-b7px9" in namespace "gc-4184"
  Aug  4 20:13:03.438: INFO: Deleting pod "simpletest.rc-bbmk4" in namespace "gc-4184"
  Aug  4 20:13:03.507: INFO: Deleting pod "simpletest.rc-bgwnl" in namespace "gc-4184"
  Aug  4 20:13:03.521: INFO: Deleting pod "simpletest.rc-bjxpz" in namespace "gc-4184"
  Aug  4 20:13:03.537: INFO: Deleting pod "simpletest.rc-bkfhh" in namespace "gc-4184"
  Aug  4 20:13:03.609: INFO: Deleting pod "simpletest.rc-blhbn" in namespace "gc-4184"
  Aug  4 20:13:03.641: INFO: Deleting pod "simpletest.rc-cc8fn" in namespace "gc-4184"
  Aug  4 20:13:03.715: INFO: Deleting pod "simpletest.rc-cdzm2" in namespace "gc-4184"
  Aug  4 20:13:03.746: INFO: Deleting pod "simpletest.rc-d2p9j" in namespace "gc-4184"
  Aug  4 20:13:03.817: INFO: Deleting pod "simpletest.rc-dq2sb" in namespace "gc-4184"
  Aug  4 20:13:03.830: INFO: Deleting pod "simpletest.rc-dt7c5" in namespace "gc-4184"
  Aug  4 20:13:03.919: INFO: Deleting pod "simpletest.rc-dxl8c" in namespace "gc-4184"
  Aug  4 20:13:03.931: INFO: Deleting pod "simpletest.rc-fthpw" in namespace "gc-4184"
  Aug  4 20:13:04.020: INFO: Deleting pod "simpletest.rc-fzh67" in namespace "gc-4184"
  Aug  4 20:13:04.108: INFO: Deleting pod "simpletest.rc-g8f24" in namespace "gc-4184"
  Aug  4 20:13:04.123: INFO: Deleting pod "simpletest.rc-gs95j" in namespace "gc-4184"
  Aug  4 20:13:04.207: INFO: Deleting pod "simpletest.rc-h2c5z" in namespace "gc-4184"
  Aug  4 20:13:04.221: INFO: Deleting pod "simpletest.rc-ht4bc" in namespace "gc-4184"
  Aug  4 20:13:04.308: INFO: Deleting pod "simpletest.rc-hwhd4" in namespace "gc-4184"
  E0804 20:13:04.328647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:04.334: INFO: Deleting pod "simpletest.rc-j2gwj" in namespace "gc-4184"
  Aug  4 20:13:04.406: INFO: Deleting pod "simpletest.rc-jbksk" in namespace "gc-4184"
  Aug  4 20:13:04.420: INFO: Deleting pod "simpletest.rc-jlhc4" in namespace "gc-4184"
  Aug  4 20:13:04.510: INFO: Deleting pod "simpletest.rc-jv9ct" in namespace "gc-4184"
  Aug  4 20:13:04.529: INFO: Deleting pod "simpletest.rc-k4dz5" in namespace "gc-4184"
  Aug  4 20:13:04.625: INFO: Deleting pod "simpletest.rc-k8zgg" in namespace "gc-4184"
  Aug  4 20:13:04.707: INFO: Deleting pod "simpletest.rc-kdfbx" in namespace "gc-4184"
  Aug  4 20:13:04.716: INFO: Deleting pod "simpletest.rc-kj9sp" in namespace "gc-4184"
  Aug  4 20:13:04.824: INFO: Deleting pod "simpletest.rc-l5pmd" in namespace "gc-4184"
  Aug  4 20:13:04.911: INFO: Deleting pod "simpletest.rc-l92zw" in namespace "gc-4184"
  Aug  4 20:13:04.924: INFO: Deleting pod "simpletest.rc-l9d9p" in namespace "gc-4184"
  Aug  4 20:13:05.008: INFO: Deleting pod "simpletest.rc-lbbpw" in namespace "gc-4184"
  Aug  4 20:13:05.022: INFO: Deleting pod "simpletest.rc-lnz68" in namespace "gc-4184"
  Aug  4 20:13:05.035: INFO: Deleting pod "simpletest.rc-m7t77" in namespace "gc-4184"
  Aug  4 20:13:05.116: INFO: Deleting pod "simpletest.rc-mcq9q" in namespace "gc-4184"
  Aug  4 20:13:05.204: INFO: Deleting pod "simpletest.rc-mdv9w" in namespace "gc-4184"
  Aug  4 20:13:05.218: INFO: Deleting pod "simpletest.rc-mkwl9" in namespace "gc-4184"
  Aug  4 20:13:05.232: INFO: Deleting pod "simpletest.rc-mqb6m" in namespace "gc-4184"
  Aug  4 20:13:05.246: INFO: Deleting pod "simpletest.rc-n4ttx" in namespace "gc-4184"
  Aug  4 20:13:05.322: INFO: Deleting pod "simpletest.rc-ncgxp" in namespace "gc-4184"
  E0804 20:13:05.329038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:05.343: INFO: Deleting pod "simpletest.rc-ncpp9" in namespace "gc-4184"
  Aug  4 20:13:05.408: INFO: Deleting pod "simpletest.rc-nkh82" in namespace "gc-4184"
  Aug  4 20:13:05.419: INFO: Deleting pod "simpletest.rc-nngcf" in namespace "gc-4184"
  Aug  4 20:13:05.428: INFO: Deleting pod "simpletest.rc-p6zdx" in namespace "gc-4184"
  Aug  4 20:13:05.439: INFO: Deleting pod "simpletest.rc-pgkkt" in namespace "gc-4184"
  Aug  4 20:13:05.511: INFO: Deleting pod "simpletest.rc-phtkw" in namespace "gc-4184"
  Aug  4 20:13:05.520: INFO: Deleting pod "simpletest.rc-pk9vk" in namespace "gc-4184"
  Aug  4 20:13:05.529: INFO: Deleting pod "simpletest.rc-pzqnv" in namespace "gc-4184"
  Aug  4 20:13:05.541: INFO: Deleting pod "simpletest.rc-q7cft" in namespace "gc-4184"
  Aug  4 20:13:05.613: INFO: Deleting pod "simpletest.rc-qgcp8" in namespace "gc-4184"
  Aug  4 20:13:05.622: INFO: Deleting pod "simpletest.rc-qlp5f" in namespace "gc-4184"
  Aug  4 20:13:05.632: INFO: Deleting pod "simpletest.rc-qs9pr" in namespace "gc-4184"
  Aug  4 20:13:05.644: INFO: Deleting pod "simpletest.rc-rgvs8" in namespace "gc-4184"
  Aug  4 20:13:05.712: INFO: Deleting pod "simpletest.rc-rnkwp" in namespace "gc-4184"
  Aug  4 20:13:05.727: INFO: Deleting pod "simpletest.rc-s8w5x" in namespace "gc-4184"
  Aug  4 20:13:05.740: INFO: Deleting pod "simpletest.rc-sthlf" in namespace "gc-4184"
  Aug  4 20:13:05.809: INFO: Deleting pod "simpletest.rc-swr7s" in namespace "gc-4184"
  Aug  4 20:13:05.819: INFO: Deleting pod "simpletest.rc-t49xg" in namespace "gc-4184"
  Aug  4 20:13:05.913: INFO: Deleting pod "simpletest.rc-t67pv" in namespace "gc-4184"
  Aug  4 20:13:05.927: INFO: Deleting pod "simpletest.rc-tc8bv" in namespace "gc-4184"
  Aug  4 20:13:06.010: INFO: Deleting pod "simpletest.rc-thdp6" in namespace "gc-4184"
  Aug  4 20:13:06.032: INFO: Deleting pod "simpletest.rc-tj5cp" in namespace "gc-4184"
  Aug  4 20:13:06.048: INFO: Deleting pod "simpletest.rc-ttfkw" in namespace "gc-4184"
  Aug  4 20:13:06.114: INFO: Deleting pod "simpletest.rc-tz2qv" in namespace "gc-4184"
  Aug  4 20:13:06.131: INFO: Deleting pod "simpletest.rc-v294c" in namespace "gc-4184"
  Aug  4 20:13:06.147: INFO: Deleting pod "simpletest.rc-vjskf" in namespace "gc-4184"
  Aug  4 20:13:06.209: INFO: Deleting pod "simpletest.rc-wnxxf" in namespace "gc-4184"
  Aug  4 20:13:06.224: INFO: Deleting pod "simpletest.rc-wzd29" in namespace "gc-4184"
  Aug  4 20:13:06.241: INFO: Deleting pod "simpletest.rc-xkfr7" in namespace "gc-4184"
  Aug  4 20:13:06.256: INFO: Deleting pod "simpletest.rc-xl8g9" in namespace "gc-4184"
  Aug  4 20:13:06.303: INFO: Deleting pod "simpletest.rc-xqkjn" in namespace "gc-4184"
  Aug  4 20:13:06.319: INFO: Deleting pod "simpletest.rc-xtt4m" in namespace "gc-4184"
  E0804 20:13:06.329467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:06.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4184" for this suite. @ 08/04/23 20:13:06.343
• [44.980 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 08/04/23 20:13:06.403
  Aug  4 20:13:06.403: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:13:06.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:13:06.425
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:13:06.43
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 08/04/23 20:13:06.433
  E0804 20:13:07.329780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:08.330136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:09.330421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:10.330777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:11.331381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:12.331444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:13:12.516
  Aug  4 20:13:12.518: INFO: Trying to get logs from node k8sconformance-m02 pod pod-22820690-c806-45e4-873b-13451ff23f68 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:13:12.524
  Aug  4 20:13:12.535: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6550" for this suite. @ 08/04/23 20:13:12.537
• [6.191 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 08/04/23 20:13:12.541
  Aug  4 20:13:12.541: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replicaset @ 08/04/23 20:13:12.542
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:13:12.551
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:13:12.553
  Aug  4 20:13:12.564: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0804 20:13:13.331907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:14.332890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:15.333150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:16.333988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:17.334608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:17.566: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/04/23 20:13:17.566
  STEP: Scaling up "test-rs" replicaset  @ 08/04/23 20:13:17.567
  Aug  4 20:13:17.573: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 08/04/23 20:13:17.573
  W0804 20:13:17.579491      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug  4 20:13:17.580: INFO: observed ReplicaSet test-rs in namespace replicaset-2475 with ReadyReplicas 1, AvailableReplicas 1
  Aug  4 20:13:17.590: INFO: observed ReplicaSet test-rs in namespace replicaset-2475 with ReadyReplicas 1, AvailableReplicas 1
  Aug  4 20:13:17.600: INFO: observed ReplicaSet test-rs in namespace replicaset-2475 with ReadyReplicas 1, AvailableReplicas 1
  Aug  4 20:13:17.609: INFO: observed ReplicaSet test-rs in namespace replicaset-2475 with ReadyReplicas 1, AvailableReplicas 1
  E0804 20:13:18.334996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:19.133: INFO: observed ReplicaSet test-rs in namespace replicaset-2475 with ReadyReplicas 2, AvailableReplicas 2
  Aug  4 20:13:19.157: INFO: observed Replicaset test-rs in namespace replicaset-2475 with ReadyReplicas 3 found true
  Aug  4 20:13:19.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-2475" for this suite. @ 08/04/23 20:13:19.16
• [6.622 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 08/04/23 20:13:19.166
  Aug  4 20:13:19.166: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename deployment @ 08/04/23 20:13:19.166
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:13:19.176
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:13:19.178
  Aug  4 20:13:19.185: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  E0804 20:13:19.335967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:20.336340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:21.336648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:22.337055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:23.337873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:24.188: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/04/23 20:13:24.188
  Aug  4 20:13:24.188: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 08/04/23 20:13:24.197
  Aug  4 20:13:24.209: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9164  877e4f85-b1e4-4987-9054-52bad4fa708f 6347 1 2023-08-04 20:13:24 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-08-04 20:13:24 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d35f18 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Aug  4 20:13:24.215: INFO: New ReplicaSet "test-cleanup-deployment-68b75d69f8" of Deployment "test-cleanup-deployment":
  &ReplicaSet{ObjectMeta:{test-cleanup-deployment-68b75d69f8  deployment-9164  b8584afd-745a-44d0-8390-5e70026c6a65 6354 1 2023-08-04 20:13:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-cleanup-deployment 877e4f85-b1e4-4987-9054-52bad4fa708f 0xc00496c157 0xc00496c158}] [] [{kube-controller-manager Update apps/v1 2023-08-04 20:13:24 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"877e4f85-b1e4-4987-9054-52bad4fa708f\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod-template-hash: 68b75d69f8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00496c1e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:0,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 20:13:24.215: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Aug  4 20:13:24.215: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9164  50ebc3be-6bed-4fd0-9219-3973e24f508c 6353 1 2023-08-04 20:13:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 877e4f85-b1e4-4987-9054-52bad4fa708f 0xc00496c02f 0xc00496c040}] [] [{e2e.test Update apps/v1 2023-08-04 20:13:19 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 20:13:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-08-04 20:13:24 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"877e4f85-b1e4-4987-9054-52bad4fa708f\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc00496c0f8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 20:13:24.225: INFO: Pod "test-cleanup-controller-94rfj" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-94rfj test-cleanup-controller- deployment-9164  71f1ba96-696a-4d99-83bb-f64bbe294725 6339 0 2023-08-04 20:13:19 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller 50ebc3be-6bed-4fd0-9219-3973e24f508c 0xc004c5227f 0xc004c52290}] [] [{kube-controller-manager Update v1 2023-08-04 20:13:19 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"50ebc3be-6bed-4fd0-9219-3973e24f508c\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:13:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.116\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q7bxd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q7bxd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:13:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:13:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:13:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:13:19 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.116,StartTime:2023-08-04 20:13:19 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:13:19 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://f0608e06899fe4cd32f99fa77e700e50f08d87de5e40a8a87299e014179a9273,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.116,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:13:24.225: INFO: Pod "test-cleanup-deployment-68b75d69f8-htfbw" is not available:
  &Pod{ObjectMeta:{test-cleanup-deployment-68b75d69f8-htfbw test-cleanup-deployment-68b75d69f8- deployment-9164  4ab1eac1-a294-4da0-9c08-f3f693a101f0 6360 0 2023-08-04 20:13:24 +0000 UTC <nil> <nil> map[name:cleanup-pod pod-template-hash:68b75d69f8] map[] [{apps/v1 ReplicaSet test-cleanup-deployment-68b75d69f8 b8584afd-745a-44d0-8390-5e70026c6a65 0xc004c52467 0xc004c52468}] [] [{kube-controller-manager Update v1 2023-08-04 20:13:24 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b8584afd-745a-44d0-8390-5e70026c6a65\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sr99r,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sr99r,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:13:24 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:13:24.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9164" for this suite. @ 08/04/23 20:13:24.228
• [5.066 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 08/04/23 20:13:24.232
  Aug  4 20:13:24.232: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/04/23 20:13:24.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:13:24.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:13:24.246
  Aug  4 20:13:24.248: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:13:24.339765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:25.339258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:26.339277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:27.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-7558" for this suite. @ 08/04/23 20:13:27.318
• [3.091 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 08/04/23 20:13:27.324
  Aug  4 20:13:27.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:13:27.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:13:27.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:13:27.334
  E0804 20:13:27.339652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Setting up server cert @ 08/04/23 20:13:27.348
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:13:27.753
  STEP: Deploying the webhook pod @ 08/04/23 20:13:27.759
  STEP: Wait for the deployment to be ready @ 08/04/23 20:13:27.768
  Aug  4 20:13:27.773: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 20:13:28.340132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:29.340592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:13:29.78
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:13:29.791
  E0804 20:13:30.341679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:30.791: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 08/04/23 20:13:30.794
  STEP: create a configmap that should be updated by the webhook @ 08/04/23 20:13:30.808
  Aug  4 20:13:30.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3269" for this suite. @ 08/04/23 20:13:30.852
  STEP: Destroying namespace "webhook-markers-9353" for this suite. @ 08/04/23 20:13:30.855
• [3.537 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 08/04/23 20:13:30.861
  Aug  4 20:13:30.861: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename subpath @ 08/04/23 20:13:30.862
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:13:30.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:13:30.872
  STEP: Setting up data @ 08/04/23 20:13:30.874
  STEP: Creating pod pod-subpath-test-configmap-ljnk @ 08/04/23 20:13:30.882
  STEP: Creating a pod to test atomic-volume-subpath @ 08/04/23 20:13:30.882
  E0804 20:13:31.342644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:32.342994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:33.343336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:34.344042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:35.344302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:36.345333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:37.345356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:38.346441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:39.346887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:40.346944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:41.347205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:42.347638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:43.347997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:44.348247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:45.349047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:46.349104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:47.349261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:48.349300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:49.349996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:50.350211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:51.351127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:52.352040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:53.352499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:54.352741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:13:54.925
  Aug  4 20:13:54.927: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-ljnk container test-container-subpath-configmap-ljnk: <nil>
  STEP: delete the pod @ 08/04/23 20:13:54.933
  STEP: Deleting pod pod-subpath-test-configmap-ljnk @ 08/04/23 20:13:54.943
  Aug  4 20:13:54.943: INFO: Deleting pod "pod-subpath-test-configmap-ljnk" in namespace "subpath-8300"
  Aug  4 20:13:54.944: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8300" for this suite. @ 08/04/23 20:13:54.946
• [24.089 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 08/04/23 20:13:54.951
  Aug  4 20:13:54.951: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:13:54.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:13:54.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:13:54.962
  STEP: Creating the pod @ 08/04/23 20:13:54.964
  E0804 20:13:55.353066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:56.353418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:57.353974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:13:57.495: INFO: Successfully updated pod "labelsupdate99002621-3f2d-42b8-adba-6f9bfb340644"
  E0804 20:13:58.354757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:13:59.354971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:00.355053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:01.355287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:14:01.516: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2934" for this suite. @ 08/04/23 20:14:01.518
• [6.570 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 08/04/23 20:14:01.522
  Aug  4 20:14:01.522: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:14:01.523
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:14:01.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:14:01.532
  STEP: Setting up server cert @ 08/04/23 20:14:01.543
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:14:01.921
  STEP: Deploying the webhook pod @ 08/04/23 20:14:01.927
  STEP: Wait for the deployment to be ready @ 08/04/23 20:14:01.935
  Aug  4 20:14:01.941: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 20:14:02.355502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:03.356067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:14:03.947
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:14:03.955
  E0804 20:14:04.356172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:14:04.955: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 08/04/23 20:14:04.959
  STEP: Creating a custom resource definition that should be denied by the webhook @ 08/04/23 20:14:04.971
  Aug  4 20:14:04.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:14:04.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4826" for this suite. @ 08/04/23 20:14:05.008
  STEP: Destroying namespace "webhook-markers-4340" for this suite. @ 08/04/23 20:14:05.013
• [3.494 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 08/04/23 20:14:05.016
  Aug  4 20:14:05.016: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:14:05.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:14:05.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:14:05.026
  STEP: Creating pod liveness-1cce81fe-469a-4325-8500-e6a5c19361a6 in namespace container-probe-3454 @ 08/04/23 20:14:05.028
  E0804 20:14:05.356306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:06.356531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:14:07.038: INFO: Started pod liveness-1cce81fe-469a-4325-8500-e6a5c19361a6 in namespace container-probe-3454
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/04/23 20:14:07.038
  Aug  4 20:14:07.040: INFO: Initial restart count of pod liveness-1cce81fe-469a-4325-8500-e6a5c19361a6 is 0
  E0804 20:14:07.357150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:08.358096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:09.358609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:10.358870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:11.359420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:12.359919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:13.360530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:14.360784      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:15.361407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:16.361623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:17.362682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:18.362856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:19.363451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:20.364077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:21.364840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:22.365226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:23.365325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:24.365546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:25.366413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:26.366613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:14:27.072: INFO: Restart count of pod container-probe-3454/liveness-1cce81fe-469a-4325-8500-e6a5c19361a6 is now 1 (20.0325334s elapsed)
  E0804 20:14:27.366834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:28.367948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:29.368478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:30.368743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:31.369475      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:32.369972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:33.370927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:34.371004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:35.371956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:36.373014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:37.373954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:38.374161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:39.374876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:40.375122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:41.375643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:42.376074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:43.376586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:44.376792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:45.377776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:46.378300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:14:47.102: INFO: Restart count of pod container-probe-3454/liveness-1cce81fe-469a-4325-8500-e6a5c19361a6 is now 2 (40.062221188s elapsed)
  E0804 20:14:47.378583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:48.378908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:49.379434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:50.379660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:51.380327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:52.380780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:53.381367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:54.381579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:55.382404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:56.382924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:57.383874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:58.384100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:14:59.384725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:00.384951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:01.385495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:02.385742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:03.385759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:04.385845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:05.386948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:06.387833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:15:07.132: INFO: Restart count of pod container-probe-3454/liveness-1cce81fe-469a-4325-8500-e6a5c19361a6 is now 3 (1m0.092252956s elapsed)
  E0804 20:15:07.388653      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:08.388897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:09.389360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:10.389464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:11.390471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:12.390730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:13.391349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:14.392102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:15.392329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:16.392997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:17.393896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:18.394165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:19.394933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:20.395303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:21.396035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:22.396239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:23.396405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:24.396618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:25.397466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:26.397933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:15:27.163: INFO: Restart count of pod container-probe-3454/liveness-1cce81fe-469a-4325-8500-e6a5c19361a6 is now 4 (1m20.123385382s elapsed)
  E0804 20:15:27.398757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:28.399195      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:29.400232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:30.400439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:31.401110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:32.401387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:33.402029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:34.402254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:35.402774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:36.403323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:37.403371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:38.403422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:39.403806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:40.404646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:41.405675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:42.405911      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:43.406098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:44.406337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:45.406997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:46.407574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:47.408402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:48.408615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:49.409505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:50.409750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:51.410071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:52.410324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:53.410946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:54.411048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:55.411987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:56.412582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:57.413632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:58.413865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:15:59.414830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:00.414968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:01.415944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:02.416185      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:03.417237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:04.417450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:05.418565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:06.418912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:07.419599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:08.420080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:09.420863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:10.421449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:11.422538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:12.422757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:13.423870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:14.424069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:15.424254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:16.424885      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:17.425632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:18.425853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:19.425924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:20.426124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:21.426913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:22.427007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:23.427572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:24.428037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:25.429001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:26.429722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:27.430781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:28.430946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:29.431397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:30.432026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:16:31.256: INFO: Restart count of pod container-probe-3454/liveness-1cce81fe-469a-4325-8500-e6a5c19361a6 is now 5 (2m24.216229756s elapsed)
  Aug  4 20:16:31.256: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:16:31.258
  STEP: Destroying namespace "container-probe-3454" for this suite. @ 08/04/23 20:16:31.264
• [146.252 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 08/04/23 20:16:31.268
  Aug  4 20:16:31.268: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename cronjob @ 08/04/23 20:16:31.269
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:16:31.277
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:16:31.279
  STEP: Creating a cronjob @ 08/04/23 20:16:31.281
  STEP: creating @ 08/04/23 20:16:31.281
  STEP: getting @ 08/04/23 20:16:31.286
  STEP: listing @ 08/04/23 20:16:31.288
  STEP: watching @ 08/04/23 20:16:31.29
  Aug  4 20:16:31.290: INFO: starting watch
  STEP: cluster-wide listing @ 08/04/23 20:16:31.291
  STEP: cluster-wide watching @ 08/04/23 20:16:31.293
  Aug  4 20:16:31.293: INFO: starting watch
  STEP: patching @ 08/04/23 20:16:31.293
  STEP: updating @ 08/04/23 20:16:31.298
  Aug  4 20:16:31.304: INFO: waiting for watch events with expected annotations
  Aug  4 20:16:31.304: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/04/23 20:16:31.304
  STEP: updating /status @ 08/04/23 20:16:31.309
  STEP: get /status @ 08/04/23 20:16:31.316
  STEP: deleting @ 08/04/23 20:16:31.318
  STEP: deleting a collection @ 08/04/23 20:16:31.326
  Aug  4 20:16:31.331: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2029" for this suite. @ 08/04/23 20:16:31.333
• [0.069 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 08/04/23 20:16:31.338
  Aug  4 20:16:31.338: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-runtime @ 08/04/23 20:16:31.339
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:16:31.345
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:16:31.347
  STEP: create the container @ 08/04/23 20:16:31.349
  W0804 20:16:31.355401      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 08/04/23 20:16:31.355
  E0804 20:16:31.432712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:32.433589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:33.434218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/04/23 20:16:34.366
  STEP: the container should be terminated @ 08/04/23 20:16:34.367
  STEP: the termination message should be set @ 08/04/23 20:16:34.367
  Aug  4 20:16:34.367: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/04/23 20:16:34.367
  Aug  4 20:16:34.375: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4982" for this suite. @ 08/04/23 20:16:34.379
• [3.044 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 08/04/23 20:16:34.383
  Aug  4 20:16:34.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:16:34.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:16:34.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:16:34.393
  STEP: Creating the pod @ 08/04/23 20:16:34.394
  E0804 20:16:34.435071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:35.435248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:36.436273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:16:36.931: INFO: Successfully updated pod "annotationupdate98ae8fe7-2c89-44de-b8e1-523abcfe002f"
  E0804 20:16:37.436362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:38.436867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:39.437822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:40.438831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:16:40.951: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1324" for this suite. @ 08/04/23 20:16:40.953
• [6.573 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 08/04/23 20:16:40.957
  Aug  4 20:16:40.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:16:40.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:16:40.967
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:16:40.969
  STEP: Creating configMap with name projected-configmap-test-volume-map-8d451fd7-4732-4918-99f1-331a649709b0 @ 08/04/23 20:16:40.971
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:16:40.974
  E0804 20:16:41.439299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:42.439383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:16:42.985
  Aug  4 20:16:42.987: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-a396dbe7-dea2-4df7-8bf4-d92a2cc3a856 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:16:42.992
  Aug  4 20:16:43.002: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6198" for this suite. @ 08/04/23 20:16:43.004
• [2.051 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 08/04/23 20:16:43.008
  Aug  4 20:16:43.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:16:43.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:16:43.017
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:16:43.019
  Aug  4 20:16:43.023: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4009" for this suite. @ 08/04/23 20:16:43.025
• [0.021 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 08/04/23 20:16:43.03
  Aug  4 20:16:43.030: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:16:43.03
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:16:43.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:16:43.041
  STEP: creating service endpoint-test2 in namespace services-1403 @ 08/04/23 20:16:43.043
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[] @ 08/04/23 20:16:43.052
  Aug  4 20:16:43.054: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  E0804 20:16:43.439742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:16:44.059: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-1403 @ 08/04/23 20:16:44.059
  E0804 20:16:44.440086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:45.440409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[pod1:[80]] @ 08/04/23 20:16:46.072
  Aug  4 20:16:46.077: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 08/04/23 20:16:46.077
  Aug  4 20:16:46.077: INFO: Creating new exec pod
  E0804 20:16:46.441165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:47.441398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:48.441938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:16:49.085: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-1403 exec execpodhq75p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug  4 20:16:49.209: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug  4 20:16:49.209: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:16:49.209: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-1403 exec execpodhq75p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.245.106 80'
  Aug  4 20:16:49.336: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.245.106 80\nConnection to 10.99.245.106 80 port [tcp/http] succeeded!\n"
  Aug  4 20:16:49.336: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-1403 @ 08/04/23 20:16:49.336
  E0804 20:16:49.442690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:50.443312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[pod1:[80] pod2:[80]] @ 08/04/23 20:16:51.348
  Aug  4 20:16:51.356: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 08/04/23 20:16:51.356
  E0804 20:16:51.443618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:16:52.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-1403 exec execpodhq75p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  E0804 20:16:52.444061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:16:52.473: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug  4 20:16:52.473: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:16:52.473: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-1403 exec execpodhq75p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.245.106 80'
  Aug  4 20:16:52.593: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.245.106 80\nConnection to 10.99.245.106 80 port [tcp/http] succeeded!\n"
  Aug  4 20:16:52.593: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-1403 @ 08/04/23 20:16:52.593
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[pod2:[80]] @ 08/04/23 20:16:52.602
  Aug  4 20:16:52.612: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 08/04/23 20:16:52.612
  E0804 20:16:53.444873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:16:53.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-1403 exec execpodhq75p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Aug  4 20:16:53.736: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Aug  4 20:16:53.736: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:16:53.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-1403 exec execpodhq75p -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.245.106 80'
  Aug  4 20:16:53.869: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.245.106 80\nConnection to 10.99.245.106 80 port [tcp/http] succeeded!\n"
  Aug  4 20:16:53.869: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-1403 @ 08/04/23 20:16:53.869
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-1403 to expose endpoints map[] @ 08/04/23 20:16:53.88
  Aug  4 20:16:53.886: INFO: successfully validated that service endpoint-test2 in namespace services-1403 exposes endpoints map[]
  Aug  4 20:16:53.886: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-1403" for this suite. @ 08/04/23 20:16:53.9
• [10.875 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 08/04/23 20:16:53.904
  Aug  4 20:16:53.904: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:16:53.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:16:53.915
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:16:53.917
  STEP: Creating a pod to test downward api env vars @ 08/04/23 20:16:53.919
  E0804 20:16:54.445367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:55.445873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:56.446743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:57.447928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:16:57.934
  Aug  4 20:16:57.936: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-b12d0e6d-1a5f-456a-a975-98d29d8499a6 container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 20:16:57.942
  Aug  4 20:16:57.950: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7257" for this suite. @ 08/04/23 20:16:57.952
• [4.052 seconds]
------------------------------
SSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 08/04/23 20:16:57.957
  Aug  4 20:16:57.957: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:16:57.958
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:16:57.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:16:57.97
  STEP: Creating configMap with name configmap-projected-all-test-volume-efbd12c0-2642-4615-85a0-fb511a9bbbce @ 08/04/23 20:16:57.972
  STEP: Creating secret with name secret-projected-all-test-volume-2289df3d-7e6e-44fd-b09b-7bb9626d71f0 @ 08/04/23 20:16:57.975
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 08/04/23 20:16:57.978
  E0804 20:16:58.448230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:16:59.449198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:16:59.987
  Aug  4 20:16:59.989: INFO: Trying to get logs from node k8sconformance-m02 pod projected-volume-1a7b2f22-d5de-4872-839b-5158710abfbe container projected-all-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:16:59.994
  Aug  4 20:17:00.004: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1364" for this suite. @ 08/04/23 20:17:00.006
• [2.052 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 08/04/23 20:17:00.011
  Aug  4 20:17:00.011: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:17:00.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:00.019
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:00.021
  STEP: Creating a pod to test downward api env vars @ 08/04/23 20:17:00.023
  E0804 20:17:00.449639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:01.450303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:17:02.033
  Aug  4 20:17:02.035: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-4dcfbbb6-abba-4ac4-9206-fa873d82f436 container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 20:17:02.042
  Aug  4 20:17:02.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7629" for this suite. @ 08/04/23 20:17:02.054
• [2.046 seconds]
------------------------------
S
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 08/04/23 20:17:02.058
  Aug  4 20:17:02.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:17:02.058
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:02.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:02.068
  STEP: creating service in namespace services-3459 @ 08/04/23 20:17:02.07
  STEP: creating service affinity-nodeport in namespace services-3459 @ 08/04/23 20:17:02.07
  STEP: creating replication controller affinity-nodeport in namespace services-3459 @ 08/04/23 20:17:02.082
  I0804 20:17:02.088264      22 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-3459, replica count: 3
  E0804 20:17:02.451261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:03.452065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:04.452609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:17:05.139526      22 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 20:17:05.145: INFO: Creating new exec pod
  E0804 20:17:05.452894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:06.453676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:07.453982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:17:08.155: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3459 exec execpod-affinityc2m7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Aug  4 20:17:08.272: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Aug  4 20:17:08.272: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:17:08.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3459 exec execpod-affinityc2m7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.13.223 80'
  Aug  4 20:17:08.388: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.13.223 80\nConnection to 10.101.13.223 80 port [tcp/http] succeeded!\n"
  Aug  4 20:17:08.388: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:17:08.388: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3459 exec execpod-affinityc2m7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.2 31267'
  E0804 20:17:08.454939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:17:08.517: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.2 31267\nConnection to 192.168.49.2 31267 port [tcp/*] succeeded!\n"
  Aug  4 20:17:08.517: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:17:08.517: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3459 exec execpod-affinityc2m7x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.3 31267'
  Aug  4 20:17:08.648: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.3 31267\nConnection to 192.168.49.3 31267 port [tcp/*] succeeded!\n"
  Aug  4 20:17:08.648: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:17:08.648: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3459 exec execpod-affinityc2m7x -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:31267/ ; done'
  Aug  4 20:17:08.816: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:31267/\n"
  Aug  4 20:17:08.816: INFO: stdout: "\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv\naffinity-nodeport-hdjkv"
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.816: INFO: Received response from host: affinity-nodeport-hdjkv
  Aug  4 20:17:08.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 20:17:08.819: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-3459, will wait for the garbage collector to delete the pods @ 08/04/23 20:17:08.824
  Aug  4 20:17:08.881: INFO: Deleting ReplicationController affinity-nodeport took: 3.852513ms
  Aug  4 20:17:08.982: INFO: Terminating ReplicationController affinity-nodeport pods took: 101.143411ms
  E0804 20:17:09.455872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:10.456744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-3459" for this suite. @ 08/04/23 20:17:11.194
• [9.141 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 08/04/23 20:17:11.199
  Aug  4 20:17:11.199: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:17:11.2
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:11.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:11.21
  STEP: creating an Endpoint @ 08/04/23 20:17:11.214
  STEP: waiting for available Endpoint @ 08/04/23 20:17:11.217
  STEP: listing all Endpoints @ 08/04/23 20:17:11.217
  STEP: updating the Endpoint @ 08/04/23 20:17:11.219
  STEP: fetching the Endpoint @ 08/04/23 20:17:11.224
  STEP: patching the Endpoint @ 08/04/23 20:17:11.225
  STEP: fetching the Endpoint @ 08/04/23 20:17:11.231
  STEP: deleting the Endpoint by Collection @ 08/04/23 20:17:11.232
  STEP: waiting for Endpoint deletion @ 08/04/23 20:17:11.236
  STEP: fetching the Endpoint @ 08/04/23 20:17:11.237
  Aug  4 20:17:11.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4848" for this suite. @ 08/04/23 20:17:11.24
• [0.045 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 08/04/23 20:17:11.244
  Aug  4 20:17:11.244: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 20:17:11.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:11.253
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:11.255
  Aug  4 20:17:11.258: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:17:11.457796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:12.458598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 08/04/23 20:17:12.556
  Aug  4 20:17:12.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 create -f -'
  Aug  4 20:17:13.076: INFO: stderr: ""
  Aug  4 20:17:13.076: INFO: stdout: "e2e-test-crd-publish-openapi-4830-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug  4 20:17:13.076: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 delete e2e-test-crd-publish-openapi-4830-crds test-foo'
  Aug  4 20:17:13.134: INFO: stderr: ""
  Aug  4 20:17:13.134: INFO: stdout: "e2e-test-crd-publish-openapi-4830-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Aug  4 20:17:13.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 apply -f -'
  Aug  4 20:17:13.298: INFO: stderr: ""
  Aug  4 20:17:13.298: INFO: stdout: "e2e-test-crd-publish-openapi-4830-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Aug  4 20:17:13.298: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 delete e2e-test-crd-publish-openapi-4830-crds test-foo'
  Aug  4 20:17:13.357: INFO: stderr: ""
  Aug  4 20:17:13.357: INFO: stdout: "e2e-test-crd-publish-openapi-4830-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 08/04/23 20:17:13.357
  Aug  4 20:17:13.357: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 create -f -'
  E0804 20:17:13.458992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:17:13.540: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 08/04/23 20:17:13.54
  Aug  4 20:17:13.540: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 create -f -'
  Aug  4 20:17:13.697: INFO: rc: 1
  Aug  4 20:17:13.697: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 apply -f -'
  Aug  4 20:17:13.859: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 08/04/23 20:17:13.859
  Aug  4 20:17:13.860: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 create -f -'
  Aug  4 20:17:14.030: INFO: rc: 1
  Aug  4 20:17:14.030: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 --namespace=crd-publish-openapi-9045 apply -f -'
  Aug  4 20:17:14.190: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 08/04/23 20:17:14.19
  Aug  4 20:17:14.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 explain e2e-test-crd-publish-openapi-4830-crds'
  Aug  4 20:17:14.347: INFO: stderr: ""
  Aug  4 20:17:14.347: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4830-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 08/04/23 20:17:14.347
  Aug  4 20:17:14.347: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 explain e2e-test-crd-publish-openapi-4830-crds.metadata'
  E0804 20:17:14.459200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:17:14.504: INFO: stderr: ""
  Aug  4 20:17:14.504: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4830-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Aug  4 20:17:14.504: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 explain e2e-test-crd-publish-openapi-4830-crds.spec'
  Aug  4 20:17:14.664: INFO: stderr: ""
  Aug  4 20:17:14.664: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4830-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Aug  4 20:17:14.664: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 explain e2e-test-crd-publish-openapi-4830-crds.spec.bars'
  Aug  4 20:17:14.818: INFO: stderr: ""
  Aug  4 20:17:14.818: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-4830-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 08/04/23 20:17:14.818
  Aug  4 20:17:14.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-9045 explain e2e-test-crd-publish-openapi-4830-crds.spec.bars2'
  Aug  4 20:17:14.970: INFO: rc: 1
  E0804 20:17:15.460049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:17:16.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-9045" for this suite. @ 08/04/23 20:17:16.228
• [4.988 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 08/04/23 20:17:16.234
  Aug  4 20:17:16.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename svcaccounts @ 08/04/23 20:17:16.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:16.241
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:16.243
  STEP: Creating a pod to test service account token:  @ 08/04/23 20:17:16.245
  E0804 20:17:16.460934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:17.461140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:18.462038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:19.462277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:17:20.258
  Aug  4 20:17:20.260: INFO: Trying to get logs from node k8sconformance-m02 pod test-pod-67e5d63a-2a0b-4a90-a933-a622ff577755 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:17:20.267
  Aug  4 20:17:20.276: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-2457" for this suite. @ 08/04/23 20:17:20.278
• [4.048 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 08/04/23 20:17:20.281
  Aug  4 20:17:20.282: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:17:20.282
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:20.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:20.293
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/04/23 20:17:20.295
  E0804 20:17:20.462899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:21.463722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:17:22.307
  Aug  4 20:17:22.309: INFO: Trying to get logs from node k8sconformance-m02 pod pod-017e2692-202d-4dac-bc07-dfe2e57ebbfa container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:17:22.315
  Aug  4 20:17:22.324: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3429" for this suite. @ 08/04/23 20:17:22.326
• [2.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 08/04/23 20:17:22.331
  Aug  4 20:17:22.331: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename csistoragecapacity @ 08/04/23 20:17:22.331
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:22.34
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:22.342
  STEP: getting /apis @ 08/04/23 20:17:22.343
  STEP: getting /apis/storage.k8s.io @ 08/04/23 20:17:22.346
  STEP: getting /apis/storage.k8s.io/v1 @ 08/04/23 20:17:22.347
  STEP: creating @ 08/04/23 20:17:22.348
  STEP: watching @ 08/04/23 20:17:22.358
  Aug  4 20:17:22.358: INFO: starting watch
  STEP: getting @ 08/04/23 20:17:22.362
  STEP: listing in namespace @ 08/04/23 20:17:22.363
  STEP: listing across namespaces @ 08/04/23 20:17:22.365
  STEP: patching @ 08/04/23 20:17:22.366
  STEP: updating @ 08/04/23 20:17:22.369
  Aug  4 20:17:22.372: INFO: waiting for watch events with expected annotations in namespace
  Aug  4 20:17:22.372: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 08/04/23 20:17:22.372
  STEP: deleting a collection @ 08/04/23 20:17:22.378
  Aug  4 20:17:22.386: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-3327" for this suite. @ 08/04/23 20:17:22.388
• [0.060 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 08/04/23 20:17:22.391
  Aug  4 20:17:22.391: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename disruption @ 08/04/23 20:17:22.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:22.399
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:22.401
  STEP: Waiting for the pdb to be processed @ 08/04/23 20:17:22.406
  E0804 20:17:22.464174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:23.464664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 08/04/23 20:17:24.428
  Aug  4 20:17:24.431: INFO: running pods: 0 < 3
  E0804 20:17:24.465327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:25.465551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:17:26.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-1896" for this suite. @ 08/04/23 20:17:26.438
• [4.050 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 08/04/23 20:17:26.441
  Aug  4 20:17:26.441: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename gc @ 08/04/23 20:17:26.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:26.449
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:26.451
  STEP: create the deployment @ 08/04/23 20:17:26.453
  W0804 20:17:26.457923      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 08/04/23 20:17:26.458
  E0804 20:17:26.465928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the deployment @ 08/04/23 20:17:26.962
  STEP: wait for all rs to be garbage collected @ 08/04/23 20:17:26.966
  STEP: expected 0 rs, got 1 rs @ 08/04/23 20:17:26.97
  STEP: expected 0 pods, got 2 pods @ 08/04/23 20:17:26.972
  E0804 20:17:27.466002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/04/23 20:17:27.477
  Aug  4 20:17:27.573: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  4 20:17:27.573: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5227" for this suite. @ 08/04/23 20:17:27.576
• [1.138 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:825
  STEP: Creating a kubernetes client @ 08/04/23 20:17:27.579
  Aug  4 20:17:27.579: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename daemonsets @ 08/04/23 20:17:27.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:27.587
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:27.589
  STEP: Creating simple DaemonSet "daemon-set" @ 08/04/23 20:17:27.598
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/04/23 20:17:27.601
  Aug  4 20:17:27.607: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 20:17:27.607: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:17:28.466688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:17:28.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 20:17:28.612: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:17:29.466880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:17:29.612: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 20:17:29.612: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: listing all DaemonSets @ 08/04/23 20:17:29.614
  STEP: DeleteCollection of the DaemonSets @ 08/04/23 20:17:29.616
  STEP: Verify that ReplicaSets have been deleted @ 08/04/23 20:17:29.62
  Aug  4 20:17:29.627: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7444"},"items":null}

  Aug  4 20:17:29.630: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7444"},"items":[{"metadata":{"name":"daemon-set-6sx9p","generateName":"daemon-set-","namespace":"daemonsets-639","uid":"ab0e7f58-5fee-4e9e-a72e-379292a82ff5","resourceVersion":"7444","creationTimestamp":"2023-08-04T20:17:27Z","deletionTimestamp":"2023-08-04T20:17:59Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee04cebb-0994-44d1-9e53-c8ba2cbaa666","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-04T20:17:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee04cebb-0994-44d1-9e53-c8ba2cbaa666\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-04T20:17:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.139\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-rj57w","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-rj57w","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8sconformance-m02","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8sconformance-m02"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-04T20:17:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-04T20:17:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-04T20:17:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-04T20:17:27Z"}],"hostIP":"192.168.49.3","podIP":"10.244.1.139","podIPs":[{"ip":"10.244.1.139"}],"startTime":"2023-08-04T20:17:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-04T20:17:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"docker://1deaacb04d53e6cebbb323b39d9b6994eefada92cc2e2fc9b0ee7b8fcf129ebf","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-dfwfj","generateName":"daemon-set-","namespace":"daemonsets-639","uid":"ed16c25a-dc24-4b69-aeb2-6cc8e62107bc","resourceVersion":"7443","creationTimestamp":"2023-08-04T20:17:27Z","deletionTimestamp":"2023-08-04T20:17:59Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ee04cebb-0994-44d1-9e53-c8ba2cbaa666","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-08-04T20:17:27Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ee04cebb-0994-44d1-9e53-c8ba2cbaa666\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-08-04T20:17:28Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-6hgsx","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-6hgsx","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"k8sconformance","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["k8sconformance"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-04T20:17:27Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-04T20:17:28Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-04T20:17:28Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-08-04T20:17:27Z"}],"hostIP":"192.168.49.2","podIP":"10.244.0.71","podIPs":[{"ip":"10.244.0.71"}],"startTime":"2023-08-04T20:17:27Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-08-04T20:17:28Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"docker://ab6d50193a3d4c2c0dca9017b6bc51ba77c97037b178728747f621d06165ae94","started":true}],"qosClass":"BestEffort"}}]}

  Aug  4 20:17:29.635: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-639" for this suite. @ 08/04/23 20:17:29.637
• [2.061 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 08/04/23 20:17:29.641
  Aug  4 20:17:29.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename ingress @ 08/04/23 20:17:29.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:29.65
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:29.652
  STEP: getting /apis @ 08/04/23 20:17:29.654
  STEP: getting /apis/networking.k8s.io @ 08/04/23 20:17:29.656
  STEP: getting /apis/networking.k8s.iov1 @ 08/04/23 20:17:29.657
  STEP: creating @ 08/04/23 20:17:29.658
  STEP: getting @ 08/04/23 20:17:29.668
  STEP: listing @ 08/04/23 20:17:29.67
  STEP: watching @ 08/04/23 20:17:29.672
  Aug  4 20:17:29.672: INFO: starting watch
  STEP: cluster-wide listing @ 08/04/23 20:17:29.672
  STEP: cluster-wide watching @ 08/04/23 20:17:29.674
  Aug  4 20:17:29.674: INFO: starting watch
  STEP: patching @ 08/04/23 20:17:29.675
  STEP: updating @ 08/04/23 20:17:29.678
  Aug  4 20:17:29.683: INFO: waiting for watch events with expected annotations
  Aug  4 20:17:29.683: INFO: saw patched and updated annotations
  STEP: patching /status @ 08/04/23 20:17:29.683
  STEP: updating /status @ 08/04/23 20:17:29.687
  STEP: get /status @ 08/04/23 20:17:29.694
  STEP: deleting @ 08/04/23 20:17:29.698
  STEP: deleting a collection @ 08/04/23 20:17:29.705
  Aug  4 20:17:29.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-9202" for this suite. @ 08/04/23 20:17:29.714
• [0.077 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 08/04/23 20:17:29.717
  Aug  4 20:17:29.717: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:17:29.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:17:29.728
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:17:29.73
  E0804 20:17:30.467060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:31.468053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:32.468449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:33.468540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:34.469051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:35.469260      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:36.470375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:37.470959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:38.472029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:39.472811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:40.473236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:41.474038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:42.474652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:43.474922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:44.475407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:45.476085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:46.477012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:47.477581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:48.478060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:49.478982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:50.480040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:51.480969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:52.481624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:53.482102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:54.482728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:55.483150      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:56.484066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:57.484654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:58.484763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:17:59.485756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:00.486299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:01.487122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:02.487848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:03.488249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:04.488370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:05.488773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:06.489002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:07.489269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:08.489840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:09.490883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:10.490982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:11.491630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:12.492157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:13.492314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:14.492765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:15.493327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:16.494392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:17.494584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:18.494924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:19.496027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:20.496870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:21.497657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:22.498110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:23.498415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:24.498574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:25.498963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:26.499978      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:27.500487      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:28.500866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:29.501732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:18:29.739: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-3964" for this suite. @ 08/04/23 20:18:29.741
• [60.027 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:864
  STEP: Creating a kubernetes client @ 08/04/23 20:18:29.745
  Aug  4 20:18:29.745: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename daemonsets @ 08/04/23 20:18:29.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:18:29.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:18:29.757
  STEP: Creating simple DaemonSet "daemon-set" @ 08/04/23 20:18:29.769
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/04/23 20:18:29.773
  Aug  4 20:18:29.777: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 20:18:29.777: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:18:30.502613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:18:30.782: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 20:18:30.782: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:18:31.502909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:18:31.783: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 20:18:31.783: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Getting /status @ 08/04/23 20:18:31.784
  Aug  4 20:18:31.787: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 08/04/23 20:18:31.787
  Aug  4 20:18:31.793: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 08/04/23 20:18:31.793
  Aug  4 20:18:31.795: INFO: Observed &DaemonSet event: ADDED
  Aug  4 20:18:31.795: INFO: Observed &DaemonSet event: MODIFIED
  Aug  4 20:18:31.795: INFO: Observed &DaemonSet event: MODIFIED
  Aug  4 20:18:31.795: INFO: Observed &DaemonSet event: MODIFIED
  Aug  4 20:18:31.795: INFO: Found daemon set daemon-set in namespace daemonsets-3886 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug  4 20:18:31.795: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 08/04/23 20:18:31.795
  STEP: watching for the daemon set status to be patched @ 08/04/23 20:18:31.8
  Aug  4 20:18:31.801: INFO: Observed &DaemonSet event: ADDED
  Aug  4 20:18:31.801: INFO: Observed &DaemonSet event: MODIFIED
  Aug  4 20:18:31.801: INFO: Observed &DaemonSet event: MODIFIED
  Aug  4 20:18:31.802: INFO: Observed &DaemonSet event: MODIFIED
  Aug  4 20:18:31.802: INFO: Observed daemon set daemon-set in namespace daemonsets-3886 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug  4 20:18:31.802: INFO: Observed &DaemonSet event: MODIFIED
  Aug  4 20:18:31.802: INFO: Found daemon set daemon-set in namespace daemonsets-3886 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Aug  4 20:18:31.802: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 08/04/23 20:18:31.804
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3886, will wait for the garbage collector to delete the pods @ 08/04/23 20:18:31.804
  Aug  4 20:18:31.860: INFO: Deleting DaemonSet.extensions daemon-set took: 4.083844ms
  Aug  4 20:18:31.961: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.085599ms
  E0804 20:18:32.503779      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:33.504244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:18:34.464: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 20:18:34.464: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  4 20:18:34.466: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"7657"},"items":null}

  Aug  4 20:18:34.468: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"7657"},"items":null}

  Aug  4 20:18:34.473: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3886" for this suite. @ 08/04/23 20:18:34.475
• [4.733 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 08/04/23 20:18:34.479
  Aug  4 20:18:34.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename svcaccounts @ 08/04/23 20:18:34.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:18:34.489
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:18:34.491
  E0804 20:18:34.504978      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:35.505312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 08/04/23 20:18:36.505
  Aug  4 20:18:36.505: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-338 pod-service-account-647f8447-6068-4fda-9db6-9cb910d19b63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  E0804 20:18:36.505546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 08/04/23 20:18:36.623
  Aug  4 20:18:36.623: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-338 pod-service-account-647f8447-6068-4fda-9db6-9cb910d19b63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 08/04/23 20:18:36.747
  Aug  4 20:18:36.747: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-338 pod-service-account-647f8447-6068-4fda-9db6-9cb910d19b63 -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Aug  4 20:18:36.863: INFO: Got root ca configmap in namespace "svcaccounts-338"
  Aug  4 20:18:36.865: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-338" for this suite. @ 08/04/23 20:18:36.867
• [2.391 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 08/04/23 20:18:36.871
  Aug  4 20:18:36.871: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:18:36.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:18:36.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:18:36.882
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/04/23 20:18:36.884
  E0804 20:18:37.506067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:38.506218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:39.506353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:40.506565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:18:40.898
  Aug  4 20:18:40.900: INFO: Trying to get logs from node k8sconformance-m02 pod pod-52a97fcb-be06-4356-ac0d-463265bd159c container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:18:40.907
  Aug  4 20:18:40.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4554" for this suite. @ 08/04/23 20:18:40.917
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 08/04/23 20:18:40.92
  Aug  4 20:18:40.920: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename gc @ 08/04/23 20:18:40.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:18:40.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:18:40.932
  STEP: create the rc1 @ 08/04/23 20:18:40.936
  STEP: create the rc2 @ 08/04/23 20:18:40.939
  E0804 20:18:41.509061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:42.509437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:43.510469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:44.511338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:45.511796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:46.512481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 08/04/23 20:18:47.006
  E0804 20:18:47.513450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc simpletest-rc-to-be-deleted @ 08/04/23 20:18:48.338
  STEP: wait for the rc to be deleted @ 08/04/23 20:18:48.342
  E0804 20:18:48.514029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:49.515111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:50.515284      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:51.517556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:52.518048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:18:53.419: INFO: 71 pods remaining
  Aug  4 20:18:53.420: INFO: 71 pods has nil DeletionTimestamp
  Aug  4 20:18:53.420: INFO: 
  E0804 20:18:53.518489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:54.518771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:55.519044      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:56.519745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:18:57.520897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/04/23 20:18:58.413
  E0804 20:18:58.521488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:18:59.022: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  4 20:18:59.022: INFO: Deleting pod "simpletest-rc-to-be-deleted-276jq" in namespace "gc-567"
  Aug  4 20:18:59.033: INFO: Deleting pod "simpletest-rc-to-be-deleted-2jjrf" in namespace "gc-567"
  Aug  4 20:18:59.120: INFO: Deleting pod "simpletest-rc-to-be-deleted-2sldh" in namespace "gc-567"
  Aug  4 20:18:59.138: INFO: Deleting pod "simpletest-rc-to-be-deleted-42hph" in namespace "gc-567"
  Aug  4 20:18:59.214: INFO: Deleting pod "simpletest-rc-to-be-deleted-4h4sw" in namespace "gc-567"
  Aug  4 20:18:59.230: INFO: Deleting pod "simpletest-rc-to-be-deleted-4xncb" in namespace "gc-567"
  Aug  4 20:18:59.242: INFO: Deleting pod "simpletest-rc-to-be-deleted-52v49" in namespace "gc-567"
  Aug  4 20:18:59.327: INFO: Deleting pod "simpletest-rc-to-be-deleted-56k6d" in namespace "gc-567"
  Aug  4 20:18:59.340: INFO: Deleting pod "simpletest-rc-to-be-deleted-5bmpv" in namespace "gc-567"
  Aug  4 20:18:59.351: INFO: Deleting pod "simpletest-rc-to-be-deleted-5d8mz" in namespace "gc-567"
  Aug  4 20:18:59.415: INFO: Deleting pod "simpletest-rc-to-be-deleted-62v7d" in namespace "gc-567"
  Aug  4 20:18:59.437: INFO: Deleting pod "simpletest-rc-to-be-deleted-6cqvz" in namespace "gc-567"
  Aug  4 20:18:59.449: INFO: Deleting pod "simpletest-rc-to-be-deleted-6h2hp" in namespace "gc-567"
  Aug  4 20:18:59.519: INFO: Deleting pod "simpletest-rc-to-be-deleted-6jv74" in namespace "gc-567"
  E0804 20:18:59.524428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:18:59.537: INFO: Deleting pod "simpletest-rc-to-be-deleted-7grrc" in namespace "gc-567"
  Aug  4 20:18:59.622: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kldq" in namespace "gc-567"
  Aug  4 20:18:59.641: INFO: Deleting pod "simpletest-rc-to-be-deleted-896r7" in namespace "gc-567"
  Aug  4 20:18:59.654: INFO: Deleting pod "simpletest-rc-to-be-deleted-8b2g8" in namespace "gc-567"
  Aug  4 20:18:59.716: INFO: Deleting pod "simpletest-rc-to-be-deleted-8gp9q" in namespace "gc-567"
  Aug  4 20:18:59.728: INFO: Deleting pod "simpletest-rc-to-be-deleted-8hd2r" in namespace "gc-567"
  Aug  4 20:18:59.750: INFO: Deleting pod "simpletest-rc-to-be-deleted-8mrv8" in namespace "gc-567"
  Aug  4 20:18:59.816: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tr9h" in namespace "gc-567"
  Aug  4 20:18:59.834: INFO: Deleting pod "simpletest-rc-to-be-deleted-8tzwg" in namespace "gc-567"
  Aug  4 20:18:59.919: INFO: Deleting pod "simpletest-rc-to-be-deleted-92rzc" in namespace "gc-567"
  Aug  4 20:18:59.928: INFO: Deleting pod "simpletest-rc-to-be-deleted-9hlhr" in namespace "gc-567"
  Aug  4 20:18:59.943: INFO: Deleting pod "simpletest-rc-to-be-deleted-b7gxh" in namespace "gc-567"
  Aug  4 20:19:00.030: INFO: Deleting pod "simpletest-rc-to-be-deleted-bphdh" in namespace "gc-567"
  Aug  4 20:19:00.105: INFO: Deleting pod "simpletest-rc-to-be-deleted-bvr2p" in namespace "gc-567"
  Aug  4 20:19:00.130: INFO: Deleting pod "simpletest-rc-to-be-deleted-c4fqh" in namespace "gc-567"
  Aug  4 20:19:00.139: INFO: Deleting pod "simpletest-rc-to-be-deleted-c5tqb" in namespace "gc-567"
  Aug  4 20:19:00.206: INFO: Deleting pod "simpletest-rc-to-be-deleted-c9rw8" in namespace "gc-567"
  Aug  4 20:19:00.219: INFO: Deleting pod "simpletest-rc-to-be-deleted-cjvdd" in namespace "gc-567"
  Aug  4 20:19:00.234: INFO: Deleting pod "simpletest-rc-to-be-deleted-cvvk4" in namespace "gc-567"
  Aug  4 20:19:00.245: INFO: Deleting pod "simpletest-rc-to-be-deleted-d9q67" in namespace "gc-567"
  Aug  4 20:19:00.322: INFO: Deleting pod "simpletest-rc-to-be-deleted-dh95x" in namespace "gc-567"
  Aug  4 20:19:00.337: INFO: Deleting pod "simpletest-rc-to-be-deleted-dq9c7" in namespace "gc-567"
  Aug  4 20:19:00.416: INFO: Deleting pod "simpletest-rc-to-be-deleted-dv6cv" in namespace "gc-567"
  Aug  4 20:19:00.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-dz58t" in namespace "gc-567"
  Aug  4 20:19:00.509: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8hwx" in namespace "gc-567"
  Aug  4 20:19:00.521: INFO: Deleting pod "simpletest-rc-to-be-deleted-f8j2m" in namespace "gc-567"
  E0804 20:19:00.525348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:19:00.533: INFO: Deleting pod "simpletest-rc-to-be-deleted-ffqc4" in namespace "gc-567"
  Aug  4 20:19:00.550: INFO: Deleting pod "simpletest-rc-to-be-deleted-flvkv" in namespace "gc-567"
  Aug  4 20:19:00.604: INFO: Deleting pod "simpletest-rc-to-be-deleted-fzsn5" in namespace "gc-567"
  Aug  4 20:19:00.618: INFO: Deleting pod "simpletest-rc-to-be-deleted-gfwd5" in namespace "gc-567"
  Aug  4 20:19:00.629: INFO: Deleting pod "simpletest-rc-to-be-deleted-gsvpp" in namespace "gc-567"
  Aug  4 20:19:00.638: INFO: Deleting pod "simpletest-rc-to-be-deleted-h7qm4" in namespace "gc-567"
  Aug  4 20:19:00.712: INFO: Deleting pod "simpletest-rc-to-be-deleted-hcmfj" in namespace "gc-567"
  Aug  4 20:19:00.725: INFO: Deleting pod "simpletest-rc-to-be-deleted-hd9ng" in namespace "gc-567"
  Aug  4 20:19:00.741: INFO: Deleting pod "simpletest-rc-to-be-deleted-j4wp5" in namespace "gc-567"
  Aug  4 20:19:00.841: INFO: Deleting pod "simpletest-rc-to-be-deleted-jgrfr" in namespace "gc-567"
  Aug  4 20:19:00.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-567" for this suite. @ 08/04/23 20:19:00.864
• [19.991 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 08/04/23 20:19:00.912
  Aug  4 20:19:00.912: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:19:00.913
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:19:00.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:19:00.933
  E0804 20:19:01.526250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:02.526512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:03.527055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:04.528115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:05.528416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:06.529085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:07.529735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:08.529951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:09.530765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:10.531031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:11.531902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:12.532128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:13.532348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:14.533272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:15.534006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:16.534704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:17.535403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:18.536102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:19.536640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:20.536873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:21.537629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:22.537959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:19:23.037: INFO: Container started at 2023-08-04 20:19:03 +0000 UTC, pod became ready at 2023-08-04 20:19:21 +0000 UTC
  Aug  4 20:19:23.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1561" for this suite. @ 08/04/23 20:19:23.039
• [22.131 seconds]
------------------------------
SSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 08/04/23 20:19:23.043
  Aug  4 20:19:23.043: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename init-container @ 08/04/23 20:19:23.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:19:23.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:19:23.054
  STEP: creating the pod @ 08/04/23 20:19:23.056
  Aug  4 20:19:23.056: INFO: PodSpec: initContainers in spec.initContainers
  E0804 20:19:23.538008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:24.538523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:25.538723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:26.538766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:19:27.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2431" for this suite. @ 08/04/23 20:19:27.428
• [4.388 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 08/04/23 20:19:27.433
  Aug  4 20:19:27.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubelet-test @ 08/04/23 20:19:27.433
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:19:27.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:19:27.444
  E0804 20:19:27.539681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:28.540061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:19:29.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-815" for this suite. @ 08/04/23 20:19:29.465
• [2.036 seconds]
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 08/04/23 20:19:29.469
  Aug  4 20:19:29.469: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:19:29.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:19:29.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:19:29.507
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:19:29.509
  E0804 20:19:29.540683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:30.541169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:19:31.521
  Aug  4 20:19:31.523: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-e9bc5149-f4b8-4860-b24f-f6c60c3e9274 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:19:31.529
  Aug  4 20:19:31.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0804 20:19:31.541984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "projected-4821" for this suite. @ 08/04/23 20:19:31.542
• [2.077 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 08/04/23 20:19:31.546
  Aug  4 20:19:31.546: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:19:31.546
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:19:31.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:19:31.555
  STEP: Creating the pod @ 08/04/23 20:19:31.557
  E0804 20:19:32.542165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:33.542582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:19:34.085: INFO: Successfully updated pod "labelsupdate825f4f57-ab43-46f0-a050-9fd88e162567"
  E0804 20:19:34.542949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:35.543530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:19:36.099: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4037" for this suite. @ 08/04/23 20:19:36.102
• [4.560 seconds]
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 08/04/23 20:19:36.106
  Aug  4 20:19:36.106: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:19:36.106
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:19:36.116
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:19:36.118
  STEP: Creating pod liveness-9109d420-d6a4-4d5a-b465-c43771fa4fff in namespace container-probe-6604 @ 08/04/23 20:19:36.12
  E0804 20:19:36.544473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:37.544741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:19:38.130: INFO: Started pod liveness-9109d420-d6a4-4d5a-b465-c43771fa4fff in namespace container-probe-6604
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/04/23 20:19:38.13
  Aug  4 20:19:38.132: INFO: Initial restart count of pod liveness-9109d420-d6a4-4d5a-b465-c43771fa4fff is 0
  E0804 20:19:38.544781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:39.544869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:40.545171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:41.546238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:42.546535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:43.546774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:44.547718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:45.547917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:46.548683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:47.548912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:48.549674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:49.549846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:50.550486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:51.550917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:52.550992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:53.552019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:54.552717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:55.552911      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:56.553597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:57.553721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:19:58.161: INFO: Restart count of pod container-probe-6604/liveness-9109d420-d6a4-4d5a-b465-c43771fa4fff is now 1 (20.029325398s elapsed)
  Aug  4 20:19:58.162: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:19:58.164
  STEP: Destroying namespace "container-probe-6604" for this suite. @ 08/04/23 20:19:58.174
• [22.072 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 08/04/23 20:19:58.178
  Aug  4 20:19:58.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:19:58.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:19:58.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:19:58.189
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/04/23 20:19:58.191
  E0804 20:19:58.554561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:19:59.555369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:00.556010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:01.556456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:20:02.205
  Aug  4 20:20:02.207: INFO: Trying to get logs from node k8sconformance-m02 pod pod-295fcccd-5d95-40aa-ab4d-d08102dfffa6 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:20:02.213
  Aug  4 20:20:02.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5458" for this suite. @ 08/04/23 20:20:02.224
• [4.049 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 08/04/23 20:20:02.228
  Aug  4 20:20:02.228: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename namespaces @ 08/04/23 20:20:02.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:20:02.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:20:02.238
  STEP: creating a Namespace @ 08/04/23 20:20:02.24
  STEP: patching the Namespace @ 08/04/23 20:20:02.246
  STEP: get the Namespace and ensuring it has the label @ 08/04/23 20:20:02.251
  Aug  4 20:20:02.253: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-350" for this suite. @ 08/04/23 20:20:02.254
  STEP: Destroying namespace "nspatchtest-1416ebe3-8b26-4302-b706-310832dafc2b-9478" for this suite. @ 08/04/23 20:20:02.258
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 08/04/23 20:20:02.263
  Aug  4 20:20:02.263: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:20:02.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:20:02.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:20:02.273
  STEP: Creating pod busybox-39b05d22-d8a3-46c7-ab46-c600bb27f7ae in namespace container-probe-8054 @ 08/04/23 20:20:02.275
  E0804 20:20:02.556706      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:03.557049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:20:04.285: INFO: Started pod busybox-39b05d22-d8a3-46c7-ab46-c600bb27f7ae in namespace container-probe-8054
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/04/23 20:20:04.285
  Aug  4 20:20:04.287: INFO: Initial restart count of pod busybox-39b05d22-d8a3-46c7-ab46-c600bb27f7ae is 0
  E0804 20:20:04.558058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:05.558354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:06.558509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:07.558724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:08.559420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:09.560495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:10.561133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:11.561644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:12.562397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:13.562613      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:14.563615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:15.564078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:16.564753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:17.564989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:18.565278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:19.565427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:20.565665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:21.566211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:22.567174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:23.568037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:24.569119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:25.569347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:26.570023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:27.570242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:28.570428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:29.571258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:30.571873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:31.572290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:32.572832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:33.573031      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:34.574136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:35.574395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:36.575402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:37.576083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:38.576666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:39.576900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:40.577379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:41.577811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:42.577991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:43.578269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:44.579336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:45.579565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:46.580349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:47.580591      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:48.581217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:49.581449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:50.582010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:51.582516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:52.583209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:53.584038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:20:54.364: INFO: Restart count of pod container-probe-8054/busybox-39b05d22-d8a3-46c7-ab46-c600bb27f7ae is now 1 (50.077405129s elapsed)
  Aug  4 20:20:54.364: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:20:54.367
  STEP: Destroying namespace "container-probe-8054" for this suite. @ 08/04/23 20:20:54.376
• [52.117 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:374
  STEP: Creating a kubernetes client @ 08/04/23 20:20:54.381
  Aug  4 20:20:54.381: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename daemonsets @ 08/04/23 20:20:54.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:20:54.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:20:54.392
  Aug  4 20:20:54.404: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/04/23 20:20:54.407
  Aug  4 20:20:54.411: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 20:20:54.411: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:20:54.584601      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:20:55.418: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 20:20:55.418: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Update daemon pods image. @ 08/04/23 20:20:55.425
  STEP: Check that daemon pods images are updated. @ 08/04/23 20:20:55.43
  Aug  4 20:20:55.434: INFO: Wrong image for pod: daemon-set-dzwfx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  4 20:20:55.434: INFO: Wrong image for pod: daemon-set-j2qhg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0804 20:20:55.585311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:20:56.439: INFO: Wrong image for pod: daemon-set-j2qhg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  E0804 20:20:56.585623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:20:57.439: INFO: Wrong image for pod: daemon-set-j2qhg. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Aug  4 20:20:57.439: INFO: Pod daemon-set-t7h5w is not available
  E0804 20:20:57.586532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:20:58.587180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:20:59.439: INFO: Pod daemon-set-wnt4l is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 08/04/23 20:20:59.441
  Aug  4 20:20:59.444: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 20:20:59.444: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
  E0804 20:20:59.588014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:21:00.449: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 20:21:00.449: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/04/23 20:21:00.459
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6264, will wait for the garbage collector to delete the pods @ 08/04/23 20:21:00.459
  Aug  4 20:21:00.514: INFO: Deleting DaemonSet.extensions daemon-set took: 4.135817ms
  E0804 20:21:00.588309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:21:00.615: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.650287ms
  E0804 20:21:01.589065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:21:02.221: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 20:21:02.221: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  4 20:21:02.223: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"9904"},"items":null}

  Aug  4 20:21:02.225: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"9904"},"items":null}

  Aug  4 20:21:02.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6264" for this suite. @ 08/04/23 20:21:02.233
• [7.857 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 08/04/23 20:21:02.239
  Aug  4 20:21:02.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:21:02.24
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:21:02.249
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:21:02.251
  STEP: Setting up server cert @ 08/04/23 20:21:02.268
  E0804 20:21:02.589760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:21:02.67
  STEP: Deploying the webhook pod @ 08/04/23 20:21:02.675
  STEP: Wait for the deployment to be ready @ 08/04/23 20:21:02.684
  Aug  4 20:21:02.691: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 20:21:03.589822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:04.590068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:21:04.7
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:21:04.708
  E0804 20:21:05.590688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:21:05.708: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug  4 20:21:05.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1226-crds.webhook.example.com via the AdmissionRegistration API @ 08/04/23 20:21:06.219
  STEP: Creating a custom resource while v1 is storage version @ 08/04/23 20:21:06.232
  E0804 20:21:06.591678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:07.592573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Patching Custom Resource Definition to set v2 as storage @ 08/04/23 20:21:08.261
  STEP: Patching the custom resource while v2 is storage version @ 08/04/23 20:21:08.274
  Aug  4 20:21:08.301: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0804 20:21:08.592782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-3394" for this suite. @ 08/04/23 20:21:08.845
  STEP: Destroying namespace "webhook-markers-7172" for this suite. @ 08/04/23 20:21:08.849
• [6.614 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 08/04/23 20:21:08.853
  Aug  4 20:21:08.853: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-pred @ 08/04/23 20:21:08.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:21:08.864
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:21:08.867
  Aug  4 20:21:08.869: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug  4 20:21:08.873: INFO: Waiting for terminating namespaces to be deleted...
  Aug  4 20:21:08.874: INFO: 
  Logging pods the apiserver thinks is on node k8sconformance before test
  Aug  4 20:21:08.878: INFO: coredns-5d78c9869d-zl979 from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container coredns ready: true, restart count 1
  Aug  4 20:21:08.878: INFO: etcd-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container etcd ready: true, restart count 0
  Aug  4 20:21:08.878: INFO: kindnet-5zl6d from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container kindnet-cni ready: true, restart count 0
  Aug  4 20:21:08.878: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug  4 20:21:08.878: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug  4 20:21:08.878: INFO: kube-proxy-mslwk from kube-system started at 2023-08-04 19:46:58 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug  4 20:21:08.878: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug  4 20:21:08.878: INFO: storage-provisioner from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container storage-provisioner ready: true, restart count 1
  Aug  4 20:21:08.878: INFO: sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-fdn76 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 20:21:08.878: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 20:21:08.878: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  4 20:21:08.878: INFO: 
  Logging pods the apiserver thinks is on node k8sconformance-m02 before test
  Aug  4 20:21:08.881: INFO: kindnet-grl89 from kube-system started at 2023-08-04 20:09:19 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.881: INFO: 	Container kindnet-cni ready: true, restart count 0
  Aug  4 20:21:08.881: INFO: kube-proxy-nn2ml from kube-system started at 2023-08-04 19:47:12 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.881: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug  4 20:21:08.881: INFO: sonobuoy from sonobuoy started at 2023-08-04 19:47:19 +0000 UTC (1 container statuses recorded)
  Aug  4 20:21:08.881: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug  4 20:21:08.881: INFO: sonobuoy-e2e-job-e2b8ac8d10f74a23 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 20:21:08.881: INFO: 	Container e2e ready: true, restart count 0
  Aug  4 20:21:08.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 20:21:08.881: INFO: sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-plz82 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 20:21:08.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 20:21:08.881: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node k8sconformance @ 08/04/23 20:21:08.892
  STEP: verifying the node has the label node k8sconformance-m02 @ 08/04/23 20:21:08.902
  Aug  4 20:21:08.909: INFO: Pod coredns-5d78c9869d-zl979 requesting resource cpu=100m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod etcd-k8sconformance requesting resource cpu=100m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod kindnet-5zl6d requesting resource cpu=100m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod kindnet-grl89 requesting resource cpu=100m on Node k8sconformance-m02
  Aug  4 20:21:08.909: INFO: Pod kube-apiserver-k8sconformance requesting resource cpu=250m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod kube-controller-manager-k8sconformance requesting resource cpu=200m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod kube-proxy-mslwk requesting resource cpu=0m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod kube-proxy-nn2ml requesting resource cpu=0m on Node k8sconformance-m02
  Aug  4 20:21:08.909: INFO: Pod kube-scheduler-k8sconformance requesting resource cpu=100m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod storage-provisioner requesting resource cpu=0m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod sonobuoy requesting resource cpu=0m on Node k8sconformance-m02
  Aug  4 20:21:08.909: INFO: Pod sonobuoy-e2e-job-e2b8ac8d10f74a23 requesting resource cpu=0m on Node k8sconformance-m02
  Aug  4 20:21:08.909: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-fdn76 requesting resource cpu=0m on Node k8sconformance
  Aug  4 20:21:08.909: INFO: Pod sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-plz82 requesting resource cpu=0m on Node k8sconformance-m02
  STEP: Starting Pods to consume most of the cluster CPU. @ 08/04/23 20:21:08.909
  Aug  4 20:21:08.909: INFO: Creating a pod which consumes cpu=5005m on Node k8sconformance
  Aug  4 20:21:08.917: INFO: Creating a pod which consumes cpu=5530m on Node k8sconformance-m02
  E0804 20:21:09.592957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:10.593233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating another pod that requires unavailable amount of CPU. @ 08/04/23 20:21:10.932
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8391ce23-7900-4615-91ca-a9505e120b2f.177847ad1d7c5f72], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4414/filler-pod-8391ce23-7900-4615-91ca-a9505e120b2f to k8sconformance-m02] @ 08/04/23 20:21:10.934
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8391ce23-7900-4615-91ca-a9505e120b2f.177847ad3d1f4bad], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/04/23 20:21:10.934
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8391ce23-7900-4615-91ca-a9505e120b2f.177847ad3e5718d3], Reason = [Created], Message = [Created container filler-pod-8391ce23-7900-4615-91ca-a9505e120b2f] @ 08/04/23 20:21:10.934
  STEP: Considering event: 
  Type = [Warning], Name = [filler-pod-8391ce23-7900-4615-91ca-a9505e120b2f.177847ad44236419], Reason = [Failed], Message = [Error: failed to start container "filler-pod-8391ce23-7900-4615-91ca-a9505e120b2f": Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "553000": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/podd3605baf-7ca3-402c-99ef-275328c683f7/filler-pod-8391ce23-7900-4615-91ca-a9505e120b2f/cpu.cfs_quota_us: invalid argument: unknown] @ 08/04/23 20:21:10.934
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8b493185-f039-4447-8207-3cfc98d70783.177847ad1d8015ab], Reason = [Scheduled], Message = [Successfully assigned sched-pred-4414/filler-pod-8b493185-f039-4447-8207-3cfc98d70783 to k8sconformance] @ 08/04/23 20:21:10.934
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8b493185-f039-4447-8207-3cfc98d70783.177847ad3d1ffa1b], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 08/04/23 20:21:10.934
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-8b493185-f039-4447-8207-3cfc98d70783.177847ad3e5690f8], Reason = [Created], Message = [Created container filler-pod-8b493185-f039-4447-8207-3cfc98d70783] @ 08/04/23 20:21:10.934
  STEP: Considering event: 
  Type = [Warning], Name = [filler-pod-8b493185-f039-4447-8207-3cfc98d70783.177847ad442356ac], Reason = [Failed], Message = [Error: failed to start container "filler-pod-8b493185-f039-4447-8207-3cfc98d70783": Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: error during container init: error setting cgroup config for procHooks process: failed to write "500500": write /sys/fs/cgroup/cpu,cpuacct/kubepods/burstable/pod66cf4ac5-8054-42ee-94da-5a4dfd678542/filler-pod-8b493185-f039-4447-8207-3cfc98d70783/cpu.cfs_quota_us: invalid argument: unknown] @ 08/04/23 20:21:10.934
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.177847ad9582c2b7], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 Insufficient cpu. preemption: 0/2 nodes are available: 2 No preemption victims found for incoming pod..] @ 08/04/23 20:21:10.944
  STEP: Considering event: 
  Type = [Warning], Name = [filler-pod-8b493185-f039-4447-8207-3cfc98d70783.177847adb5395efb], Reason = [BackOff], Message = [Back-off restarting failed container filler-pod-8b493185-f039-4447-8207-3cfc98d70783 in pod filler-pod-8b493185-f039-4447-8207-3cfc98d70783_sched-pred-4414(66cf4ac5-8054-42ee-94da-5a4dfd678542)] @ 08/04/23 20:21:11.476
  E0804 20:21:11.594046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label node off the node k8sconformance @ 08/04/23 20:21:11.944
  STEP: verifying the node doesn't have the label node @ 08/04/23 20:21:11.954
  STEP: removing the label node off the node k8sconformance-m02 @ 08/04/23 20:21:11.956
  STEP: verifying the node doesn't have the label node @ 08/04/23 20:21:11.965
  Aug  4 20:21:11.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-4414" for this suite. @ 08/04/23 20:21:11.97
• [3.123 seconds]
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 08/04/23 20:21:11.976
  Aug  4 20:21:11.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename namespaces @ 08/04/23 20:21:11.977
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:21:11.986
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:21:11.988
  STEP: Updating Namespace "namespaces-4231" @ 08/04/23 20:21:11.991
  Aug  4 20:21:11.996: INFO: Namespace "namespaces-4231" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"aa536a21-1f67-4537-80d5-a1f09aaa06c5", "kubernetes.io/metadata.name":"namespaces-4231", "namespaces-4231":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Aug  4 20:21:11.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4231" for this suite. @ 08/04/23 20:21:11.999
• [0.027 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 08/04/23 20:21:12.004
  Aug  4 20:21:12.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename dns @ 08/04/23 20:21:12.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:21:12.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:21:12.014
  STEP: Creating a test headless service @ 08/04/23 20:21:12.016
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8020.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-8020.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 08/04/23 20:21:12.019
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-8020.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-8020.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 08/04/23 20:21:12.019
  STEP: creating a pod to probe DNS @ 08/04/23 20:21:12.019
  STEP: submitting the pod to kubernetes @ 08/04/23 20:21:12.019
  E0804 20:21:12.594893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:13.594974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:14.596052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:15.596521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:16.597393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:17.598146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:18.598341      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:19.598529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 20:21:20.04
  STEP: looking for the results for each expected name from probers @ 08/04/23 20:21:20.042
  Aug  4 20:21:20.051: INFO: DNS probes using dns-8020/dns-test-a14af8f9-7638-444a-9fbb-aa372d69dfa7 succeeded

  Aug  4 20:21:20.051: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:21:20.053
  STEP: deleting the test headless service @ 08/04/23 20:21:20.069
  STEP: Destroying namespace "dns-8020" for this suite. @ 08/04/23 20:21:20.076
• [8.106 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 08/04/23 20:21:20.11
  Aug  4 20:21:20.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:21:20.111
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:21:20.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:21:20.122
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-c9df96e4-4de2-4aa6-974e-81d54ef62837 @ 08/04/23 20:21:20.126
  STEP: Creating the pod @ 08/04/23 20:21:20.128
  E0804 20:21:20.599330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:21.599495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-c9df96e4-4de2-4aa6-974e-81d54ef62837 @ 08/04/23 20:21:22.147
  STEP: waiting to observe update in volume @ 08/04/23 20:21:22.152
  E0804 20:21:22.599618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:23.600036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:24.600441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:25.600639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:26.601201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:27.601415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:28.602401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:29.602646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:30.603587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:31.604064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:32.604178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:33.604539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:34.605400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:35.605629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:36.606711      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:37.606936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:38.606950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:39.607173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:40.608005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:41.608748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:42.609453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:43.609673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:44.610316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:45.610519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:46.611555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:47.612095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:48.613176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:49.613285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:50.613511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:51.613926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:52.613977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:53.614194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:54.614336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:55.614533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:56.614644      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:57.614976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:58.615149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:21:59.616066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:00.616600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:01.617190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:02.617978      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:03.618257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:04.618390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:05.618747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:06.619198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:07.620056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:08.620530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:09.620785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:10.621310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:11.621443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:12.622397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:13.622643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:14.622675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:15.622918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:16.623686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:17.624056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:18.624347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:19.624573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:20.624715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:21.625186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:22.625951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:23.626298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:24.626514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:25.626712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:26.626949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:27.627330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:28.627854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:29.628036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:30.628643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:31.629205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:32.630050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:33.630282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:34.630411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:35.630624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:36.630931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:37.630954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:38.631326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:39.633084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:40.633188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:41.633274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:42.633752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:43.633943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:44.635014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:45.636064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:46.636733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:47.636923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:22:48.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4099" for this suite. @ 08/04/23 20:22:48.495
• [88.389 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 08/04/23 20:22:48.5
  Aug  4 20:22:48.500: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename gc @ 08/04/23 20:22:48.501
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:22:48.509
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:22:48.511
  STEP: create the rc @ 08/04/23 20:22:48.515
  W0804 20:22:48.519994      22 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  E0804 20:22:48.637302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:49.639051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:50.639619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:51.640564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:52.641281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:22:53.641629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the rc @ 08/04/23 20:22:54.525
  STEP: wait for the rc to be deleted @ 08/04/23 20:22:54.531
  E0804 20:22:54.645741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:22:55.543: INFO: 80 pods remaining
  Aug  4 20:22:55.543: INFO: 80 pods has nil DeletionTimestamp
  Aug  4 20:22:55.543: INFO: 
  E0804 20:22:55.646110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:22:56.544: INFO: 71 pods remaining
  Aug  4 20:22:56.544: INFO: 71 pods has nil DeletionTimestamp
  Aug  4 20:22:56.544: INFO: 
  E0804 20:22:56.704038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:22:57.605: INFO: 60 pods remaining
  Aug  4 20:22:57.605: INFO: 60 pods has nil DeletionTimestamp
  Aug  4 20:22:57.605: INFO: 
  E0804 20:22:57.704193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:22:58.607: INFO: 40 pods remaining
  Aug  4 20:22:58.607: INFO: 40 pods has nil DeletionTimestamp
  Aug  4 20:22:58.607: INFO: 
  E0804 20:22:58.704983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:22:59.543: INFO: 31 pods remaining
  Aug  4 20:22:59.543: INFO: 31 pods has nil DeletionTimestamp
  Aug  4 20:22:59.543: INFO: 
  E0804 20:22:59.705358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:23:00.607: INFO: 20 pods remaining
  Aug  4 20:23:00.607: INFO: 20 pods has nil DeletionTimestamp
  Aug  4 20:23:00.607: INFO: 
  E0804 20:23:00.705733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 08/04/23 20:23:01.536
  E0804 20:23:01.706547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:23:02.138: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Aug  4 20:23:02.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-3418" for this suite. @ 08/04/23 20:23:02.142
• [13.708 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 08/04/23 20:23:02.208
  Aug  4 20:23:02.208: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:23:02.209
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:23:02.222
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:23:02.224
  STEP: creating the pod @ 08/04/23 20:23:02.226
  STEP: submitting the pod to kubernetes @ 08/04/23 20:23:02.226
  STEP: verifying QOS class is set on the pod @ 08/04/23 20:23:02.233
  Aug  4 20:23:02.238: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4178" for this suite. @ 08/04/23 20:23:02.241
• [0.098 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 08/04/23 20:23:02.307
  Aug  4 20:23:02.307: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:23:02.308
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:23:02.321
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:23:02.324
  Aug  4 20:23:02.328: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-8571 version'
  Aug  4 20:23:02.633: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Aug  4 20:23:02.633: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:53:42Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.3\", GitCommit:\"25b4e43193bcda6c7328a6d147b1fb73a33f1598\", GitTreeState:\"clean\", BuildDate:\"2023-06-14T09:47:40Z\", GoVersion:\"go1.20.5\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Aug  4 20:23:02.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8571" for this suite. @ 08/04/23 20:23:02.637
  E0804 20:23:02.706641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
• [0.400 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 08/04/23 20:23:02.707
  Aug  4 20:23:02.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-runtime @ 08/04/23 20:23:02.708
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:23:02.721
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:23:02.724
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 08/04/23 20:23:02.733
  E0804 20:23:03.708233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:04.708287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:05.708412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:06.708750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:07.709174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:08.709297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:09.709413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:10.709624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:11.709950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:12.711034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:13.711132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:14.711816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:15.712095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:16.712280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:17.713362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:18.714001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:19.714429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:20.714983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:21.715714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:22.716003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:23.716277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:24.716556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:25.716946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 08/04/23 20:23:25.954
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 08/04/23 20:23:25.956
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 08/04/23 20:23:25.96
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 08/04/23 20:23:25.96
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 08/04/23 20:23:25.974
  E0804 20:23:26.717741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:27.718085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 08/04/23 20:23:27.982
  E0804 20:23:28.718337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 08/04/23 20:23:28.987
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 08/04/23 20:23:28.99
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 08/04/23 20:23:28.99
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 08/04/23 20:23:29.004
  E0804 20:23:29.718928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 08/04/23 20:23:30.008
  E0804 20:23:30.719748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 08/04/23 20:23:31.013
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 08/04/23 20:23:31.017
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 08/04/23 20:23:31.017
  Aug  4 20:23:31.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4209" for this suite. @ 08/04/23 20:23:31.034
• [28.331 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 08/04/23 20:23:31.038
  Aug  4 20:23:31.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename watch @ 08/04/23 20:23:31.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:23:31.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:23:31.049
  STEP: creating a watch on configmaps with a certain label @ 08/04/23 20:23:31.051
  STEP: creating a new configmap @ 08/04/23 20:23:31.052
  STEP: modifying the configmap once @ 08/04/23 20:23:31.055
  STEP: changing the label value of the configmap @ 08/04/23 20:23:31.06
  STEP: Expecting to observe a delete notification for the watched object @ 08/04/23 20:23:31.064
  Aug  4 20:23:31.064: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9920  008cbb05-1203-4215-94cf-bb6aad4accb5 11688 0 2023-08-04 20:23:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-04 20:23:31 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:23:31.065: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9920  008cbb05-1203-4215-94cf-bb6aad4accb5 11689 0 2023-08-04 20:23:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-04 20:23:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:23:31.065: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9920  008cbb05-1203-4215-94cf-bb6aad4accb5 11690 0 2023-08-04 20:23:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-04 20:23:31 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 08/04/23 20:23:31.065
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 08/04/23 20:23:31.07
  E0804 20:23:31.719949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:32.720216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:33.721018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:34.721419      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:35.721845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:36.722512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:37.723060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:38.723288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:39.723507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:40.724051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 08/04/23 20:23:41.071
  STEP: modifying the configmap a third time @ 08/04/23 20:23:41.078
  STEP: deleting the configmap @ 08/04/23 20:23:41.082
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 08/04/23 20:23:41.085
  Aug  4 20:23:41.085: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9920  008cbb05-1203-4215-94cf-bb6aad4accb5 11719 0 2023-08-04 20:23:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-04 20:23:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:23:41.085: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9920  008cbb05-1203-4215-94cf-bb6aad4accb5 11720 0 2023-08-04 20:23:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-04 20:23:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:23:41.086: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9920  008cbb05-1203-4215-94cf-bb6aad4accb5 11721 0 2023-08-04 20:23:31 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-08-04 20:23:41 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:23:41.086: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9920" for this suite. @ 08/04/23 20:23:41.088
• [10.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 08/04/23 20:23:41.092
  Aug  4 20:23:41.092: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:23:41.092
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:23:41.101
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:23:41.103
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 08/04/23 20:23:41.105
  E0804 20:23:41.724983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:42.725061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:23:43.115
  Aug  4 20:23:43.116: INFO: Trying to get logs from node k8sconformance-m02 pod pod-33ff4fa7-2262-4ae5-8aa3-9dc3e73b5c8b container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:23:43.122
  Aug  4 20:23:43.130: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1276" for this suite. @ 08/04/23 20:23:43.132
• [2.043 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 08/04/23 20:23:43.135
  Aug  4 20:23:43.135: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename containers @ 08/04/23 20:23:43.136
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:23:43.142
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:23:43.144
  STEP: Creating a pod to test override arguments @ 08/04/23 20:23:43.146
  E0804 20:23:43.725187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:44.725441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:23:45.153
  Aug  4 20:23:45.155: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-c8705380-4e61-4fc2-be04-884178363853 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:23:45.161
  Aug  4 20:23:45.167: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-7602" for this suite. @ 08/04/23 20:23:45.169
• [2.039 seconds]
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 08/04/23 20:23:45.174
  Aug  4 20:23:45.174: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:23:45.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:23:45.181
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:23:45.183
  STEP: Creating pod test-grpc-89c9419c-cd86-420a-a133-d1964fbc1a01 in namespace container-probe-7975 @ 08/04/23 20:23:45.205
  E0804 20:23:45.725825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:46.726068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:23:47.215: INFO: Started pod test-grpc-89c9419c-cd86-420a-a133-d1964fbc1a01 in namespace container-probe-7975
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/04/23 20:23:47.215
  Aug  4 20:23:47.217: INFO: Initial restart count of pod test-grpc-89c9419c-cd86-420a-a133-d1964fbc1a01 is 0
  E0804 20:23:47.726175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:48.726535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:49.727004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:50.727262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:51.728047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:52.728135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:53.728632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:54.728865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:55.729625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:56.730008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:57.730588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:58.730976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:23:59.731479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:00.731733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:01.732079      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:02.732519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:03.733341      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:04.733557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:05.734373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:06.735170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:07.736108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:08.736344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:09.736879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:10.737176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:11.737738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:12.738880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:13.739004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:14.740094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:15.740264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:16.740959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:17.741906      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:18.742112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:19.742621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:20.742943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:21.743001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:22.743865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:23.743395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:24.744100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:25.744774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:26.745353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:27.746095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:28.746328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:29.746918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:30.747140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:31.747658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:32.748343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:33.748862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:34.749643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:35.749859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:36.750655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:37.750967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:38.752030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:39.752690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:40.752942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:41.753056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:42.753289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:43.754400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:44.754587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:45.755266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:46.755811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:47.756243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:48.756444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:49.756939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:50.757121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:51.757265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:52.757333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:53.757995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:54.758227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:55.758522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:56.759175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:57.759771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:58.760168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:24:59.760525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:00.761196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:01.761963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:02.762259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:03.762719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:04.762977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:05.763098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:06.763752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:07.764027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:08.764287      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:09.764662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:10.764878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:11.764907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:12.765106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:13.765825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:14.766013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:15.766659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:16.766718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:17.767417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:18.768049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:19.768831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:20.769059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:21.770121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:22.770723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:23.771462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:24.771702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:25.772520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:26.773411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:27.773666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:28.773880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:29.773925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:30.774146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:31.774297      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:32.774524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:33.774596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:34.774820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:35.774966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:36.776029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:37.777127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:38.777381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:39.777442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:40.777989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:41.778307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:42.778508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:43.778981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:44.779011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:45.779558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:46.780045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:47.781050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:48.781340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:49.782003      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:50.782093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:51.782438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:52.782684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:53.783038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:54.783280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:55.783552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:56.784223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:57.784748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:58.784924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:25:59.785136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:00.785309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:01.785428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:02.785722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:03.785815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:04.786087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:05.786942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:06.787667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:07.788262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:08.788520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:09.789424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:10.789692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:11.790646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:12.790902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:13.791611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:14.791914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:15.792975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:16.793648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:17.793713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:18.793948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:19.794714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:20.794882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:21.795445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:22.795680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:23.796093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:24.796309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:25.796877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:26.797343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:27.797875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:28.798073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:29.798577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:30.798848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:31.798943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:32.798986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:33.799441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:34.800017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:35.800363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:36.801070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:37.801458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:38.801677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:39.802342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:40.802567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:41.802922      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:42.803205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:43.803943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:44.804220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:45.805186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:46.805799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:47.806320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:48.806559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:49.806841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:50.806979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:51.808068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:52.808245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:53.808411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:54.808754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:55.809256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:56.809746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:57.810007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:58.810244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:26:59.810373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:00.810595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:01.810713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:02.810952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:03.811488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:04.812074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:05.812628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:06.813307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:07.814407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:08.814534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:09.815119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:10.816028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:11.817045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:12.817213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:13.817799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:14.818033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:15.818265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:16.818935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:17.818967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:18.820045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:19.820766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:20.820961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:21.821245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:22.821405      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:23.821590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:24.821850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:25.822410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:26.823141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:27.823800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:28.824030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:29.824442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:30.824639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:31.825641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:32.825820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:33.826458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:34.826640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:35.827476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:36.828105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:37.828869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:38.829101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:39.829848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:40.830062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:41.830728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:42.831181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:43.831670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:44.832121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:45.832777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:46.833252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:27:47.582: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:27:47.584
  STEP: Destroying namespace "container-probe-7975" for this suite. @ 08/04/23 20:27:47.593
• [242.424 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 08/04/23 20:27:47.598
  Aug  4 20:27:47.598: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubelet-test @ 08/04/23 20:27:47.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:27:47.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:27:47.609
  Aug  4 20:27:47.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-4811" for this suite. @ 08/04/23 20:27:47.627
• [0.032 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 08/04/23 20:27:47.631
  Aug  4 20:27:47.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:27:47.632
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:27:47.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:27:47.642
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-6297 @ 08/04/23 20:27:47.644
  STEP: changing the ExternalName service to type=ClusterIP @ 08/04/23 20:27:47.647
  STEP: creating replication controller externalname-service in namespace services-6297 @ 08/04/23 20:27:47.657
  I0804 20:27:47.663436      22 runners.go:194] Created replication controller with name: externalname-service, namespace: services-6297, replica count: 2
  E0804 20:27:47.834000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:48.834909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:49.834995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:27:50.714900      22 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 20:27:50.714: INFO: Creating new exec pod
  E0804 20:27:50.835778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:51.836205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:52.836944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:27:53.726: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-6297 exec execpod7wx7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0804 20:27:53.837462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:27:53.861: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  4 20:27:53.861: INFO: stdout: ""
  E0804 20:27:54.838571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:27:54.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-6297 exec execpod7wx7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  4 20:27:54.973: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  4 20:27:54.973: INFO: stdout: "externalname-service-vhdvz"
  Aug  4 20:27:54.973: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-6297 exec execpod7wx7f -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.103.132.227 80'
  Aug  4 20:27:55.109: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.103.132.227 80\nConnection to 10.103.132.227 80 port [tcp/http] succeeded!\n"
  Aug  4 20:27:55.109: INFO: stdout: "externalname-service-9hfbm"
  Aug  4 20:27:55.109: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 20:27:55.111: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-6297" for this suite. @ 08/04/23 20:27:55.122
• [7.496 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 08/04/23 20:27:55.131
  Aug  4 20:27:55.131: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename ingressclass @ 08/04/23 20:27:55.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:27:55.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:27:55.141
  STEP: getting /apis @ 08/04/23 20:27:55.143
  STEP: getting /apis/networking.k8s.io @ 08/04/23 20:27:55.146
  STEP: getting /apis/networking.k8s.iov1 @ 08/04/23 20:27:55.147
  STEP: creating @ 08/04/23 20:27:55.147
  STEP: getting @ 08/04/23 20:27:55.157
  STEP: listing @ 08/04/23 20:27:55.158
  STEP: watching @ 08/04/23 20:27:55.16
  Aug  4 20:27:55.160: INFO: starting watch
  STEP: patching @ 08/04/23 20:27:55.161
  STEP: updating @ 08/04/23 20:27:55.164
  Aug  4 20:27:55.167: INFO: waiting for watch events with expected annotations
  Aug  4 20:27:55.167: INFO: saw patched and updated annotations
  STEP: deleting @ 08/04/23 20:27:55.167
  STEP: deleting a collection @ 08/04/23 20:27:55.172
  Aug  4 20:27:55.179: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-7077" for this suite. @ 08/04/23 20:27:55.181
• [0.054 seconds]
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 08/04/23 20:27:55.185
  Aug  4 20:27:55.185: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename security-context @ 08/04/23 20:27:55.185
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:27:55.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:27:55.196
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/04/23 20:27:55.198
  E0804 20:27:55.839030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:56.839785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:57.840455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:27:58.840669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:27:59.212
  Aug  4 20:27:59.214: INFO: Trying to get logs from node k8sconformance-m02 pod security-context-35cc2f49-01e5-4a43-9a99-14614e77ab79 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:27:59.226
  Aug  4 20:27:59.237: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-1090" for this suite. @ 08/04/23 20:27:59.24
• [4.058 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 08/04/23 20:27:59.243
  Aug  4 20:27:59.243: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename disruption @ 08/04/23 20:27:59.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:27:59.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:27:59.254
  STEP: Creating a kubernetes client @ 08/04/23 20:27:59.256
  Aug  4 20:27:59.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename disruption-2 @ 08/04/23 20:27:59.257
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:27:59.264
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:27:59.266
  STEP: Waiting for the pdb to be processed @ 08/04/23 20:27:59.27
  E0804 20:27:59.840809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:00.841282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 08/04/23 20:28:01.283
  E0804 20:28:01.841388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:02.841816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be processed @ 08/04/23 20:28:03.292
  E0804 20:28:03.842327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:04.842635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: listing a collection of PDBs across all namespaces @ 08/04/23 20:28:05.297
  STEP: listing a collection of PDBs in namespace disruption-6593 @ 08/04/23 20:28:05.299
  STEP: deleting a collection of PDBs @ 08/04/23 20:28:05.301
  STEP: Waiting for the PDB collection to be deleted @ 08/04/23 20:28:05.31
  Aug  4 20:28:05.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 20:28:05.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-6025" for this suite. @ 08/04/23 20:28:05.316
  STEP: Destroying namespace "disruption-6593" for this suite. @ 08/04/23 20:28:05.32
• [6.080 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 08/04/23 20:28:05.324
  Aug  4 20:28:05.324: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:28:05.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:05.332
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:05.334
  STEP: Creating secret with name secret-test-4cdb832d-82f5-4679-ba93-ef1206d87970 @ 08/04/23 20:28:05.349
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:28:05.352
  E0804 20:28:05.843157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:06.843812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:07.844586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:08.844849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:28:09.366
  Aug  4 20:28:09.368: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-df1a3e68-44bb-46cf-9228-60fc46e36678 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:28:09.374
  Aug  4 20:28:09.385: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-4562" for this suite. @ 08/04/23 20:28:09.387
  STEP: Destroying namespace "secret-namespace-2934" for this suite. @ 08/04/23 20:28:09.391
• [4.071 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 08/04/23 20:28:09.395
  Aug  4 20:28:09.395: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename svcaccounts @ 08/04/23 20:28:09.395
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:09.406
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:09.409
  Aug  4 20:28:09.412: INFO: Got root ca configmap in namespace "svcaccounts-7035"
  Aug  4 20:28:09.417: INFO: Deleted root ca configmap in namespace "svcaccounts-7035"
  E0804 20:28:09.845007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for a new root ca configmap created @ 08/04/23 20:28:09.918
  Aug  4 20:28:09.920: INFO: Recreated root ca configmap in namespace "svcaccounts-7035"
  Aug  4 20:28:09.924: INFO: Updated root ca configmap in namespace "svcaccounts-7035"
  STEP: waiting for the root ca configmap reconciled @ 08/04/23 20:28:10.424
  Aug  4 20:28:10.426: INFO: Reconciled root ca configmap in namespace "svcaccounts-7035"
  Aug  4 20:28:10.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7035" for this suite. @ 08/04/23 20:28:10.428
• [1.038 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 08/04/23 20:28:10.434
  Aug  4 20:28:10.434: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:28:10.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:10.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:10.447
  STEP: Creating secret with name secret-test-3f7738d8-0dae-4fef-a507-bbdb1d710ab1 @ 08/04/23 20:28:10.449
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:28:10.452
  E0804 20:28:10.845040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:11.845952      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:12.846682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:13.846957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:28:14.465
  Aug  4 20:28:14.467: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-6217006c-e458-49c9-848b-012bfa3b3841 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:28:14.511
  Aug  4 20:28:14.521: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2493" for this suite. @ 08/04/23 20:28:14.524
• [4.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 08/04/23 20:28:14.529
  Aug  4 20:28:14.529: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename gc @ 08/04/23 20:28:14.53
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:14.538
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:14.54
  Aug  4 20:28:14.611: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"6f642031-3161-4192-91a0-13063f48fa20", Controller:(*bool)(0xc003794716), BlockOwnerDeletion:(*bool)(0xc003794717)}}
  Aug  4 20:28:14.619: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"007bf1eb-b38e-47fe-b412-a53ef1fe8ef8", Controller:(*bool)(0xc003794976), BlockOwnerDeletion:(*bool)(0xc003794977)}}
  Aug  4 20:28:14.627: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"092ab394-d7fd-43ad-944f-52e5e5f077eb", Controller:(*bool)(0xc003f2635e), BlockOwnerDeletion:(*bool)(0xc003f2635f)}}
  E0804 20:28:14.847570      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:15.848073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:16.848560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:17.848797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:18.849289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:19.634: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-4816" for this suite. @ 08/04/23 20:28:19.637
• [5.111 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 08/04/23 20:28:19.641
  Aug  4 20:28:19.641: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename podtemplate @ 08/04/23 20:28:19.641
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:19.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:19.651
  STEP: Create set of pod templates @ 08/04/23 20:28:19.654
  Aug  4 20:28:19.659: INFO: created test-podtemplate-1
  Aug  4 20:28:19.662: INFO: created test-podtemplate-2
  Aug  4 20:28:19.665: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 08/04/23 20:28:19.665
  STEP: delete collection of pod templates @ 08/04/23 20:28:19.667
  Aug  4 20:28:19.667: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 08/04/23 20:28:19.678
  Aug  4 20:28:19.678: INFO: requesting list of pod templates to confirm quantity
  Aug  4 20:28:19.680: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-146" for this suite. @ 08/04/23 20:28:19.682
• [0.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 08/04/23 20:28:19.689
  Aug  4 20:28:19.689: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 20:28:19.69
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:19.697
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:19.699
  Aug  4 20:28:19.701: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:28:19.850000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:20.850750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/04/23 20:28:21.006
  Aug  4 20:28:21.006: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-8454 --namespace=crd-publish-openapi-8454 create -f -'
  Aug  4 20:28:21.491: INFO: stderr: ""
  Aug  4 20:28:21.491: INFO: stdout: "e2e-test-crd-publish-openapi-4748-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug  4 20:28:21.491: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-8454 --namespace=crd-publish-openapi-8454 delete e2e-test-crd-publish-openapi-4748-crds test-cr'
  Aug  4 20:28:21.551: INFO: stderr: ""
  Aug  4 20:28:21.551: INFO: stdout: "e2e-test-crd-publish-openapi-4748-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Aug  4 20:28:21.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-8454 --namespace=crd-publish-openapi-8454 apply -f -'
  Aug  4 20:28:21.718: INFO: stderr: ""
  Aug  4 20:28:21.718: INFO: stdout: "e2e-test-crd-publish-openapi-4748-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Aug  4 20:28:21.718: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-8454 --namespace=crd-publish-openapi-8454 delete e2e-test-crd-publish-openapi-4748-crds test-cr'
  Aug  4 20:28:21.781: INFO: stderr: ""
  Aug  4 20:28:21.781: INFO: stdout: "e2e-test-crd-publish-openapi-4748-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 08/04/23 20:28:21.781
  Aug  4 20:28:21.781: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-8454 explain e2e-test-crd-publish-openapi-4748-crds'
  E0804 20:28:21.851035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:21.942: INFO: stderr: ""
  Aug  4 20:28:21.942: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-4748-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  E0804 20:28:22.851008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:23.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-8454" for this suite. @ 08/04/23 20:28:23.222
• [3.536 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 08/04/23 20:28:23.227
  Aug  4 20:28:23.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename var-expansion @ 08/04/23 20:28:23.228
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:23.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:23.237
  E0804 20:28:23.852098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:24.852366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:25.250: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 20:28:25.252: INFO: Deleting pod "var-expansion-3ade5864-e2c6-40af-9cc4-1bcd20f42c71" in namespace "var-expansion-6998"
  Aug  4 20:28:25.257: INFO: Wait up to 5m0s for pod "var-expansion-3ade5864-e2c6-40af-9cc4-1bcd20f42c71" to be fully deleted
  E0804 20:28:25.852929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:26.853924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-6998" for this suite. @ 08/04/23 20:28:27.261
• [4.038 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 08/04/23 20:28:27.265
  Aug  4 20:28:27.265: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:28:27.266
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:27.274
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:27.276
  STEP: creating a Pod with a static label @ 08/04/23 20:28:27.28
  STEP: watching for Pod to be ready @ 08/04/23 20:28:27.285
  Aug  4 20:28:27.286: INFO: observed Pod pod-test in namespace pods-399 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Aug  4 20:28:27.290: INFO: observed Pod pod-test in namespace pods-399 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:27 +0000 UTC  }]
  Aug  4 20:28:27.298: INFO: observed Pod pod-test in namespace pods-399 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:27 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:27 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:27 +0000 UTC  }]
  E0804 20:28:27.854956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:28.555: INFO: Found Pod pod-test in namespace pods-399 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:27 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 20:28:27 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 08/04/23 20:28:28.557
  STEP: getting the Pod and ensuring that it's patched @ 08/04/23 20:28:28.564
  STEP: replacing the Pod's status Ready condition to False @ 08/04/23 20:28:28.566
  STEP: check the Pod again to ensure its Ready conditions are False @ 08/04/23 20:28:28.575
  STEP: deleting the Pod via a Collection with a LabelSelector @ 08/04/23 20:28:28.575
  STEP: watching for the Pod to be deleted @ 08/04/23 20:28:28.579
  Aug  4 20:28:28.580: INFO: observed event type MODIFIED
  E0804 20:28:28.854987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:29.855511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:30.576: INFO: observed event type MODIFIED
  Aug  4 20:28:30.665: INFO: observed event type MODIFIED
  E0804 20:28:30.855935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:31.585: INFO: observed event type MODIFIED
  Aug  4 20:28:31.593: INFO: observed event type MODIFIED
  Aug  4 20:28:31.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-399" for this suite. @ 08/04/23 20:28:31.599
• [4.337 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 08/04/23 20:28:31.603
  Aug  4 20:28:31.603: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename job @ 08/04/23 20:28:31.604
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:31.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:31.614
  STEP: Creating a job @ 08/04/23 20:28:31.616
  STEP: Ensuring job reaches completions @ 08/04/23 20:28:31.621
  E0804 20:28:31.856531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:32.856825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:33.857873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:34.858122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:35.859146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:36.859638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:37.860078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:38.860294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:39.860888      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:40.861295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:41.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2672" for this suite. @ 08/04/23 20:28:41.625
• [10.027 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 08/04/23 20:28:41.631
  Aug  4 20:28:41.631: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:28:41.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:41.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:41.641
  STEP: creating secret secrets-1044/secret-test-e3a6a387-d506-4d56-b9f8-d6cf83b0d20d @ 08/04/23 20:28:41.643
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:28:41.646
  E0804 20:28:41.861966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:42.862428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:43.863187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:44.864084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:28:45.66
  Aug  4 20:28:45.662: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-c0a13d2d-800f-4d45-9127-23f873ce6ada container env-test: <nil>
  STEP: delete the pod @ 08/04/23 20:28:45.667
  Aug  4 20:28:45.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1044" for this suite. @ 08/04/23 20:28:45.678
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 08/04/23 20:28:45.682
  Aug  4 20:28:45.682: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename proxy @ 08/04/23 20:28:45.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:45.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:45.695
  Aug  4 20:28:45.697: INFO: Creating pod...
  E0804 20:28:45.864890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:46.865352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:47.707: INFO: Creating service...
  Aug  4 20:28:47.735: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/pods/agnhost/proxy/some/path/with/DELETE
  Aug  4 20:28:47.807: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug  4 20:28:47.807: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/pods/agnhost/proxy/some/path/with/GET
  Aug  4 20:28:47.811: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug  4 20:28:47.811: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/pods/agnhost/proxy/some/path/with/HEAD
  Aug  4 20:28:47.813: INFO: http.Client request:HEAD | StatusCode:200
  Aug  4 20:28:47.813: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/pods/agnhost/proxy/some/path/with/OPTIONS
  Aug  4 20:28:47.815: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug  4 20:28:47.815: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/pods/agnhost/proxy/some/path/with/PATCH
  Aug  4 20:28:47.817: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug  4 20:28:47.817: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/pods/agnhost/proxy/some/path/with/POST
  Aug  4 20:28:47.819: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug  4 20:28:47.819: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/pods/agnhost/proxy/some/path/with/PUT
  Aug  4 20:28:47.821: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug  4 20:28:47.821: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/services/test-service/proxy/some/path/with/DELETE
  Aug  4 20:28:47.823: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Aug  4 20:28:47.823: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/services/test-service/proxy/some/path/with/GET
  Aug  4 20:28:47.825: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Aug  4 20:28:47.825: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/services/test-service/proxy/some/path/with/HEAD
  Aug  4 20:28:47.827: INFO: http.Client request:HEAD | StatusCode:200
  Aug  4 20:28:47.827: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/services/test-service/proxy/some/path/with/OPTIONS
  Aug  4 20:28:47.829: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Aug  4 20:28:47.829: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/services/test-service/proxy/some/path/with/PATCH
  Aug  4 20:28:47.832: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Aug  4 20:28:47.832: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/services/test-service/proxy/some/path/with/POST
  Aug  4 20:28:47.834: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Aug  4 20:28:47.834: INFO: Starting http.Client for https://10.96.0.1:443/api/v1/namespaces/proxy-7757/services/test-service/proxy/some/path/with/PUT
  Aug  4 20:28:47.837: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Aug  4 20:28:47.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7757" for this suite. @ 08/04/23 20:28:47.839
• [2.160 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 08/04/23 20:28:47.842
  Aug  4 20:28:47.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:28:47.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:47.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:47.854
  STEP: Creating secret with name secret-test-map-ec06e36e-2663-4119-a270-36188d620ee6 @ 08/04/23 20:28:47.856
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:28:47.858
  E0804 20:28:47.865804      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:48.866015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:49.866327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:50.866968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:51.867155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:28:51.87
  Aug  4 20:28:51.872: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-02f82d5a-c448-47e4-a7fe-13dfad41c508 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:28:51.879
  Aug  4 20:28:51.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7938" for this suite. @ 08/04/23 20:28:51.89
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 08/04/23 20:28:51.894
  Aug  4 20:28:51.894: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:28:51.895
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:51.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:51.905
  STEP: validating api versions @ 08/04/23 20:28:51.906
  Aug  4 20:28:51.907: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7678 api-versions'
  Aug  4 20:28:51.960: INFO: stderr: ""
  Aug  4 20:28:51.960: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nmygroup.example.com/v1\nmygroup.example.com/v1beta1\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Aug  4 20:28:51.960: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7678" for this suite. @ 08/04/23 20:28:51.964
• [0.073 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 08/04/23 20:28:51.968
  Aug  4 20:28:51.968: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replication-controller @ 08/04/23 20:28:51.969
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:51.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:51.977
  STEP: Creating ReplicationController "e2e-rc-zvqmg" @ 08/04/23 20:28:51.979
  Aug  4 20:28:51.982: INFO: Get Replication Controller "e2e-rc-zvqmg" to confirm replicas
  E0804 20:28:52.868067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:52.984: INFO: Get Replication Controller "e2e-rc-zvqmg" to confirm replicas
  Aug  4 20:28:52.986: INFO: Found 1 replicas for "e2e-rc-zvqmg" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-zvqmg" @ 08/04/23 20:28:52.986
  STEP: Updating a scale subresource @ 08/04/23 20:28:52.988
  STEP: Verifying replicas where modified for replication controller "e2e-rc-zvqmg" @ 08/04/23 20:28:52.993
  Aug  4 20:28:52.993: INFO: Get Replication Controller "e2e-rc-zvqmg" to confirm replicas
  E0804 20:28:53.868903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:28:53.995: INFO: Get Replication Controller "e2e-rc-zvqmg" to confirm replicas
  Aug  4 20:28:53.997: INFO: Found 2 replicas for "e2e-rc-zvqmg" replication controller
  Aug  4 20:28:53.997: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-7460" for this suite. @ 08/04/23 20:28:53.999
• [2.035 seconds]
------------------------------
S
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 08/04/23 20:28:54.003
  Aug  4 20:28:54.003: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:28:54.003
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:54.01
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:54.012
  STEP: Creating secret with name secret-test-67124076-cc93-45fb-9f19-7cc54f68b58b @ 08/04/23 20:28:54.014
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:28:54.018
  E0804 20:28:54.869737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:55.869945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:56.870398      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:57.870648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:28:58.029
  Aug  4 20:28:58.031: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-b75a06ed-fc98-497d-95ff-0662cf797c1e container secret-env-test: <nil>
  STEP: delete the pod @ 08/04/23 20:28:58.037
  Aug  4 20:28:58.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2671" for this suite. @ 08/04/23 20:28:58.047
• [4.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 08/04/23 20:28:58.051
  Aug  4 20:28:58.051: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-preemption @ 08/04/23 20:28:58.051
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:28:58.058
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:28:58.06
  Aug  4 20:28:58.071: INFO: Waiting up to 1m0s for all nodes to be ready
  E0804 20:28:58.871178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:28:59.872036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:00.872526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:01.872899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:02.873519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:03.873775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:04.874283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:05.874547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:06.874839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:07.875155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:08.875328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:09.876067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:10.876357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:11.876722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:12.877337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:13.877577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:14.878558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:15.878696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:16.878975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:17.880028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:18.880556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:19.880801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:20.881183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:21.881526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:22.882080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:23.882970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:24.883080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:25.883323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:26.884048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:27.884270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:28.884811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:29.885022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:30.885496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:31.885945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:32.886141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:33.886373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:34.886658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:35.886924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:36.887700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:37.888037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:38.888162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:39.888413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:40.888514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:41.889524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:42.889852      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:43.890114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:44.890404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:45.890645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:46.891493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:47.891765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:48.892565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:49.892809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:50.893472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:51.893848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:52.893976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:53.894221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:54.894556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:55.894745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:56.895520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:57.895750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:29:58.091: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/04/23 20:29:58.093
  Aug  4 20:29:58.109: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug  4 20:29:58.116: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug  4 20:29:58.131: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug  4 20:29:58.136: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/04/23 20:29:58.136
  E0804 20:29:58.895898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:29:59.896125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 08/04/23 20:30:00.149
  E0804 20:30:00.896596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:01.897045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:02.897650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:03.897951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:30:04.184: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-2942" for this suite. @ 08/04/23 20:30:04.23
• [66.185 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 08/04/23 20:30:04.236
  Aug  4 20:30:04.236: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 20:30:04.237
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:30:04.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:30:04.315
  STEP: Creating configMap with name configmap-test-volume-b5dde5a2-5ad7-4176-9fca-838aa90ca63c @ 08/04/23 20:30:04.317
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:30:04.323
  E0804 20:30:04.898927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:05.898994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:06.899300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:07.899967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:30:08.337
  Aug  4 20:30:08.339: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-33a96425-55ad-4b41-bad4-4f8dc9df14bc container configmap-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:30:08.345
  Aug  4 20:30:08.356: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3206" for this suite. @ 08/04/23 20:30:08.358
• [4.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 08/04/23 20:30:08.363
  Aug  4 20:30:08.363: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename dns @ 08/04/23 20:30:08.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:30:08.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:30:08.375
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/04/23 20:30:08.377
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 08/04/23 20:30:08.377
  STEP: creating a pod to probe DNS @ 08/04/23 20:30:08.377
  STEP: submitting the pod to kubernetes @ 08/04/23 20:30:08.377
  E0804 20:30:08.900099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:09.900517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 20:30:10.388
  STEP: looking for the results for each expected name from probers @ 08/04/23 20:30:10.39
  Aug  4 20:30:10.399: INFO: DNS probes using dns-8405/dns-test-9bda387c-0c3f-4d6b-b605-8ca2c1b04dff succeeded

  Aug  4 20:30:10.399: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:30:10.401
  STEP: Destroying namespace "dns-8405" for this suite. @ 08/04/23 20:30:10.41
• [2.051 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 08/04/23 20:30:10.415
  Aug  4 20:30:10.415: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:30:10.415
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:30:10.427
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:30:10.43
  STEP: Creating projection with secret that has name projected-secret-test-1bc36d9a-909a-412a-bf6e-09c0fcaa320c @ 08/04/23 20:30:10.432
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:30:10.436
  E0804 20:30:10.900946      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:11.901770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:12.902270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:13.902523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:30:14.448
  Aug  4 20:30:14.450: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-392b0248-fb18-441e-91c5-16f4471f4858 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:30:14.456
  Aug  4 20:30:14.466: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-188" for this suite. @ 08/04/23 20:30:14.468
• [4.058 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 08/04/23 20:30:14.473
  Aug  4 20:30:14.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/04/23 20:30:14.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:30:14.483
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:30:14.485
  STEP: create the container to handle the HTTPGet hook request. @ 08/04/23 20:30:14.488
  E0804 20:30:14.902848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:15.902989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/04/23 20:30:16.502
  E0804 20:30:16.903622      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:17.903887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 08/04/23 20:30:18.513
  STEP: delete the pod with lifecycle hook @ 08/04/23 20:30:18.518
  E0804 20:30:18.904505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:19.904668      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:30:20.528: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-3901" for this suite. @ 08/04/23 20:30:20.531
• [6.064 seconds]
------------------------------
SS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 08/04/23 20:30:20.537
  Aug  4 20:30:20.537: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:30:20.538
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:30:20.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:30:20.548
  Aug  4 20:30:20.550: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: creating the pod @ 08/04/23 20:30:20.55
  STEP: submitting the pod to kubernetes @ 08/04/23 20:30:20.55
  E0804 20:30:20.905563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:21.906246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:30:22.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9979" for this suite. @ 08/04/23 20:30:22.648
• [2.116 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 08/04/23 20:30:22.654
  Aug  4 20:30:22.654: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:30:22.655
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:30:22.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:30:22.667
  STEP: Setting up server cert @ 08/04/23 20:30:22.682
  E0804 20:30:22.906342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:30:23.134
  STEP: Deploying the webhook pod @ 08/04/23 20:30:23.137
  STEP: Wait for the deployment to be ready @ 08/04/23 20:30:23.147
  Aug  4 20:30:23.150: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0804 20:30:23.906453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:24.906785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:30:25.159
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:30:25.167
  E0804 20:30:25.906998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:30:26.167: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/04/23 20:30:26.225
  STEP: Creating a configMap that should be mutated @ 08/04/23 20:30:26.234
  STEP: Deleting the collection of validation webhooks @ 08/04/23 20:30:26.257
  STEP: Creating a configMap that should not be mutated @ 08/04/23 20:30:26.287
  Aug  4 20:30:26.294: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6537" for this suite. @ 08/04/23 20:30:26.318
  STEP: Destroying namespace "webhook-markers-2588" for this suite. @ 08/04/23 20:30:26.322
• [3.671 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 08/04/23 20:30:26.326
  Aug  4 20:30:26.326: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 20:30:26.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:30:26.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:30:26.337
  Aug  4 20:30:26.339: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:30:26.907985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/04/23 20:30:27.573
  Aug  4 20:30:27.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-1995 --namespace=crd-publish-openapi-1995 create -f -'
  E0804 20:30:27.909065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:30:28.174: INFO: stderr: ""
  Aug  4 20:30:28.174: INFO: stdout: "e2e-test-crd-publish-openapi-2860-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug  4 20:30:28.174: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-1995 --namespace=crd-publish-openapi-1995 delete e2e-test-crd-publish-openapi-2860-crds test-cr'
  Aug  4 20:30:28.236: INFO: stderr: ""
  Aug  4 20:30:28.236: INFO: stdout: "e2e-test-crd-publish-openapi-2860-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Aug  4 20:30:28.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-1995 --namespace=crd-publish-openapi-1995 apply -f -'
  Aug  4 20:30:28.420: INFO: stderr: ""
  Aug  4 20:30:28.420: INFO: stdout: "e2e-test-crd-publish-openapi-2860-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Aug  4 20:30:28.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-1995 --namespace=crd-publish-openapi-1995 delete e2e-test-crd-publish-openapi-2860-crds test-cr'
  Aug  4 20:30:28.479: INFO: stderr: ""
  Aug  4 20:30:28.479: INFO: stdout: "e2e-test-crd-publish-openapi-2860-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/04/23 20:30:28.479
  Aug  4 20:30:28.479: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-1995 explain e2e-test-crd-publish-openapi-2860-crds'
  Aug  4 20:30:28.642: INFO: stderr: ""
  Aug  4 20:30:28.643: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-2860-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0804 20:30:28.909979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:30:29.882: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1995" for this suite. @ 08/04/23 20:30:29.888
• [3.566 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 08/04/23 20:30:29.892
  Aug  4 20:30:29.892: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 20:30:29.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:30:29.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:30:29.905
  STEP: Creating configMap with name cm-test-opt-del-db9685ba-5bcb-48e1-99b7-9bc3dcda015c @ 08/04/23 20:30:29.909
  E0804 20:30:29.910154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating configMap with name cm-test-opt-upd-4157495b-2e9d-4b72-876e-03fc31734bfe @ 08/04/23 20:30:29.911
  STEP: Creating the pod @ 08/04/23 20:30:29.915
  E0804 20:30:30.911129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:31.912096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-db9685ba-5bcb-48e1-99b7-9bc3dcda015c @ 08/04/23 20:30:31.944
  STEP: Updating configmap cm-test-opt-upd-4157495b-2e9d-4b72-876e-03fc31734bfe @ 08/04/23 20:30:31.948
  STEP: Creating configMap with name cm-test-opt-create-6868f424-f38b-4309-8d8b-4c8a47098021 @ 08/04/23 20:30:31.951
  STEP: waiting to observe update in volume @ 08/04/23 20:30:31.954
  E0804 20:30:32.913158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:33.913399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:34.913918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:35.914124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:36.915129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:37.916029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:38.916699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:39.916928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:40.917703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:41.918111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:42.918919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:43.918997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:44.920054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:45.920256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:46.920996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:47.921215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:48.922299      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:49.922542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:50.922962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:51.922993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:52.924074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:53.924309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:54.925018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:55.925987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:56.926020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:57.926261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:58.926438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:30:59.926936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:00.926991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:01.928067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:02.928705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:03.928791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:04.929116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:05.930100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:06.930342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:07.930580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:08.930971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:09.931243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:10.932056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:11.932460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:12.933162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:13.933963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:14.934778      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:15.935019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:16.936060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:17.936324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:18.936683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:19.936916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:20.937907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:21.938360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:22.939202      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:23.940076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:24.940895      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:25.941133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:26.942157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:27.942288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:28.943294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:29.944063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:30.944668      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:31.945117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:32.946087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:33.946408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:34.946762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:35.946967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:31:36.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3483" for this suite. @ 08/04/23 20:31:36.236
• [66.347 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 08/04/23 20:31:36.241
  Aug  4 20:31:36.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename containers @ 08/04/23 20:31:36.242
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:31:36.252
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:31:36.254
  STEP: Creating a pod to test override command @ 08/04/23 20:31:36.256
  E0804 20:31:36.948058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:37.948200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:38.949263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:39.949696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:31:40.272
  Aug  4 20:31:40.274: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-f2e45ed5-49f3-4303-8921-ecab3cf4bf93 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:31:40.279
  Aug  4 20:31:40.288: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-5920" for this suite. @ 08/04/23 20:31:40.29
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 08/04/23 20:31:40.295
  Aug  4 20:31:40.295: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename subpath @ 08/04/23 20:31:40.296
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:31:40.302
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:31:40.304
  STEP: Setting up data @ 08/04/23 20:31:40.306
  STEP: Creating pod pod-subpath-test-configmap-zf7p @ 08/04/23 20:31:40.311
  STEP: Creating a pod to test atomic-volume-subpath @ 08/04/23 20:31:40.311
  E0804 20:31:40.949897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:41.950271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:42.951149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:43.952044      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:44.952638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:45.952943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:46.953401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:47.953741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:48.953994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:49.954229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:50.955247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:51.955782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:52.956213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:53.956408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:54.957189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:55.957386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:56.958274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:57.958510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:58.958985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:31:59.960028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:00.960480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:01.960801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:02.961840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:03.962073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:32:04.353
  Aug  4 20:32:04.355: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-configmap-zf7p container test-container-subpath-configmap-zf7p: <nil>
  STEP: delete the pod @ 08/04/23 20:32:04.362
  STEP: Deleting pod pod-subpath-test-configmap-zf7p @ 08/04/23 20:32:04.372
  Aug  4 20:32:04.372: INFO: Deleting pod "pod-subpath-test-configmap-zf7p" in namespace "subpath-5950"
  Aug  4 20:32:04.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-5950" for this suite. @ 08/04/23 20:32:04.375
• [24.085 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 08/04/23 20:32:04.382
  Aug  4 20:32:04.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/04/23 20:32:04.383
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:32:04.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:32:04.393
  Aug  4 20:32:04.394: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:32:04.962908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:32:05.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-6903" for this suite. @ 08/04/23 20:32:05.413
• [1.037 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 08/04/23 20:32:05.42
  Aug  4 20:32:05.420: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:32:05.42
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:32:05.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:32:05.43
  STEP: creating service nodeport-test with type=NodePort in namespace services-5742 @ 08/04/23 20:32:05.432
  STEP: creating replication controller nodeport-test in namespace services-5742 @ 08/04/23 20:32:05.444
  I0804 20:32:05.448054      22 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-5742, replica count: 2
  E0804 20:32:05.963013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:06.963158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:07.964023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:32:08.498712      22 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 20:32:08.498: INFO: Creating new exec pod
  E0804 20:32:08.964987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:09.965215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:10.965247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:32:11.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-5742 exec execpodpd47x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Aug  4 20:32:11.634: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Aug  4 20:32:11.634: INFO: stdout: "nodeport-test-kvfjn"
  Aug  4 20:32:11.634: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-5742 exec execpodpd47x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.127.17 80'
  Aug  4 20:32:11.770: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.127.17 80\nConnection to 10.107.127.17 80 port [tcp/http] succeeded!\n"
  Aug  4 20:32:11.770: INFO: stdout: "nodeport-test-t9wv7"
  Aug  4 20:32:11.770: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-5742 exec execpodpd47x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.2 30913'
  Aug  4 20:32:11.910: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.2 30913\nConnection to 192.168.49.2 30913 port [tcp/*] succeeded!\n"
  Aug  4 20:32:11.910: INFO: stdout: "nodeport-test-t9wv7"
  Aug  4 20:32:11.910: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-5742 exec execpodpd47x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.3 30913'
  E0804 20:32:11.966215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:32:12.040: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.3 30913\nConnection to 192.168.49.3 30913 port [tcp/*] succeeded!\n"
  Aug  4 20:32:12.040: INFO: stdout: ""
  E0804 20:32:12.966263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:32:13.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-5742 exec execpodpd47x -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.3 30913'
  Aug  4 20:32:13.165: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.3 30913\nConnection to 192.168.49.3 30913 port [tcp/*] succeeded!\n"
  Aug  4 20:32:13.165: INFO: stdout: "nodeport-test-kvfjn"
  Aug  4 20:32:13.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5742" for this suite. @ 08/04/23 20:32:13.167
• [7.752 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 08/04/23 20:32:13.172
  Aug  4 20:32:13.172: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/04/23 20:32:13.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:32:13.184
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:32:13.186
  STEP: fetching the /apis discovery document @ 08/04/23 20:32:13.188
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 08/04/23 20:32:13.189
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 08/04/23 20:32:13.189
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 08/04/23 20:32:13.189
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 08/04/23 20:32:13.19
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 08/04/23 20:32:13.19
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 08/04/23 20:32:13.19
  Aug  4 20:32:13.190: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1294" for this suite. @ 08/04/23 20:32:13.193
• [0.024 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 08/04/23 20:32:13.197
  Aug  4 20:32:13.197: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-watch @ 08/04/23 20:32:13.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:32:13.206
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:32:13.208
  Aug  4 20:32:13.210: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:32:13.966718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:14.966996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating first CR  @ 08/04/23 20:32:15.739
  Aug  4 20:32:15.742: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-04T20:32:15Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-04T20:32:15Z]] name:name1 resourceVersion:13464 uid:1f87e591-d2c2-40e2-a94b-c0a5fcfb94f3] num:map[num1:9223372036854775807 num2:1000000]]}
  E0804 20:32:15.968007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:16.968462      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:17.968702      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:18.968967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:19.969176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:20.969426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:21.969705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:22.969859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:23.970537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:24.970736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating second CR @ 08/04/23 20:32:25.743
  Aug  4 20:32:25.747: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-04T20:32:25Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-04T20:32:25Z]] name:name2 resourceVersion:13516 uid:e186f105-b816-4761-b733-85a3cd73d1bd] num:map[num1:9223372036854775807 num2:1000000]]}
  E0804 20:32:25.971085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:26.972060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:27.972293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:28.972490      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:29.972998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:30.973380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:31.973444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:32.973671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:33.973856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:34.974081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying first CR @ 08/04/23 20:32:35.748
  Aug  4 20:32:35.752: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-04T20:32:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-04T20:32:35Z]] name:name1 resourceVersion:13526 uid:1f87e591-d2c2-40e2-a94b-c0a5fcfb94f3] num:map[num1:9223372036854775807 num2:1000000]]}
  E0804 20:32:35.974987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:36.976052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:37.976257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:38.976478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:39.976703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:40.976894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:41.977018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:42.977253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:43.977555      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:44.977772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Modifying second CR @ 08/04/23 20:32:45.753
  Aug  4 20:32:45.758: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-04T20:32:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-04T20:32:45Z]] name:name2 resourceVersion:13536 uid:e186f105-b816-4761-b733-85a3cd73d1bd] num:map[num1:9223372036854775807 num2:1000000]]}
  E0804 20:32:45.978730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:46.978968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:47.980084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:48.980294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:49.980488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:50.980753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:51.981147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:52.981358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:53.981546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:54.981761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting first CR @ 08/04/23 20:32:55.758
  Aug  4 20:32:55.763: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-04T20:32:15Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-04T20:32:35Z]] name:name1 resourceVersion:13546 uid:1f87e591-d2c2-40e2-a94b-c0a5fcfb94f3] num:map[num1:9223372036854775807 num2:1000000]]}
  E0804 20:32:55.982737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:56.982990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:57.984053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:58.984275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:32:59.984463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:00.984684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:01.984758      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:02.984991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:03.985192      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:04.986275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting second CR @ 08/04/23 20:33:05.764
  Aug  4 20:33:05.769: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-08-04T20:32:25Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-08-04T20:32:45Z]] name:name2 resourceVersion:13556 uid:e186f105-b816-4761-b733-85a3cd73d1bd] num:map[num1:9223372036854775807 num2:1000000]]}
  E0804 20:33:05.986817      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:06.987561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:07.987681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:08.988098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:09.988375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:10.988597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:11.989042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:12.989247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:13.989428      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:14.989720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:15.990175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:16.280: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-8081" for this suite. @ 08/04/23 20:33:16.283
• [63.092 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 08/04/23 20:33:16.289
  Aug  4 20:33:16.289: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:33:16.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:16.297
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:16.299
  STEP: creating a replication controller @ 08/04/23 20:33:16.301
  Aug  4 20:33:16.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 create -f -'
  Aug  4 20:33:16.939: INFO: stderr: ""
  Aug  4 20:33:16.939: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/04/23 20:33:16.939
  Aug  4 20:33:16.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  E0804 20:33:16.990668      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:17.003: INFO: stderr: ""
  Aug  4 20:33:17.003: INFO: stdout: "update-demo-nautilus-79glh update-demo-nautilus-g5fb2 "
  Aug  4 20:33:17.003: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-79glh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:33:17.057: INFO: stderr: ""
  Aug  4 20:33:17.057: INFO: stdout: ""
  Aug  4 20:33:17.057: INFO: update-demo-nautilus-79glh is created but not running
  E0804 20:33:17.991100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:18.991264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:19.992060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:20.992319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:21.992655      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:22.058: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  4 20:33:22.116: INFO: stderr: ""
  Aug  4 20:33:22.116: INFO: stdout: "update-demo-nautilus-79glh update-demo-nautilus-g5fb2 "
  Aug  4 20:33:22.116: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-79glh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:33:22.170: INFO: stderr: ""
  Aug  4 20:33:22.170: INFO: stdout: "true"
  Aug  4 20:33:22.170: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-79glh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  4 20:33:22.225: INFO: stderr: ""
  Aug  4 20:33:22.225: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  4 20:33:22.225: INFO: validating pod update-demo-nautilus-79glh
  Aug  4 20:33:22.228: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  4 20:33:22.228: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  4 20:33:22.228: INFO: update-demo-nautilus-79glh is verified up and running
  Aug  4 20:33:22.228: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-g5fb2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:33:22.284: INFO: stderr: ""
  Aug  4 20:33:22.284: INFO: stdout: "true"
  Aug  4 20:33:22.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-g5fb2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  4 20:33:22.338: INFO: stderr: ""
  Aug  4 20:33:22.338: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  4 20:33:22.338: INFO: validating pod update-demo-nautilus-g5fb2
  Aug  4 20:33:22.341: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  4 20:33:22.341: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  4 20:33:22.341: INFO: update-demo-nautilus-g5fb2 is verified up and running
  STEP: scaling down the replication controller @ 08/04/23 20:33:22.341
  Aug  4 20:33:22.342: INFO: scanned /root for discovery docs: <nil>
  Aug  4 20:33:22.342: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  E0804 20:33:22.993239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:23.411: INFO: stderr: ""
  Aug  4 20:33:23.411: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/04/23 20:33:23.411
  Aug  4 20:33:23.411: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  4 20:33:23.468: INFO: stderr: ""
  Aug  4 20:33:23.468: INFO: stdout: "update-demo-nautilus-79glh update-demo-nautilus-g5fb2 "
  STEP: Replicas for name=update-demo: expected=1 actual=2 @ 08/04/23 20:33:23.468
  E0804 20:33:23.994172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:24.994408      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:25.995404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:26.995998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:27.996196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:28.468: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  4 20:33:28.522: INFO: stderr: ""
  Aug  4 20:33:28.522: INFO: stdout: "update-demo-nautilus-g5fb2 "
  Aug  4 20:33:28.522: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-g5fb2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:33:28.577: INFO: stderr: ""
  Aug  4 20:33:28.577: INFO: stdout: "true"
  Aug  4 20:33:28.577: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-g5fb2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  4 20:33:28.632: INFO: stderr: ""
  Aug  4 20:33:28.632: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  4 20:33:28.632: INFO: validating pod update-demo-nautilus-g5fb2
  Aug  4 20:33:28.634: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  4 20:33:28.634: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  4 20:33:28.634: INFO: update-demo-nautilus-g5fb2 is verified up and running
  STEP: scaling up the replication controller @ 08/04/23 20:33:28.634
  Aug  4 20:33:28.635: INFO: scanned /root for discovery docs: <nil>
  Aug  4 20:33:28.635: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  E0804 20:33:28.997337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:29.699: INFO: stderr: ""
  Aug  4 20:33:29.699: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 08/04/23 20:33:29.699
  Aug  4 20:33:29.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Aug  4 20:33:29.755: INFO: stderr: ""
  Aug  4 20:33:29.755: INFO: stdout: "update-demo-nautilus-9f9js update-demo-nautilus-g5fb2 "
  Aug  4 20:33:29.755: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-9f9js -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:33:29.809: INFO: stderr: ""
  Aug  4 20:33:29.809: INFO: stdout: "true"
  Aug  4 20:33:29.809: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-9f9js -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  4 20:33:29.862: INFO: stderr: ""
  Aug  4 20:33:29.862: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  4 20:33:29.862: INFO: validating pod update-demo-nautilus-9f9js
  Aug  4 20:33:29.864: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  4 20:33:29.864: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  4 20:33:29.864: INFO: update-demo-nautilus-9f9js is verified up and running
  Aug  4 20:33:29.864: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-g5fb2 -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Aug  4 20:33:29.918: INFO: stderr: ""
  Aug  4 20:33:29.918: INFO: stdout: "true"
  Aug  4 20:33:29.918: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods update-demo-nautilus-g5fb2 -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Aug  4 20:33:29.973: INFO: stderr: ""
  Aug  4 20:33:29.973: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Aug  4 20:33:29.973: INFO: validating pod update-demo-nautilus-g5fb2
  Aug  4 20:33:29.975: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Aug  4 20:33:29.975: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Aug  4 20:33:29.975: INFO: update-demo-nautilus-g5fb2 is verified up and running
  STEP: using delete to clean up resources @ 08/04/23 20:33:29.975
  Aug  4 20:33:29.975: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 delete --grace-period=0 --force -f -'
  E0804 20:33:29.998144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:30.032: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:33:30.032: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Aug  4 20:33:30.032: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get rc,svc -l name=update-demo --no-headers'
  Aug  4 20:33:30.097: INFO: stderr: "No resources found in kubectl-7802 namespace.\n"
  Aug  4 20:33:30.097: INFO: stdout: ""
  Aug  4 20:33:30.097: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7802 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug  4 20:33:30.157: INFO: stderr: ""
  Aug  4 20:33:30.157: INFO: stdout: ""
  Aug  4 20:33:30.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7802" for this suite. @ 08/04/23 20:33:30.16
• [13.876 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 08/04/23 20:33:30.165
  Aug  4 20:33:30.165: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename watch @ 08/04/23 20:33:30.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:30.175
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:30.177
  STEP: getting a starting resourceVersion @ 08/04/23 20:33:30.179
  STEP: starting a background goroutine to produce watch events @ 08/04/23 20:33:30.18
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 08/04/23 20:33:30.18
  E0804 20:33:30.998204      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:31.998572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:32.969: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0804 20:33:32.998832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "watch-7116" for this suite. @ 08/04/23 20:33:33.018
• [2.905 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 08/04/23 20:33:33.07
  Aug  4 20:33:33.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename certificates @ 08/04/23 20:33:33.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:33.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:33.081
  STEP: getting /apis @ 08/04/23 20:33:33.424
  STEP: getting /apis/certificates.k8s.io @ 08/04/23 20:33:33.428
  STEP: getting /apis/certificates.k8s.io/v1 @ 08/04/23 20:33:33.429
  STEP: creating @ 08/04/23 20:33:33.429
  STEP: getting @ 08/04/23 20:33:33.44
  STEP: listing @ 08/04/23 20:33:33.442
  STEP: watching @ 08/04/23 20:33:33.443
  Aug  4 20:33:33.443: INFO: starting watch
  STEP: patching @ 08/04/23 20:33:33.444
  STEP: updating @ 08/04/23 20:33:33.449
  Aug  4 20:33:33.452: INFO: waiting for watch events with expected annotations
  Aug  4 20:33:33.452: INFO: saw patched and updated annotations
  STEP: getting /approval @ 08/04/23 20:33:33.452
  STEP: patching /approval @ 08/04/23 20:33:33.454
  STEP: updating /approval @ 08/04/23 20:33:33.459
  STEP: getting /status @ 08/04/23 20:33:33.464
  STEP: patching /status @ 08/04/23 20:33:33.465
  STEP: updating /status @ 08/04/23 20:33:33.471
  STEP: deleting @ 08/04/23 20:33:33.476
  STEP: deleting a collection @ 08/04/23 20:33:33.483
  Aug  4 20:33:33.491: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-5677" for this suite. @ 08/04/23 20:33:33.493
• [0.427 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 08/04/23 20:33:33.498
  Aug  4 20:33:33.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename field-validation @ 08/04/23 20:33:33.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:33.507
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:33.508
  STEP: apply creating a deployment @ 08/04/23 20:33:33.51
  Aug  4 20:33:33.511: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9252" for this suite. @ 08/04/23 20:33:33.518
• [0.024 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 08/04/23 20:33:33.523
  Aug  4 20:33:33.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 20:33:33.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:33.53
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:33.532
  STEP: Counting existing ResourceQuota @ 08/04/23 20:33:33.534
  E0804 20:33:33.999866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:35.000086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:36.000502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:37.000540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:38.000966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/04/23 20:33:38.536
  STEP: Ensuring resource quota status is calculated @ 08/04/23 20:33:38.539
  E0804 20:33:39.001291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:40.001542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 08/04/23 20:33:40.542
  STEP: Ensuring ResourceQuota status captures the pod usage @ 08/04/23 20:33:40.553
  E0804 20:33:41.002422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:42.003014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 08/04/23 20:33:42.556
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 08/04/23 20:33:42.558
  STEP: Ensuring a pod cannot update its resource requirements @ 08/04/23 20:33:42.56
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 08/04/23 20:33:42.562
  E0804 20:33:43.003484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:44.003543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/04/23 20:33:44.565
  STEP: Ensuring resource quota status released the pod usage @ 08/04/23 20:33:44.572
  E0804 20:33:45.004623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:46.004919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:33:46.575: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-246" for this suite. @ 08/04/23 20:33:46.577
• [13.058 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 08/04/23 20:33:46.581
  Aug  4 20:33:46.581: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename var-expansion @ 08/04/23 20:33:46.582
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:46.591
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:46.592
  STEP: Creating a pod to test substitution in container's command @ 08/04/23 20:33:46.594
  E0804 20:33:47.005039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:48.005521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:49.005641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:50.005875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:33:50.607
  Aug  4 20:33:50.609: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-d6cca681-ba4a-4ab0-be3c-4480009e57e0 container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 20:33:50.622
  Aug  4 20:33:50.630: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9376" for this suite. @ 08/04/23 20:33:50.631
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 08/04/23 20:33:50.636
  Aug  4 20:33:50.636: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename svcaccounts @ 08/04/23 20:33:50.636
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:50.644
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:50.646
  STEP: Creating ServiceAccount "e2e-sa-szs7m"  @ 08/04/23 20:33:50.648
  Aug  4 20:33:50.650: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-szs7m"  @ 08/04/23 20:33:50.65
  Aug  4 20:33:50.654: INFO: AutomountServiceAccountToken: true
  Aug  4 20:33:50.654: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8588" for this suite. @ 08/04/23 20:33:50.656
• [0.024 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 08/04/23 20:33:50.664
  Aug  4 20:33:50.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:33:50.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:50.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:50.673
  STEP: Creating a pod to test emptydir volume type on node default medium @ 08/04/23 20:33:50.675
  E0804 20:33:51.006872      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:52.006945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:53.007630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:54.008050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:33:54.688
  Aug  4 20:33:54.690: INFO: Trying to get logs from node k8sconformance-m02 pod pod-3b32799a-f859-442e-9f49-15ab132940c9 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:33:54.696
  Aug  4 20:33:54.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4942" for this suite. @ 08/04/23 20:33:54.705
• [4.045 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 08/04/23 20:33:54.71
  Aug  4 20:33:54.710: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:33:54.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:54.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:54.72
  STEP: Creating Pod @ 08/04/23 20:33:54.722
  E0804 20:33:55.008144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:56.009074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 08/04/23 20:33:56.733
  Aug  4 20:33:56.733: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-5504 PodName:pod-sharedvolume-f71cc430-61f4-4c90-bbab-395d05336407 ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:33:56.733: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:33:56.733: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:33:56.733: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/emptydir-5504/pods/pod-sharedvolume-f71cc430-61f4-4c90-bbab-395d05336407/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Aug  4 20:33:56.814: INFO: Exec stderr: ""
  Aug  4 20:33:56.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-5504" for this suite. @ 08/04/23 20:33:56.817
• [2.111 seconds]
------------------------------
S
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 08/04/23 20:33:56.821
  Aug  4 20:33:56.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename podtemplate @ 08/04/23 20:33:56.822
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:56.832
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:56.835
  Aug  4 20:33:56.855: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-6855" for this suite. @ 08/04/23 20:33:56.857
• [0.040 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 08/04/23 20:33:56.862
  Aug  4 20:33:56.862: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:33:56.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:33:56.872
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:33:56.874
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 08/04/23 20:33:56.876
  E0804 20:33:57.009271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:58.009635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:33:59.010241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:00.010466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:34:00.891
  Aug  4 20:34:00.893: INFO: Trying to get logs from node k8sconformance-m02 pod pod-7b62a6ae-9c3f-4af4-bdbf-3d2d66ba8daf container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:34:00.899
  Aug  4 20:34:00.906: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1390" for this suite. @ 08/04/23 20:34:00.908
• [4.051 seconds]
------------------------------
SSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 08/04/23 20:34:00.913
  Aug  4 20:34:00.913: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename svcaccounts @ 08/04/23 20:34:00.914
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:34:00.922
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:34:00.924
  STEP: creating a ServiceAccount @ 08/04/23 20:34:00.926
  STEP: watching for the ServiceAccount to be added @ 08/04/23 20:34:00.93
  STEP: patching the ServiceAccount @ 08/04/23 20:34:00.932
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 08/04/23 20:34:00.937
  STEP: deleting the ServiceAccount @ 08/04/23 20:34:00.939
  Aug  4 20:34:00.945: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-5621" for this suite. @ 08/04/23 20:34:00.947
• [0.037 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 08/04/23 20:34:00.952
  Aug  4 20:34:00.952: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename svc-latency @ 08/04/23 20:34:00.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:34:00.96
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:34:00.962
  Aug  4 20:34:00.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-4525 @ 08/04/23 20:34:00.965
  I0804 20:34:00.968821      22 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-4525, replica count: 1
  E0804 20:34:01.011551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:02.012189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:34:02.020509      22 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  E0804 20:34:03.012748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:34:03.021054      22 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 20:34:03.131: INFO: Created: latency-svc-5ls95
  Aug  4 20:34:03.135: INFO: Got endpoints: latency-svc-5ls95 [14.5891ms]
  Aug  4 20:34:03.149: INFO: Created: latency-svc-jthzc
  Aug  4 20:34:03.153: INFO: Got endpoints: latency-svc-jthzc [17.752402ms]
  Aug  4 20:34:03.154: INFO: Created: latency-svc-cwsmd
  Aug  4 20:34:03.160: INFO: Got endpoints: latency-svc-cwsmd [24.521377ms]
  Aug  4 20:34:03.161: INFO: Created: latency-svc-tw4kd
  Aug  4 20:34:03.167: INFO: Got endpoints: latency-svc-tw4kd [31.349074ms]
  Aug  4 20:34:03.168: INFO: Created: latency-svc-77mpk
  Aug  4 20:34:03.174: INFO: Got endpoints: latency-svc-77mpk [38.602016ms]
  Aug  4 20:34:03.175: INFO: Created: latency-svc-58lc7
  Aug  4 20:34:03.177: INFO: Got endpoints: latency-svc-58lc7 [41.920078ms]
  Aug  4 20:34:03.182: INFO: Created: latency-svc-jkwxv
  Aug  4 20:34:03.187: INFO: Got endpoints: latency-svc-jkwxv [51.884159ms]
  Aug  4 20:34:03.204: INFO: Created: latency-svc-swq7z
  Aug  4 20:34:03.209: INFO: Got endpoints: latency-svc-swq7z [73.094549ms]
  Aug  4 20:34:03.210: INFO: Created: latency-svc-ctkt6
  Aug  4 20:34:03.221: INFO: Got endpoints: latency-svc-ctkt6 [85.486177ms]
  Aug  4 20:34:03.223: INFO: Created: latency-svc-fh7dj
  Aug  4 20:34:03.226: INFO: Got endpoints: latency-svc-fh7dj [90.835462ms]
  Aug  4 20:34:03.231: INFO: Created: latency-svc-dd7kf
  Aug  4 20:34:03.237: INFO: Got endpoints: latency-svc-dd7kf [101.300596ms]
  Aug  4 20:34:03.238: INFO: Created: latency-svc-pbhx5
  Aug  4 20:34:03.243: INFO: Got endpoints: latency-svc-pbhx5 [107.43864ms]
  Aug  4 20:34:03.245: INFO: Created: latency-svc-dcssq
  Aug  4 20:34:03.249: INFO: Got endpoints: latency-svc-dcssq [113.731216ms]
  Aug  4 20:34:03.254: INFO: Created: latency-svc-82pv8
  Aug  4 20:34:03.258: INFO: Got endpoints: latency-svc-82pv8 [122.603045ms]
  Aug  4 20:34:03.260: INFO: Created: latency-svc-fszzf
  Aug  4 20:34:03.265: INFO: Got endpoints: latency-svc-fszzf [129.997682ms]
  Aug  4 20:34:03.270: INFO: Created: latency-svc-6jcxz
  Aug  4 20:34:03.306: INFO: Created: latency-svc-s2b57
  Aug  4 20:34:03.307: INFO: Got endpoints: latency-svc-6jcxz [172.12508ms]
  Aug  4 20:34:03.311: INFO: Got endpoints: latency-svc-s2b57 [157.327452ms]
  Aug  4 20:34:03.312: INFO: Created: latency-svc-tmbhs
  Aug  4 20:34:03.317: INFO: Got endpoints: latency-svc-tmbhs [157.330662ms]
  Aug  4 20:34:03.318: INFO: Created: latency-svc-cs7lr
  Aug  4 20:34:03.326: INFO: Got endpoints: latency-svc-cs7lr [158.624679ms]
  Aug  4 20:34:03.328: INFO: Created: latency-svc-vjbpz
  Aug  4 20:34:03.333: INFO: Got endpoints: latency-svc-vjbpz [158.33465ms]
  Aug  4 20:34:03.334: INFO: Created: latency-svc-bvlzz
  Aug  4 20:34:03.339: INFO: Got endpoints: latency-svc-bvlzz [161.452639ms]
  Aug  4 20:34:03.344: INFO: Created: latency-svc-k2d8j
  Aug  4 20:34:03.348: INFO: Got endpoints: latency-svc-k2d8j [160.985129ms]
  Aug  4 20:34:03.350: INFO: Created: latency-svc-8k9l8
  Aug  4 20:34:03.353: INFO: Got endpoints: latency-svc-8k9l8 [144.235953ms]
  Aug  4 20:34:03.359: INFO: Created: latency-svc-jgxnr
  Aug  4 20:34:03.364: INFO: Got endpoints: latency-svc-jgxnr [143.404325ms]
  Aug  4 20:34:03.405: INFO: Created: latency-svc-crv7b
  Aug  4 20:34:03.411: INFO: Got endpoints: latency-svc-crv7b [184.051673ms]
  Aug  4 20:34:03.416: INFO: Created: latency-svc-6z9n2
  Aug  4 20:34:03.421: INFO: Got endpoints: latency-svc-6z9n2 [183.984213ms]
  Aug  4 20:34:03.423: INFO: Created: latency-svc-ksz2k
  Aug  4 20:34:03.430: INFO: Got endpoints: latency-svc-ksz2k [186.827151ms]
  Aug  4 20:34:03.433: INFO: Created: latency-svc-9jxhs
  Aug  4 20:34:03.440: INFO: Got endpoints: latency-svc-9jxhs [190.384121ms]
  Aug  4 20:34:03.442: INFO: Created: latency-svc-gbmmf
  Aug  4 20:34:03.448: INFO: Got endpoints: latency-svc-gbmmf [189.371331ms]
  Aug  4 20:34:03.450: INFO: Created: latency-svc-z6mb6
  Aug  4 20:34:03.454: INFO: Got endpoints: latency-svc-z6mb6 [188.221062ms]
  Aug  4 20:34:03.464: INFO: Created: latency-svc-wt2p2
  Aug  4 20:34:03.469: INFO: Got endpoints: latency-svc-wt2p2 [161.166227ms]
  Aug  4 20:34:03.470: INFO: Created: latency-svc-vm6dx
  Aug  4 20:34:03.507: INFO: Got endpoints: latency-svc-vm6dx [196.720768ms]
  Aug  4 20:34:03.511: INFO: Created: latency-svc-knpcd
  Aug  4 20:34:03.516: INFO: Got endpoints: latency-svc-knpcd [198.696208ms]
  Aug  4 20:34:03.519: INFO: Created: latency-svc-grtnd
  Aug  4 20:34:03.523: INFO: Got endpoints: latency-svc-grtnd [197.155079ms]
  Aug  4 20:34:03.528: INFO: Created: latency-svc-5zqkx
  Aug  4 20:34:03.533: INFO: Got endpoints: latency-svc-5zqkx [200.907792ms]
  Aug  4 20:34:03.536: INFO: Created: latency-svc-mwnvj
  Aug  4 20:34:03.541: INFO: Got endpoints: latency-svc-mwnvj [202.107221ms]
  Aug  4 20:34:03.546: INFO: Created: latency-svc-9wrgz
  Aug  4 20:34:03.552: INFO: Got endpoints: latency-svc-9wrgz [203.29952ms]
  Aug  4 20:34:03.553: INFO: Created: latency-svc-nlrrb
  Aug  4 20:34:03.558: INFO: Got endpoints: latency-svc-nlrrb [204.640299ms]
  Aug  4 20:34:03.561: INFO: Created: latency-svc-pzxsv
  Aug  4 20:34:03.568: INFO: Created: latency-svc-6w8s8
  Aug  4 20:34:03.574: INFO: Created: latency-svc-scxm4
  Aug  4 20:34:03.607: INFO: Created: latency-svc-8lmkd
  Aug  4 20:34:03.608: INFO: Got endpoints: latency-svc-pzxsv [243.177201ms]
  Aug  4 20:34:03.613: INFO: Created: latency-svc-4n5sb
  Aug  4 20:34:03.620: INFO: Created: latency-svc-fcdbq
  Aug  4 20:34:03.628: INFO: Created: latency-svc-zlmzr
  Aug  4 20:34:03.652: INFO: Got endpoints: latency-svc-6w8s8 [241.252524ms]
  Aug  4 20:34:03.652: INFO: Created: latency-svc-92kcw
  Aug  4 20:34:03.716: INFO: Created: latency-svc-r4t7q
  Aug  4 20:34:03.717: INFO: Got endpoints: latency-svc-scxm4 [296.345343ms]
  Aug  4 20:34:03.724: INFO: Created: latency-svc-sqh9n
  Aug  4 20:34:03.730: INFO: Created: latency-svc-nfrk9
  Aug  4 20:34:03.735: INFO: Got endpoints: latency-svc-8lmkd [305.245301ms]
  Aug  4 20:34:03.736: INFO: Created: latency-svc-gxljr
  Aug  4 20:34:03.743: INFO: Created: latency-svc-5qllr
  Aug  4 20:34:03.749: INFO: Created: latency-svc-stt96
  Aug  4 20:34:03.759: INFO: Created: latency-svc-xndpg
  Aug  4 20:34:03.808: INFO: Got endpoints: latency-svc-4n5sb [368.242591ms]
  Aug  4 20:34:03.811: INFO: Created: latency-svc-psj2p
  Aug  4 20:34:03.816: INFO: Created: latency-svc-wd8mm
  Aug  4 20:34:03.823: INFO: Created: latency-svc-wwspx
  Aug  4 20:34:03.827: INFO: Created: latency-svc-rm77f
  Aug  4 20:34:03.833: INFO: Created: latency-svc-qjsh4
  Aug  4 20:34:03.834: INFO: Got endpoints: latency-svc-fcdbq [386.61212ms]
  Aug  4 20:34:03.846: INFO: Created: latency-svc-7s8qh
  Aug  4 20:34:03.885: INFO: Got endpoints: latency-svc-zlmzr [431.413265ms]
  Aug  4 20:34:03.895: INFO: Created: latency-svc-mbbr8
  Aug  4 20:34:03.936: INFO: Got endpoints: latency-svc-92kcw [467.120004ms]
  Aug  4 20:34:03.948: INFO: Created: latency-svc-7thp2
  Aug  4 20:34:03.985: INFO: Got endpoints: latency-svc-r4t7q [478.067473ms]
  Aug  4 20:34:03.996: INFO: Created: latency-svc-9q4n4
  E0804 20:34:04.013536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:04.036: INFO: Got endpoints: latency-svc-sqh9n [520.256171ms]
  Aug  4 20:34:04.049: INFO: Created: latency-svc-bjs7r
  Aug  4 20:34:04.084: INFO: Got endpoints: latency-svc-nfrk9 [561.6013ms]
  Aug  4 20:34:04.095: INFO: Created: latency-svc-dt282
  Aug  4 20:34:04.135: INFO: Got endpoints: latency-svc-gxljr [601.585504ms]
  Aug  4 20:34:04.145: INFO: Created: latency-svc-h2q9r
  Aug  4 20:34:04.187: INFO: Got endpoints: latency-svc-5qllr [645.498237ms]
  Aug  4 20:34:04.196: INFO: Created: latency-svc-cwkbm
  Aug  4 20:34:04.236: INFO: Got endpoints: latency-svc-stt96 [684.40296ms]
  Aug  4 20:34:04.246: INFO: Created: latency-svc-89bld
  Aug  4 20:34:04.285: INFO: Got endpoints: latency-svc-xndpg [727.465395ms]
  Aug  4 20:34:04.294: INFO: Created: latency-svc-f2zhz
  Aug  4 20:34:04.337: INFO: Got endpoints: latency-svc-psj2p [729.530449ms]
  Aug  4 20:34:04.347: INFO: Created: latency-svc-hhwdx
  Aug  4 20:34:04.390: INFO: Got endpoints: latency-svc-wd8mm [737.661074ms]
  Aug  4 20:34:04.400: INFO: Created: latency-svc-bjwfd
  Aug  4 20:34:04.435: INFO: Got endpoints: latency-svc-wwspx [717.546069ms]
  Aug  4 20:34:04.444: INFO: Created: latency-svc-jfvfq
  Aug  4 20:34:04.487: INFO: Got endpoints: latency-svc-rm77f [751.686473ms]
  Aug  4 20:34:04.501: INFO: Created: latency-svc-44bvg
  Aug  4 20:34:04.536: INFO: Got endpoints: latency-svc-qjsh4 [728.057976ms]
  Aug  4 20:34:04.546: INFO: Created: latency-svc-5qs2d
  Aug  4 20:34:04.586: INFO: Got endpoints: latency-svc-7s8qh [751.260279ms]
  Aug  4 20:34:04.595: INFO: Created: latency-svc-m22jd
  Aug  4 20:34:04.637: INFO: Got endpoints: latency-svc-mbbr8 [751.499082ms]
  Aug  4 20:34:04.647: INFO: Created: latency-svc-6vgxc
  Aug  4 20:34:04.686: INFO: Got endpoints: latency-svc-7thp2 [749.624262ms]
  Aug  4 20:34:04.695: INFO: Created: latency-svc-qw25k
  Aug  4 20:34:04.734: INFO: Got endpoints: latency-svc-9q4n4 [749.013105ms]
  Aug  4 20:34:04.744: INFO: Created: latency-svc-lbkdj
  Aug  4 20:34:04.787: INFO: Got endpoints: latency-svc-bjs7r [750.860594ms]
  Aug  4 20:34:04.799: INFO: Created: latency-svc-8dmrb
  Aug  4 20:34:04.835: INFO: Got endpoints: latency-svc-dt282 [750.507672ms]
  Aug  4 20:34:04.846: INFO: Created: latency-svc-gtvxb
  Aug  4 20:34:04.889: INFO: Got endpoints: latency-svc-h2q9r [753.699706ms]
  Aug  4 20:34:04.899: INFO: Created: latency-svc-5t9fg
  Aug  4 20:34:04.938: INFO: Got endpoints: latency-svc-cwkbm [750.856029ms]
  Aug  4 20:34:04.948: INFO: Created: latency-svc-v7xd6
  Aug  4 20:34:04.986: INFO: Got endpoints: latency-svc-89bld [749.448555ms]
  Aug  4 20:34:04.996: INFO: Created: latency-svc-vwlxf
  E0804 20:34:05.014548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:05.037: INFO: Got endpoints: latency-svc-f2zhz [751.905896ms]
  Aug  4 20:34:05.047: INFO: Created: latency-svc-r9gd9
  Aug  4 20:34:05.086: INFO: Got endpoints: latency-svc-hhwdx [748.886083ms]
  Aug  4 20:34:05.096: INFO: Created: latency-svc-7zfl4
  Aug  4 20:34:05.136: INFO: Got endpoints: latency-svc-bjwfd [746.04551ms]
  Aug  4 20:34:05.146: INFO: Created: latency-svc-nm2jn
  Aug  4 20:34:05.186: INFO: Got endpoints: latency-svc-jfvfq [750.768275ms]
  Aug  4 20:34:05.197: INFO: Created: latency-svc-pmfvc
  Aug  4 20:34:05.236: INFO: Got endpoints: latency-svc-44bvg [749.02138ms]
  Aug  4 20:34:05.246: INFO: Created: latency-svc-pj6hl
  Aug  4 20:34:05.285: INFO: Got endpoints: latency-svc-5qs2d [749.251523ms]
  Aug  4 20:34:05.295: INFO: Created: latency-svc-gv2s5
  Aug  4 20:34:05.337: INFO: Got endpoints: latency-svc-m22jd [751.364505ms]
  Aug  4 20:34:05.348: INFO: Created: latency-svc-k9bhs
  Aug  4 20:34:05.385: INFO: Got endpoints: latency-svc-6vgxc [748.764491ms]
  Aug  4 20:34:05.395: INFO: Created: latency-svc-qrqwg
  Aug  4 20:34:05.439: INFO: Got endpoints: latency-svc-qw25k [753.647709ms]
  Aug  4 20:34:05.449: INFO: Created: latency-svc-d59xs
  Aug  4 20:34:05.485: INFO: Got endpoints: latency-svc-lbkdj [750.116219ms]
  Aug  4 20:34:05.498: INFO: Created: latency-svc-j82jg
  Aug  4 20:34:05.535: INFO: Got endpoints: latency-svc-8dmrb [747.490347ms]
  Aug  4 20:34:05.547: INFO: Created: latency-svc-w9ft5
  Aug  4 20:34:05.585: INFO: Got endpoints: latency-svc-gtvxb [750.143948ms]
  Aug  4 20:34:05.596: INFO: Created: latency-svc-b9m95
  Aug  4 20:34:05.637: INFO: Got endpoints: latency-svc-5t9fg [748.330058ms]
  Aug  4 20:34:05.654: INFO: Created: latency-svc-h4sfh
  Aug  4 20:34:05.685: INFO: Got endpoints: latency-svc-v7xd6 [747.434159ms]
  Aug  4 20:34:05.696: INFO: Created: latency-svc-7j82g
  Aug  4 20:34:05.735: INFO: Got endpoints: latency-svc-vwlxf [749.705321ms]
  Aug  4 20:34:05.749: INFO: Created: latency-svc-d68ts
  Aug  4 20:34:05.787: INFO: Got endpoints: latency-svc-r9gd9 [749.756725ms]
  Aug  4 20:34:05.798: INFO: Created: latency-svc-lvpt8
  Aug  4 20:34:05.835: INFO: Got endpoints: latency-svc-7zfl4 [749.195666ms]
  Aug  4 20:34:05.846: INFO: Created: latency-svc-cmspr
  Aug  4 20:34:05.887: INFO: Got endpoints: latency-svc-nm2jn [751.072399ms]
  Aug  4 20:34:05.898: INFO: Created: latency-svc-c65w7
  Aug  4 20:34:05.937: INFO: Got endpoints: latency-svc-pmfvc [750.973947ms]
  Aug  4 20:34:05.947: INFO: Created: latency-svc-gsd9h
  Aug  4 20:34:05.985: INFO: Got endpoints: latency-svc-pj6hl [748.706047ms]
  E0804 20:34:06.015364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:06.025: INFO: Created: latency-svc-qxrtf
  Aug  4 20:34:06.107: INFO: Got endpoints: latency-svc-gv2s5 [821.241675ms]
  Aug  4 20:34:06.108: INFO: Got endpoints: latency-svc-k9bhs [771.357699ms]
  Aug  4 20:34:06.117: INFO: Created: latency-svc-x9v6p
  Aug  4 20:34:06.124: INFO: Created: latency-svc-9rc44
  Aug  4 20:34:06.135: INFO: Got endpoints: latency-svc-qrqwg [749.528623ms]
  Aug  4 20:34:06.147: INFO: Created: latency-svc-wxg6h
  Aug  4 20:34:06.211: INFO: Got endpoints: latency-svc-d59xs [771.521285ms]
  Aug  4 20:34:06.221: INFO: Created: latency-svc-bl89w
  Aug  4 20:34:06.236: INFO: Got endpoints: latency-svc-j82jg [750.84138ms]
  Aug  4 20:34:06.245: INFO: Created: latency-svc-fxggf
  Aug  4 20:34:06.285: INFO: Got endpoints: latency-svc-w9ft5 [750.206555ms]
  Aug  4 20:34:06.294: INFO: Created: latency-svc-clpg6
  Aug  4 20:34:06.337: INFO: Got endpoints: latency-svc-b9m95 [751.443392ms]
  Aug  4 20:34:06.348: INFO: Created: latency-svc-r9fpq
  Aug  4 20:34:06.385: INFO: Got endpoints: latency-svc-h4sfh [747.670233ms]
  Aug  4 20:34:06.394: INFO: Created: latency-svc-w5tmf
  Aug  4 20:34:06.435: INFO: Got endpoints: latency-svc-7j82g [749.46847ms]
  Aug  4 20:34:06.444: INFO: Created: latency-svc-g6nps
  Aug  4 20:34:06.486: INFO: Got endpoints: latency-svc-d68ts [751.06757ms]
  Aug  4 20:34:06.496: INFO: Created: latency-svc-cdwpj
  Aug  4 20:34:06.536: INFO: Got endpoints: latency-svc-lvpt8 [749.158174ms]
  Aug  4 20:34:06.545: INFO: Created: latency-svc-lvbbv
  Aug  4 20:34:06.587: INFO: Got endpoints: latency-svc-cmspr [751.614334ms]
  Aug  4 20:34:06.598: INFO: Created: latency-svc-kpz5s
  Aug  4 20:34:06.635: INFO: Got endpoints: latency-svc-c65w7 [748.257602ms]
  Aug  4 20:34:06.645: INFO: Created: latency-svc-9t5fn
  Aug  4 20:34:06.685: INFO: Got endpoints: latency-svc-gsd9h [748.876732ms]
  Aug  4 20:34:06.694: INFO: Created: latency-svc-jkgvg
  Aug  4 20:34:06.736: INFO: Got endpoints: latency-svc-qxrtf [750.787696ms]
  Aug  4 20:34:06.745: INFO: Created: latency-svc-rgpgh
  Aug  4 20:34:06.787: INFO: Got endpoints: latency-svc-x9v6p [680.616061ms]
  Aug  4 20:34:06.797: INFO: Created: latency-svc-hk5nm
  Aug  4 20:34:06.835: INFO: Got endpoints: latency-svc-9rc44 [726.538792ms]
  Aug  4 20:34:06.847: INFO: Created: latency-svc-tq2wn
  Aug  4 20:34:06.886: INFO: Got endpoints: latency-svc-wxg6h [750.751891ms]
  Aug  4 20:34:06.894: INFO: Created: latency-svc-5pbm5
  Aug  4 20:34:06.935: INFO: Got endpoints: latency-svc-bl89w [723.943532ms]
  Aug  4 20:34:06.947: INFO: Created: latency-svc-89wzl
  Aug  4 20:34:06.985: INFO: Got endpoints: latency-svc-fxggf [749.752357ms]
  Aug  4 20:34:06.995: INFO: Created: latency-svc-ncl54
  E0804 20:34:07.016190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:07.036: INFO: Got endpoints: latency-svc-clpg6 [750.508872ms]
  Aug  4 20:34:07.050: INFO: Created: latency-svc-4gkcw
  Aug  4 20:34:07.084: INFO: Got endpoints: latency-svc-r9fpq [747.698767ms]
  Aug  4 20:34:07.094: INFO: Created: latency-svc-rwfnt
  Aug  4 20:34:07.137: INFO: Got endpoints: latency-svc-w5tmf [752.376441ms]
  Aug  4 20:34:07.148: INFO: Created: latency-svc-v5v2w
  Aug  4 20:34:07.186: INFO: Got endpoints: latency-svc-g6nps [751.555325ms]
  Aug  4 20:34:07.195: INFO: Created: latency-svc-z4xrg
  Aug  4 20:34:07.238: INFO: Got endpoints: latency-svc-cdwpj [750.991622ms]
  Aug  4 20:34:07.247: INFO: Created: latency-svc-w6wg4
  Aug  4 20:34:07.286: INFO: Got endpoints: latency-svc-lvbbv [750.067304ms]
  Aug  4 20:34:07.297: INFO: Created: latency-svc-2qxzs
  Aug  4 20:34:07.335: INFO: Got endpoints: latency-svc-kpz5s [747.483657ms]
  Aug  4 20:34:07.344: INFO: Created: latency-svc-jcl2l
  Aug  4 20:34:07.387: INFO: Got endpoints: latency-svc-9t5fn [752.140811ms]
  Aug  4 20:34:07.399: INFO: Created: latency-svc-848nw
  Aug  4 20:34:07.435: INFO: Got endpoints: latency-svc-jkgvg [749.943346ms]
  Aug  4 20:34:07.445: INFO: Created: latency-svc-gldzv
  Aug  4 20:34:07.485: INFO: Got endpoints: latency-svc-rgpgh [749.445465ms]
  Aug  4 20:34:07.498: INFO: Created: latency-svc-vxkxx
  Aug  4 20:34:07.535: INFO: Got endpoints: latency-svc-hk5nm [748.098362ms]
  Aug  4 20:34:07.545: INFO: Created: latency-svc-pmbsm
  Aug  4 20:34:07.587: INFO: Got endpoints: latency-svc-tq2wn [752.299307ms]
  Aug  4 20:34:07.596: INFO: Created: latency-svc-rhpjb
  Aug  4 20:34:07.635: INFO: Got endpoints: latency-svc-5pbm5 [749.22327ms]
  Aug  4 20:34:07.646: INFO: Created: latency-svc-p8q9n
  Aug  4 20:34:07.685: INFO: Got endpoints: latency-svc-89wzl [750.611535ms]
  Aug  4 20:34:07.696: INFO: Created: latency-svc-f6mtd
  Aug  4 20:34:07.736: INFO: Got endpoints: latency-svc-ncl54 [750.543112ms]
  Aug  4 20:34:07.745: INFO: Created: latency-svc-rxd8c
  Aug  4 20:34:07.785: INFO: Got endpoints: latency-svc-4gkcw [749.155246ms]
  Aug  4 20:34:07.794: INFO: Created: latency-svc-rbk27
  Aug  4 20:34:07.834: INFO: Got endpoints: latency-svc-rwfnt [749.817486ms]
  Aug  4 20:34:07.844: INFO: Created: latency-svc-gdjzz
  Aug  4 20:34:07.889: INFO: Got endpoints: latency-svc-v5v2w [751.665441ms]
  Aug  4 20:34:07.899: INFO: Created: latency-svc-8rg4r
  Aug  4 20:34:07.935: INFO: Got endpoints: latency-svc-z4xrg [749.153994ms]
  Aug  4 20:34:07.944: INFO: Created: latency-svc-dvcqh
  Aug  4 20:34:07.993: INFO: Got endpoints: latency-svc-w6wg4 [755.340989ms]
  Aug  4 20:34:08.002: INFO: Created: latency-svc-pt47d
  E0804 20:34:08.016353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:08.036: INFO: Got endpoints: latency-svc-2qxzs [749.798549ms]
  Aug  4 20:34:08.045: INFO: Created: latency-svc-t568h
  Aug  4 20:34:08.087: INFO: Got endpoints: latency-svc-jcl2l [752.080661ms]
  Aug  4 20:34:08.099: INFO: Created: latency-svc-ds92j
  Aug  4 20:34:08.135: INFO: Got endpoints: latency-svc-848nw [748.090117ms]
  Aug  4 20:34:08.146: INFO: Created: latency-svc-5sxd6
  Aug  4 20:34:08.185: INFO: Got endpoints: latency-svc-gldzv [749.391807ms]
  Aug  4 20:34:08.194: INFO: Created: latency-svc-74srv
  Aug  4 20:34:08.237: INFO: Got endpoints: latency-svc-vxkxx [751.945075ms]
  Aug  4 20:34:08.247: INFO: Created: latency-svc-hxlbj
  Aug  4 20:34:08.286: INFO: Got endpoints: latency-svc-pmbsm [750.810121ms]
  Aug  4 20:34:08.295: INFO: Created: latency-svc-4hkpk
  Aug  4 20:34:08.335: INFO: Got endpoints: latency-svc-rhpjb [747.46894ms]
  Aug  4 20:34:08.344: INFO: Created: latency-svc-bw5tg
  Aug  4 20:34:08.386: INFO: Got endpoints: latency-svc-p8q9n [750.656501ms]
  Aug  4 20:34:08.396: INFO: Created: latency-svc-crg25
  Aug  4 20:34:08.437: INFO: Got endpoints: latency-svc-f6mtd [751.102802ms]
  Aug  4 20:34:08.446: INFO: Created: latency-svc-p6lsx
  Aug  4 20:34:08.485: INFO: Got endpoints: latency-svc-rxd8c [748.96439ms]
  Aug  4 20:34:08.495: INFO: Created: latency-svc-kncst
  Aug  4 20:34:08.537: INFO: Got endpoints: latency-svc-rbk27 [752.306171ms]
  Aug  4 20:34:08.547: INFO: Created: latency-svc-h658f
  Aug  4 20:34:08.584: INFO: Got endpoints: latency-svc-gdjzz [749.917289ms]
  Aug  4 20:34:08.616: INFO: Created: latency-svc-xcg4s
  Aug  4 20:34:08.635: INFO: Got endpoints: latency-svc-8rg4r [746.016551ms]
  Aug  4 20:34:08.644: INFO: Created: latency-svc-hdc2j
  Aug  4 20:34:08.687: INFO: Got endpoints: latency-svc-dvcqh [751.825743ms]
  Aug  4 20:34:08.697: INFO: Created: latency-svc-6g24q
  Aug  4 20:34:08.735: INFO: Got endpoints: latency-svc-pt47d [742.009449ms]
  Aug  4 20:34:08.745: INFO: Created: latency-svc-pvjhr
  Aug  4 20:34:08.785: INFO: Got endpoints: latency-svc-t568h [749.353086ms]
  Aug  4 20:34:08.795: INFO: Created: latency-svc-x5zf5
  Aug  4 20:34:08.834: INFO: Got endpoints: latency-svc-ds92j [747.692738ms]
  Aug  4 20:34:08.843: INFO: Created: latency-svc-7dmbt
  Aug  4 20:34:08.886: INFO: Got endpoints: latency-svc-5sxd6 [750.6727ms]
  Aug  4 20:34:08.895: INFO: Created: latency-svc-2f9gr
  Aug  4 20:34:08.935: INFO: Got endpoints: latency-svc-74srv [750.080121ms]
  Aug  4 20:34:08.945: INFO: Created: latency-svc-d7cjh
  Aug  4 20:34:08.985: INFO: Got endpoints: latency-svc-hxlbj [747.561681ms]
  Aug  4 20:34:08.994: INFO: Created: latency-svc-chrq7
  E0804 20:34:09.016828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:09.035: INFO: Got endpoints: latency-svc-4hkpk [748.409051ms]
  Aug  4 20:34:09.046: INFO: Created: latency-svc-9rx9g
  Aug  4 20:34:09.085: INFO: Got endpoints: latency-svc-bw5tg [749.693717ms]
  Aug  4 20:34:09.094: INFO: Created: latency-svc-6zd7g
  Aug  4 20:34:09.136: INFO: Got endpoints: latency-svc-crg25 [749.968468ms]
  Aug  4 20:34:09.147: INFO: Created: latency-svc-pbfdb
  Aug  4 20:34:09.185: INFO: Got endpoints: latency-svc-p6lsx [748.814051ms]
  Aug  4 20:34:09.196: INFO: Created: latency-svc-kdt79
  Aug  4 20:34:09.235: INFO: Got endpoints: latency-svc-kncst [750.102663ms]
  Aug  4 20:34:09.249: INFO: Created: latency-svc-h4jws
  Aug  4 20:34:09.285: INFO: Got endpoints: latency-svc-h658f [747.702865ms]
  Aug  4 20:34:09.294: INFO: Created: latency-svc-p6qzn
  Aug  4 20:34:09.335: INFO: Got endpoints: latency-svc-xcg4s [750.70313ms]
  Aug  4 20:34:09.345: INFO: Created: latency-svc-694dv
  Aug  4 20:34:09.387: INFO: Got endpoints: latency-svc-hdc2j [751.525736ms]
  Aug  4 20:34:09.396: INFO: Created: latency-svc-jx47q
  Aug  4 20:34:09.435: INFO: Got endpoints: latency-svc-6g24q [747.497302ms]
  Aug  4 20:34:09.444: INFO: Created: latency-svc-67r9d
  Aug  4 20:34:09.487: INFO: Got endpoints: latency-svc-pvjhr [752.040308ms]
  Aug  4 20:34:09.501: INFO: Created: latency-svc-9hvjg
  Aug  4 20:34:09.535: INFO: Got endpoints: latency-svc-x5zf5 [749.154854ms]
  Aug  4 20:34:09.546: INFO: Created: latency-svc-j78qb
  Aug  4 20:34:09.586: INFO: Got endpoints: latency-svc-7dmbt [751.057804ms]
  Aug  4 20:34:09.595: INFO: Created: latency-svc-r89j7
  Aug  4 20:34:09.636: INFO: Got endpoints: latency-svc-2f9gr [750.127647ms]
  Aug  4 20:34:09.647: INFO: Created: latency-svc-9wzrt
  Aug  4 20:34:09.686: INFO: Got endpoints: latency-svc-d7cjh [750.690505ms]
  Aug  4 20:34:09.696: INFO: Created: latency-svc-4s7p6
  Aug  4 20:34:09.735: INFO: Got endpoints: latency-svc-chrq7 [750.741773ms]
  Aug  4 20:34:09.748: INFO: Created: latency-svc-j6n79
  Aug  4 20:34:09.785: INFO: Got endpoints: latency-svc-9rx9g [750.538721ms]
  Aug  4 20:34:09.795: INFO: Created: latency-svc-ng98m
  Aug  4 20:34:09.836: INFO: Got endpoints: latency-svc-6zd7g [751.30062ms]
  Aug  4 20:34:09.846: INFO: Created: latency-svc-4vqgh
  Aug  4 20:34:09.884: INFO: Got endpoints: latency-svc-pbfdb [748.615867ms]
  Aug  4 20:34:09.895: INFO: Created: latency-svc-plnf9
  Aug  4 20:34:09.935: INFO: Got endpoints: latency-svc-kdt79 [749.971186ms]
  Aug  4 20:34:09.945: INFO: Created: latency-svc-r96bs
  Aug  4 20:34:09.986: INFO: Got endpoints: latency-svc-h4jws [751.105014ms]
  Aug  4 20:34:09.996: INFO: Created: latency-svc-94g4m
  E0804 20:34:10.017393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:10.037: INFO: Got endpoints: latency-svc-p6qzn [752.251691ms]
  Aug  4 20:34:10.048: INFO: Created: latency-svc-svzqj
  Aug  4 20:34:10.089: INFO: Got endpoints: latency-svc-694dv [754.491051ms]
  Aug  4 20:34:10.099: INFO: Created: latency-svc-n2f4f
  Aug  4 20:34:10.135: INFO: Got endpoints: latency-svc-jx47q [747.831083ms]
  Aug  4 20:34:10.146: INFO: Created: latency-svc-wj5nt
  Aug  4 20:34:10.185: INFO: Got endpoints: latency-svc-67r9d [750.387367ms]
  Aug  4 20:34:10.196: INFO: Created: latency-svc-d8xgn
  Aug  4 20:34:10.235: INFO: Got endpoints: latency-svc-9hvjg [747.820116ms]
  Aug  4 20:34:10.245: INFO: Created: latency-svc-h6n2f
  Aug  4 20:34:10.285: INFO: Got endpoints: latency-svc-j78qb [750.242269ms]
  Aug  4 20:34:10.298: INFO: Created: latency-svc-r7vqd
  Aug  4 20:34:10.336: INFO: Got endpoints: latency-svc-r89j7 [750.306907ms]
  Aug  4 20:34:10.346: INFO: Created: latency-svc-f6dt8
  Aug  4 20:34:10.385: INFO: Got endpoints: latency-svc-9wzrt [749.067788ms]
  Aug  4 20:34:10.396: INFO: Created: latency-svc-5w8nk
  Aug  4 20:34:10.435: INFO: Got endpoints: latency-svc-4s7p6 [748.765455ms]
  Aug  4 20:34:10.445: INFO: Created: latency-svc-dm97c
  Aug  4 20:34:10.485: INFO: Got endpoints: latency-svc-j6n79 [749.457181ms]
  Aug  4 20:34:10.496: INFO: Created: latency-svc-kc688
  Aug  4 20:34:10.535: INFO: Got endpoints: latency-svc-ng98m [749.859564ms]
  Aug  4 20:34:10.544: INFO: Created: latency-svc-gc62b
  Aug  4 20:34:10.587: INFO: Got endpoints: latency-svc-4vqgh [751.077369ms]
  Aug  4 20:34:10.596: INFO: Created: latency-svc-qjjsg
  Aug  4 20:34:10.634: INFO: Got endpoints: latency-svc-plnf9 [749.981509ms]
  Aug  4 20:34:10.643: INFO: Created: latency-svc-2rscs
  Aug  4 20:34:10.687: INFO: Got endpoints: latency-svc-r96bs [751.402311ms]
  Aug  4 20:34:10.697: INFO: Created: latency-svc-vjs9s
  Aug  4 20:34:10.735: INFO: Got endpoints: latency-svc-94g4m [748.676052ms]
  Aug  4 20:34:10.745: INFO: Created: latency-svc-lb7jl
  Aug  4 20:34:10.787: INFO: Got endpoints: latency-svc-svzqj [749.542899ms]
  Aug  4 20:34:10.796: INFO: Created: latency-svc-bpwrg
  Aug  4 20:34:10.839: INFO: Got endpoints: latency-svc-n2f4f [749.166435ms]
  Aug  4 20:34:10.851: INFO: Created: latency-svc-8mmsm
  Aug  4 20:34:10.885: INFO: Got endpoints: latency-svc-wj5nt [750.7836ms]
  Aug  4 20:34:10.895: INFO: Created: latency-svc-cm6sd
  Aug  4 20:34:10.937: INFO: Got endpoints: latency-svc-d8xgn [751.446539ms]
  Aug  4 20:34:10.949: INFO: Created: latency-svc-ghzqg
  Aug  4 20:34:10.984: INFO: Got endpoints: latency-svc-h6n2f [749.517298ms]
  E0804 20:34:11.018080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:11.038: INFO: Got endpoints: latency-svc-r7vqd [752.664215ms]
  Aug  4 20:34:11.085: INFO: Got endpoints: latency-svc-f6dt8 [748.807828ms]
  Aug  4 20:34:11.136: INFO: Got endpoints: latency-svc-5w8nk [750.534326ms]
  Aug  4 20:34:11.186: INFO: Got endpoints: latency-svc-dm97c [751.388107ms]
  Aug  4 20:34:11.235: INFO: Got endpoints: latency-svc-kc688 [749.876327ms]
  Aug  4 20:34:11.285: INFO: Got endpoints: latency-svc-gc62b [749.507816ms]
  Aug  4 20:34:11.337: INFO: Got endpoints: latency-svc-qjjsg [749.840244ms]
  Aug  4 20:34:11.385: INFO: Got endpoints: latency-svc-2rscs [750.091624ms]
  Aug  4 20:34:11.435: INFO: Got endpoints: latency-svc-vjs9s [747.681224ms]
  Aug  4 20:34:11.487: INFO: Got endpoints: latency-svc-lb7jl [752.085949ms]
  Aug  4 20:34:11.535: INFO: Got endpoints: latency-svc-bpwrg [747.839386ms]
  Aug  4 20:34:11.587: INFO: Got endpoints: latency-svc-8mmsm [747.766099ms]
  Aug  4 20:34:11.635: INFO: Got endpoints: latency-svc-cm6sd [749.146391ms]
  Aug  4 20:34:11.687: INFO: Got endpoints: latency-svc-ghzqg [750.090437ms]
  Aug  4 20:34:11.687: INFO: Latencies: [17.752402ms 24.521377ms 31.349074ms 38.602016ms 41.920078ms 51.884159ms 73.094549ms 85.486177ms 90.835462ms 101.300596ms 107.43864ms 113.731216ms 122.603045ms 129.997682ms 143.404325ms 144.235953ms 157.327452ms 157.330662ms 158.33465ms 158.624679ms 160.985129ms 161.166227ms 161.452639ms 172.12508ms 183.984213ms 184.051673ms 186.827151ms 188.221062ms 189.371331ms 190.384121ms 196.720768ms 197.155079ms 198.696208ms 200.907792ms 202.107221ms 203.29952ms 204.640299ms 241.252524ms 243.177201ms 296.345343ms 305.245301ms 368.242591ms 386.61212ms 431.413265ms 467.120004ms 478.067473ms 520.256171ms 561.6013ms 601.585504ms 645.498237ms 680.616061ms 684.40296ms 717.546069ms 723.943532ms 726.538792ms 727.465395ms 728.057976ms 729.530449ms 737.661074ms 742.009449ms 746.016551ms 746.04551ms 747.434159ms 747.46894ms 747.483657ms 747.490347ms 747.497302ms 747.561681ms 747.670233ms 747.681224ms 747.692738ms 747.698767ms 747.702865ms 747.766099ms 747.820116ms 747.831083ms 747.839386ms 748.090117ms 748.098362ms 748.257602ms 748.330058ms 748.409051ms 748.615867ms 748.676052ms 748.706047ms 748.764491ms 748.765455ms 748.807828ms 748.814051ms 748.876732ms 748.886083ms 748.96439ms 749.013105ms 749.02138ms 749.067788ms 749.146391ms 749.153994ms 749.154854ms 749.155246ms 749.158174ms 749.166435ms 749.195666ms 749.22327ms 749.251523ms 749.353086ms 749.391807ms 749.445465ms 749.448555ms 749.457181ms 749.46847ms 749.507816ms 749.517298ms 749.528623ms 749.542899ms 749.624262ms 749.693717ms 749.705321ms 749.752357ms 749.756725ms 749.798549ms 749.817486ms 749.840244ms 749.859564ms 749.876327ms 749.917289ms 749.943346ms 749.968468ms 749.971186ms 749.981509ms 750.067304ms 750.080121ms 750.090437ms 750.091624ms 750.102663ms 750.116219ms 750.127647ms 750.143948ms 750.206555ms 750.242269ms 750.306907ms 750.387367ms 750.507672ms 750.508872ms 750.534326ms 750.538721ms 750.543112ms 750.611535ms 750.656501ms 750.6727ms 750.690505ms 750.70313ms 750.741773ms 750.751891ms 750.768275ms 750.7836ms 750.787696ms 750.810121ms 750.84138ms 750.856029ms 750.860594ms 750.973947ms 750.991622ms 751.057804ms 751.06757ms 751.072399ms 751.077369ms 751.102802ms 751.105014ms 751.260279ms 751.30062ms 751.364505ms 751.388107ms 751.402311ms 751.443392ms 751.446539ms 751.499082ms 751.525736ms 751.555325ms 751.614334ms 751.665441ms 751.686473ms 751.825743ms 751.905896ms 751.945075ms 752.040308ms 752.080661ms 752.085949ms 752.140811ms 752.251691ms 752.299307ms 752.306171ms 752.376441ms 752.664215ms 753.647709ms 753.699706ms 754.491051ms 755.340989ms 771.357699ms 771.521285ms 821.241675ms]
  Aug  4 20:34:11.687: INFO: 50 %ile: 749.166435ms
  Aug  4 20:34:11.687: INFO: 90 %ile: 751.686473ms
  Aug  4 20:34:11.687: INFO: 99 %ile: 771.521285ms
  Aug  4 20:34:11.687: INFO: Total sample count: 200
  Aug  4 20:34:11.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-4525" for this suite. @ 08/04/23 20:34:11.69
• [10.742 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 08/04/23 20:34:11.694
  Aug  4 20:34:11.694: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename svcaccounts @ 08/04/23 20:34:11.695
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:34:11.703
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:34:11.705
  Aug  4 20:34:11.715: INFO: created pod
  E0804 20:34:12.018672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:13.019018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:34:13.72
  E0804 20:34:14.019556      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:15.020110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:16.020420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:17.021055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:18.021423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:19.022001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:20.022943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:21.022986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:22.024067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:23.024320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:24.024461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:25.025431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:26.025630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:27.026496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:28.026962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:29.027062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:30.027595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:31.027744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:32.028174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:33.028426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:34.028454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:35.028883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:36.029184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:37.029539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:38.029924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:39.030123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:40.030321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:41.030669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:42.030954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:43.032047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:43.720: INFO: polling logs
  Aug  4 20:34:43.727: INFO: Pod logs: 
  I0804 20:34:12.345246       1 log.go:198] OK: Got token
  I0804 20:34:12.345283       1 log.go:198] validating with in-cluster discovery
  I0804 20:34:12.345591       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0804 20:34:12.345617       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1798:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691181851, NotBefore:1691181251, IssuedAt:1691181251, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1798", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f0a5029b-7163-4072-8d51-620ff87252b6"}}}
  I0804 20:34:12.373505       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0804 20:34:12.377867       1 log.go:198] OK: Validated signature on JWT
  I0804 20:34:12.377927       1 log.go:198] OK: Got valid claims from token!
  I0804 20:34:12.377947       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-1798:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1691181851, NotBefore:1691181251, IssuedAt:1691181251, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-1798", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"f0a5029b-7163-4072-8d51-620ff87252b6"}}}

  Aug  4 20:34:43.727: INFO: completed pod
  Aug  4 20:34:43.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-1798" for this suite. @ 08/04/23 20:34:43.733
• [32.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 08/04/23 20:34:43.738
  Aug  4 20:34:43.738: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename watch @ 08/04/23 20:34:43.739
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:34:43.751
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:34:43.753
  STEP: creating a watch on configmaps @ 08/04/23 20:34:43.756
  STEP: creating a new configmap @ 08/04/23 20:34:43.757
  STEP: modifying the configmap once @ 08/04/23 20:34:43.76
  STEP: closing the watch once it receives two notifications @ 08/04/23 20:34:43.764
  Aug  4 20:34:43.764: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1234  220e21be-3a62-45e4-9f2f-89f369e0dd4b 15697 0 2023-08-04 20:34:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-04 20:34:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:34:43.764: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1234  220e21be-3a62-45e4-9f2f-89f369e0dd4b 15698 0 2023-08-04 20:34:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-04 20:34:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 08/04/23 20:34:43.765
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 08/04/23 20:34:43.769
  STEP: deleting the configmap @ 08/04/23 20:34:43.77
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 08/04/23 20:34:43.773
  Aug  4 20:34:43.773: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1234  220e21be-3a62-45e4-9f2f-89f369e0dd4b 15699 0 2023-08-04 20:34:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-04 20:34:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:34:43.773: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-1234  220e21be-3a62-45e4-9f2f-89f369e0dd4b 15700 0 2023-08-04 20:34:43 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-08-04 20:34:43 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:34:43.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-1234" for this suite. @ 08/04/23 20:34:43.775
• [0.040 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 08/04/23 20:34:43.778
  Aug  4 20:34:43.778: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replication-controller @ 08/04/23 20:34:43.779
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:34:43.787
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:34:43.789
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 08/04/23 20:34:43.791
  E0804 20:34:44.032725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:45.032940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replication controller with a matching selector is created @ 08/04/23 20:34:45.811
  STEP: Then the orphan pod is adopted @ 08/04/23 20:34:45.817
  E0804 20:34:46.033278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:46.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-3281" for this suite. @ 08/04/23 20:34:46.824
• [3.049 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 08/04/23 20:34:46.828
  Aug  4 20:34:46.828: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:34:46.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:34:46.838
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:34:46.839
  STEP: Creating a pod to test downward api env vars @ 08/04/23 20:34:46.841
  E0804 20:34:47.033850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:48.034233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:49.034918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:50.034989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:34:50.858
  Aug  4 20:34:50.860: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-7d92ab8b-54af-48dd-b43b-dbc9cd06c22c container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 20:34:50.866
  Aug  4 20:34:50.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4770" for this suite. @ 08/04/23 20:34:50.878
• [4.056 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 08/04/23 20:34:50.884
  Aug  4 20:34:50.884: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 20:34:50.885
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:34:50.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:34:50.895
  STEP: Creating configMap with name configmap-test-volume-1a520213-ca62-4f5b-942c-fda0c27d9b63 @ 08/04/23 20:34:50.897
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:34:50.9
  E0804 20:34:51.035565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:52.036332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:53.036514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:54.036760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:34:54.914
  Aug  4 20:34:54.915: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-be487c33-9037-4da5-b190-8e24d7221193 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:34:54.921
  Aug  4 20:34:54.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7835" for this suite. @ 08/04/23 20:34:54.931
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 08/04/23 20:34:54.935
  Aug  4 20:34:54.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:34:54.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:34:54.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:34:54.944
  STEP: Creating pod test-grpc-54207643-3394-4bc0-b2d1-11387f63f364 in namespace container-probe-5804 @ 08/04/23 20:34:54.946
  E0804 20:34:55.037596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:56.038085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:34:56.954: INFO: Started pod test-grpc-54207643-3394-4bc0-b2d1-11387f63f364 in namespace container-probe-5804
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/04/23 20:34:56.954
  Aug  4 20:34:56.956: INFO: Initial restart count of pod test-grpc-54207643-3394-4bc0-b2d1-11387f63f364 is 0
  E0804 20:34:57.038168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:58.038426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:34:59.038980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:00.040059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:01.040681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:02.041085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:03.041434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:04.041658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:05.042377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:06.042610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:07.043637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:08.044066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:09.045060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:10.045268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:11.045560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:12.045934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:13.046936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:14.046962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:15.047362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:16.048092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:17.048735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:18.048990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:19.049980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:20.050209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:21.050900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:22.050985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:23.051337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:24.051449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:25.051921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:26.052127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:27.052598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:28.052810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:29.053603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:30.053931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:31.054599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:32.054997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:33.055724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:34.056073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:35.056830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:36.057048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:37.057584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:38.057796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:39.058215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:40.058419      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:41.059115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:42.059492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:43.059510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:44.059686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:45.060080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:46.060265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:47.061336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:48.061599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:49.062378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:50.062630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:51.063573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:52.064611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:53.065298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:54.065529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:55.065998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:56.066245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:57.066593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:58.067602      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:35:59.068314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:00.068547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:01.069289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:02.069643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:36:03.059: INFO: Restart count of pod container-probe-5804/test-grpc-54207643-3394-4bc0-b2d1-11387f63f364 is now 1 (1m6.102832869s elapsed)
  Aug  4 20:36:03.059: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:36:03.061
  E0804 20:36:03.069936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "container-probe-5804" for this suite. @ 08/04/23 20:36:03.074
• [68.145 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 08/04/23 20:36:03.081
  Aug  4 20:36:03.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:36:03.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:03.099
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:03.102
  STEP: creating all guestbook components @ 08/04/23 20:36:03.104
  Aug  4 20:36:03.104: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Aug  4 20:36:03.104: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 create -f -'
  Aug  4 20:36:03.365: INFO: stderr: ""
  Aug  4 20:36:03.365: INFO: stdout: "service/agnhost-replica created\n"
  Aug  4 20:36:03.365: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Aug  4 20:36:03.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 create -f -'
  Aug  4 20:36:03.585: INFO: stderr: ""
  Aug  4 20:36:03.585: INFO: stdout: "service/agnhost-primary created\n"
  Aug  4 20:36:03.586: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Aug  4 20:36:03.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 create -f -'
  Aug  4 20:36:03.799: INFO: stderr: ""
  Aug  4 20:36:03.799: INFO: stdout: "service/frontend created\n"
  Aug  4 20:36:03.799: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Aug  4 20:36:03.799: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 create -f -'
  Aug  4 20:36:03.986: INFO: stderr: ""
  Aug  4 20:36:03.986: INFO: stdout: "deployment.apps/frontend created\n"
  Aug  4 20:36:03.986: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug  4 20:36:03.986: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 create -f -'
  E0804 20:36:04.070985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:36:04.182: INFO: stderr: ""
  Aug  4 20:36:04.182: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Aug  4 20:36:04.182: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Aug  4 20:36:04.182: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 create -f -'
  Aug  4 20:36:04.384: INFO: stderr: ""
  Aug  4 20:36:04.384: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 08/04/23 20:36:04.384
  Aug  4 20:36:04.384: INFO: Waiting for all frontend pods to be Running.
  E0804 20:36:05.070993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:06.071024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:07.071577      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:08.072039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:09.072231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:36:09.434: INFO: Waiting for frontend to serve content.
  Aug  4 20:36:09.441: INFO: Trying to add a new entry to the guestbook.
  Aug  4 20:36:09.448: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 08/04/23 20:36:09.453
  Aug  4 20:36:09.453: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 delete --grace-period=0 --force -f -'
  Aug  4 20:36:09.521: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:36:09.521: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 08/04/23 20:36:09.521
  Aug  4 20:36:09.521: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 delete --grace-period=0 --force -f -'
  Aug  4 20:36:09.588: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:36:09.588: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/04/23 20:36:09.588
  Aug  4 20:36:09.588: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 delete --grace-period=0 --force -f -'
  Aug  4 20:36:09.654: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:36:09.654: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/04/23 20:36:09.654
  Aug  4 20:36:09.655: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 delete --grace-period=0 --force -f -'
  Aug  4 20:36:09.712: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:36:09.712: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 08/04/23 20:36:09.712
  Aug  4 20:36:09.712: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 delete --grace-period=0 --force -f -'
  Aug  4 20:36:09.822: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:36:09.822: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 08/04/23 20:36:09.822
  Aug  4 20:36:09.822: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4644 delete --grace-period=0 --force -f -'
  Aug  4 20:36:09.912: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:36:09.912: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Aug  4 20:36:09.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4644" for this suite. @ 08/04/23 20:36:09.916
• [6.841 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 08/04/23 20:36:09.922
  Aug  4 20:36:09.922: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replication-controller @ 08/04/23 20:36:09.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:09.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:09.946
  Aug  4 20:36:09.949: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  E0804 20:36:10.072753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 08/04/23 20:36:10.958
  STEP: Checking rc "condition-test" has the desired failure condition set @ 08/04/23 20:36:10.964
  E0804 20:36:11.073499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 08/04/23 20:36:11.969
  Aug  4 20:36:11.974: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 08/04/23 20:36:11.974
  E0804 20:36:12.073909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:36:12.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-6150" for this suite. @ 08/04/23 20:36:12.982
• [3.063 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 08/04/23 20:36:12.986
  Aug  4 20:36:12.986: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:36:12.987
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:12.995
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:12.997
  Aug  4 20:36:13.021: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-291" for this suite. @ 08/04/23 20:36:13.023
• [0.041 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 08/04/23 20:36:13.028
  Aug  4 20:36:13.028: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 20:36:13.029
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:13.036
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:13.038
  STEP: Creating configMap with name configmap-test-volume-map-f60cb8bf-df19-425f-95a8-e1ab52dbe360 @ 08/04/23 20:36:13.04
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:36:13.043
  E0804 20:36:13.074643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:14.075039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:15.075934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:16.076312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:36:17.059
  Aug  4 20:36:17.061: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-b4e40c17-653f-4a96-b337-7e21fc1506ea container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:36:17.066
  E0804 20:36:17.076803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:36:17.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9216" for this suite. @ 08/04/23 20:36:17.079
• [4.055 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 08/04/23 20:36:17.084
  Aug  4 20:36:17.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:36:17.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:17.095
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:17.097
  STEP: creating a Service @ 08/04/23 20:36:17.1
  STEP: watching for the Service to be added @ 08/04/23 20:36:17.108
  Aug  4 20:36:17.109: INFO: Found Service test-service-64rlq in namespace services-6793 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Aug  4 20:36:17.109: INFO: Service test-service-64rlq created
  STEP: Getting /status @ 08/04/23 20:36:17.109
  Aug  4 20:36:17.114: INFO: Service test-service-64rlq has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 08/04/23 20:36:17.114
  STEP: watching for the Service to be patched @ 08/04/23 20:36:17.118
  Aug  4 20:36:17.119: INFO: observed Service test-service-64rlq in namespace services-6793 with annotations: map[] & LoadBalancer: {[]}
  Aug  4 20:36:17.120: INFO: Found Service test-service-64rlq in namespace services-6793 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Aug  4 20:36:17.120: INFO: Service test-service-64rlq has service status patched
  STEP: updating the ServiceStatus @ 08/04/23 20:36:17.12
  Aug  4 20:36:17.125: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 08/04/23 20:36:17.126
  Aug  4 20:36:17.127: INFO: Observed Service test-service-64rlq in namespace services-6793 with annotations: map[] & Conditions: {[]}
  Aug  4 20:36:17.127: INFO: Observed event: &Service{ObjectMeta:{test-service-64rlq  services-6793  9b053c99-b21e-467a-b9ea-b91799bc12c9 16201 0 2023-08-04 20:36:17 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-08-04 20:36:17 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-08-04 20:36:17 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.99.186.90,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.99.186.90],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Aug  4 20:36:17.127: INFO: Found Service test-service-64rlq in namespace services-6793 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug  4 20:36:17.127: INFO: Service test-service-64rlq has service status updated
  STEP: patching the service @ 08/04/23 20:36:17.127
  STEP: watching for the Service to be patched @ 08/04/23 20:36:17.137
  Aug  4 20:36:17.138: INFO: observed Service test-service-64rlq in namespace services-6793 with labels: map[test-service-static:true]
  Aug  4 20:36:17.138: INFO: observed Service test-service-64rlq in namespace services-6793 with labels: map[test-service-static:true]
  Aug  4 20:36:17.138: INFO: observed Service test-service-64rlq in namespace services-6793 with labels: map[test-service-static:true]
  Aug  4 20:36:17.139: INFO: Found Service test-service-64rlq in namespace services-6793 with labels: map[test-service:patched test-service-static:true]
  Aug  4 20:36:17.139: INFO: Service test-service-64rlq patched
  STEP: deleting the service @ 08/04/23 20:36:17.139
  STEP: watching for the Service to be deleted @ 08/04/23 20:36:17.147
  Aug  4 20:36:17.148: INFO: Observed event: ADDED
  Aug  4 20:36:17.149: INFO: Observed event: MODIFIED
  Aug  4 20:36:17.149: INFO: Observed event: MODIFIED
  Aug  4 20:36:17.149: INFO: Observed event: MODIFIED
  Aug  4 20:36:17.149: INFO: Found Service test-service-64rlq in namespace services-6793 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Aug  4 20:36:17.149: INFO: Service test-service-64rlq deleted
  Aug  4 20:36:17.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6793" for this suite. @ 08/04/23 20:36:17.151
• [0.070 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 08/04/23 20:36:17.155
  Aug  4 20:36:17.155: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:36:17.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:17.164
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:17.165
  STEP: Setting up server cert @ 08/04/23 20:36:17.179
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:36:17.545
  STEP: Deploying the webhook pod @ 08/04/23 20:36:17.55
  STEP: Wait for the deployment to be ready @ 08/04/23 20:36:17.561
  Aug  4 20:36:17.564: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0804 20:36:18.077105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:19.077340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:36:19.571
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:36:19.58
  E0804 20:36:20.077735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:36:20.581: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug  4 20:36:20.584: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:36:21.077921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-6263-crds.webhook.example.com via the AdmissionRegistration API @ 08/04/23 20:36:21.092
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/04/23 20:36:21.105
  E0804 20:36:22.078919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:23.079168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:36:23.133: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3854" for this suite. @ 08/04/23 20:36:23.738
  STEP: Destroying namespace "webhook-markers-4411" for this suite. @ 08/04/23 20:36:23.742
• [6.591 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 08/04/23 20:36:23.746
  Aug  4 20:36:23.746: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename runtimeclass @ 08/04/23 20:36:23.746
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:23.755
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:23.757
  STEP: Deleting RuntimeClass runtimeclass-5942-delete-me @ 08/04/23 20:36:23.762
  STEP: Waiting for the RuntimeClass to disappear @ 08/04/23 20:36:23.765
  Aug  4 20:36:23.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5942" for this suite. @ 08/04/23 20:36:23.772
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 08/04/23 20:36:23.776
  Aug  4 20:36:23.776: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/04/23 20:36:23.776
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:23.785
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:23.787
  STEP: creating @ 08/04/23 20:36:23.789
  STEP: getting @ 08/04/23 20:36:23.8
  STEP: listing in namespace @ 08/04/23 20:36:23.803
  STEP: patching @ 08/04/23 20:36:23.805
  STEP: deleting @ 08/04/23 20:36:23.809
  Aug  4 20:36:23.815: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-3089" for this suite. @ 08/04/23 20:36:23.819
• [0.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 08/04/23 20:36:23.824
  Aug  4 20:36:23.825: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:36:23.825
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:23.834
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:23.836
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:36:23.838
  E0804 20:36:24.080062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:25.080348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:36:25.851
  Aug  4 20:36:25.857: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-a40823d7-8e55-4a7c-9e71-1788c716dbce container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:36:25.863
  Aug  4 20:36:25.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9788" for this suite. @ 08/04/23 20:36:25.878
• [2.059 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 08/04/23 20:36:25.885
  Aug  4 20:36:25.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:36:25.886
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:36:25.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:36:25.896
  STEP: Creating pod liveness-4322f32f-d121-4312-84f0-4c88e716caae in namespace container-probe-7504 @ 08/04/23 20:36:25.898
  E0804 20:36:26.080748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:27.081520      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:36:27.910: INFO: Started pod liveness-4322f32f-d121-4312-84f0-4c88e716caae in namespace container-probe-7504
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/04/23 20:36:27.91
  Aug  4 20:36:27.912: INFO: Initial restart count of pod liveness-4322f32f-d121-4312-84f0-4c88e716caae is 0
  E0804 20:36:28.082152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:29.082439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:30.082912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:31.083240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:32.083267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:33.083508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:34.083573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:35.083824      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:36.084464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:37.084949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:38.085441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:39.085712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:40.085796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:41.086133      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:42.086476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:43.086682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:44.087243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:45.088049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:46.088592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:47.089231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:48.089821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:49.090037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:50.091026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:51.091394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:52.092028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:53.092245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:54.092616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:55.092677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:56.093309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:57.093545      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:58.094530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:36:59.094764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:00.095093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:01.095540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:02.096078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:03.096301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:04.096775      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:05.097000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:06.097479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:07.098026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:08.098387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:09.098745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:10.099105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:11.099325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:12.100060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:13.100317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:14.101028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:15.101248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:16.102282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:17.102774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:18.103421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:19.104030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:20.104691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:21.104907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:22.105134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:23.105341      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:24.106027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:25.106302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:26.107048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:27.108055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:28.108459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:29.108677      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:30.109753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:31.109890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:32.110708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:33.110941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:34.111881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:35.112087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:36.112980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:37.113519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:38.113988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:39.114241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:40.115296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:41.115420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:42.116056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:43.116282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:44.116912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:45.117183      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:46.117590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:47.118305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:48.119239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:49.120050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:50.120903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:51.121170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:52.121712      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:53.121909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:54.122390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:55.123465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:56.123931      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:57.124558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:58.125101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:37:59.125345      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:00.126145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:01.126225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:02.127089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:03.127326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:04.128001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:05.128212      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:06.128514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:07.129163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:08.129901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:09.130161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:10.131203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:11.132213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:12.132663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:13.132794      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:14.133472      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:15.133743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:16.134334      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:17.134927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:18.135787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:19.136051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:20.136599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:21.136821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:22.137734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:23.137988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:24.138691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:25.139131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:26.139997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:27.140662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:28.141381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:29.141638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:30.142376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:31.142849      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:32.143330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:33.144087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:34.144539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:35.144769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:36.144998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:37.145535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:38.145990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:39.146225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:40.147285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:41.147522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:42.148030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:43.149108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:44.149692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:45.149904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:46.150394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:47.151274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:48.151476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:49.151700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:50.151749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:51.152026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:52.152791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:53.153050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:54.153789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:55.154055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:56.154552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:57.155213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:58.155473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:38:59.156038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:00.157054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:01.157236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:02.157850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:03.158045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:04.158579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:05.158772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:06.158989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:07.159585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:08.160227      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:09.160451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:10.161182      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:11.161933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:12.162563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:13.162986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:14.163170      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:15.163275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:16.164075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:17.164300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:18.164533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:19.165312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:20.165407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:21.166244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:22.166671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:23.166968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:24.168054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:25.168802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:26.169135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:27.169634      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:28.169769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:29.170445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:30.170704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:31.171686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:32.172124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:33.172760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:34.173026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:35.173624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:36.173889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:37.174057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:38.174365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:39.175239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:40.176086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:41.176357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:42.176805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:43.176940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:44.177323      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:45.178144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:46.179207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:47.180100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:48.180214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:49.181012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:50.181211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:51.181300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:52.181679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:53.182050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:54.182286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:55.183241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:56.184086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:57.185114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:58.185369      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:39:59.185492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:00.185729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:01.185820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:02.186345      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:03.186966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:04.187235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:05.188086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:06.188353      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:07.189029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:08.189346      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:09.190295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:10.190586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:11.191343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:12.191798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:13.192479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:14.192732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:15.193675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:16.193914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:17.194101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:18.194347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:19.195002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:20.195256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:21.196113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:22.196579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:23.197000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:24.197237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:25.198045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:26.198321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:27.198619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:28.198938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:28.282: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:40:28.285
  STEP: Destroying namespace "container-probe-7504" for this suite. @ 08/04/23 20:40:28.296
• [242.416 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 08/04/23 20:40:28.301
  Aug  4 20:40:28.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename controllerrevisions @ 08/04/23 20:40:28.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:28.313
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:28.316
  STEP: Creating DaemonSet "e2e-lbqvz-daemon-set" @ 08/04/23 20:40:28.332
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/04/23 20:40:28.336
  Aug  4 20:40:28.343: INFO: Number of nodes with available pods controlled by daemonset e2e-lbqvz-daemon-set: 0
  Aug  4 20:40:28.343: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:40:29.199675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:29.349: INFO: Number of nodes with available pods controlled by daemonset e2e-lbqvz-daemon-set: 0
  Aug  4 20:40:29.349: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:40:30.200168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:30.349: INFO: Number of nodes with available pods controlled by daemonset e2e-lbqvz-daemon-set: 2
  Aug  4 20:40:30.349: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset e2e-lbqvz-daemon-set
  STEP: Confirm DaemonSet "e2e-lbqvz-daemon-set" successfully created with "daemonset-name=e2e-lbqvz-daemon-set" label @ 08/04/23 20:40:30.351
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-lbqvz-daemon-set" @ 08/04/23 20:40:30.355
  Aug  4 20:40:30.357: INFO: Located ControllerRevision: "e2e-lbqvz-daemon-set-6c558b69b8"
  STEP: Patching ControllerRevision "e2e-lbqvz-daemon-set-6c558b69b8" @ 08/04/23 20:40:30.36
  Aug  4 20:40:30.366: INFO: e2e-lbqvz-daemon-set-6c558b69b8 has been patched
  STEP: Create a new ControllerRevision @ 08/04/23 20:40:30.366
  Aug  4 20:40:30.371: INFO: Created ControllerRevision: e2e-lbqvz-daemon-set-56499f65d
  STEP: Confirm that there are two ControllerRevisions @ 08/04/23 20:40:30.371
  Aug  4 20:40:30.371: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  4 20:40:30.374: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-lbqvz-daemon-set-6c558b69b8" @ 08/04/23 20:40:30.374
  STEP: Confirm that there is only one ControllerRevision @ 08/04/23 20:40:30.378
  Aug  4 20:40:30.378: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  4 20:40:30.380: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-lbqvz-daemon-set-56499f65d" @ 08/04/23 20:40:30.382
  Aug  4 20:40:30.390: INFO: e2e-lbqvz-daemon-set-56499f65d has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 08/04/23 20:40:30.39
  W0804 20:40:30.395139      22 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 08/04/23 20:40:30.395
  Aug  4 20:40:30.395: INFO: Requesting list of ControllerRevisions to confirm quantity
  E0804 20:40:31.200579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:31.398: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  4 20:40:31.400: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-lbqvz-daemon-set-56499f65d=updated" @ 08/04/23 20:40:31.4
  STEP: Confirm that there is only one ControllerRevision @ 08/04/23 20:40:31.405
  Aug  4 20:40:31.405: INFO: Requesting list of ControllerRevisions to confirm quantity
  Aug  4 20:40:31.407: INFO: Found 1 ControllerRevisions
  Aug  4 20:40:31.409: INFO: ControllerRevision "e2e-lbqvz-daemon-set-59db54bd79" has revision 3
  STEP: Deleting DaemonSet "e2e-lbqvz-daemon-set" @ 08/04/23 20:40:31.411
  STEP: deleting DaemonSet.extensions e2e-lbqvz-daemon-set in namespace controllerrevisions-716, will wait for the garbage collector to delete the pods @ 08/04/23 20:40:31.411
  Aug  4 20:40:31.469: INFO: Deleting DaemonSet.extensions e2e-lbqvz-daemon-set took: 7.052909ms
  Aug  4 20:40:31.570: INFO: Terminating DaemonSet.extensions e2e-lbqvz-daemon-set pods took: 100.195453ms
  E0804 20:40:32.201080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:33.201485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:33.572: INFO: Number of nodes with available pods controlled by daemonset e2e-lbqvz-daemon-set: 0
  Aug  4 20:40:33.572: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-lbqvz-daemon-set
  Aug  4 20:40:33.574: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"16707"},"items":null}

  Aug  4 20:40:33.575: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"16707"},"items":null}

  Aug  4 20:40:33.581: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-716" for this suite. @ 08/04/23 20:40:33.583
• [5.285 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 08/04/23 20:40:33.587
  Aug  4 20:40:33.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:40:33.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:33.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:33.598
  STEP: Setting up server cert @ 08/04/23 20:40:33.61
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:40:33.898
  STEP: Deploying the webhook pod @ 08/04/23 20:40:33.903
  STEP: Wait for the deployment to be ready @ 08/04/23 20:40:33.912
  Aug  4 20:40:33.919: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 20:40:34.201643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:35.202035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:40:35.926
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:40:35.934
  E0804 20:40:36.202934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:36.935: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 08/04/23 20:40:36.937
  STEP: create a pod that should be updated by the webhook @ 08/04/23 20:40:36.948
  Aug  4 20:40:36.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4738" for this suite. @ 08/04/23 20:40:36.99
  STEP: Destroying namespace "webhook-markers-8952" for this suite. @ 08/04/23 20:40:36.994
• [3.411 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 08/04/23 20:40:36.999
  Aug  4 20:40:36.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:40:36.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:37.006
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:37.008
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:40:37.01
  E0804 20:40:37.203116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:38.203493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:39.204333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:40.204565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:40:41.024
  Aug  4 20:40:41.026: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-37e0f5df-71d0-4e30-ba4f-d175a7d358e6 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:40:41.038
  Aug  4 20:40:41.048: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3214" for this suite. @ 08/04/23 20:40:41.05
• [4.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 08/04/23 20:40:41.056
  Aug  4 20:40:41.056: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename podtemplate @ 08/04/23 20:40:41.056
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:41.064
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:41.065
  STEP: Create a pod template @ 08/04/23 20:40:41.067
  STEP: Replace a pod template @ 08/04/23 20:40:41.071
  Aug  4 20:40:41.076: INFO: Found updated podtemplate annotation: "true"

  Aug  4 20:40:41.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3210" for this suite. @ 08/04/23 20:40:41.078
• [0.025 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 08/04/23 20:40:41.082
  Aug  4 20:40:41.082: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:40:41.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:41.089
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:41.09
  Aug  4 20:40:41.093: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-3869 create -f -'
  E0804 20:40:41.205470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:41.368: INFO: stderr: ""
  Aug  4 20:40:41.369: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Aug  4 20:40:41.369: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-3869 create -f -'
  Aug  4 20:40:41.588: INFO: stderr: ""
  Aug  4 20:40:41.588: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/04/23 20:40:41.588
  E0804 20:40:42.205587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:42.591: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 20:40:42.591: INFO: Found 1 / 1
  Aug  4 20:40:42.591: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Aug  4 20:40:42.593: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 20:40:42.593: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug  4 20:40:42.593: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-3869 describe pod agnhost-primary-c946r'
  Aug  4 20:40:42.654: INFO: stderr: ""
  Aug  4 20:40:42.654: INFO: stdout: "Name:             agnhost-primary-c946r\nNamespace:        kubectl-3869\nPriority:         0\nService Account:  default\nNode:             k8sconformance-m02/192.168.49.3\nStart Time:       Fri, 04 Aug 2023 20:40:41 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.244.1.69\nIPs:\n  IP:           10.244.1.69\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   docker://c9e98037fc890feae4e4a1d71916d54d26a8a0bd8bab8f71c1716b7bc91d4fb1\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Fri, 04 Aug 2023 20:40:41 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-ddbk2 (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-ddbk2:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  1s    default-scheduler  Successfully assigned kubectl-3869/agnhost-primary-c946r to k8sconformance-m02\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Aug  4 20:40:42.654: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-3869 describe rc agnhost-primary'
  Aug  4 20:40:42.717: INFO: stderr: ""
  Aug  4 20:40:42.717: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-3869\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  1s    replication-controller  Created pod: agnhost-primary-c946r\n"
  Aug  4 20:40:42.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-3869 describe service agnhost-primary'
  Aug  4 20:40:42.776: INFO: stderr: ""
  Aug  4 20:40:42.776: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-3869\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.100.245.142\nIPs:               10.100.245.142\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.244.1.69:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Aug  4 20:40:42.779: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-3869 describe node k8sconformance'
  Aug  4 20:40:42.853: INFO: stderr: ""
  Aug  4 20:40:42.853: INFO: stdout: "Name:               k8sconformance\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=k8sconformance\n                    kubernetes.io/os=linux\n                    minikube.k8s.io/commit=e4823aa495551ce86337c9c1a7344a96cc604a34\n                    minikube.k8s.io/name=k8sconformance\n                    minikube.k8s.io/primary=true\n                    minikube.k8s.io/updated_at=2023_08_04T19_46_46_0700\n                    minikube.k8s.io/version=v1.31.1\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/cri-dockerd.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Fri, 04 Aug 2023 19:46:43 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  k8sconformance\n  AcquireTime:     <unset>\n  RenewTime:       Fri, 04 Aug 2023 20:40:37 +0000\nConditions:\n  Type             Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----             ------  -----------------                 ------------------                ------                       -------\n  MemoryPressure   False   Fri, 04 Aug 2023 20:37:00 +0000   Fri, 04 Aug 2023 19:46:41 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure     False   Fri, 04 Aug 2023 20:37:00 +0000   Fri, 04 Aug 2023 19:46:41 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure      False   Fri, 04 Aug 2023 20:37:00 +0000   Fri, 04 Aug 2023 19:46:41 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready            True    Fri, 04 Aug 2023 20:37:00 +0000   Fri, 04 Aug 2023 19:46:46 +0000   KubeletReady                 kubelet is posting ready status\nAddresses:\n  InternalIP:  192.168.49.2\n  Hostname:    k8sconformance\nCapacity:\n  cpu:                    8\n  ephemeral-storage:      304681132Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 32859436Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nAllocatable:\n  cpu:                    8\n  ephemeral-storage:      304681132Ki\n  hugepages-1Gi:          0\n  hugepages-2Mi:          0\n  memory:                 32859436Ki\n  pods:                   110\n  scheduling.k8s.io/foo:  5\nSystem Info:\n  Machine ID:                 2174407b9f1f4f1187fbff59059ee36a\n  System UUID:                a70d6f0a-373f-4953-acd5-a8d3afb039e3\n  Boot ID:                    2eb4e049-c492-41ec-b178-09310ed5c01e\n  Kernel Version:             5.15.0-1038-gcp\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  docker://24.0.5\n  Kubelet Version:            v1.27.3\n  Kube-Proxy Version:         v1.27.3\nPodCIDR:                      10.244.0.0/24\nPodCIDRs:                     10.244.0.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 coredns-5d78c9869d-zl979                                   100m (1%)     0 (0%)      70Mi (0%)        170Mi (0%)     53m\n  kube-system                 etcd-k8sconformance                                        100m (1%)     0 (0%)      100Mi (0%)       0 (0%)         53m\n  kube-system                 kindnet-5zl6d                                              100m (1%)     100m (1%)   50Mi (0%)        50Mi (0%)      53m\n  kube-system                 kube-apiserver-k8sconformance                              250m (3%)     0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-controller-manager-k8sconformance                     200m (2%)     0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-proxy-mslwk                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 kube-scheduler-k8sconformance                              100m (1%)     0 (0%)      0 (0%)           0 (0%)         53m\n  kube-system                 storage-provisioner                                        0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-fdn76    0 (0%)        0 (0%)      0 (0%)           0 (0%)         53m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource               Requests    Limits\n  --------               --------    ------\n  cpu                    850m (10%)  100m (1%)\n  memory                 220Mi (0%)  220Mi (0%)\n  ephemeral-storage      0 (0%)      0 (0%)\n  hugepages-1Gi          0 (0%)      0 (0%)\n  hugepages-2Mi          0 (0%)      0 (0%)\n  scheduling.k8s.io/foo  0           0\nEvents:\n  Type    Reason                   Age                From             Message\n  ----    ------                   ----               ----             -------\n  Normal  Starting                 53m                kube-proxy       \n  Normal  Starting                 54m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  54m (x8 over 54m)  kubelet          Node k8sconformance status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    54m (x8 over 54m)  kubelet          Node k8sconformance status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     54m (x7 over 54m)  kubelet          Node k8sconformance status is now: NodeHasSufficientPID\n  Normal  NodeAllocatableEnforced  54m                kubelet          Updated Node Allocatable limit across pods\n  Normal  Starting                 53m                kubelet          Starting kubelet.\n  Normal  NodeHasSufficientMemory  53m                kubelet          Node k8sconformance status is now: NodeHasSufficientMemory\n  Normal  NodeHasNoDiskPressure    53m                kubelet          Node k8sconformance status is now: NodeHasNoDiskPressure\n  Normal  NodeHasSufficientPID     53m                kubelet          Node k8sconformance status is now: NodeHasSufficientPID\n  Normal  NodeNotReady             53m                kubelet          Node k8sconformance status is now: NodeNotReady\n  Normal  NodeAllocatableEnforced  53m                kubelet          Updated Node Allocatable limit across pods\n  Normal  NodeReady                53m                kubelet          Node k8sconformance status is now: NodeReady\n  Normal  RegisteredNode           53m                node-controller  Node k8sconformance event: Registered Node k8sconformance in Controller\n"
  Aug  4 20:40:42.853: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-3869 describe namespace kubectl-3869'
  Aug  4 20:40:42.914: INFO: stderr: ""
  Aug  4 20:40:42.914: INFO: stdout: "Name:         kubectl-3869\nLabels:       e2e-framework=kubectl\n              e2e-run=aa536a21-1f67-4537-80d5-a1f09aaa06c5\n              kubernetes.io/metadata.name=kubectl-3869\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Aug  4 20:40:42.914: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3869" for this suite. @ 08/04/23 20:40:42.916
• [1.838 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 08/04/23 20:40:42.921
  Aug  4 20:40:42.921: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:40:42.922
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:42.929
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:42.932
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:40:42.933
  E0804 20:40:43.206127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:44.206891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:45.207397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:46.208073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:40:46.946
  Aug  4 20:40:46.947: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-efeb635f-3175-4207-8b0f-233285bd59a8 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:40:46.953
  Aug  4 20:40:46.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8533" for this suite. @ 08/04/23 20:40:46.964
• [4.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 08/04/23 20:40:46.969
  Aug  4 20:40:46.969: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename events @ 08/04/23 20:40:46.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:46.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:46.979
  STEP: Create set of events @ 08/04/23 20:40:46.981
  STEP: get a list of Events with a label in the current namespace @ 08/04/23 20:40:46.991
  STEP: delete a list of events @ 08/04/23 20:40:46.993
  Aug  4 20:40:46.993: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 08/04/23 20:40:47.001
  Aug  4 20:40:47.003: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8223" for this suite. @ 08/04/23 20:40:47.005
• [0.039 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 08/04/23 20:40:47.008
  Aug  4 20:40:47.008: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename security-context-test @ 08/04/23 20:40:47.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:47.016
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:47.018
  E0804 20:40:47.208840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:48.209920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:49.210719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:50.210984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:51.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6332" for this suite. @ 08/04/23 20:40:51.034
• [4.029 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 08/04/23 20:40:51.038
  Aug  4 20:40:51.038: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:40:51.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:51.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:51.048
  Aug  4 20:40:51.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: creating the pod @ 08/04/23 20:40:51.05
  STEP: submitting the pod to kubernetes @ 08/04/23 20:40:51.05
  E0804 20:40:51.212054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:52.212226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:53.071: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7818" for this suite. @ 08/04/23 20:40:53.073
• [2.039 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 08/04/23 20:40:53.077
  Aug  4 20:40:53.077: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replicaset @ 08/04/23 20:40:53.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:53.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:53.089
  STEP: Create a Replicaset @ 08/04/23 20:40:53.093
  STEP: Verify that the required pods have come up. @ 08/04/23 20:40:53.097
  Aug  4 20:40:53.098: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0804 20:40:53.213136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:54.213393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:55.213610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:56.213830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:57.214378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:40:58.101: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/04/23 20:40:58.101
  STEP: Getting /status @ 08/04/23 20:40:58.101
  Aug  4 20:40:58.103: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 08/04/23 20:40:58.104
  Aug  4 20:40:58.110: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 08/04/23 20:40:58.11
  Aug  4 20:40:58.112: INFO: Observed &ReplicaSet event: ADDED
  Aug  4 20:40:58.112: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  4 20:40:58.112: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  4 20:40:58.112: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  4 20:40:58.112: INFO: Found replicaset test-rs in namespace replicaset-6424 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Aug  4 20:40:58.112: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 08/04/23 20:40:58.112
  Aug  4 20:40:58.112: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug  4 20:40:58.117: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 08/04/23 20:40:58.117
  Aug  4 20:40:58.119: INFO: Observed &ReplicaSet event: ADDED
  Aug  4 20:40:58.119: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  4 20:40:58.119: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  4 20:40:58.119: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  4 20:40:58.119: INFO: Observed replicaset test-rs in namespace replicaset-6424 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  4 20:40:58.119: INFO: Observed &ReplicaSet event: MODIFIED
  Aug  4 20:40:58.119: INFO: Found replicaset test-rs in namespace replicaset-6424 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Aug  4 20:40:58.119: INFO: Replicaset test-rs has a patched status
  Aug  4 20:40:58.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6424" for this suite. @ 08/04/23 20:40:58.121
• [5.048 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 08/04/23 20:40:58.125
  Aug  4 20:40:58.125: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename server-version @ 08/04/23 20:40:58.126
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:58.134
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:58.137
  STEP: Request ServerVersion @ 08/04/23 20:40:58.139
  STEP: Confirm major version @ 08/04/23 20:40:58.139
  Aug  4 20:40:58.139: INFO: Major version: 1
  STEP: Confirm minor version @ 08/04/23 20:40:58.139
  Aug  4 20:40:58.139: INFO: cleanMinorVersion: 27
  Aug  4 20:40:58.139: INFO: Minor version: 27
  Aug  4 20:40:58.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-3190" for this suite. @ 08/04/23 20:40:58.141
• [0.019 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 08/04/23 20:40:58.145
  Aug  4 20:40:58.145: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replication-controller @ 08/04/23 20:40:58.146
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:40:58.155
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:40:58.156
  STEP: Creating replication controller my-hostname-basic-a65ae93f-5cc8-49ee-9279-6d3ab2e9e5bf @ 08/04/23 20:40:58.159
  Aug  4 20:40:58.163: INFO: Pod name my-hostname-basic-a65ae93f-5cc8-49ee-9279-6d3ab2e9e5bf: Found 0 pods out of 1
  E0804 20:40:58.215178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:40:59.215558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:00.215803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:01.216071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:02.216445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:41:03.167: INFO: Pod name my-hostname-basic-a65ae93f-5cc8-49ee-9279-6d3ab2e9e5bf: Found 1 pods out of 1
  Aug  4 20:41:03.167: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-a65ae93f-5cc8-49ee-9279-6d3ab2e9e5bf" are running
  Aug  4 20:41:03.170: INFO: Pod "my-hostname-basic-a65ae93f-5cc8-49ee-9279-6d3ab2e9e5bf-pgn4z" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-04 20:40:58 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-04 20:40:59 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-04 20:40:59 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-08-04 20:40:58 +0000 UTC Reason: Message:}])
  Aug  4 20:41:03.170: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 08/04/23 20:41:03.17
  Aug  4 20:41:03.176: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2440" for this suite. @ 08/04/23 20:41:03.178
• [5.037 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 08/04/23 20:41:03.182
  Aug  4 20:41:03.182: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename var-expansion @ 08/04/23 20:41:03.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:41:03.208
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:41:03.21
  STEP: creating the pod @ 08/04/23 20:41:03.213
  E0804 20:41:03.217048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for pod running @ 08/04/23 20:41:03.219
  E0804 20:41:04.217803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:05.218094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating a file in subpath @ 08/04/23 20:41:05.225
  Aug  4 20:41:05.227: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-2597 PodName:var-expansion-7c11bd69-6e66-40df-bee0-3d29c8a241f3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:41:05.227: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:41:05.228: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:41:05.228: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2597/pods/var-expansion-7c11bd69-6e66-40df-bee0-3d29c8a241f3/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 08/04/23 20:41:05.299
  Aug  4 20:41:05.301: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-2597 PodName:var-expansion-7c11bd69-6e66-40df-bee0-3d29c8a241f3 ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:41:05.301: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:41:05.302: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:41:05.302: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/var-expansion-2597/pods/var-expansion-7c11bd69-6e66-40df-bee0-3d29c8a241f3/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 08/04/23 20:41:05.39
  Aug  4 20:41:05.900: INFO: Successfully updated pod "var-expansion-7c11bd69-6e66-40df-bee0-3d29c8a241f3"
  STEP: waiting for annotated pod running @ 08/04/23 20:41:05.9
  STEP: deleting the pod gracefully @ 08/04/23 20:41:05.902
  Aug  4 20:41:05.902: INFO: Deleting pod "var-expansion-7c11bd69-6e66-40df-bee0-3d29c8a241f3" in namespace "var-expansion-2597"
  Aug  4 20:41:05.907: INFO: Wait up to 5m0s for pod "var-expansion-7c11bd69-6e66-40df-bee0-3d29c8a241f3" to be fully deleted
  E0804 20:41:06.218391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:07.218426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:08.219190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:09.219471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:10.220027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:11.220329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:12.221274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:13.221402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:14.221889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:15.222118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:16.222572      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:17.222972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:18.223016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:19.224077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:20.224654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:21.224899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:22.224934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:23.225145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:24.225713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:25.225899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:26.226759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:27.227074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:28.227321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:29.228036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:30.228754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:31.228994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:32.229473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:33.229723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:34.230159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:35.230429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:36.230889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:37.230967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:41:37.957: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-2597" for this suite. @ 08/04/23 20:41:37.961
• [34.782 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 08/04/23 20:41:37.965
  Aug  4 20:41:37.965: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 20:41:37.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:41:37.974
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:41:37.976
  STEP: Creating configMap with name configmap-test-upd-7d23d98b-8d53-4850-8853-fa53279d88e1 @ 08/04/23 20:41:37.98
  STEP: Creating the pod @ 08/04/23 20:41:37.985
  E0804 20:41:38.232005      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:39.232263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap configmap-test-upd-7d23d98b-8d53-4850-8853-fa53279d88e1 @ 08/04/23 20:41:40.005
  STEP: waiting to observe update in volume @ 08/04/23 20:41:40.008
  E0804 20:41:40.232790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:41.233046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:42.233881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:43.234119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:41:44.034: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7337" for this suite. @ 08/04/23 20:41:44.036
• [6.076 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 08/04/23 20:41:44.042
  Aug  4 20:41:44.042: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 20:41:44.043
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:41:44.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:41:44.054
  STEP: Creating configMap configmap-1980/configmap-test-1075b598-a72c-4afe-9b08-9cf0f628a934 @ 08/04/23 20:41:44.057
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:41:44.06
  E0804 20:41:44.234676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:45.235007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:46.235830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:47.236352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:41:48.075
  Aug  4 20:41:48.077: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-5b821212-69af-4945-9721-335b8dfe8d11 container env-test: <nil>
  STEP: delete the pod @ 08/04/23 20:41:48.083
  Aug  4 20:41:48.092: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-1980" for this suite. @ 08/04/23 20:41:48.094
• [4.056 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:344
  STEP: Creating a kubernetes client @ 08/04/23 20:41:48.098
  Aug  4 20:41:48.098: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename field-validation @ 08/04/23 20:41:48.098
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:41:48.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:41:48.108
  Aug  4 20:41:48.110: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  W0804 20:41:48.111237      22 field_validation.go:417] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc00141dc90 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  E0804 20:41:48.237262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:49.237573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:50.237760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0804 20:41:50.642058      22 warnings.go:70] unknown field "alpha"
  W0804 20:41:50.642079      22 warnings.go:70] unknown field "beta"
  W0804 20:41:50.642082      22 warnings.go:70] unknown field "delta"
  W0804 20:41:50.642086      22 warnings.go:70] unknown field "epsilon"
  W0804 20:41:50.642089      22 warnings.go:70] unknown field "gamma"
  Aug  4 20:41:50.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7937" for this suite. @ 08/04/23 20:41:50.657
• [2.562 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 08/04/23 20:41:50.66
  Aug  4 20:41:50.660: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename init-container @ 08/04/23 20:41:50.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:41:50.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:41:50.671
  STEP: creating the pod @ 08/04/23 20:41:50.672
  Aug  4 20:41:50.672: INFO: PodSpec: initContainers in spec.initContainers
  E0804 20:41:51.238675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:52.239469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:53.240091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:41:53.804: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-321" for this suite. @ 08/04/23 20:41:53.808
• [3.151 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 08/04/23 20:41:53.813
  Aug  4 20:41:53.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:41:53.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:41:53.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:41:53.825
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:41:53.828
  E0804 20:41:54.240583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:55.241403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:56.241768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:57.242330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:41:57.842
  Aug  4 20:41:57.844: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-289d61de-517b-423e-bf92-6fd44019d047 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:41:57.851
  Aug  4 20:41:57.861: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1398" for this suite. @ 08/04/23 20:41:57.863
• [4.054 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 08/04/23 20:41:57.867
  Aug  4 20:41:57.867: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 20:41:57.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:41:57.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:41:57.877
  STEP: Creating a ResourceQuota with terminating scope @ 08/04/23 20:41:57.879
  STEP: Ensuring ResourceQuota status is calculated @ 08/04/23 20:41:57.883
  E0804 20:41:58.243214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:41:59.244149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not terminating scope @ 08/04/23 20:41:59.886
  STEP: Ensuring ResourceQuota status is calculated @ 08/04/23 20:41:59.889
  E0804 20:42:00.244550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:01.244865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a long running pod @ 08/04/23 20:42:01.892
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 08/04/23 20:42:01.902
  E0804 20:42:02.245749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:03.245994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 08/04/23 20:42:03.905
  E0804 20:42:04.246943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:05.247145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/04/23 20:42:05.908
  STEP: Ensuring resource quota status released the pod usage @ 08/04/23 20:42:05.915
  E0804 20:42:06.247813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:07.248470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a terminating pod @ 08/04/23 20:42:07.918
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 08/04/23 20:42:07.926
  E0804 20:42:08.249406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:09.249623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 08/04/23 20:42:09.928
  E0804 20:42:10.250317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:11.250546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/04/23 20:42:11.931
  STEP: Ensuring resource quota status released the pod usage @ 08/04/23 20:42:11.936
  E0804 20:42:12.251126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:13.252182      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:42:13.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8540" for this suite. @ 08/04/23 20:42:13.942
• [16.079 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 08/04/23 20:42:13.947
  Aug  4 20:42:13.947: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:42:13.947
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:42:13.955
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:42:13.957
  STEP: Setting up server cert @ 08/04/23 20:42:13.97
  E0804 20:42:14.252293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:42:14.264
  STEP: Deploying the webhook pod @ 08/04/23 20:42:14.268
  STEP: Wait for the deployment to be ready @ 08/04/23 20:42:14.277
  Aug  4 20:42:14.282: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 20:42:15.253097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:16.253338      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:42:16.29
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:42:16.299
  E0804 20:42:17.254139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:42:17.300: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/04/23 20:42:17.302
  STEP: create a pod @ 08/04/23 20:42:17.315
  E0804 20:42:18.254743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:19.254966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 08/04/23 20:42:19.325
  Aug  4 20:42:19.325: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=webhook-5887 attach --namespace=webhook-5887 to-be-attached-pod -i -c=container1'
  Aug  4 20:42:19.390: INFO: rc: 1
  Aug  4 20:42:19.390: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5887" for this suite. @ 08/04/23 20:42:19.416
  STEP: Destroying namespace "webhook-markers-7848" for this suite. @ 08/04/23 20:42:19.42
• [5.476 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 08/04/23 20:42:19.424
  Aug  4 20:42:19.424: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:42:19.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:42:19.433
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:42:19.435
  STEP: Creating configMap with name projected-configmap-test-volume-9317642f-7c5c-45ec-a223-c34f23b7c323 @ 08/04/23 20:42:19.437
  STEP: Creating a pod to test consume configMaps @ 08/04/23 20:42:19.439
  E0804 20:42:20.256048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:21.256335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:22.256366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:23.256989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:42:23.454
  Aug  4 20:42:23.456: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-8d1b8f4f-42f3-48f8-b86a-e6848c289a24 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:42:23.461
  Aug  4 20:42:23.471: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3774" for this suite. @ 08/04/23 20:42:23.473
• [4.054 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 08/04/23 20:42:23.479
  Aug  4 20:42:23.479: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:42:23.48
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:42:23.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:42:23.489
  STEP: Setting up server cert @ 08/04/23 20:42:23.502
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:42:24.039
  STEP: Deploying the webhook pod @ 08/04/23 20:42:24.043
  STEP: Wait for the deployment to be ready @ 08/04/23 20:42:24.052
  Aug  4 20:42:24.058: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 20:42:24.257463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:25.257856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:42:26.064
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:42:26.072
  E0804 20:42:26.258107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:42:27.073: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 08/04/23 20:42:27.075
  STEP: create a pod that should be denied by the webhook @ 08/04/23 20:42:27.088
  STEP: create a pod that causes the webhook to hang @ 08/04/23 20:42:27.098
  E0804 20:42:27.258603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:28.258869      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:29.259114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:30.259311      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:31.259494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:32.260294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:33.260513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:34.260720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:35.260925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:36.261160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create a configmap that should be denied by the webhook @ 08/04/23 20:42:37.103
  STEP: create a configmap that should be admitted by the webhook @ 08/04/23 20:42:37.109
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/04/23 20:42:37.116
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 08/04/23 20:42:37.12
  STEP: create a namespace that bypass the webhook @ 08/04/23 20:42:37.124
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 08/04/23 20:42:37.131
  Aug  4 20:42:37.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3588" for this suite. @ 08/04/23 20:42:37.17
  STEP: Destroying namespace "webhook-markers-8876" for this suite. @ 08/04/23 20:42:37.175
  STEP: Destroying namespace "exempted-namespace-4049" for this suite. @ 08/04/23 20:42:37.179
• [13.703 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 08/04/23 20:42:37.183
  Aug  4 20:42:37.183: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:42:37.183
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:42:37.193
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:42:37.195
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:42:37.197
  E0804 20:42:37.261476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:38.261666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:39.262329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:40.262669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:42:41.215
  Aug  4 20:42:41.217: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-0249b758-1225-447e-972c-dc5c97635c3f container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:42:41.222
  Aug  4 20:42:41.233: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4785" for this suite. @ 08/04/23 20:42:41.235
• [4.056 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 08/04/23 20:42:41.239
  Aug  4 20:42:41.239: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename statefulset @ 08/04/23 20:42:41.239
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:42:41.247
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:42:41.249
  STEP: Creating service test in namespace statefulset-4563 @ 08/04/23 20:42:41.251
  STEP: Creating a new StatefulSet @ 08/04/23 20:42:41.256
  E0804 20:42:41.263034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:42:41.263: INFO: Found 0 stateful pods, waiting for 3
  E0804 20:42:42.263823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:43.264605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:44.264697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:45.264976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:46.265367      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:47.266073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:48.266303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:49.266516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:50.267159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:42:51.266: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 20:42:51.266: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 20:42:51.266: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  E0804 20:42:51.267899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/04/23 20:42:51.272
  Aug  4 20:42:51.289: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/04/23 20:42:51.289
  E0804 20:42:52.268111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:53.268396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:54.268533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:55.268746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:56.268944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:57.269114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:58.269329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:42:59.269559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:00.269781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:01.269971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not applying an update when the partition is greater than the number of replicas @ 08/04/23 20:43:01.299
  STEP: Performing a canary update @ 08/04/23 20:43:01.299
  Aug  4 20:43:01.318: INFO: Updating stateful set ss2
  Aug  4 20:43:01.321: INFO: Waiting for Pod statefulset-4563/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0804 20:43:02.270069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:03.270352      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:04.270598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:05.271523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:06.271830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:07.272477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:08.272667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:09.272854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:10.273050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:11.273273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Restoring Pods to the correct revision when they are deleted @ 08/04/23 20:43:11.327
  Aug  4 20:43:11.350: INFO: Found 1 stateful pods, waiting for 3
  E0804 20:43:12.273630      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:13.273883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:14.274114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:15.274336      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:16.274566      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:17.275209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:18.276048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:19.276289      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:20.276549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:21.276691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:43:21.354: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 20:43:21.354: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 20:43:21.354: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 08/04/23 20:43:21.357
  Aug  4 20:43:21.375: INFO: Updating stateful set ss2
  Aug  4 20:43:21.379: INFO: Waiting for Pod statefulset-4563/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0804 20:43:22.277769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:23.278054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:24.278600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:25.278691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:26.278929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:27.278987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:28.279016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:29.280030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:30.280247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:31.280454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:43:31.401: INFO: Updating stateful set ss2
  Aug  4 20:43:31.405: INFO: Waiting for StatefulSet statefulset-4563/ss2 to complete update
  Aug  4 20:43:31.405: INFO: Waiting for Pod statefulset-4563/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  E0804 20:43:32.281485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:33.281717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:34.281923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:35.282123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:36.282375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:37.283050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:38.284042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:39.284281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:40.284516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:41.285403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:43:41.410: INFO: Deleting all statefulset in ns statefulset-4563
  Aug  4 20:43:41.412: INFO: Scaling statefulset ss2 to 0
  E0804 20:43:42.285550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:43.286397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:44.286661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:45.286945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:46.287185      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:47.287997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:48.288201      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:49.288419      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:50.288636      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:51.288901      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:43:51.507: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 20:43:51.509: INFO: Deleting statefulset ss2
  Aug  4 20:43:51.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4563" for this suite. @ 08/04/23 20:43:51.517
• [70.282 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 08/04/23 20:43:51.521
  Aug  4 20:43:51.521: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename field-validation @ 08/04/23 20:43:51.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:43:51.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:43:51.53
  Aug  4 20:43:51.532: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:43:52.289840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:53.290094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0804 20:43:54.062168      22 warnings.go:70] unknown field "alpha"
  W0804 20:43:54.062194      22 warnings.go:70] unknown field "beta"
  W0804 20:43:54.062200      22 warnings.go:70] unknown field "delta"
  W0804 20:43:54.062205      22 warnings.go:70] unknown field "epsilon"
  W0804 20:43:54.062211      22 warnings.go:70] unknown field "gamma"
  Aug  4 20:43:54.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-4607" for this suite. @ 08/04/23 20:43:54.08
• [2.563 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 08/04/23 20:43:54.084
  Aug  4 20:43:54.084: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename job @ 08/04/23 20:43:54.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:43:54.094
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:43:54.096
  STEP: Creating Indexed job @ 08/04/23 20:43:54.098
  STEP: Ensuring job reaches completions @ 08/04/23 20:43:54.101
  E0804 20:43:54.291139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:55.291317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:56.291858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:57.292157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:58.292605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:43:59.292679      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring pods with index for job exist @ 08/04/23 20:44:00.104
  Aug  4 20:44:00.107: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1005" for this suite. @ 08/04/23 20:44:00.109
• [6.028 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 08/04/23 20:44:00.113
  Aug  4 20:44:00.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 20:44:00.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:00.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:00.125
  STEP: creating a secret @ 08/04/23 20:44:00.127
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 08/04/23 20:44:00.129
  STEP: patching the secret @ 08/04/23 20:44:00.131
  STEP: deleting the secret using a LabelSelector @ 08/04/23 20:44:00.137
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 08/04/23 20:44:00.141
  Aug  4 20:44:00.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-916" for this suite. @ 08/04/23 20:44:00.145
• [0.036 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 08/04/23 20:44:00.149
  Aug  4 20:44:00.149: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:44:00.149
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:00.157
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:00.159
  STEP: Setting up server cert @ 08/04/23 20:44:00.17
  E0804 20:44:00.292928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:44:00.524
  STEP: Deploying the webhook pod @ 08/04/23 20:44:00.531
  STEP: Wait for the deployment to be ready @ 08/04/23 20:44:00.538
  Aug  4 20:44:00.541: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0804 20:44:01.294025      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:02.294400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:44:02.548
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:44:02.556
  E0804 20:44:03.295210      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:03.556: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug  4 20:44:03.559: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-9101-crds.webhook.example.com via the AdmissionRegistration API @ 08/04/23 20:44:04.065
  STEP: Creating a custom resource that should be mutated by the webhook @ 08/04/23 20:44:04.078
  E0804 20:44:04.295854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:05.296200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:06.118: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0804 20:44:06.296247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-5299" for this suite. @ 08/04/23 20:44:06.657
  STEP: Destroying namespace "webhook-markers-5149" for this suite. @ 08/04/23 20:44:06.661
• [6.516 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:294
  STEP: Creating a kubernetes client @ 08/04/23 20:44:06.664
  Aug  4 20:44:06.664: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename daemonsets @ 08/04/23 20:44:06.665
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:06.675
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:06.677
  STEP: Creating a simple DaemonSet "daemon-set" @ 08/04/23 20:44:06.687
  STEP: Check that daemon pods launch on every node of the cluster. @ 08/04/23 20:44:06.691
  Aug  4 20:44:06.695: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 20:44:06.695: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:44:07.296427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:07.700: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 20:44:07.700: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:44:08.296910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:08.704: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 20:44:08.704: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 08/04/23 20:44:08.705
  Aug  4 20:44:08.719: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 20:44:08.719: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:44:09.297971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:09.724: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 20:44:09.724: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 20:44:10.298110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:10.724: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 20:44:10.724: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 08/04/23 20:44:10.724
  STEP: Deleting DaemonSet "daemon-set" @ 08/04/23 20:44:10.727
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4556, will wait for the garbage collector to delete the pods @ 08/04/23 20:44:10.727
  Aug  4 20:44:10.782: INFO: Deleting DaemonSet.extensions daemon-set took: 4.075419ms
  Aug  4 20:44:10.883: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.371394ms
  E0804 20:44:11.298934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:11.985: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 20:44:11.985: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  4 20:44:11.987: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"18166"},"items":null}

  Aug  4 20:44:11.989: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"18166"},"items":null}

  Aug  4 20:44:11.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4556" for this suite. @ 08/04/23 20:44:11.996
• [5.334 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 08/04/23 20:44:11.999
  Aug  4 20:44:11.999: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:44:12
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:12.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:12.01
  STEP: creating service multi-endpoint-test in namespace services-6129 @ 08/04/23 20:44:12.012
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6129 to expose endpoints map[] @ 08/04/23 20:44:12.021
  Aug  4 20:44:12.023: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  E0804 20:44:12.299859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:13.028: INFO: successfully validated that service multi-endpoint-test in namespace services-6129 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-6129 @ 08/04/23 20:44:13.028
  E0804 20:44:13.300381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:14.300595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6129 to expose endpoints map[pod1:[100]] @ 08/04/23 20:44:15.041
  Aug  4 20:44:15.047: INFO: successfully validated that service multi-endpoint-test in namespace services-6129 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-6129 @ 08/04/23 20:44:15.047
  E0804 20:44:15.301526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:16.301727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6129 to expose endpoints map[pod1:[100] pod2:[101]] @ 08/04/23 20:44:17.055
  Aug  4 20:44:17.063: INFO: successfully validated that service multi-endpoint-test in namespace services-6129 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 08/04/23 20:44:17.063
  Aug  4 20:44:17.063: INFO: Creating new exec pod
  E0804 20:44:17.302360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:18.303132      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:19.303484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:20.071: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-6129 exec execpodnxs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Aug  4 20:44:20.189: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Aug  4 20:44:20.189: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:44:20.189: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-6129 exec execpodnxs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.143.252 80'
  E0804 20:44:20.303848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:20.312: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.143.252 80\nConnection to 10.105.143.252 80 port [tcp/http] succeeded!\n"
  Aug  4 20:44:20.312: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:44:20.312: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-6129 exec execpodnxs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Aug  4 20:44:20.445: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Aug  4 20:44:20.445: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:44:20.445: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-6129 exec execpodnxs2j -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.105.143.252 81'
  Aug  4 20:44:20.584: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.105.143.252 81\nConnection to 10.105.143.252 81 port [tcp/*] succeeded!\n"
  Aug  4 20:44:20.584: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-6129 @ 08/04/23 20:44:20.584
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6129 to expose endpoints map[pod2:[101]] @ 08/04/23 20:44:20.593
  Aug  4 20:44:20.601: INFO: successfully validated that service multi-endpoint-test in namespace services-6129 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-6129 @ 08/04/23 20:44:20.601
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-6129 to expose endpoints map[] @ 08/04/23 20:44:20.61
  E0804 20:44:21.304704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:21.622: INFO: successfully validated that service multi-endpoint-test in namespace services-6129 exposes endpoints map[]
  Aug  4 20:44:21.622: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6129" for this suite. @ 08/04/23 20:44:21.635
• [9.640 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 08/04/23 20:44:21.639
  Aug  4 20:44:21.639: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename endpointslice @ 08/04/23 20:44:21.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:21.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:21.651
  Aug  4 20:44:21.657: INFO: Endpoints addresses: [192.168.49.2] , ports: [8443]
  Aug  4 20:44:21.657: INFO: EndpointSlices addresses: [192.168.49.2] , ports: [8443]
  Aug  4 20:44:21.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-6447" for this suite. @ 08/04/23 20:44:21.659
• [0.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 08/04/23 20:44:21.663
  Aug  4 20:44:21.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename watch @ 08/04/23 20:44:21.664
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:21.673
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:21.675
  STEP: creating a new configmap @ 08/04/23 20:44:21.676
  STEP: modifying the configmap once @ 08/04/23 20:44:21.679
  STEP: modifying the configmap a second time @ 08/04/23 20:44:21.683
  STEP: deleting the configmap @ 08/04/23 20:44:21.687
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 08/04/23 20:44:21.69
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 08/04/23 20:44:21.691
  Aug  4 20:44:21.691: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6477  6562937f-6e19-4d92-b917-3f440a2e398d 18262 0 2023-08-04 20:44:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-04 20:44:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:44:21.691: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-6477  6562937f-6e19-4d92-b917-3f440a2e398d 18263 0 2023-08-04 20:44:21 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-08-04 20:44:21 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:44:21.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-6477" for this suite. @ 08/04/23 20:44:21.693
• [0.034 seconds]
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 08/04/23 20:44:21.698
  Aug  4 20:44:21.698: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename hostport @ 08/04/23 20:44:21.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:21.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:21.707
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 08/04/23 20:44:21.711
  E0804 20:44:22.304848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:23.305049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.49.3 on the node which pod1 resides and expect scheduled @ 08/04/23 20:44:23.723
  E0804 20:44:24.305562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:25.305791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.49.3 but use UDP protocol on the node which pod2 resides @ 08/04/23 20:44:25.732
  E0804 20:44:26.306689      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:27.306970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:28.307747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:29.307891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:30.308109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:31.308373      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:32.309124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:33.309337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:34.310000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:35.310238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:36.310789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:37.310954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:38.310995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:39.312060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 08/04/23 20:44:39.764
  Aug  4 20:44:39.764: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.49.3 http://127.0.0.1:54323/hostname] Namespace:hostport-5774 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:44:39.764: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:44:39.765: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:44:39.765: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5774/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.49.3+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.49.3, port: 54323 @ 08/04/23 20:44:39.845
  Aug  4 20:44:39.845: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.49.3:54323/hostname] Namespace:hostport-5774 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:44:39.845: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:44:39.846: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:44:39.846: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5774/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.49.3%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.49.3, port: 54323 UDP @ 08/04/23 20:44:39.924
  Aug  4 20:44:39.924: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.49.3 54323] Namespace:hostport-5774 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 20:44:39.924: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 20:44:39.925: INFO: ExecWithOptions: Clientset creation
  Aug  4 20:44:39.925: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/hostport-5774/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.49.3+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  E0804 20:44:40.313042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:41.313262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:42.313632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:43.313865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:44.314059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:44.973: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-5774" for this suite. @ 08/04/23 20:44:44.976
• [23.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 08/04/23 20:44:44.982
  Aug  4 20:44:44.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 20:44:44.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:44.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:44.992
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 08/04/23 20:44:44.993
  Aug  4 20:44:44.994: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:44:45.314151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:46.234: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:44:46.314675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:47.314686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:48.314842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:49.315268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:50.315557      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:51.316294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:51.333: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7166" for this suite. @ 08/04/23 20:44:51.34
• [6.363 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 08/04/23 20:44:51.346
  Aug  4 20:44:51.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:44:51.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:51.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:51.356
  STEP: Setting up server cert @ 08/04/23 20:44:51.408
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:44:51.673
  STEP: Deploying the webhook pod @ 08/04/23 20:44:51.681
  STEP: Wait for the deployment to be ready @ 08/04/23 20:44:51.688
  Aug  4 20:44:51.694: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 20:44:52.316664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:53.316801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:44:53.701
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:44:53.709
  E0804 20:44:54.316983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:44:54.709: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/04/23 20:44:54.711
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 08/04/23 20:44:54.723
  STEP: Creating a dummy validating-webhook-configuration object @ 08/04/23 20:44:54.735
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 08/04/23 20:44:54.743
  STEP: Creating a dummy mutating-webhook-configuration object @ 08/04/23 20:44:54.746
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 08/04/23 20:44:54.752
  Aug  4 20:44:54.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-4330" for this suite. @ 08/04/23 20:44:54.791
  STEP: Destroying namespace "webhook-markers-9041" for this suite. @ 08/04/23 20:44:54.795
• [3.452 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 08/04/23 20:44:54.801
  Aug  4 20:44:54.801: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename endpointslice @ 08/04/23 20:44:54.802
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:44:54.812
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:44:54.815
  E0804 20:44:55.317342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:56.317848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:57.317997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:58.318432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:44:59.319464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing a single matching pod @ 08/04/23 20:44:59.909
  E0804 20:45:00.320106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:01.320337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:02.320846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:03.321123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:04.321344      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: referencing matching pods with named port @ 08/04/23 20:45:04.914
  E0804 20:45:05.321683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:06.321934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:07.322526      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:08.322753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:09.322980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 08/04/23 20:45:09.918
  E0804 20:45:10.323519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:11.323752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:12.324161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:13.324366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:14.324623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: recreating EndpointSlices after they've been deleted @ 08/04/23 20:45:14.923
  Aug  4 20:45:14.935: INFO: EndpointSlice for Service endpointslice-7144/example-named-port not found
  E0804 20:45:15.324681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:16.324895      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:17.325610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:18.325806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:19.326041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:20.326095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:21.326271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:22.326950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:23.327169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:24.328089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:45:24.942: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-7144" for this suite. @ 08/04/23 20:45:24.944
• [30.147 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 08/04/23 20:45:24.949
  Aug  4 20:45:24.949: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename security-context-test @ 08/04/23 20:45:24.949
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:45:24.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:45:24.96
  E0804 20:45:25.328326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:26.328618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:45:26.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6217" for this suite. @ 08/04/23 20:45:26.974
• [2.031 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 08/04/23 20:45:26.98
  Aug  4 20:45:26.980: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename events @ 08/04/23 20:45:26.981
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:45:26.988
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:45:26.991
  STEP: creating a test event @ 08/04/23 20:45:26.992
  STEP: listing events in all namespaces @ 08/04/23 20:45:26.997
  STEP: listing events in test namespace @ 08/04/23 20:45:27.001
  STEP: listing events with field selection filtering on source @ 08/04/23 20:45:27.003
  STEP: listing events with field selection filtering on reportingController @ 08/04/23 20:45:27.004
  STEP: getting the test event @ 08/04/23 20:45:27.005
  STEP: patching the test event @ 08/04/23 20:45:27.007
  STEP: getting the test event @ 08/04/23 20:45:27.014
  STEP: updating the test event @ 08/04/23 20:45:27.016
  STEP: getting the test event @ 08/04/23 20:45:27.019
  STEP: deleting the test event @ 08/04/23 20:45:27.021
  STEP: listing events in all namespaces @ 08/04/23 20:45:27.025
  STEP: listing events in test namespace @ 08/04/23 20:45:27.028
  Aug  4 20:45:27.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-9414" for this suite. @ 08/04/23 20:45:27.032
• [0.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 08/04/23 20:45:27.037
  Aug  4 20:45:27.037: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename runtimeclass @ 08/04/23 20:45:27.038
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:45:27.045
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:45:27.047
  E0804 20:45:27.328772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:28.329320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:45:29.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6780" for this suite. @ 08/04/23 20:45:29.066
• [2.032 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 08/04/23 20:45:29.07
  Aug  4 20:45:29.070: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:45:29.071
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:45:29.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:45:29.081
  STEP: Creating projection with secret that has name projected-secret-test-2da07ed6-91f2-4e06-8e51-17a52e84c84c @ 08/04/23 20:45:29.082
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:45:29.085
  E0804 20:45:29.329774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:30.330026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:45:31.096
  Aug  4 20:45:31.098: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-301f726c-1742-4271-b4f8-fa1be99053bf container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:45:31.111
  Aug  4 20:45:31.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8098" for this suite. @ 08/04/23 20:45:31.123
• [2.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 08/04/23 20:45:31.128
  Aug  4 20:45:31.128: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:45:31.129
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:45:31.137
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:45:31.139
  STEP: Creating projection with secret that has name projected-secret-test-map-3e10607f-8977-4088-b148-5634e650fb8f @ 08/04/23 20:45:31.141
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:45:31.144
  E0804 20:45:31.330614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:32.331100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:45:33.154
  Aug  4 20:45:33.155: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-3e0f78be-a4c1-4c14-87f3-1374937e2168 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:45:33.161
  Aug  4 20:45:33.170: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5831" for this suite. @ 08/04/23 20:45:33.172
• [2.048 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 08/04/23 20:45:33.176
  Aug  4 20:45:33.176: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename dns @ 08/04/23 20:45:33.177
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:45:33.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:45:33.184
  STEP: Creating a test headless service @ 08/04/23 20:45:33.186
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-742.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-742.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local;sleep 1; done
   @ 08/04/23 20:45:33.189
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-742.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-742.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-742.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local;sleep 1; done
   @ 08/04/23 20:45:33.189
  STEP: creating a pod to probe DNS @ 08/04/23 20:45:33.189
  STEP: submitting the pod to kubernetes @ 08/04/23 20:45:33.189
  E0804 20:45:33.332008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:34.332406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 20:45:35.2
  STEP: looking for the results for each expected name from probers @ 08/04/23 20:45:35.202
  Aug  4 20:45:35.204: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:35.206: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:35.207: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:35.209: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:35.210: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:35.212: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:35.214: INFO: Unable to read jessie_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:35.215: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:35.215: INFO: Lookups using dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local]

  E0804 20:45:35.332976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:36.334018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:37.334095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:38.334304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:39.334781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:45:40.218: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:40.220: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:40.222: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:40.224: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:40.225: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:40.227: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:40.228: INFO: Unable to read jessie_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:40.230: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:40.230: INFO: Lookups using dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local]

  E0804 20:45:40.335734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:41.336038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:42.336431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:43.336492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:44.336699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:45:45.218: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:45.220: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:45.222: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:45.223: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:45.225: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:45.227: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:45.228: INFO: Unable to read jessie_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:45.230: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:45.230: INFO: Lookups using dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local]

  E0804 20:45:45.337751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:46.338658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:47.338959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:48.338984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:49.339076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:45:50.218: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:50.220: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:50.221: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:50.223: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:50.225: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:50.227: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:50.228: INFO: Unable to read jessie_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:50.230: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:50.230: INFO: Lookups using dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local]

  E0804 20:45:50.339680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:51.339939      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:52.340437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:53.340691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:54.340889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:45:55.218: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:55.220: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:55.222: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:55.224: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:55.226: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:55.227: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:55.229: INFO: Unable to read jessie_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:55.230: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:45:55.230: INFO: Lookups using dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local]

  E0804 20:45:55.341029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:56.341231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:57.341788      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:58.342002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:45:59.342228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:00.218: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:46:00.220: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:46:00.222: INFO: Unable to read wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:46:00.224: INFO: Unable to read wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:46:00.226: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:46:00.228: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:46:00.229: INFO: Unable to read jessie_udp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:46:00.231: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local from pod dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554: the server could not find the requested resource (get pods dns-test-432f162b-3824-42f6-a353-f91bf7c15554)
  Aug  4 20:46:00.231: INFO: Lookups using dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local wheezy_udp@dns-test-service-2.dns-742.svc.cluster.local wheezy_tcp@dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-742.svc.cluster.local jessie_udp@dns-test-service-2.dns-742.svc.cluster.local jessie_tcp@dns-test-service-2.dns-742.svc.cluster.local]

  E0804 20:46:00.342953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:01.343217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:02.344082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:03.344221      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:04.344458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:05.234: INFO: DNS probes using dns-742/dns-test-432f162b-3824-42f6-a353-f91bf7c15554 succeeded

  Aug  4 20:46:05.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:46:05.236
  STEP: deleting the test headless service @ 08/04/23 20:46:05.248
  STEP: Destroying namespace "dns-742" for this suite. @ 08/04/23 20:46:05.265
• [32.093 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 08/04/23 20:46:05.269
  Aug  4 20:46:05.269: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/04/23 20:46:05.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:46:05.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:46:05.284
  Aug  4 20:46:05.286: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:46:05.345444      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:06.346455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:07.347589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:08.348151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:09.349066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:10.350060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:11.350325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:11.424: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-5959" for this suite. @ 08/04/23 20:46:11.426
• [6.161 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 08/04/23 20:46:11.43
  Aug  4 20:46:11.430: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename var-expansion @ 08/04/23 20:46:11.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:46:11.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:46:11.44
  STEP: Creating a pod to test env composition @ 08/04/23 20:46:11.442
  E0804 20:46:12.350426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:13.350621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:46:13.451
  Aug  4 20:46:13.453: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-d7022969-2392-4c23-9ca7-9be6f6419994 container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 20:46:13.46
  Aug  4 20:46:13.470: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8548" for this suite. @ 08/04/23 20:46:13.473
• [2.046 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 08/04/23 20:46:13.477
  Aug  4 20:46:13.477: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:46:13.477
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:46:13.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:46:13.486
  STEP: creating the pod @ 08/04/23 20:46:13.488
  Aug  4 20:46:13.489: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4387 create -f -'
  Aug  4 20:46:14.079: INFO: stderr: ""
  Aug  4 20:46:14.079: INFO: stdout: "pod/pause created\n"
  E0804 20:46:14.351129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:15.351355      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 08/04/23 20:46:16.084
  Aug  4 20:46:16.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4387 label pods pause testing-label=testing-label-value'
  Aug  4 20:46:16.145: INFO: stderr: ""
  Aug  4 20:46:16.145: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 08/04/23 20:46:16.145
  Aug  4 20:46:16.145: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4387 get pod pause -L testing-label'
  Aug  4 20:46:16.200: INFO: stderr: ""
  Aug  4 20:46:16.200: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 08/04/23 20:46:16.2
  Aug  4 20:46:16.200: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4387 label pods pause testing-label-'
  Aug  4 20:46:16.262: INFO: stderr: ""
  Aug  4 20:46:16.262: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 08/04/23 20:46:16.262
  Aug  4 20:46:16.262: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4387 get pod pause -L testing-label'
  Aug  4 20:46:16.319: INFO: stderr: ""
  Aug  4 20:46:16.319: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          2s    \n"
  STEP: using delete to clean up resources @ 08/04/23 20:46:16.319
  Aug  4 20:46:16.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4387 delete --grace-period=0 --force -f -'
  E0804 20:46:16.351612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:16.379: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Aug  4 20:46:16.379: INFO: stdout: "pod \"pause\" force deleted\n"
  Aug  4 20:46:16.379: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4387 get rc,svc -l name=pause --no-headers'
  Aug  4 20:46:16.439: INFO: stderr: "No resources found in kubectl-4387 namespace.\n"
  Aug  4 20:46:16.439: INFO: stdout: ""
  Aug  4 20:46:16.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-4387 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Aug  4 20:46:16.503: INFO: stderr: ""
  Aug  4 20:46:16.503: INFO: stdout: ""
  Aug  4 20:46:16.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4387" for this suite. @ 08/04/23 20:46:16.505
• [3.032 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 08/04/23 20:46:16.509
  Aug  4 20:46:16.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:46:16.51
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:46:16.519
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:46:16.521
  STEP: creating service in namespace services-4529 @ 08/04/23 20:46:16.523
  STEP: creating service affinity-clusterip-transition in namespace services-4529 @ 08/04/23 20:46:16.523
  STEP: creating replication controller affinity-clusterip-transition in namespace services-4529 @ 08/04/23 20:46:16.531
  I0804 20:46:16.537248      22 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-4529, replica count: 3
  E0804 20:46:17.352068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:18.352176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:19.352416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:46:19.588842      22 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 20:46:19.592: INFO: Creating new exec pod
  E0804 20:46:20.353062      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:21.353504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:22.354280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:22.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-4529 exec execpod-affinity89kzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Aug  4 20:46:22.725: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Aug  4 20:46:22.725: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:46:22.725: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-4529 exec execpod-affinity89kzt -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.99.65.93 80'
  Aug  4 20:46:22.844: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.99.65.93 80\nConnection to 10.99.65.93 80 port [tcp/http] succeeded!\n"
  Aug  4 20:46:22.844: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:46:22.850: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-4529 exec execpod-affinity89kzt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.65.93:80/ ; done'
  Aug  4 20:46:23.031: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n"
  Aug  4 20:46:23.031: INFO: stdout: "\naffinity-clusterip-transition-ch9nh\naffinity-clusterip-transition-ch9nh\naffinity-clusterip-transition-ch9nh\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-ch9nh\naffinity-clusterip-transition-plr7z\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-plr7z\naffinity-clusterip-transition-ch9nh\naffinity-clusterip-transition-plr7z\naffinity-clusterip-transition-ch9nh\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-ch9nh"
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-ch9nh
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-ch9nh
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-ch9nh
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-ch9nh
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-plr7z
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-plr7z
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-ch9nh
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-plr7z
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-ch9nh
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.031: INFO: Received response from host: affinity-clusterip-transition-ch9nh
  Aug  4 20:46:23.037: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-4529 exec execpod-affinity89kzt -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.99.65.93:80/ ; done'
  Aug  4 20:46:23.218: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.99.65.93:80/\n"
  Aug  4 20:46:23.218: INFO: stdout: "\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs\naffinity-clusterip-transition-j27hs"
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Received response from host: affinity-clusterip-transition-j27hs
  Aug  4 20:46:23.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 20:46:23.221: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-4529, will wait for the garbage collector to delete the pods @ 08/04/23 20:46:23.229
  Aug  4 20:46:23.286: INFO: Deleting ReplicationController affinity-clusterip-transition took: 3.938265ms
  E0804 20:46:23.354813      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:23.387: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.818375ms
  E0804 20:46:24.355128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:25.355764      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-4529" for this suite. @ 08/04/23 20:46:25.598
• [9.093 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 08/04/23 20:46:25.602
  Aug  4 20:46:25.602: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename statefulset @ 08/04/23 20:46:25.603
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:46:25.611
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:46:25.613
  STEP: Creating service test in namespace statefulset-4352 @ 08/04/23 20:46:25.614
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 08/04/23 20:46:25.618
  STEP: Creating stateful set ss in namespace statefulset-4352 @ 08/04/23 20:46:25.619
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-4352 @ 08/04/23 20:46:25.624
  Aug  4 20:46:25.625: INFO: Found 0 stateful pods, waiting for 1
  E0804 20:46:26.355848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:27.356433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:28.356671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:29.356899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:30.357123      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:31.357364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:32.357800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:33.358001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:34.358226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:35.358418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:35.629: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 08/04/23 20:46:35.629
  Aug  4 20:46:35.631: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-4352 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 20:46:35.758: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 20:46:35.758: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 20:46:35.758: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  4 20:46:35.761: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0804 20:46:36.358517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:37.358964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:38.359001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:39.360068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:40.360261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:41.360509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:42.360891      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:43.361124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:44.361418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:45.361627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:45.763: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug  4 20:46:45.763: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 20:46:45.772: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999773s
  E0804 20:46:46.362438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:46.776: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.997362826s
  E0804 20:46:47.362976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:47.779: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.994353353s
  E0804 20:46:48.363916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:48.781: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.991395636s
  E0804 20:46:49.364706      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:49.784: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.988642454s
  E0804 20:46:50.365483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:50.787: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.985851801s
  E0804 20:46:51.365958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:51.790: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.9831387s
  E0804 20:46:52.366011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:52.793: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.97926247s
  E0804 20:46:53.367080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:53.797: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.976175994s
  E0804 20:46:54.367925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:46:54.800: INFO: Verifying statefulset ss doesn't scale past 1 for another 973.532421ms
  E0804 20:46:55.368304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-4352 @ 08/04/23 20:46:55.8
  Aug  4 20:46:55.803: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-4352 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  4 20:46:55.923: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  4 20:46:55.923: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 20:46:55.923: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  4 20:46:55.926: INFO: Found 1 stateful pods, waiting for 3
  E0804 20:46:56.368549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:57.368950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:58.369629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:46:59.369805      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:00.370027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:01.370243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:02.370714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:03.370971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:04.371236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:05.372022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:05.929: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 20:47:05.929: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 20:47:05.929: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 08/04/23 20:47:05.929
  STEP: Scale down will halt with unhealthy stateful pod @ 08/04/23 20:47:05.929
  Aug  4 20:47:05.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-4352 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 20:47:06.059: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 20:47:06.059: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 20:47:06.059: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  4 20:47:06.059: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-4352 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 20:47:06.190: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 20:47:06.190: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 20:47:06.190: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  4 20:47:06.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-4352 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 20:47:06.317: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 20:47:06.317: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 20:47:06.317: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  4 20:47:06.317: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 20:47:06.319: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0804 20:47:06.372734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:07.373229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:08.373468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:09.373725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:10.373960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:11.374162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:12.374571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:13.374841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:14.375072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:15.375283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:16.325: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug  4 20:47:16.325: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug  4 20:47:16.325: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug  4 20:47:16.335: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.99999968s
  E0804 20:47:16.375554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:17.338: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.996890486s
  E0804 20:47:17.376396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:18.341: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994533253s
  E0804 20:47:18.377302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:19.344: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991591496s
  E0804 20:47:19.378279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:20.347: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988481531s
  E0804 20:47:20.379332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:21.350: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.985044361s
  E0804 20:47:21.379780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:22.353: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.981885607s
  E0804 20:47:22.380064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:23.356: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.978855332s
  E0804 20:47:23.380953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:24.359: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.975945745s
  E0804 20:47:24.381720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:25.362: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.034616ms
  E0804 20:47:25.382664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-4352 @ 08/04/23 20:47:26.362
  Aug  4 20:47:26.365: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-4352 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0804 20:47:26.383409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:26.488: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  4 20:47:26.488: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 20:47:26.488: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  4 20:47:26.488: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-4352 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  4 20:47:26.604: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  4 20:47:26.604: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 20:47:26.604: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  4 20:47:26.604: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-4352 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  4 20:47:26.764: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  4 20:47:26.764: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 20:47:26.764: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  4 20:47:26.764: INFO: Scaling statefulset ss to 0
  E0804 20:47:27.383649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:28.384024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:29.384230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:30.385037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:31.385441      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:32.385617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:33.385846      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:34.386523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:35.386716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:36.387333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 08/04/23 20:47:36.774
  Aug  4 20:47:36.774: INFO: Deleting all statefulset in ns statefulset-4352
  Aug  4 20:47:36.776: INFO: Scaling statefulset ss to 0
  Aug  4 20:47:36.782: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 20:47:36.783: INFO: Deleting statefulset ss
  Aug  4 20:47:36.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4352" for this suite. @ 08/04/23 20:47:36.792
• [71.195 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 08/04/23 20:47:36.797
  Aug  4 20:47:36.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:47:36.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:47:36.806
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:47:36.808
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/04/23 20:47:36.809
  E0804 20:47:37.387686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:38.388087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:39.388597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:40.388825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:47:40.822
  Aug  4 20:47:40.824: INFO: Trying to get logs from node k8sconformance-m02 pod pod-c2bfe820-67a5-4f50-973a-32e5b436a9b0 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:47:40.83
  Aug  4 20:47:40.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7493" for this suite. @ 08/04/23 20:47:40.84
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 08/04/23 20:47:40.846
  Aug  4 20:47:40.846: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename conformance-tests @ 08/04/23 20:47:40.847
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:47:40.853
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:47:40.856
  STEP: Getting node addresses @ 08/04/23 20:47:40.857
  Aug  4 20:47:40.858: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Aug  4 20:47:40.860: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-6444" for this suite. @ 08/04/23 20:47:40.862
• [0.021 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 08/04/23 20:47:40.868
  Aug  4 20:47:40.868: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:47:40.868
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:47:40.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:47:40.877
  STEP: Create a pod @ 08/04/23 20:47:40.879
  E0804 20:47:41.389485      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:42.390629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 08/04/23 20:47:42.89
  Aug  4 20:47:42.896: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Aug  4 20:47:42.896: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-6343" for this suite. @ 08/04/23 20:47:42.898
• [2.034 seconds]
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 08/04/23 20:47:42.902
  Aug  4 20:47:42.902: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:47:42.902
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:47:42.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:47:42.911
  STEP: Setting up server cert @ 08/04/23 20:47:42.922
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:47:43.27
  STEP: Deploying the webhook pod @ 08/04/23 20:47:43.275
  STEP: Wait for the deployment to be ready @ 08/04/23 20:47:43.283
  Aug  4 20:47:43.288: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 20:47:43.391672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:44.392035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:47:45.294
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:47:45.301
  E0804 20:47:45.393110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:46.302: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 08/04/23 20:47:46.304
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 08/04/23 20:47:46.305
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 08/04/23 20:47:46.305
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 08/04/23 20:47:46.305
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 08/04/23 20:47:46.306
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/04/23 20:47:46.306
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 08/04/23 20:47:46.306
  Aug  4 20:47:46.307: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-2267" for this suite. @ 08/04/23 20:47:46.327
  STEP: Destroying namespace "webhook-markers-9765" for this suite. @ 08/04/23 20:47:46.33
• [3.433 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 08/04/23 20:47:46.336
  Aug  4 20:47:46.336: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:47:46.336
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:47:46.343
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:47:46.345
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:47:46.347
  E0804 20:47:46.393800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:47.394165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:47:48.358
  Aug  4 20:47:48.359: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-33454400-4b36-4c3c-ac56-9b59e55c84a5 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:47:48.365
  Aug  4 20:47:48.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2678" for this suite. @ 08/04/23 20:47:48.378
• [2.046 seconds]
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 08/04/23 20:47:48.382
  Aug  4 20:47:48.382: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename deployment @ 08/04/23 20:47:48.382
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:47:48.389
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:47:48.391
  Aug  4 20:47:48.394: INFO: Creating deployment "webserver-deployment"
  E0804 20:47:48.394151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:48.397: INFO: Waiting for observed generation 1
  E0804 20:47:49.394397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:50.394646      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:50.401: INFO: Waiting for all required pods to come up
  Aug  4 20:47:50.404: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 08/04/23 20:47:50.404
  E0804 20:47:51.395554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:52.395670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:52.411: INFO: Waiting for deployment "webserver-deployment" to complete
  Aug  4 20:47:52.415: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Aug  4 20:47:52.421: INFO: Updating deployment webserver-deployment
  Aug  4 20:47:52.421: INFO: Waiting for observed generation 2
  E0804 20:47:53.396762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:54.396962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:54.426: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Aug  4 20:47:54.428: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Aug  4 20:47:54.429: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug  4 20:47:54.434: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Aug  4 20:47:54.434: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Aug  4 20:47:54.436: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Aug  4 20:47:54.439: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Aug  4 20:47:54.439: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Aug  4 20:47:54.445: INFO: Updating deployment webserver-deployment
  Aug  4 20:47:54.445: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Aug  4 20:47:54.448: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Aug  4 20:47:54.452: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  E0804 20:47:55.403876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:56.404825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:47:56.511: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-9443  9554a6d1-285e-49c1-997f-7ffa167e3626 19709 3 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004675df8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:33,UpdatedReplicas:13,AvailableReplicas:8,UnavailableReplicas:25,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-04 20:47:54 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-08-04 20:47:54 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Aug  4 20:47:56.514: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-9443  7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 19705 3 2023-08-04 20:47:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment 9554a6d1-285e-49c1-997f-7ffa167e3626 0xc003e78c57 0xc003e78c58}] [] [{kube-controller-manager Update apps/v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9554a6d1-285e-49c1-997f-7ffa167e3626\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e78cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:13,FullyLabeledReplicas:13,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 20:47:56.514: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Aug  4 20:47:56.514: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-9443  66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 19698 3 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment 9554a6d1-285e-49c1-997f-7ffa167e3626 0xc003e78b67 0xc003e78b68}] [] [{kube-controller-manager Update apps/v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"9554a6d1-285e-49c1-997f-7ffa167e3626\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e78bf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:20,FullyLabeledReplicas:20,ObservedGeneration:3,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 20:47:56.520: INFO: Pod "webserver-deployment-67bd4bf6dc-28lbf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-28lbf webserver-deployment-67bd4bf6dc- deployment-9443  72ab9b4e-af09-455e-ba10-962050de0681 19682 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e791e7 0xc003e791e8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8x5vq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8x5vq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.520: INFO: Pod "webserver-deployment-67bd4bf6dc-2kcr6" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-2kcr6 webserver-deployment-67bd4bf6dc- deployment-9443  2acaf86c-3fd8-4509-8047-86e62916930b 19735 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e793a7 0xc003e793a8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79xwv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79xwv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.520: INFO: Pod "webserver-deployment-67bd4bf6dc-4lnsd" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-4lnsd webserver-deployment-67bd4bf6dc- deployment-9443  d3b21df7-bec2-4fda-8dab-9de199502bea 19723 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e79577 0xc003e79578}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-z44jc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-z44jc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.520: INFO: Pod "webserver-deployment-67bd4bf6dc-649qf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-649qf webserver-deployment-67bd4bf6dc- deployment-9443  5cc17bcf-93b0-451b-8539-ba40bf1d6ea8 19732 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e79747 0xc003e79748}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5pzpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5pzpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.521: INFO: Pod "webserver-deployment-67bd4bf6dc-96pzl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-96pzl webserver-deployment-67bd4bf6dc- deployment-9443  b10abffb-bfa4-4783-9cc9-05ae1a75b961 19553 0 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e79907 0xc003e79908}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.123\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kcjrh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kcjrh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.123,StartTime:2023-08-04 20:47:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:47:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://4e3f08688b2bd44bc9573ee197d38f04e568ddc228cbecd244a63c17151ad654,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.123,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.521: INFO: Pod "webserver-deployment-67bd4bf6dc-9mdkt" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9mdkt webserver-deployment-67bd4bf6dc- deployment-9443  5fae92c8-dbc1-4c36-9b99-9719818ee9a9 19526 0 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e79ae7 0xc003e79ae8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.195\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-72257,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-72257,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.195,StartTime:2023-08-04 20:47:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:47:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://5babf1624b980f4739ab7dd9f498af32444908665ac7373eb9daf1b2c0b95cb5,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.195,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.522: INFO: Pod "webserver-deployment-67bd4bf6dc-cdwmq" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-cdwmq webserver-deployment-67bd4bf6dc- deployment-9443  01df4360-19af-4872-9998-43d3e3f8bf70 19746 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e79ce7 0xc003e79ce8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-srdjb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-srdjb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.522: INFO: Pod "webserver-deployment-67bd4bf6dc-dkc7c" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dkc7c webserver-deployment-67bd4bf6dc- deployment-9443  9cbbea5f-be57-4528-a978-e7dbc4e96f26 19529 0 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e79ec7 0xc003e79ec8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.198\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5jknq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5jknq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.198,StartTime:2023-08-04 20:47:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:47:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://782cfcd2c6ee36831e02356f0fe083d3e5d468065de21a3ec130e15759c6b0f8,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.198,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.522: INFO: Pod "webserver-deployment-67bd4bf6dc-dkj2p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dkj2p webserver-deployment-67bd4bf6dc- deployment-9443  3fcaf9cb-6fcf-4368-a6b1-534fd03ce947 19720 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e48c97 0xc003e48c98}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v9cqn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v9cqn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.522: INFO: Pod "webserver-deployment-67bd4bf6dc-dsrnv" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dsrnv webserver-deployment-67bd4bf6dc- deployment-9443  d45d9ff7-cdfb-456f-8667-28236357558b 19550 0 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e49297 0xc003e49298}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.121\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2jd8l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2jd8l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.121,StartTime:2023-08-04 20:47:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:47:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://6707bafbee828f98f5704b9c0c8be284fb343ed36a7dc0d8f2100689cb3900ab,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.121,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.523: INFO: Pod "webserver-deployment-67bd4bf6dc-f6xz2" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-f6xz2 webserver-deployment-67bd4bf6dc- deployment-9443  80c2a196-1f61-4f84-af47-c3a5347a59e5 19535 0 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e498a7 0xc003e498a8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.194\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w86n4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w86n4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.194,StartTime:2023-08-04 20:47:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:47:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://e6340c869c8ee023d037bcda118f6ea68b1a2ed671ab8df491ce44da63618d8c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.194,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.523: INFO: Pod "webserver-deployment-67bd4bf6dc-h9s44" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-h9s44 webserver-deployment-67bd4bf6dc- deployment-9443  db69178f-9a99-4d8a-8a2a-6993c284752c 19547 0 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc003e49e97 0xc003e49e98}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.122\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jbbv4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jbbv4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.122,StartTime:2023-08-04 20:47:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:47:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://cdde895652d8f653bd33b43e97244e0c0934ab34ccdf3f1286f0a46442e76215,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.122,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.523: INFO: Pod "webserver-deployment-67bd4bf6dc-kn62x" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-kn62x webserver-deployment-67bd4bf6dc- deployment-9443  0ad032dc-4826-4df0-a129-016083052efe 19744 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc000ce7ed7 0xc000ce7ed8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4r628,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4r628,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.523: INFO: Pod "webserver-deployment-67bd4bf6dc-nwwgt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-nwwgt webserver-deployment-67bd4bf6dc- deployment-9443  8db956d5-97ca-498b-8cd7-837983d1d7ec 19743 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc004502097 0xc004502098}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7lgpb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7lgpb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.524: INFO: Pod "webserver-deployment-67bd4bf6dc-sg4ns" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-sg4ns webserver-deployment-67bd4bf6dc- deployment-9443  85d4cd3b-a725-4613-a45e-ceef7e1a1801 19715 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc004502267 0xc004502268}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jlmmr,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jlmmr,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.524: INFO: Pod "webserver-deployment-67bd4bf6dc-vtltc" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vtltc webserver-deployment-67bd4bf6dc- deployment-9443  ec4157af-e49d-4590-b3c2-07578564dd8f 19531 0 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc004502437 0xc004502438}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-c2q5h,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-c2q5h,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.196,StartTime:2023-08-04 20:47:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:47:50 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://43776a3184d6a054dfe1b5aa702c100a3ca444d88bfd8df3683e4d73d4a74408,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.196,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.524: INFO: Pod "webserver-deployment-67bd4bf6dc-w7m6g" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-w7m6g webserver-deployment-67bd4bf6dc- deployment-9443  2ee1aaa2-6db4-4ca2-9e28-43a2a2ef31d8 19537 0 2023-08-04 20:47:48 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc004502627 0xc004502628}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:48 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:50 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.197\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nlx2k,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nlx2k,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:50 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:48 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.197,StartTime:2023-08-04 20:47:48 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 20:47:49 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://8a4f39eddbcf0510cc453f881aeaa5bce127f3690255bbdcf44375309dee4bc2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.197,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.524: INFO: Pod "webserver-deployment-67bd4bf6dc-wgn4p" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wgn4p webserver-deployment-67bd4bf6dc- deployment-9443  aaedb123-a621-4d4c-81fa-02568e322a06 19710 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc004502817 0xc004502818}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-v488q,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-v488q,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.525: INFO: Pod "webserver-deployment-67bd4bf6dc-whl4n" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-whl4n webserver-deployment-67bd4bf6dc- deployment-9443  00f8f6bb-30fd-4ed2-8dbd-c3e03d494b54 19700 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc0045029d7 0xc0045029d8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-48vv5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-48vv5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.525: INFO: Pod "webserver-deployment-67bd4bf6dc-wn859" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-wn859 webserver-deployment-67bd4bf6dc- deployment-9443  d9576c98-b616-41ce-9b3c-03c8d4354482 19714 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc 66bd3dbe-4dae-48dc-bd71-f9471b3f22b9 0xc004502ba7 0xc004502ba8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"66bd3dbe-4dae-48dc-bd71-f9471b3f22b9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d6kf2,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d6kf2,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.525: INFO: Pod "webserver-deployment-7b75d79cf5-45tjf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-45tjf webserver-deployment-7b75d79cf5- deployment-9443  0a8f27d7-5262-48d4-9b61-35cb4997eb4d 19706 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc004503057 0xc004503058}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2qz5j,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2qz5j,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.526: INFO: Pod "webserver-deployment-7b75d79cf5-49n5z" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-49n5z webserver-deployment-7b75d79cf5- deployment-9443  f27f4aa9-b87e-4eb2-a00f-abebc2d36f49 19737 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc004503307 0xc004503308}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fkpr6,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fkpr6,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.526: INFO: Pod "webserver-deployment-7b75d79cf5-7qfqg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-7qfqg webserver-deployment-7b75d79cf5- deployment-9443  5eec7ce3-7a54-4bec-bf6c-d759de35edbb 19615 0 2023-08-04 20:47:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc0045034e7 0xc0045034e8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-w6jq7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-w6jq7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.527: INFO: Pod "webserver-deployment-7b75d79cf5-8wxmt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8wxmt webserver-deployment-7b75d79cf5- deployment-9443  2b0c6de3-6e8d-41c8-b63d-199cd3bd8430 19601 0 2023-08-04 20:47:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc004503887 0xc004503888}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bmhw8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bmhw8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.527: INFO: Pod "webserver-deployment-7b75d79cf5-c94jk" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-c94jk webserver-deployment-7b75d79cf5- deployment-9443  59757b5a-8ea4-41c0-bcd2-04596369f2bd 19739 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc004503a77 0xc004503a78}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6k8k9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6k8k9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.527: INFO: Pod "webserver-deployment-7b75d79cf5-kqtjb" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-kqtjb webserver-deployment-7b75d79cf5- deployment-9443  f7fcea6c-25ea-426b-a024-ed3cb4f6e5e0 19741 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc004503c57 0xc004503c58}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fd8df,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fd8df,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.528: INFO: Pod "webserver-deployment-7b75d79cf5-l22j9" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-l22j9 webserver-deployment-7b75d79cf5- deployment-9443  fb9c33ab-412a-432a-bd58-02ab97f22902 19641 0 2023-08-04 20:47:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc004503e47 0xc004503e48}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.126\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2b8sg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2b8sg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.126,StartTime:2023-08-04 20:47:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.126,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.528: INFO: Pod "webserver-deployment-7b75d79cf5-lfq7k" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lfq7k webserver-deployment-7b75d79cf5- deployment-9443  6eaf0c66-4796-43ba-a9c2-b9f104eb14e5 19728 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc000664057 0xc000664058}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rpcsb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rpcsb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.528: INFO: Pod "webserver-deployment-7b75d79cf5-lhrhw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-lhrhw webserver-deployment-7b75d79cf5- deployment-9443  05ae61d2-fcb2-4b70-82ef-c94a5c639bb5 19617 0 2023-08-04 20:47:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc000664247 0xc000664248}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:52 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4sn7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4sn7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.528: INFO: Pod "webserver-deployment-7b75d79cf5-npxwx" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-npxwx webserver-deployment-7b75d79cf5- deployment-9443  61d3e2a0-3a30-4fdc-b159-b2e60f5f2af6 19731 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc000664427 0xc000664428}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bv5t5,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bv5t5,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.529: INFO: Pod "webserver-deployment-7b75d79cf5-q789c" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-q789c webserver-deployment-7b75d79cf5- deployment-9443  ecaf662e-a0ce-4e3d-a62b-27c28838b9a3 19727 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc000664617 0xc000664618}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-2grwd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-2grwd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.529: INFO: Pod "webserver-deployment-7b75d79cf5-vdh4g" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vdh4g webserver-deployment-7b75d79cf5- deployment-9443  218ba4c9-b98a-4860-a813-77f7555046eb 19747 0 2023-08-04 20:47:54 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc0006647f7 0xc0006647f8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h7zhl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h7zhl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:54 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 20:47:54 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.529: INFO: Pod "webserver-deployment-7b75d79cf5-vhk9h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-vhk9h webserver-deployment-7b75d79cf5- deployment-9443  d3f07ff3-5d4f-458c-be78-d0c8d08272a8 19751 0 2023-08-04 20:47:52 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3 0xc0006649f7 0xc0006649f8}] [] [{kube-controller-manager Update v1 2023-08-04 20:47:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7e98bbdf-51e4-4a4f-8e5e-77637beb9ac3\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 20:47:54 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.199\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mgx75,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mgx75,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 20:47:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.199,StartTime:2023-08-04 20:47:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ErrImagePull,Message:rpc error: code = Unknown desc = Error response from daemon: pull access denied for webserver, repository does not exist or may require 'docker login': denied: requested access to the resource is denied,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.199,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 20:47:56.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9443" for this suite. @ 08/04/23 20:47:56.533
• [8.160 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 08/04/23 20:47:56.543
  Aug  4 20:47:56.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:47:56.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:47:56.607
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:47:56.61
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-3085 @ 08/04/23 20:47:56.613
  STEP: changing the ExternalName service to type=NodePort @ 08/04/23 20:47:56.621
  STEP: creating replication controller externalname-service in namespace services-3085 @ 08/04/23 20:47:56.637
  I0804 20:47:56.645799      22 runners.go:194] Created replication controller with name: externalname-service, namespace: services-3085, replica count: 2
  E0804 20:47:57.405423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:58.405718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:47:59.405951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:47:59.696401      22 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 20:47:59.696: INFO: Creating new exec pod
  E0804 20:48:00.406121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:01.406533      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:02.408612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:02.717: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3085 exec execpodntzcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  4 20:48:03.107: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  4 20:48:03.107: INFO: stdout: ""
  E0804 20:48:03.409226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:04.108: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3085 exec execpodntzcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Aug  4 20:48:04.237: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Aug  4 20:48:04.237: INFO: stdout: "externalname-service-lsk29"
  Aug  4 20:48:04.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3085 exec execpodntzcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.102.149.215 80'
  Aug  4 20:48:04.373: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.102.149.215 80\nConnection to 10.102.149.215 80 port [tcp/http] succeeded!\n"
  Aug  4 20:48:04.373: INFO: stdout: "externalname-service-lsk29"
  Aug  4 20:48:04.373: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3085 exec execpodntzcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.2 30128'
  E0804 20:48:04.409588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:04.493: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.2 30128\nConnection to 192.168.49.2 30128 port [tcp/*] succeeded!\n"
  Aug  4 20:48:04.493: INFO: stdout: "externalname-service-lsk29"
  Aug  4 20:48:04.493: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3085 exec execpodntzcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.3 30128'
  Aug  4 20:48:04.612: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.3 30128\nConnection to 192.168.49.3 30128 port [tcp/*] succeeded!\n"
  Aug  4 20:48:04.612: INFO: stdout: ""
  E0804 20:48:05.410403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:05.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3085 exec execpodntzcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.3 30128'
  Aug  4 20:48:05.732: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.3 30128\nConnection to 192.168.49.3 30128 port [tcp/*] succeeded!\n"
  Aug  4 20:48:05.732: INFO: stdout: ""
  E0804 20:48:06.410588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:06.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-3085 exec execpodntzcn -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.3 30128'
  Aug  4 20:48:06.748: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.3 30128\nConnection to 192.168.49.3 30128 port [tcp/*] succeeded!\n"
  Aug  4 20:48:06.748: INFO: stdout: "externalname-service-lsk29"
  Aug  4 20:48:06.748: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 20:48:06.751: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-3085" for this suite. @ 08/04/23 20:48:06.762
• [10.222 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 08/04/23 20:48:06.766
  Aug  4 20:48:06.766: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replicaset @ 08/04/23 20:48:06.767
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:06.776
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:06.777
  STEP: Create a ReplicaSet @ 08/04/23 20:48:06.779
  STEP: Verify that the required pods have come up @ 08/04/23 20:48:06.782
  Aug  4 20:48:06.784: INFO: Pod name sample-pod: Found 0 pods out of 3
  E0804 20:48:07.410986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:08.412067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:09.412259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:10.412481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:11.412958      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:11.786: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 08/04/23 20:48:11.786
  Aug  4 20:48:11.788: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 08/04/23 20:48:11.788
  STEP: DeleteCollection of the ReplicaSets @ 08/04/23 20:48:11.789
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 08/04/23 20:48:11.794
  Aug  4 20:48:11.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-6747" for this suite. @ 08/04/23 20:48:11.798
• [5.041 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 08/04/23 20:48:11.808
  Aug  4 20:48:11.808: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename discovery @ 08/04/23 20:48:11.808
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:11.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:11.826
  STEP: Setting up server cert @ 08/04/23 20:48:11.829
  E0804 20:48:12.413609      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:12.474: INFO: Checking APIGroup: apiregistration.k8s.io
  Aug  4 20:48:12.474: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Aug  4 20:48:12.474: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Aug  4 20:48:12.474: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Aug  4 20:48:12.475: INFO: Checking APIGroup: apps
  Aug  4 20:48:12.475: INFO: PreferredVersion.GroupVersion: apps/v1
  Aug  4 20:48:12.475: INFO: Versions found [{apps/v1 v1}]
  Aug  4 20:48:12.475: INFO: apps/v1 matches apps/v1
  Aug  4 20:48:12.475: INFO: Checking APIGroup: events.k8s.io
  Aug  4 20:48:12.476: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Aug  4 20:48:12.476: INFO: Versions found [{events.k8s.io/v1 v1}]
  Aug  4 20:48:12.476: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Aug  4 20:48:12.476: INFO: Checking APIGroup: authentication.k8s.io
  Aug  4 20:48:12.476: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Aug  4 20:48:12.476: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Aug  4 20:48:12.476: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Aug  4 20:48:12.476: INFO: Checking APIGroup: authorization.k8s.io
  Aug  4 20:48:12.477: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Aug  4 20:48:12.477: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Aug  4 20:48:12.477: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Aug  4 20:48:12.477: INFO: Checking APIGroup: autoscaling
  Aug  4 20:48:12.478: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Aug  4 20:48:12.478: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Aug  4 20:48:12.478: INFO: autoscaling/v2 matches autoscaling/v2
  Aug  4 20:48:12.478: INFO: Checking APIGroup: batch
  Aug  4 20:48:12.479: INFO: PreferredVersion.GroupVersion: batch/v1
  Aug  4 20:48:12.479: INFO: Versions found [{batch/v1 v1}]
  Aug  4 20:48:12.479: INFO: batch/v1 matches batch/v1
  Aug  4 20:48:12.479: INFO: Checking APIGroup: certificates.k8s.io
  Aug  4 20:48:12.479: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Aug  4 20:48:12.479: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Aug  4 20:48:12.479: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Aug  4 20:48:12.479: INFO: Checking APIGroup: networking.k8s.io
  Aug  4 20:48:12.480: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Aug  4 20:48:12.480: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Aug  4 20:48:12.480: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Aug  4 20:48:12.480: INFO: Checking APIGroup: policy
  Aug  4 20:48:12.481: INFO: PreferredVersion.GroupVersion: policy/v1
  Aug  4 20:48:12.481: INFO: Versions found [{policy/v1 v1}]
  Aug  4 20:48:12.481: INFO: policy/v1 matches policy/v1
  Aug  4 20:48:12.481: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Aug  4 20:48:12.481: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Aug  4 20:48:12.481: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Aug  4 20:48:12.481: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Aug  4 20:48:12.481: INFO: Checking APIGroup: storage.k8s.io
  Aug  4 20:48:12.482: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Aug  4 20:48:12.482: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Aug  4 20:48:12.482: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Aug  4 20:48:12.482: INFO: Checking APIGroup: admissionregistration.k8s.io
  Aug  4 20:48:12.483: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Aug  4 20:48:12.483: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Aug  4 20:48:12.483: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Aug  4 20:48:12.483: INFO: Checking APIGroup: apiextensions.k8s.io
  Aug  4 20:48:12.483: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Aug  4 20:48:12.484: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Aug  4 20:48:12.484: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Aug  4 20:48:12.484: INFO: Checking APIGroup: scheduling.k8s.io
  Aug  4 20:48:12.484: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Aug  4 20:48:12.484: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Aug  4 20:48:12.484: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Aug  4 20:48:12.484: INFO: Checking APIGroup: coordination.k8s.io
  Aug  4 20:48:12.485: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Aug  4 20:48:12.485: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Aug  4 20:48:12.485: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Aug  4 20:48:12.485: INFO: Checking APIGroup: node.k8s.io
  Aug  4 20:48:12.485: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Aug  4 20:48:12.485: INFO: Versions found [{node.k8s.io/v1 v1}]
  Aug  4 20:48:12.485: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Aug  4 20:48:12.485: INFO: Checking APIGroup: discovery.k8s.io
  Aug  4 20:48:12.486: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Aug  4 20:48:12.486: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Aug  4 20:48:12.486: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Aug  4 20:48:12.486: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Aug  4 20:48:12.487: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Aug  4 20:48:12.487: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Aug  4 20:48:12.487: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Aug  4 20:48:12.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-8944" for this suite. @ 08/04/23 20:48:12.489
• [0.685 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 08/04/23 20:48:12.493
  Aug  4 20:48:12.493: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 20:48:12.493
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:12.502
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:12.505
  Aug  4 20:48:12.507: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 20:48:13.414282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 08/04/23 20:48:13.735
  Aug  4 20:48:13.736: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-6127 --namespace=crd-publish-openapi-6127 create -f -'
  Aug  4 20:48:14.360: INFO: stderr: ""
  Aug  4 20:48:14.360: INFO: stdout: "e2e-test-crd-publish-openapi-6824-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug  4 20:48:14.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-6127 --namespace=crd-publish-openapi-6127 delete e2e-test-crd-publish-openapi-6824-crds test-cr'
  E0804 20:48:14.414513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:14.418: INFO: stderr: ""
  Aug  4 20:48:14.418: INFO: stdout: "e2e-test-crd-publish-openapi-6824-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Aug  4 20:48:14.418: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-6127 --namespace=crd-publish-openapi-6127 apply -f -'
  Aug  4 20:48:14.584: INFO: stderr: ""
  Aug  4 20:48:14.584: INFO: stdout: "e2e-test-crd-publish-openapi-6824-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Aug  4 20:48:14.584: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-6127 --namespace=crd-publish-openapi-6127 delete e2e-test-crd-publish-openapi-6824-crds test-cr'
  Aug  4 20:48:14.641: INFO: stderr: ""
  Aug  4 20:48:14.641: INFO: stdout: "e2e-test-crd-publish-openapi-6824-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 08/04/23 20:48:14.641
  Aug  4 20:48:14.641: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=crd-publish-openapi-6127 explain e2e-test-crd-publish-openapi-6824-crds'
  Aug  4 20:48:14.797: INFO: stderr: ""
  Aug  4 20:48:14.797: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-6824-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  E0804 20:48:15.414845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:16.044: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-6127" for this suite. @ 08/04/23 20:48:16.05
• [3.560 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 08/04/23 20:48:16.054
  Aug  4 20:48:16.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:48:16.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:16.062
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:16.064
  STEP: Setting up server cert @ 08/04/23 20:48:16.112
  E0804 20:48:16.414971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:48:16.472
  STEP: Deploying the webhook pod @ 08/04/23 20:48:16.477
  STEP: Wait for the deployment to be ready @ 08/04/23 20:48:16.486
  Aug  4 20:48:16.489: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0804 20:48:17.415548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:18.415743      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:48:18.495
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:48:18.503
  E0804 20:48:19.416580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:19.503: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 08/04/23 20:48:19.545
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/04/23 20:48:19.575
  STEP: Deleting the collection of validation webhooks @ 08/04/23 20:48:19.596
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/04/23 20:48:19.622
  Aug  4 20:48:19.628: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5078" for this suite. @ 08/04/23 20:48:19.65
  STEP: Destroying namespace "webhook-markers-1107" for this suite. @ 08/04/23 20:48:19.653
• [3.603 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 08/04/23 20:48:19.659
  Aug  4 20:48:19.659: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 20:48:19.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:19.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:19.67
  E0804 20:48:20.417650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:21.418035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:22.418962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:23.419001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:24.419476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:25.420188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:26.420937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:27.421304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:28.421623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:29.422075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:30.422153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:31.422965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:32.422980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:33.424092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:34.424629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:35.425172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:36.426247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Counting existing ResourceQuota @ 08/04/23 20:48:36.675
  E0804 20:48:37.426731      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:38.426964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:39.428057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:40.428578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:41.429265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/04/23 20:48:41.678
  STEP: Ensuring resource quota status is calculated @ 08/04/23 20:48:41.682
  E0804 20:48:42.429399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:43.430379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ConfigMap @ 08/04/23 20:48:43.685
  STEP: Ensuring resource quota status captures configMap creation @ 08/04/23 20:48:43.694
  E0804 20:48:44.430464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:45.430734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ConfigMap @ 08/04/23 20:48:45.697
  STEP: Ensuring resource quota status released usage @ 08/04/23 20:48:45.701
  E0804 20:48:46.431580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:47.432036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:47.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-299" for this suite. @ 08/04/23 20:48:47.706
• [28.052 seconds]
------------------------------
S
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 08/04/23 20:48:47.711
  Aug  4 20:48:47.711: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/04/23 20:48:47.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:47.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:47.722
  STEP: create the container to handle the HTTPGet hook request. @ 08/04/23 20:48:47.726
  E0804 20:48:48.432172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:49.432450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/04/23 20:48:49.739
  E0804 20:48:50.432870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:51.433293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/04/23 20:48:51.752
  E0804 20:48:52.433972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:53.434188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/04/23 20:48:53.762
  Aug  4 20:48:53.774: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2826" for this suite. @ 08/04/23 20:48:53.777
• [6.069 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 08/04/23 20:48:53.782
  Aug  4 20:48:53.782: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 20:48:53.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:53.791
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:53.793
  STEP: Setting up server cert @ 08/04/23 20:48:53.805
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 20:48:54.15
  STEP: Deploying the webhook pod @ 08/04/23 20:48:54.155
  STEP: Wait for the deployment to be ready @ 08/04/23 20:48:54.164
  Aug  4 20:48:54.167: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0804 20:48:54.435288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:55.436120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 20:48:56.174
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 20:48:56.182
  E0804 20:48:56.436436      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:57.182: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 08/04/23 20:48:57.185
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/04/23 20:48:57.196
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 08/04/23 20:48:57.201
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/04/23 20:48:57.208
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 08/04/23 20:48:57.215
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 08/04/23 20:48:57.22
  Aug  4 20:48:57.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9974" for this suite. @ 08/04/23 20:48:57.249
  STEP: Destroying namespace "webhook-markers-8653" for this suite. @ 08/04/23 20:48:57.252
• [3.473 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 08/04/23 20:48:57.256
  Aug  4 20:48:57.256: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:48:57.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:57.263
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:57.265
  STEP: create deployment with httpd image @ 08/04/23 20:48:57.267
  Aug  4 20:48:57.267: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-5372 create -f -'
  E0804 20:48:57.436801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:48:57.791: INFO: stderr: ""
  Aug  4 20:48:57.791: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 08/04/23 20:48:57.791
  Aug  4 20:48:57.791: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-5372 diff -f -'
  Aug  4 20:48:57.969: INFO: rc: 1
  Aug  4 20:48:57.969: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-5372 delete -f -'
  Aug  4 20:48:58.027: INFO: stderr: ""
  Aug  4 20:48:58.027: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Aug  4 20:48:58.027: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-5372" for this suite. @ 08/04/23 20:48:58.029
• [0.777 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 08/04/23 20:48:58.033
  Aug  4 20:48:58.033: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename csiinlinevolumes @ 08/04/23 20:48:58.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:58.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:58.045
  STEP: creating @ 08/04/23 20:48:58.047
  STEP: getting @ 08/04/23 20:48:58.057
  STEP: listing @ 08/04/23 20:48:58.06
  STEP: deleting @ 08/04/23 20:48:58.061
  Aug  4 20:48:58.070: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5530" for this suite. @ 08/04/23 20:48:58.072
• [0.043 seconds]
------------------------------
SS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 08/04/23 20:48:58.076
  Aug  4 20:48:58.076: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename containers @ 08/04/23 20:48:58.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:48:58.084
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:48:58.086
  E0804 20:48:58.436942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:48:59.437340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:49:00.106: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-101" for this suite. @ 08/04/23 20:49:00.108
• [2.035 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 08/04/23 20:49:00.114
  Aug  4 20:49:00.114: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 20:49:00.114
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:00.122
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:00.123
  STEP: starting the proxy server @ 08/04/23 20:49:00.126
  Aug  4 20:49:00.126: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-2325 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 08/04/23 20:49:00.167
  Aug  4 20:49:00.174: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2325" for this suite. @ 08/04/23 20:49:00.176
• [0.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 08/04/23 20:49:00.181
  Aug  4 20:49:00.181: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:49:00.181
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:00.191
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:00.192
  STEP: Creating the pod @ 08/04/23 20:49:00.194
  E0804 20:49:00.438247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:01.438935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:02.439483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:49:02.721: INFO: Successfully updated pod "annotationupdatea2ab6ceb-1615-4b2c-9ea4-853f16ea6776"
  E0804 20:49:03.440549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:04.440763      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:05.441144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:06.441696      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:49:06.742: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5504" for this suite. @ 08/04/23 20:49:06.744
• [6.567 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 08/04/23 20:49:06.748
  Aug  4 20:49:06.748: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 20:49:06.749
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:06.756
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:06.758
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 20:49:06.761
  E0804 20:49:07.442314      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:08.442675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:09.443001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:10.444034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:49:10.773
  Aug  4 20:49:10.774: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-c4f29cda-df3a-41af-853c-0ed3032a3eae container client-container: <nil>
  STEP: delete the pod @ 08/04/23 20:49:10.78
  Aug  4 20:49:10.790: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5136" for this suite. @ 08/04/23 20:49:10.792
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 08/04/23 20:49:10.797
  Aug  4 20:49:10.797: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-pred @ 08/04/23 20:49:10.798
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:10.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:10.807
  Aug  4 20:49:10.809: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug  4 20:49:10.813: INFO: Waiting for terminating namespaces to be deleted...
  Aug  4 20:49:10.814: INFO: 
  Logging pods the apiserver thinks is on node k8sconformance before test
  Aug  4 20:49:10.817: INFO: coredns-5d78c9869d-zl979 from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container coredns ready: true, restart count 1
  Aug  4 20:49:10.817: INFO: etcd-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container etcd ready: true, restart count 0
  Aug  4 20:49:10.817: INFO: kindnet-5zl6d from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container kindnet-cni ready: true, restart count 0
  Aug  4 20:49:10.817: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug  4 20:49:10.817: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug  4 20:49:10.817: INFO: kube-proxy-mslwk from kube-system started at 2023-08-04 19:46:58 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug  4 20:49:10.817: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug  4 20:49:10.817: INFO: storage-provisioner from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container storage-provisioner ready: true, restart count 1
  Aug  4 20:49:10.817: INFO: sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-fdn76 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 20:49:10.817: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 20:49:10.817: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  4 20:49:10.817: INFO: 
  Logging pods the apiserver thinks is on node k8sconformance-m02 before test
  Aug  4 20:49:10.820: INFO: kindnet-grl89 from kube-system started at 2023-08-04 20:09:19 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.820: INFO: 	Container kindnet-cni ready: true, restart count 0
  Aug  4 20:49:10.820: INFO: kube-proxy-nn2ml from kube-system started at 2023-08-04 19:47:12 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.820: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug  4 20:49:10.820: INFO: httpd-deployment-5cd84d4f9-lxd6j from kubectl-5372 started at 2023-08-04 20:48:57 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.820: INFO: 	Container httpd ready: false, restart count 0
  Aug  4 20:49:10.820: INFO: annotationupdatea2ab6ceb-1615-4b2c-9ea4-853f16ea6776 from projected-5504 started at 2023-08-04 20:49:00 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.820: INFO: 	Container client-container ready: true, restart count 0
  Aug  4 20:49:10.820: INFO: sonobuoy from sonobuoy started at 2023-08-04 19:47:19 +0000 UTC (1 container statuses recorded)
  Aug  4 20:49:10.820: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug  4 20:49:10.820: INFO: sonobuoy-e2e-job-e2b8ac8d10f74a23 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 20:49:10.821: INFO: 	Container e2e ready: true, restart count 0
  Aug  4 20:49:10.821: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 20:49:10.821: INFO: sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-plz82 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 20:49:10.821: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 20:49:10.821: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 08/04/23 20:49:10.821
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.17784934b6e7b451], Reason = [FailedScheduling], Message = [0/2 nodes are available: 2 node(s) didn't match Pod's node affinity/selector. preemption: 0/2 nodes are available: 2 Preemption is not helpful for scheduling..] @ 08/04/23 20:49:10.836
  E0804 20:49:11.445078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:49:11.836: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5992" for this suite. @ 08/04/23 20:49:11.838
• [1.044 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 08/04/23 20:49:11.842
  Aug  4 20:49:11.842: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename endpointslicemirroring @ 08/04/23 20:49:11.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:11.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:11.855
  STEP: mirroring a new custom Endpoint @ 08/04/23 20:49:11.866
  Aug  4 20:49:11.871: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  E0804 20:49:12.445397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:13.445856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring an update to a custom Endpoint @ 08/04/23 20:49:13.874
  Aug  4 20:49:13.880: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  E0804 20:49:14.445920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:15.446256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mirroring deletion of a custom Endpoint @ 08/04/23 20:49:15.908
  Aug  4 20:49:15.917: INFO: Waiting for 0 EndpointSlices to exist, got 1
  E0804 20:49:16.446757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:17.447007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:49:17.920: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-1696" for this suite. @ 08/04/23 20:49:17.923
• [6.085 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 08/04/23 20:49:17.927
  Aug  4 20:49:17.927: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replicaset @ 08/04/23 20:49:17.928
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:17.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:17.937
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 08/04/23 20:49:17.939
  Aug  4 20:49:17.944: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0804 20:49:18.447960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:19.448912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:20.449085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:21.449541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:22.449863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:49:22.946: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/04/23 20:49:22.946
  STEP: getting scale subresource @ 08/04/23 20:49:22.946
  STEP: updating a scale subresource @ 08/04/23 20:49:22.948
  STEP: verifying the replicaset Spec.Replicas was modified @ 08/04/23 20:49:22.953
  STEP: Patch a scale subresource @ 08/04/23 20:49:22.955
  Aug  4 20:49:22.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4071" for this suite. @ 08/04/23 20:49:22.966
• [5.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 08/04/23 20:49:22.971
  Aug  4 20:49:22.971: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename watch @ 08/04/23 20:49:22.972
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:22.981
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:22.983
  STEP: creating a watch on configmaps with label A @ 08/04/23 20:49:22.984
  STEP: creating a watch on configmaps with label B @ 08/04/23 20:49:22.985
  STEP: creating a watch on configmaps with label A or B @ 08/04/23 20:49:23.004
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 08/04/23 20:49:23.005
  Aug  4 20:49:23.009: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-629  2e7b35e4-c2bc-49ab-aaf0-85a71f961685 20711 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:49:23.010: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-629  2e7b35e4-c2bc-49ab-aaf0-85a71f961685 20711 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 08/04/23 20:49:23.01
  Aug  4 20:49:23.014: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-629  2e7b35e4-c2bc-49ab-aaf0-85a71f961685 20712 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:49:23.014: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-629  2e7b35e4-c2bc-49ab-aaf0-85a71f961685 20712 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 08/04/23 20:49:23.014
  Aug  4 20:49:23.019: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-629  2e7b35e4-c2bc-49ab-aaf0-85a71f961685 20713 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:49:23.019: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-629  2e7b35e4-c2bc-49ab-aaf0-85a71f961685 20713 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 08/04/23 20:49:23.019
  Aug  4 20:49:23.022: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-629  2e7b35e4-c2bc-49ab-aaf0-85a71f961685 20714 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:49:23.022: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-629  2e7b35e4-c2bc-49ab-aaf0-85a71f961685 20714 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 08/04/23 20:49:23.022
  Aug  4 20:49:23.025: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-629  f1634f5f-8c7d-4619-a045-f2a91a55fa8a 20715 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:49:23.025: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-629  f1634f5f-8c7d-4619-a045-f2a91a55fa8a 20715 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0804 20:49:23.450355      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:24.450907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:25.451002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:26.451783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:27.452047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:28.452331      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:29.452535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:30.453584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:31.454004      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:32.454226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 08/04/23 20:49:33.027
  Aug  4 20:49:33.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-629  f1634f5f-8c7d-4619-a045-f2a91a55fa8a 20746 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Aug  4 20:49:33.031: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-629  f1634f5f-8c7d-4619-a045-f2a91a55fa8a 20746 0 2023-08-04 20:49:23 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-08-04 20:49:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  E0804 20:49:33.454994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:34.456039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:35.456310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:36.457033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:37.457242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:38.457616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:39.457782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:40.457899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:41.458306      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:42.458461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:49:43.032: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-629" for this suite. @ 08/04/23 20:49:43.034
• [20.067 seconds]
------------------------------
SSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 08/04/23 20:49:43.039
  Aug  4 20:49:43.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename subjectreview @ 08/04/23 20:49:43.039
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:43.049
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:43.051
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-9454" @ 08/04/23 20:49:43.053
  Aug  4 20:49:43.056: INFO: saUsername: "system:serviceaccount:subjectreview-9454:e2e"
  Aug  4 20:49:43.056: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-9454"}
  Aug  4 20:49:43.056: INFO: saUID: "ae57cceb-0ba0-441a-b3b5-9aca4cbbe578"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-9454:e2e" @ 08/04/23 20:49:43.056
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-9454:e2e" @ 08/04/23 20:49:43.056
  Aug  4 20:49:43.058: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-9454:e2e" api 'list' configmaps in "subjectreview-9454" namespace @ 08/04/23 20:49:43.058
  Aug  4 20:49:43.059: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-9454:e2e" @ 08/04/23 20:49:43.059
  Aug  4 20:49:43.060: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Aug  4 20:49:43.060: INFO: LocalSubjectAccessReview has been verified
  Aug  4 20:49:43.060: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-9454" for this suite. @ 08/04/23 20:49:43.063
• [0.029 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 08/04/23 20:49:43.069
  Aug  4 20:49:43.069: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename containers @ 08/04/23 20:49:43.069
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:43.078
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:43.08
  STEP: Creating a pod to test override all @ 08/04/23 20:49:43.081
  E0804 20:49:43.459368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:44.459659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:45.460060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:46.460666      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:49:47.093
  Aug  4 20:49:47.095: INFO: Trying to get logs from node k8sconformance-m02 pod client-containers-46c9b2d7-6429-49d5-8bb6-f28b8a30b3d3 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 20:49:47.1
  Aug  4 20:49:47.108: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-9300" for this suite. @ 08/04/23 20:49:47.111
• [4.047 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 08/04/23 20:49:47.116
  Aug  4 20:49:47.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename statefulset @ 08/04/23 20:49:47.116
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:49:47.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:49:47.125
  STEP: Creating service test in namespace statefulset-2830 @ 08/04/23 20:49:47.127
  STEP: Looking for a node to schedule stateful set and pod @ 08/04/23 20:49:47.132
  STEP: Creating pod with conflicting port in namespace statefulset-2830 @ 08/04/23 20:49:47.134
  STEP: Waiting until pod test-pod will start running in namespace statefulset-2830 @ 08/04/23 20:49:47.139
  E0804 20:49:47.461294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:48.461518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating statefulset with conflicting port in namespace statefulset-2830 @ 08/04/23 20:49:49.143
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-2830 @ 08/04/23 20:49:49.148
  Aug  4 20:49:49.159: INFO: Observed stateful pod in namespace: statefulset-2830, name: ss-0, uid: bb80dec3-54d8-4ec6-b1a7-61f5c77f3cbd, status phase: Pending. Waiting for statefulset controller to delete.
  Aug  4 20:49:49.168: INFO: Observed stateful pod in namespace: statefulset-2830, name: ss-0, uid: bb80dec3-54d8-4ec6-b1a7-61f5c77f3cbd, status phase: Failed. Waiting for statefulset controller to delete.
  Aug  4 20:49:49.174: INFO: Observed stateful pod in namespace: statefulset-2830, name: ss-0, uid: bb80dec3-54d8-4ec6-b1a7-61f5c77f3cbd, status phase: Failed. Waiting for statefulset controller to delete.
  Aug  4 20:49:49.175: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-2830
  STEP: Removing pod with conflicting port in namespace statefulset-2830 @ 08/04/23 20:49:49.175
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-2830 and will be in running state @ 08/04/23 20:49:49.187
  E0804 20:49:49.462246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:50.462457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:49:51.194: INFO: Deleting all statefulset in ns statefulset-2830
  Aug  4 20:49:51.196: INFO: Scaling statefulset ss to 0
  E0804 20:49:51.462469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:52.462730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:53.462992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:54.464023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:55.464280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:56.464820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:57.465028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:58.465254      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:49:59.465483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:00.465799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:50:01.207: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 20:50:01.209: INFO: Deleting statefulset ss
  Aug  4 20:50:01.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-2830" for this suite. @ 08/04/23 20:50:01.219
• [14.108 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 08/04/23 20:50:01.225
  Aug  4 20:50:01.225: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:50:01.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:50:01.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:50:01.238
  STEP: creating service in namespace services-655 @ 08/04/23 20:50:01.24
  STEP: creating service affinity-nodeport-transition in namespace services-655 @ 08/04/23 20:50:01.24
  STEP: creating replication controller affinity-nodeport-transition in namespace services-655 @ 08/04/23 20:50:01.253
  I0804 20:50:01.258315      22 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-655, replica count: 3
  E0804 20:50:01.465873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:02.466137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:03.466837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:50:04.309836      22 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 20:50:04.316: INFO: Creating new exec pod
  E0804 20:50:04.466926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:05.466979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:06.467829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:50:07.327: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-655 exec execpod-affinitytrxpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Aug  4 20:50:07.454: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Aug  4 20:50:07.454: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:50:07.454: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-655 exec execpod-affinitytrxpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.101.177.180 80'
  E0804 20:50:07.468617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:50:07.601: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.101.177.180 80\nConnection to 10.101.177.180 80 port [tcp/http] succeeded!\n"
  Aug  4 20:50:07.601: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:50:07.601: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-655 exec execpod-affinitytrxpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.2 30249'
  Aug  4 20:50:07.716: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.2 30249\nConnection to 192.168.49.2 30249 port [tcp/*] succeeded!\n"
  Aug  4 20:50:07.716: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:50:07.716: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-655 exec execpod-affinitytrxpw -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.49.3 30249'
  Aug  4 20:50:07.842: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.49.3 30249\nConnection to 192.168.49.3 30249 port [tcp/*] succeeded!\n"
  Aug  4 20:50:07.842: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 20:50:07.849: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-655 exec execpod-affinitytrxpw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:30249/ ; done'
  Aug  4 20:50:08.036: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n"
  Aug  4 20:50:08.036: INFO: stdout: "\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-6z65p\naffinity-nodeport-transition-6z65p\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-9xtjr\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-6z65p\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-9xtjr\naffinity-nodeport-transition-6z65p\naffinity-nodeport-transition-6z65p\naffinity-nodeport-transition-9xtjr\naffinity-nodeport-transition-l6fjt"
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-6z65p
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-6z65p
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-9xtjr
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-6z65p
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-9xtjr
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-6z65p
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-6z65p
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-9xtjr
  Aug  4 20:50:08.036: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-655 exec execpod-affinitytrxpw -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.49.2:30249/ ; done'
  Aug  4 20:50:08.235: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.49.2:30249/\n"
  Aug  4 20:50:08.235: INFO: stdout: "\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt\naffinity-nodeport-transition-l6fjt"
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Received response from host: affinity-nodeport-transition-l6fjt
  Aug  4 20:50:08.235: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 20:50:08.238: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-655, will wait for the garbage collector to delete the pods @ 08/04/23 20:50:08.246
  Aug  4 20:50:08.304: INFO: Deleting ReplicationController affinity-nodeport-transition took: 4.587095ms
  Aug  4 20:50:08.404: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 100.379321ms
  E0804 20:50:08.469368      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:09.470450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:10.471097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-655" for this suite. @ 08/04/23 20:50:10.519
• [9.298 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 08/04/23 20:50:10.523
  Aug  4 20:50:10.523: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:50:10.524
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:50:10.532
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:50:10.534
  STEP: creating the pod @ 08/04/23 20:50:10.535
  STEP: setting up watch @ 08/04/23 20:50:10.536
  STEP: submitting the pod to kubernetes @ 08/04/23 20:50:10.638
  STEP: verifying the pod is in kubernetes @ 08/04/23 20:50:10.645
  STEP: verifying pod creation was observed @ 08/04/23 20:50:10.647
  E0804 20:50:11.471117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:12.472059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/04/23 20:50:12.654
  STEP: verifying pod deletion was observed @ 08/04/23 20:50:12.661
  Aug  4 20:50:13.452: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3886" for this suite. @ 08/04/23 20:50:13.454
• [2.935 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 08/04/23 20:50:13.458
  Aug  4 20:50:13.459: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:50:13.459
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:50:13.467
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:50:13.469
  STEP: Creating projection with secret that has name projected-secret-test-1aaf8319-0b17-4e2f-85cc-cf318f522046 @ 08/04/23 20:50:13.471
  E0804 20:50:13.472212      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a pod to test consume secrets @ 08/04/23 20:50:13.473
  E0804 20:50:14.472550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:15.472754      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:16.473796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:17.474110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:50:17.487
  Aug  4 20:50:17.489: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-secrets-7d1cb0e6-67b2-4c19-8d80-50be599f0e8b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 20:50:17.493
  Aug  4 20:50:17.503: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8727" for this suite. @ 08/04/23 20:50:17.505
• [4.050 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 08/04/23 20:50:17.509
  Aug  4 20:50:17.509: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/04/23 20:50:17.509
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:50:17.518
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:50:17.519
  E0804 20:50:18.474216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:19.474467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:50:19.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 08/04/23 20:50:19.54
  STEP: Cleaning up the configmap @ 08/04/23 20:50:19.543
  STEP: Cleaning up the pod @ 08/04/23 20:50:19.547
  STEP: Destroying namespace "emptydir-wrapper-6484" for this suite. @ 08/04/23 20:50:19.555
• [2.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 08/04/23 20:50:19.561
  Aug  4 20:50:19.561: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename var-expansion @ 08/04/23 20:50:19.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:50:19.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:50:19.571
  STEP: creating the pod with failed condition @ 08/04/23 20:50:19.573
  E0804 20:50:20.475111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:21.475519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:22.475641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:23.475736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:24.475838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:25.476116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:26.477025      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:27.477266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:28.478364      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:29.478453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:30.478824      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:31.479388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:32.479979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:33.480190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:34.480328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:35.480744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:36.480824      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:37.481055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:38.481185      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:39.481421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:40.481547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:41.482093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:42.482661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:43.482832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:44.483315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:45.484115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:46.485027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:47.485586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:48.486506      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:49.486721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:50.486985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:51.487439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:52.488268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:53.488471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:54.489199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:55.489392      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:56.490119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:57.490357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:58.490944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:50:59.490982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:00.492038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:01.492460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:02.493012      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:03.493097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:04.493580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:05.493797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:06.493973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:07.494211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:08.494303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:09.494740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:10.494960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:11.495499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:12.496467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:13.496597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:14.497358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:15.497797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:16.498070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:17.498298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:18.498443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:19.498708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:20.499766      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:21.500051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:22.500521      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:23.500738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:24.501397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:25.501639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:26.502510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:27.502751      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:28.503667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:29.503780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:30.504656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:31.505072      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:32.505720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:33.505980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:34.506468      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:35.506657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:36.507163      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:37.508096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:38.508756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:39.508959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:40.509279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:41.509680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:42.509935      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:43.510032      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:44.510459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:45.510669      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:46.511554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:47.512111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:48.513140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:49.513235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:50.513662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:51.514136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:52.514954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:53.514981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:54.516083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:55.516310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:56.517395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:57.517515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:58.518298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:51:59.518479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:00.518991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:01.519389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:02.520020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:03.520288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:04.521020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:05.521242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:06.521770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:07.521993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:08.522729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:09.522966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:10.524023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:11.524473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:12.524816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:13.525026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:14.525467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:15.525675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:16.526109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:17.526780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:18.526920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:19.527033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pod @ 08/04/23 20:52:19.582
  Aug  4 20:52:20.091: INFO: Successfully updated pod "var-expansion-fcfed626-121b-49d0-8a68-bde8fbbd52b8"
  STEP: waiting for pod running @ 08/04/23 20:52:20.092
  E0804 20:52:20.527742      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:21.528319      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 08/04/23 20:52:22.097
  Aug  4 20:52:22.097: INFO: Deleting pod "var-expansion-fcfed626-121b-49d0-8a68-bde8fbbd52b8" in namespace "var-expansion-1240"
  Aug  4 20:52:22.102: INFO: Wait up to 5m0s for pod "var-expansion-fcfed626-121b-49d0-8a68-bde8fbbd52b8" to be fully deleted
  E0804 20:52:22.529070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:23.529295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:24.529860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:25.530610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:26.531623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:27.532056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:28.532240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:29.532394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:30.532546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:31.532966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:32.533642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:33.533862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:34.534264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:35.534433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:36.535432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:37.535780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:38.536538      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:39.536756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:40.537379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:41.537918      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:42.538587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:43.539055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:44.539842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:45.540002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:46.540798      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:47.541056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:48.542023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:49.542245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:50.542938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:51.543301      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:52.544059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:53.544281      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:52:54.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1240" for this suite. @ 08/04/23 20:52:54.156
• [154.601 seconds]
------------------------------
S
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 08/04/23 20:52:54.162
  Aug  4 20:52:54.162: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:52:54.163
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:52:54.171
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:52:54.173
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-7278 @ 08/04/23 20:52:54.175
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/04/23 20:52:54.184
  STEP: creating service externalsvc in namespace services-7278 @ 08/04/23 20:52:54.184
  STEP: creating replication controller externalsvc in namespace services-7278 @ 08/04/23 20:52:54.193
  I0804 20:52:54.198790      22 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7278, replica count: 2
  E0804 20:52:54.544541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:55.544905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:56.545612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 20:52:57.250488      22 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 08/04/23 20:52:57.253
  Aug  4 20:52:57.262: INFO: Creating new exec pod
  E0804 20:52:57.546218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:52:58.546464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:52:59.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-7278 exec execpodqpw46 -- /bin/sh -x -c nslookup clusterip-service.services-7278.svc.cluster.local'
  Aug  4 20:52:59.442: INFO: stderr: "+ nslookup clusterip-service.services-7278.svc.cluster.local\n"
  Aug  4 20:52:59.442: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nclusterip-service.services-7278.svc.cluster.local\tcanonical name = externalsvc.services-7278.svc.cluster.local.\nName:\texternalsvc.services-7278.svc.cluster.local\nAddress: 10.102.206.206\n\n"
  Aug  4 20:52:59.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7278, will wait for the garbage collector to delete the pods @ 08/04/23 20:52:59.445
  Aug  4 20:52:59.501: INFO: Deleting ReplicationController externalsvc took: 4.425314ms
  E0804 20:52:59.546451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:52:59.602: INFO: Terminating ReplicationController externalsvc pods took: 100.849693ms
  E0804 20:53:00.546776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:53:01.414: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-7278" for this suite. @ 08/04/23 20:53:01.422
• [7.264 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 08/04/23 20:53:01.427
  Aug  4 20:53:01.427: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:53:01.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:53:01.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:53:01.438
  STEP: creating the pod @ 08/04/23 20:53:01.439
  STEP: submitting the pod to kubernetes @ 08/04/23 20:53:01.439
  W0804 20:53:01.445690      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  E0804 20:53:01.547795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:02.548027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 08/04/23 20:53:03.452
  STEP: updating the pod @ 08/04/23 20:53:03.453
  E0804 20:53:03.548080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:53:03.963: INFO: Successfully updated pod "pod-update-activedeadlineseconds-6dfb9f89-83a2-46dc-ba1f-40c9da3fe195"
  E0804 20:53:04.548994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:05.549232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:06.550028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:07.550279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:53:07.971: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-4483" for this suite. @ 08/04/23 20:53:07.973
• [6.550 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 08/04/23 20:53:07.978
  Aug  4 20:53:07.978: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 20:53:07.979
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:53:07.989
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:53:07.991
  STEP: Creating configMap with name cm-test-opt-del-0af65649-1793-4e65-aba7-a0b893f86fcb @ 08/04/23 20:53:07.995
  STEP: Creating configMap with name cm-test-opt-upd-8a69e193-c9a2-43e3-81bf-f2b91934c68e @ 08/04/23 20:53:07.998
  STEP: Creating the pod @ 08/04/23 20:53:08
  E0804 20:53:08.551125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:09.552097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting configmap cm-test-opt-del-0af65649-1793-4e65-aba7-a0b893f86fcb @ 08/04/23 20:53:10.035
  STEP: Updating configmap cm-test-opt-upd-8a69e193-c9a2-43e3-81bf-f2b91934c68e @ 08/04/23 20:53:10.038
  STEP: Creating configMap with name cm-test-opt-create-afb68be1-21e7-4101-a9b6-b6da95ccb82d @ 08/04/23 20:53:10.04
  STEP: waiting to observe update in volume @ 08/04/23 20:53:10.045
  E0804 20:53:10.552632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:11.553116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:53:12.067: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5733" for this suite. @ 08/04/23 20:53:12.069
• [4.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 08/04/23 20:53:12.073
  Aug  4 20:53:12.073: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir-wrapper @ 08/04/23 20:53:12.074
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:53:12.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:53:12.084
  STEP: Creating 50 configmaps @ 08/04/23 20:53:12.086
  STEP: Creating RC which spawns configmap-volume pods @ 08/04/23 20:53:12.327
  Aug  4 20:53:12.434: INFO: Pod name wrapped-volume-race-ff06f7d3-b718-4914-9351-c50620b96f45: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/04/23 20:53:12.434
  E0804 20:53:12.554165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:13.554471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 08/04/23 20:53:14.519
  Aug  4 20:53:14.529: INFO: Pod name wrapped-volume-race-5f6814ea-ae48-4c58-9353-9f534aaf4f2e: Found 0 pods out of 5
  E0804 20:53:14.555099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:15.555229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:16.556083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:17.556111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:18.556347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:53:19.535: INFO: Pod name wrapped-volume-race-5f6814ea-ae48-4c58-9353-9f534aaf4f2e: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/04/23 20:53:19.535
  STEP: Creating RC which spawns configmap-volume pods @ 08/04/23 20:53:19.548
  E0804 20:53:19.556594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:53:19.560: INFO: Pod name wrapped-volume-race-ea089a2b-a919-4886-ac56-07e53a95e023: Found 0 pods out of 5
  E0804 20:53:20.557690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:21.558304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:22.558550      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:23.558822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:24.558996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:53:24.568: INFO: Pod name wrapped-volume-race-ea089a2b-a919-4886-ac56-07e53a95e023: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 08/04/23 20:53:24.568
  Aug  4 20:53:24.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-ea089a2b-a919-4886-ac56-07e53a95e023 in namespace emptydir-wrapper-7154, will wait for the garbage collector to delete the pods @ 08/04/23 20:53:24.581
  Aug  4 20:53:24.639: INFO: Deleting ReplicationController wrapped-volume-race-ea089a2b-a919-4886-ac56-07e53a95e023 took: 4.947932ms
  Aug  4 20:53:24.740: INFO: Terminating ReplicationController wrapped-volume-race-ea089a2b-a919-4886-ac56-07e53a95e023 pods took: 100.702918ms
  E0804 20:53:25.559874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-5f6814ea-ae48-4c58-9353-9f534aaf4f2e in namespace emptydir-wrapper-7154, will wait for the garbage collector to delete the pods @ 08/04/23 20:53:26.241
  Aug  4 20:53:26.300: INFO: Deleting ReplicationController wrapped-volume-race-5f6814ea-ae48-4c58-9353-9f534aaf4f2e took: 5.746239ms
  Aug  4 20:53:26.400: INFO: Terminating ReplicationController wrapped-volume-race-5f6814ea-ae48-4c58-9353-9f534aaf4f2e pods took: 100.596652ms
  E0804 20:53:26.560625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:27.561307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-ff06f7d3-b718-4914-9351-c50620b96f45 in namespace emptydir-wrapper-7154, will wait for the garbage collector to delete the pods @ 08/04/23 20:53:27.701
  Aug  4 20:53:27.761: INFO: Deleting ReplicationController wrapped-volume-race-ff06f7d3-b718-4914-9351-c50620b96f45 took: 6.243327ms
  Aug  4 20:53:27.862: INFO: Terminating ReplicationController wrapped-volume-race-ff06f7d3-b718-4914-9351-c50620b96f45 pods took: 101.144279ms
  E0804 20:53:28.561812      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 08/04/23 20:53:28.762
  STEP: Destroying namespace "emptydir-wrapper-7154" for this suite. @ 08/04/23 20:53:28.961
• [16.891 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 08/04/23 20:53:28.964
  Aug  4 20:53:28.964: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 20:53:28.965
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:53:28.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:53:28.974
  STEP: fetching services @ 08/04/23 20:53:28.976
  Aug  4 20:53:28.978: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-9118" for this suite. @ 08/04/23 20:53:28.98
• [0.019 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 08/04/23 20:53:28.984
  Aug  4 20:53:28.984: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 20:53:28.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:53:28.991
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:53:28.993
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 08/04/23 20:53:28.995
  E0804 20:53:29.562571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:30.562871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:31.563767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:32.563989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:53:33.008
  Aug  4 20:53:33.010: INFO: Trying to get logs from node k8sconformance-m02 pod pod-73cc68cd-5437-4369-b732-8327961b8151 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 20:53:33.016
  Aug  4 20:53:33.026: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1866" for this suite. @ 08/04/23 20:53:33.028
• [4.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 08/04/23 20:53:33.032
  Aug  4 20:53:33.032: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:53:33.033
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:53:33.042
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:53:33.044
  STEP: Creating pod busybox-df21b0ce-b6e2-491b-bbbb-d71ffedece1d in namespace container-probe-6119 @ 08/04/23 20:53:33.046
  E0804 20:53:33.564477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:34.564722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:53:35.055: INFO: Started pod busybox-df21b0ce-b6e2-491b-bbbb-d71ffedece1d in namespace container-probe-6119
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/04/23 20:53:35.055
  Aug  4 20:53:35.057: INFO: Initial restart count of pod busybox-df21b0ce-b6e2-491b-bbbb-d71ffedece1d is 0
  E0804 20:53:35.565239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:36.565949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:37.566905      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:38.567014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:39.567735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:40.567983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:41.568999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:42.569266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:43.569312      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:44.569730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:45.569760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:46.570411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:47.571088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:48.572094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:49.572868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:50.573264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:51.574138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:52.574365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:53.574466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:54.575029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:55.575792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:56.576396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:57.577116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:58.578188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:53:59.578924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:00.579006      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:01.579658      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:02.579797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:03.580665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:04.580884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:05.581279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:06.582055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:07.582867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:08.582991      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:09.583826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:10.584042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:11.585044      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:12.585220      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:13.585878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:14.586105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:15.586772      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:16.587263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:17.588040      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:18.588257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:19.588628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:20.588910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:21.589747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:22.589961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:23.590977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:24.591402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:25.592035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:26.592524      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:27.593198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:28.594168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:29.594517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:30.594714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:31.595584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:32.596049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:33.597060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:34.597313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:35.597606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:36.598154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:37.598466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:38.598589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:39.599377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:40.600000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:41.600791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:42.600995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:43.602097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:44.602886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:45.603486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:46.604073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:47.604810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:48.605034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:49.605715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:50.606452      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:51.607169      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:52.608041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:53.609034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:54.609225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:55.609340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:56.609847      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:57.610470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:58.610673      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:54:59.611466      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:00.612049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:01.613039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:02.613271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:03.613409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:04.613690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:05.614105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:06.614659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:07.615256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:08.616340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:09.616800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:10.617160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:11.618256      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:12.618495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:13.619290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:14.620065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:15.620332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:16.621141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:17.621851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:18.622002      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:19.622618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:20.622837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:21.623649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:22.624066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:23.624149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:24.624365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:25.625240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:26.625809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:27.626769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:28.626937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:29.627453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:30.628100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:31.629061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:32.629263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:33.629960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:34.630144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:35.630700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:36.631162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:37.631930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:38.632122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:39.632560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:40.632752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:41.633397      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:42.634421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:43.635074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:44.636066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:45.636189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:46.636703      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:47.637268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:48.637465      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:49.638091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:50.638887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:51.639777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:52.639972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:53.640896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:54.641082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:55.641294      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:56.641904      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:57.642154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:58.642871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:55:59.643523      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:00.644492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:01.645234      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:02.645470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:03.645795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:04.646000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:05.646683      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:06.647171      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:07.647807      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:08.648000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:09.648706      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:10.648917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:11.649676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:12.649907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:13.650744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:14.650997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:15.651637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:16.652195      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:17.652820      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:18.652963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:19.653293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:20.653469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:21.653614      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:22.653821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:23.654892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:24.654951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:25.655535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:26.656032      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:27.656605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:28.656800      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:29.657612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:30.657833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:31.658422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:32.658628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:33.658693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:34.658915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:35.659396      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:36.660001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:37.660783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:38.660975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:39.661662      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:40.661845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:41.661996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:42.662193      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:43.663087      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:44.664008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:45.664377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:46.665078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:47.665884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:48.666076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:49.666667      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:50.666892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:51.667491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:52.667631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:53.668474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:54.668657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:55.668947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:56.669086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:57.669633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:58.670363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:56:59.670637      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:00.670893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:01.671293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:02.672108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:03.672674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:04.672847      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:05.673333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:06.673909      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:07.674629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:08.674858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:09.675410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:10.676045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:11.676781      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:12.677237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:13.678021      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:14.678237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:15.678844      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:16.679414      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:17.680073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:18.680275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:19.680756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:20.680967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:21.681100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:22.681470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:23.682499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:24.682730      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:25.682986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:26.683592      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:27.684090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:28.684335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:29.684415      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:30.684480      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:31.685127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:32.686140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:33.686575      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:34.687499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:57:35.418: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 20:57:35.421
  STEP: Destroying namespace "container-probe-6119" for this suite. @ 08/04/23 20:57:35.429
• [242.402 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 08/04/23 20:57:35.435
  Aug  4 20:57:35.435: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 20:57:35.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:57:35.444
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:57:35.446
  STEP: Create set of pods @ 08/04/23 20:57:35.448
  Aug  4 20:57:35.453: INFO: created test-pod-1
  Aug  4 20:57:35.456: INFO: created test-pod-2
  Aug  4 20:57:35.459: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 08/04/23 20:57:35.459
  E0804 20:57:35.687928      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:36.688528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: waiting for all pods to be deleted @ 08/04/23 20:57:37.484
  Aug  4 20:57:37.486: INFO: Pod quantity 3 is different from expected quantity 0
  E0804 20:57:37.688664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:57:38.489: INFO: Pod quantity 3 is different from expected quantity 0
  E0804 20:57:38.689695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:57:39.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7576" for this suite. @ 08/04/23 20:57:39.492
• [4.063 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 08/04/23 20:57:39.498
  Aug  4 20:57:39.498: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename subpath @ 08/04/23 20:57:39.499
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:57:39.506
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:57:39.508
  STEP: Setting up data @ 08/04/23 20:57:39.51
  STEP: Creating pod pod-subpath-test-downwardapi-5bjj @ 08/04/23 20:57:39.516
  STEP: Creating a pod to test atomic-volume-subpath @ 08/04/23 20:57:39.516
  E0804 20:57:39.690567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:40.690956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:41.691512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:42.691802      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:43.692840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:44.693054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:45.693579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:46.694305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:47.694770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:48.694989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:49.695126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:50.696050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:51.696719      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:52.697610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:53.698302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:54.698498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:55.698619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:56.699105      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:57.700084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:58.700329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:57:59.700919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:00.701162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:01.701962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:02.702196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 20:58:03.563
  Aug  4 20:58:03.565: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-downwardapi-5bjj container test-container-subpath-downwardapi-5bjj: <nil>
  STEP: delete the pod @ 08/04/23 20:58:03.576
  STEP: Deleting pod pod-subpath-test-downwardapi-5bjj @ 08/04/23 20:58:03.588
  Aug  4 20:58:03.588: INFO: Deleting pod "pod-subpath-test-downwardapi-5bjj" in namespace "subpath-3787"
  Aug  4 20:58:03.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-3787" for this suite. @ 08/04/23 20:58:03.592
• [24.097 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 08/04/23 20:58:03.596
  Aug  4 20:58:03.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-probe @ 08/04/23 20:58:03.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 20:58:03.606
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 20:58:03.608
  STEP: Creating pod test-webserver-c85ae7d5-5b0a-4eb4-ae6b-491a9f712787 in namespace container-probe-2107 @ 08/04/23 20:58:03.61
  E0804 20:58:03.702848      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:04.702986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 20:58:05.620: INFO: Started pod test-webserver-c85ae7d5-5b0a-4eb4-ae6b-491a9f712787 in namespace container-probe-2107
  STEP: checking the pod's current state and verifying that restartCount is present @ 08/04/23 20:58:05.62
  Aug  4 20:58:05.622: INFO: Initial restart count of pod test-webserver-c85ae7d5-5b0a-4eb4-ae6b-491a9f712787 is 0
  E0804 20:58:05.703654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:06.704048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:07.704579      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:08.704833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:09.705464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:10.705716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:11.706612      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:12.706932      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:13.707968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:14.708135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:15.708625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:16.709055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:17.710174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:18.710425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:19.710514      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:20.710732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:21.711371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:22.711586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:23.711797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:24.712660      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:25.713136      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:26.713674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:27.714222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:28.714426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:29.714925      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:30.716016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:31.716638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:32.716816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:33.716837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:34.717067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:35.717544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:36.718275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:37.718870      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:38.718950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:39.719544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:40.720139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:41.720875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:42.721069      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:43.722035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:44.722288      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:45.722887      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:46.723453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:47.724308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:48.724510      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:49.724908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:50.725042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:51.725913      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:52.726114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:53.726593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:54.726777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:55.727345      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:56.728131      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:57.728732      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:58.728942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:58:59.729471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:00.730269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:01.731075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:02.732076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:03.733157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:04.733387      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:05.733883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:06.734429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:07.735453      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:08.735672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:09.736034      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:10.736263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:11.736902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:12.737109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:13.737167      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:14.737380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:15.737886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:16.738470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:17.738933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:18.739149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:19.739785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:20.740127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:21.740699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:22.740898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:23.741796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:24.741993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:25.742454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:26.743158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:27.743401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:28.744030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:29.744257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:30.744689      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:31.745233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:32.745881      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:33.746114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:34.746232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:35.746448      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:36.747300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:37.748047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:38.748620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:39.748819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:40.749451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:41.749819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:42.750418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:43.750603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:44.750678      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:45.751459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:46.752047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:47.752258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:48.752859      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:49.753102      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:50.753695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:51.754252      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:52.754945      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:53.755187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:54.756026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:55.756919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:56.757871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:57.758081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:58.758765      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 20:59:59.759015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:00.759121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:01.759499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:02.760147      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:03.760620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:04.760890      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:05.761029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:06.762121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:07.762365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:08.762969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:09.763194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:10.763710      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:11.764115      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:12.764684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:13.765607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:14.766611      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:15.766739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:16.767247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:17.767531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:18.768097      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:19.768315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:20.769208      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:21.769522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:22.770250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:23.770496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:24.770741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:25.770984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:26.772077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:27.772279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:28.773085      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:29.773285      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:30.774155      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:31.774629      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:32.774981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:33.776092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:34.776198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:35.776449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:36.777286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:37.778077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:38.778737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:39.778927      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:40.778988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:41.780055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:42.780603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:43.780792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:44.781401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:45.781598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:46.782620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:47.782792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:48.783305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:49.783529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:50.784154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:51.784488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:52.785064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:53.785792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:54.785892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:55.786125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:56.786214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:57.786435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:58.786948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:00:59.787151      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:00.788041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:01.788413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:02.789048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:03.789458      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:04.789537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:05.789774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:06.789972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:07.790196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:08.790691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:09.790851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:10.791304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:11.791680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:12.792371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:13.792565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:14.792597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:15.792791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:16.793349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:17.793569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:18.793923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:19.794101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:20.794618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:21.794943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:22.795096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:23.796057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:24.797110      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:25.797351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:26.798407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:27.798607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:28.799595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:29.799810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:30.800308      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:31.800690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:32.801047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:33.801271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:34.802326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:35.802527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:36.803413      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:37.804075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:38.804366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:39.804492      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:40.804837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:41.804997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:42.805211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:43.805407      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:44.805645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:45.805767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:46.806831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:47.806970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:48.808066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:49.808302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:50.808961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:51.809313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:52.809464      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:53.809671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:54.810320      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:55.810552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:56.811325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:57.811517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:58.811908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:01:59.812877      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:00.813076      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:01.813473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:02.813955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:03.814172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:04.815018      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:05.816089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:05.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 21:02:05.982
  STEP: Destroying namespace "container-probe-2107" for this suite. @ 08/04/23 21:02:05.991
• [242.399 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 08/04/23 21:02:05.995
  Aug  4 21:02:05.995: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 21:02:05.996
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:06.005
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:06.007
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-7160 @ 08/04/23 21:02:06.009
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 08/04/23 21:02:06.023
  STEP: creating service externalsvc in namespace services-7160 @ 08/04/23 21:02:06.023
  STEP: creating replication controller externalsvc in namespace services-7160 @ 08/04/23 21:02:06.031
  I0804 21:02:06.038040      22 runners.go:194] Created replication controller with name: externalsvc, namespace: services-7160, replica count: 2
  E0804 21:02:06.816310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:07.816322      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:08.816583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 21:02:09.089043      22 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 08/04/23 21:02:09.091
  Aug  4 21:02:09.104: INFO: Creating new exec pod
  E0804 21:02:09.817476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:10.817705      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:11.112: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-7160 exec execpodns4zp -- /bin/sh -x -c nslookup nodeport-service.services-7160.svc.cluster.local'
  Aug  4 21:02:11.292: INFO: stderr: "+ nslookup nodeport-service.services-7160.svc.cluster.local\n"
  Aug  4 21:02:11.292: INFO: stdout: "Server:\t\t10.96.0.10\nAddress:\t10.96.0.10#53\n\nnodeport-service.services-7160.svc.cluster.local\tcanonical name = externalsvc.services-7160.svc.cluster.local.\nName:\texternalsvc.services-7160.svc.cluster.local\nAddress: 10.101.135.194\n\n"
  Aug  4 21:02:11.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-7160, will wait for the garbage collector to delete the pods @ 08/04/23 21:02:11.295
  Aug  4 21:02:11.351: INFO: Deleting ReplicationController externalsvc took: 4.168835ms
  Aug  4 21:02:11.451: INFO: Terminating ReplicationController externalsvc pods took: 100.1337ms
  E0804 21:02:11.818562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:12.818768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:13.062: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-7160" for this suite. @ 08/04/23 21:02:13.068
• [7.077 seconds]
------------------------------
S
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 08/04/23 21:02:13.072
  Aug  4 21:02:13.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 21:02:13.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:13.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:13.083
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 08/04/23 21:02:13.085
  E0804 21:02:13.819007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:14.819389      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:15.819756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:16.820305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:02:17.099
  Aug  4 21:02:17.101: INFO: Trying to get logs from node k8sconformance-m02 pod pod-9fe878b6-a398-4db9-b6d0-ebf252dd9a75 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 21:02:17.115
  Aug  4 21:02:17.125: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-1924" for this suite. @ 08/04/23 21:02:17.127
• [4.059 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 08/04/23 21:02:17.132
  Aug  4 21:02:17.132: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename namespaces @ 08/04/23 21:02:17.132
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:17.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:17.142
  STEP: Read namespace status @ 08/04/23 21:02:17.144
  Aug  4 21:02:17.145: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 08/04/23 21:02:17.145
  Aug  4 21:02:17.149: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 08/04/23 21:02:17.149
  Aug  4 21:02:17.153: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Aug  4 21:02:17.153: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-777" for this suite. @ 08/04/23 21:02:17.155
• [0.028 seconds]
------------------------------
S
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 08/04/23 21:02:17.159
  Aug  4 21:02:17.159: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename security-context @ 08/04/23 21:02:17.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:17.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:17.169
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 08/04/23 21:02:17.171
  E0804 21:02:17.820366      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:18.820598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:02:19.181
  Aug  4 21:02:19.183: INFO: Trying to get logs from node k8sconformance-m02 pod security-context-dd0a73aa-8322-4305-bf84-50bdfef4734b container test-container: <nil>
  STEP: delete the pod @ 08/04/23 21:02:19.188
  Aug  4 21:02:19.197: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-1595" for this suite. @ 08/04/23 21:02:19.199
• [2.043 seconds]
------------------------------
S
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 08/04/23 21:02:19.203
  Aug  4 21:02:19.203: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sysctl @ 08/04/23 21:02:19.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:19.211
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:19.213
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 08/04/23 21:02:19.215
  STEP: Watching for error events or started pod @ 08/04/23 21:02:19.219
  E0804 21:02:19.820699      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:20.820894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 08/04/23 21:02:21.223
  STEP: Checking that the pod succeeded @ 08/04/23 21:02:21.226
  STEP: Getting logs from the pod @ 08/04/23 21:02:21.226
  STEP: Checking that the sysctl is actually updated @ 08/04/23 21:02:21.231
  Aug  4 21:02:21.231: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-6356" for this suite. @ 08/04/23 21:02:21.233
• [2.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 08/04/23 21:02:21.238
  Aug  4 21:02:21.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename aggregator @ 08/04/23 21:02:21.238
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:21.246
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:21.248
  Aug  4 21:02:21.250: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Registering the sample API server. @ 08/04/23 21:02:21.251
  Aug  4 21:02:21.782: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Aug  4 21:02:21.801: INFO: deployment "sample-apiserver-deployment" doesn't have the required revision set
  E0804 21:02:21.821840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:22.822259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:23.823632      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:23.830: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:24.824016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:25.824168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:25.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:26.824552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:27.824825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:27.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:28.825477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:29.825698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:29.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:30.826621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:31.826972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:31.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:32.828050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:33.828291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:33.834: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:34.828976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:35.829257      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:35.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:36.830268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:37.830495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:37.832: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:38.830680      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:39.830971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:39.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:40.832094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:41.832531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:41.833: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 21, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:02:42.832716      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:43.833242      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:43.948: INFO: Waited 109.12497ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 08/04/23 21:02:43.98
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 08/04/23 21:02:43.982
  STEP: List APIServices @ 08/04/23 21:02:43.988
  Aug  4 21:02:43.991: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 08/04/23 21:02:43.991
  Aug  4 21:02:44.002: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 08/04/23 21:02:44.002
  Aug  4 21:02:44.009: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.August, 4, 21, 2, 43, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 08/04/23 21:02:44.009
  Aug  4 21:02:44.012: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-08-04 21:02:43 +0000 UTC Passed all checks passed}
  Aug  4 21:02:44.012: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  4 21:02:44.012: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 08/04/23 21:02:44.012
  Aug  4 21:02:44.020: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-882013414" @ 08/04/23 21:02:44.02
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 08/04/23 21:02:44.028
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 08/04/23 21:02:44.033
  STEP: Patch APIService Status @ 08/04/23 21:02:44.035
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 08/04/23 21:02:44.04
  Aug  4 21:02:44.042: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-08-04 21:02:43 +0000 UTC Passed all checks passed}
  Aug  4 21:02:44.042: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  4 21:02:44.042: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Aug  4 21:02:44.042: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 08/04/23 21:02:44.043
  STEP: Confirm that the generated APIService has been deleted @ 08/04/23 21:02:44.045
  Aug  4 21:02:44.045: INFO: Requesting list of APIServices to confirm quantity
  Aug  4 21:02:44.048: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Aug  4 21:02:44.048: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Aug  4 21:02:44.093: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-5713" for this suite. @ 08/04/23 21:02:44.114
• [22.880 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 08/04/23 21:02:44.118
  Aug  4 21:02:44.118: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 21:02:44.119
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:44.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:44.133
  STEP: creating the pod @ 08/04/23 21:02:44.135
  STEP: submitting the pod to kubernetes @ 08/04/23 21:02:44.135
  E0804 21:02:44.833518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:45.833875      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod is in kubernetes @ 08/04/23 21:02:46.149
  STEP: updating the pod @ 08/04/23 21:02:46.151
  Aug  4 21:02:46.660: INFO: Successfully updated pod "pod-update-70349e43-14a4-4610-950d-d936a338ebf5"
  STEP: verifying the updated pod is in kubernetes @ 08/04/23 21:02:46.662
  Aug  4 21:02:46.664: INFO: Pod update OK
  Aug  4 21:02:46.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9402" for this suite. @ 08/04/23 21:02:46.667
• [2.553 seconds]
------------------------------
SSSS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 08/04/23 21:02:46.672
  Aug  4 21:02:46.672: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 08/04/23 21:02:46.672
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:46.68
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:46.682
  STEP: Setting up the test @ 08/04/23 21:02:46.683
  STEP: Creating hostNetwork=false pod @ 08/04/23 21:02:46.683
  E0804 21:02:46.834200      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:47.834457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating hostNetwork=true pod @ 08/04/23 21:02:48.696
  E0804 21:02:48.834965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:49.834998      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Running the test @ 08/04/23 21:02:50.707
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 08/04/23 21:02:50.707
  Aug  4 21:02:50.707: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:50.707: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:50.707: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:50.707: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug  4 21:02:50.770: INFO: Exec stderr: ""
  Aug  4 21:02:50.770: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:50.770: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:50.771: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:50.771: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug  4 21:02:50.813: INFO: Exec stderr: ""
  Aug  4 21:02:50.813: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:50.813: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:50.814: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:50.814: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  E0804 21:02:50.835243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:50.879: INFO: Exec stderr: ""
  Aug  4 21:02:50.879: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:50.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:50.879: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:50.879: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug  4 21:02:50.946: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 08/04/23 21:02:50.946
  Aug  4 21:02:50.946: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:50.946: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:50.947: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:50.947: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug  4 21:02:50.991: INFO: Exec stderr: ""
  Aug  4 21:02:50.991: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:50.992: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:50.992: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:50.992: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Aug  4 21:02:51.065: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 08/04/23 21:02:51.065
  Aug  4 21:02:51.066: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:51.066: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:51.066: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:51.066: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug  4 21:02:51.113: INFO: Exec stderr: ""
  Aug  4 21:02:51.113: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:51.113: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:51.114: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:51.114: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Aug  4 21:02:51.178: INFO: Exec stderr: ""
  Aug  4 21:02:51.178: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:51.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:51.178: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:51.178: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug  4 21:02:51.238: INFO: Exec stderr: ""
  Aug  4 21:02:51.238: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-9715 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:02:51.238: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:02:51.239: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:02:51.239: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-9715/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Aug  4 21:02:51.302: INFO: Exec stderr: ""
  Aug  4 21:02:51.302: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-9715" for this suite. @ 08/04/23 21:02:51.305
• [4.637 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 08/04/23 21:02:51.309
  Aug  4 21:02:51.309: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename deployment @ 08/04/23 21:02:51.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:51.318
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:51.32
  Aug  4 21:02:51.322: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Aug  4 21:02:51.327: INFO: Pod name sample-pod: Found 0 pods out of 1
  E0804 21:02:51.836114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:52.836215      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:53.836486      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:54.837107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:55.837421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:56.330: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/04/23 21:02:56.33
  Aug  4 21:02:56.330: INFO: Creating deployment "test-rolling-update-deployment"
  Aug  4 21:02:56.334: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Aug  4 21:02:56.338: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  E0804 21:02:56.838027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:57.838146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:02:58.343: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Aug  4 21:02:58.345: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Aug  4 21:02:58.350: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-2395  b60286f5-a50f-4f23-aee0-5b0533a44e2d 23040 1 2023-08-04 21:02:56 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-08-04 21:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058b95a8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-04 21:02:56 +0000 UTC,LastTransitionTime:2023-08-04 21:02:56 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-08-04 21:02:57 +0000 UTC,LastTransitionTime:2023-08-04 21:02:56 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug  4 21:02:58.352: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-2395  1b781589-e076-4104-9b44-7b5ef19bf471 23026 1 2023-08-04 21:02:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment b60286f5-a50f-4f23-aee0-5b0533a44e2d 0xc0058b9aa7 0xc0058b9aa8}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b60286f5-a50f-4f23-aee0-5b0533a44e2d\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:02:57 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058b9b58 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:02:58.352: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Aug  4 21:02:58.352: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-2395  17aa8f4d-959e-4bfc-9a65-8a57b72d5da3 23039 2 2023-08-04 21:02:51 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment b60286f5-a50f-4f23-aee0-5b0533a44e2d 0xc0058b9977 0xc0058b9978}] [] [{e2e.test Update apps/v1 2023-08-04 21:02:51 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:02:57 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b60286f5-a50f-4f23-aee0-5b0533a44e2d\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:02:57 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0058b9a38 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:02:58.354: INFO: Pod "test-rolling-update-deployment-656d657cd8-msjtv" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-msjtv test-rolling-update-deployment-656d657cd8- deployment-2395  40caa03d-5be3-4951-8102-361c6ddca770 23025 0 2023-08-04 21:02:56 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 1b781589-e076-4104-9b44-7b5ef19bf471 0xc0058b9fb7 0xc0058b9fb8}] [] [{kube-controller-manager Update v1 2023-08-04 21:02:56 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1b781589-e076-4104-9b44-7b5ef19bf471\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:02:57 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.185\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4lbf8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4lbf8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:02:56 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:02:57 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:02:56 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.185,StartTime:2023-08-04 21:02:56 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 21:02:56 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:docker://0d2168aea29ecc93980db494d796a7707886bb39cd7998230b45583cafa27e01,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.185,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 21:02:58.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-2395" for this suite. @ 08/04/23 21:02:58.356
• [7.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 08/04/23 21:02:58.361
  Aug  4 21:02:58.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pod-network-test @ 08/04/23 21:02:58.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:02:58.37
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:02:58.372
  STEP: Performing setup for networking test in namespace pod-network-test-8823 @ 08/04/23 21:02:58.374
  STEP: creating a selector @ 08/04/23 21:02:58.374
  STEP: Creating the service pods in kubernetes @ 08/04/23 21:02:58.374
  Aug  4 21:02:58.374: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0804 21:02:58.838966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:02:59.838988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:00.839720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:01.840175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:02.841120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:03.841421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:04.842077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:05.842316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:06.843362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:07.844063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:08.844120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:09.844269      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/04/23 21:03:10.413
  E0804 21:03:10.844663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:11.845135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:12.425: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Aug  4 21:03:12.425: INFO: Breadth first check of 10.244.0.226 on host 192.168.49.2...
  Aug  4 21:03:12.426: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.187:9080/dial?request=hostname&protocol=udp&host=10.244.0.226&port=8081&tries=1'] Namespace:pod-network-test-8823 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:03:12.426: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:03:12.427: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:03:12.427: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8823/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.0.226%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  4 21:03:12.509: INFO: Waiting for responses: map[]
  Aug  4 21:03:12.509: INFO: reached 10.244.0.226 after 0/1 tries
  Aug  4 21:03:12.509: INFO: Breadth first check of 10.244.1.186 on host 192.168.49.3...
  Aug  4 21:03:12.511: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.244.1.187:9080/dial?request=hostname&protocol=udp&host=10.244.1.186&port=8081&tries=1'] Namespace:pod-network-test-8823 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:03:12.511: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:03:12.512: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:03:12.512: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-8823/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.244.1.187%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.244.1.186%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Aug  4 21:03:12.580: INFO: Waiting for responses: map[]
  Aug  4 21:03:12.580: INFO: reached 10.244.1.186 after 0/1 tries
  Aug  4 21:03:12.580: INFO: Going to retry 0 out of 2 pods....
  Aug  4 21:03:12.580: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-8823" for this suite. @ 08/04/23 21:03:12.583
• [14.226 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 08/04/23 21:03:12.587
  Aug  4 21:03:12.587: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename statefulset @ 08/04/23 21:03:12.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:03:12.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:03:12.598
  STEP: Creating service test in namespace statefulset-5066 @ 08/04/23 21:03:12.6
  STEP: Creating stateful set ss in namespace statefulset-5066 @ 08/04/23 21:03:12.605
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5066 @ 08/04/23 21:03:12.608
  Aug  4 21:03:12.610: INFO: Found 0 stateful pods, waiting for 1
  E0804 21:03:12.846199      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:13.846484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:14.846686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:15.846961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:16.846962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:17.848054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:18.848292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:19.848476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:20.849298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:21.849421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:22.613: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 08/04/23 21:03:22.613
  Aug  4 21:03:22.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-5066 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 21:03:22.738: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 21:03:22.738: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 21:03:22.738: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  4 21:03:22.740: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  E0804 21:03:22.850060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:23.850265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:24.850459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:25.850652      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:26.851332      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:27.852128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:28.852354      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:29.852596      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:30.852806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:31.853243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:32.744: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug  4 21:03:32.744: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 21:03:32.753: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
  Aug  4 21:03:32.753: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:22 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:12 +0000 UTC  }]
  Aug  4 21:03:32.753: INFO: 
  Aug  4 21:03:32.753: INFO: StatefulSet ss has not reached scale 3, at 1
  E0804 21:03:32.853745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:33.756: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.997931475s
  E0804 21:03:33.854721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:34.759: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.994799939s
  E0804 21:03:34.856239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:35.762: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.991974155s
  E0804 21:03:35.856421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:36.765: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.988985911s
  E0804 21:03:36.856771      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:37.768: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.98592739s
  E0804 21:03:37.857640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:38.771: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.983066961s
  E0804 21:03:38.858439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:39.774: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.979792302s
  E0804 21:03:39.858842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:40.777: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.976840741s
  E0804 21:03:40.859713      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:41.780: INFO: Verifying statefulset ss doesn't scale past 3 for another 973.778885ms
  E0804 21:03:41.860801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5066 @ 08/04/23 21:03:42.781
  Aug  4 21:03:42.784: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-5066 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  E0804 21:03:42.861011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:42.916: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  4 21:03:42.916: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 21:03:42.916: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  4 21:03:42.916: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-5066 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  4 21:03:43.055: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug  4 21:03:43.055: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 21:03:43.055: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  4 21:03:43.056: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-5066 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  4 21:03:43.180: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Aug  4 21:03:43.180: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 21:03:43.180: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Aug  4 21:03:43.182: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 21:03:43.182: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 21:03:43.182: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 08/04/23 21:03:43.182
  Aug  4 21:03:43.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-5066 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 21:03:43.309: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 21:03:43.309: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 21:03:43.309: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  4 21:03:43.309: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-5066 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 21:03:43.431: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 21:03:43.431: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 21:03:43.431: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  4 21:03:43.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-5066 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 21:03:43.550: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 21:03:43.550: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 21:03:43.550: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Aug  4 21:03:43.550: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 21:03:43.552: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  E0804 21:03:43.862075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:44.862243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:45.862474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:46.862793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:47.862949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:48.864045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:49.864258      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:50.864491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:51.864886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:03:52.865091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:53.557: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Aug  4 21:03:53.557: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Aug  4 21:03:53.557: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Aug  4 21:03:53.565: INFO: POD   NODE                PHASE    GRACE  CONDITIONS
  Aug  4 21:03:53.565: INFO: ss-0  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:12 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:43 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:12 +0000 UTC  }]
  Aug  4 21:03:53.565: INFO: ss-1  k8sconformance      Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:32 +0000 UTC  }]
  Aug  4 21:03:53.565: INFO: ss-2  k8sconformance-m02  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:44 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-08-04 21:03:32 +0000 UTC  }]
  Aug  4 21:03:53.565: INFO: 
  Aug  4 21:03:53.565: INFO: StatefulSet ss has not reached scale 0, at 3
  E0804 21:03:53.865153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:54.567: INFO: Verifying statefulset ss doesn't scale past 0 for another 8.997724395s
  E0804 21:03:54.865286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:55.570: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.99559338s
  E0804 21:03:55.865627      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:56.573: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.992836801s
  E0804 21:03:56.865856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:57.575: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.98999346s
  E0804 21:03:57.866463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:58.578: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.987011345s
  E0804 21:03:58.867507      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:03:59.582: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.983731792s
  E0804 21:03:59.867670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:04:00.584: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.98118559s
  E0804 21:04:00.868035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:04:01.587: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.978648882s
  E0804 21:04:01.868840      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:04:02.590: INFO: Verifying statefulset ss doesn't scale past 0 for another 976.089326ms
  E0804 21:04:02.869580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5066 @ 08/04/23 21:04:03.591
  Aug  4 21:04:03.593: INFO: Scaling statefulset ss to 0
  Aug  4 21:04:03.600: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 21:04:03.601: INFO: Deleting all statefulset in ns statefulset-5066
  Aug  4 21:04:03.603: INFO: Scaling statefulset ss to 0
  Aug  4 21:04:03.608: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 21:04:03.610: INFO: Deleting statefulset ss
  Aug  4 21:04:03.617: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5066" for this suite. @ 08/04/23 21:04:03.62
• [51.036 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 08/04/23 21:04:03.624
  Aug  4 21:04:03.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-webhook @ 08/04/23 21:04:03.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:04:03.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:04:03.636
  STEP: Setting up server cert @ 08/04/23 21:04:03.638
  E0804 21:04:03.869635      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/04/23 21:04:03.876
  STEP: Deploying the custom resource conversion webhook pod @ 08/04/23 21:04:03.881
  STEP: Wait for the deployment to be ready @ 08/04/23 21:04:03.889
  Aug  4 21:04:03.893: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0804 21:04:04.870599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:05.870823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 21:04:05.899
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 21:04:05.907
  E0804 21:04:06.870988      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:04:06.908: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug  4 21:04:06.910: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 21:04:07.872017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:08.872224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/04/23 21:04:09.458
  STEP: Create a v2 custom resource @ 08/04/23 21:04:09.469
  STEP: List CRs in v1 @ 08/04/23 21:04:09.496
  STEP: List CRs in v2 @ 08/04/23 21:04:09.499
  Aug  4 21:04:09.502: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0804 21:04:09.872826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "crd-webhook-9501" for this suite. @ 08/04/23 21:04:10.036
• [6.417 seconds]
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 08/04/23 21:04:10.041
  Aug  4 21:04:10.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 21:04:10.042
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:04:10.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:04:10.052
  STEP: Counting existing ResourceQuota @ 08/04/23 21:04:10.054
  E0804 21:04:10.873361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:11.873714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:12.874235      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:13.874457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:14.875109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/04/23 21:04:15.057
  STEP: Ensuring resource quota status is calculated @ 08/04/23 21:04:15.06
  E0804 21:04:15.876035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:16.876505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:04:17.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2420" for this suite. @ 08/04/23 21:04:17.065
• [7.030 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 08/04/23 21:04:17.072
  Aug  4 21:04:17.072: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename statefulset @ 08/04/23 21:04:17.072
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:04:17.081
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:04:17.083
  STEP: Creating service test in namespace statefulset-4758 @ 08/04/23 21:04:17.084
  STEP: Creating statefulset ss in namespace statefulset-4758 @ 08/04/23 21:04:17.09
  Aug  4 21:04:17.095: INFO: Found 0 stateful pods, waiting for 1
  E0804 21:04:17.876916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:18.877149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:19.877981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:20.878243      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:21.878588      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:22.878822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:23.878979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:24.880075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:25.880272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:26.880832      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:04:27.098: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 08/04/23 21:04:27.101
  STEP: Getting /status @ 08/04/23 21:04:27.112
  Aug  4 21:04:27.115: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 08/04/23 21:04:27.115
  Aug  4 21:04:27.120: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 08/04/23 21:04:27.12
  Aug  4 21:04:27.122: INFO: Observed &StatefulSet event: ADDED
  Aug  4 21:04:27.122: INFO: Found Statefulset ss in namespace statefulset-4758 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  4 21:04:27.122: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 08/04/23 21:04:27.122
  Aug  4 21:04:27.122: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug  4 21:04:27.127: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 08/04/23 21:04:27.128
  Aug  4 21:04:27.129: INFO: Observed &StatefulSet event: ADDED
  Aug  4 21:04:27.129: INFO: Observed Statefulset ss in namespace statefulset-4758 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  4 21:04:27.129: INFO: Observed &StatefulSet event: MODIFIED
  Aug  4 21:04:27.129: INFO: Deleting all statefulset in ns statefulset-4758
  Aug  4 21:04:27.131: INFO: Scaling statefulset ss to 0
  E0804 21:04:27.881310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:28.881393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:29.881607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:30.882342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:31.882651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:32.882919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:33.882951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:34.883016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:35.884042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:36.884583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:04:37.144: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 21:04:37.146: INFO: Deleting statefulset ss
  Aug  4 21:04:37.152: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-4758" for this suite. @ 08/04/23 21:04:37.155
• [20.086 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 08/04/23 21:04:37.158
  Aug  4 21:04:37.158: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename deployment @ 08/04/23 21:04:37.159
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:04:37.168
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:04:37.17
  Aug  4 21:04:37.172: INFO: Creating simple deployment test-new-deployment
  Aug  4 21:04:37.179: INFO: new replicaset for deployment "test-new-deployment" is yet to be created
  E0804 21:04:37.885529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:38.885773      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: getting scale subresource @ 08/04/23 21:04:39.187
  STEP: updating a scale subresource @ 08/04/23 21:04:39.189
  STEP: verifying the deployment Spec.Replicas was modified @ 08/04/23 21:04:39.194
  STEP: Patch a scale subresource @ 08/04/23 21:04:39.196
  Aug  4 21:04:39.205: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-6208  a2d2ed1d-2e46-4d3f-926f-400e8c7da387 23556 3 2023-08-04 21:04:37 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-08-04 21:04:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:04:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00419fe68 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-04 21:04:38 +0000 UTC,LastTransitionTime:2023-08-04 21:04:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-08-04 21:04:38 +0000 UTC,LastTransitionTime:2023-08-04 21:04:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug  4 21:04:39.210: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-6208  7f8d7267-f628-4df1-a04a-14f0bba25ec9 23560 2 2023-08-04 21:04:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment a2d2ed1d-2e46-4d3f-926f-400e8c7da387 0xc001d49cf7 0xc001d49cf8}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a2d2ed1d-2e46-4d3f-926f-400e8c7da387\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:04:39 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc001d49d88 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:04:39.214: INFO: Pod "test-new-deployment-67bd4bf6dc-5s9m5" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-5s9m5 test-new-deployment-67bd4bf6dc- deployment-6208  9a7c6b2e-3ffd-445b-a64a-1f017bd05fe9 23561 0 2023-08-04 21:04:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 7f8d7267-f628-4df1-a04a-14f0bba25ec9 0xc0044a6307 0xc0044a6308}] [] [{kube-controller-manager Update v1 2023-08-04 21:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f8d7267-f628-4df1-a04a-14f0bba25ec9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mqz7z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mqz7z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:04:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 21:04:39.214: INFO: Pod "test-new-deployment-67bd4bf6dc-w76cn" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-w76cn test-new-deployment-67bd4bf6dc- deployment-6208  ce1921af-8c8a-421f-aee0-69d0e2aab63f 23550 0 2023-08-04 21:04:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 7f8d7267-f628-4df1-a04a-14f0bba25ec9 0xc0044a6470 0xc0044a6471}] [] [{kube-controller-manager Update v1 2023-08-04 21:04:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"7f8d7267-f628-4df1-a04a-14f0bba25ec9\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:04:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.192\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5tmxn,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5tmxn,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:04:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:04:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:04:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:04:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.192,StartTime:2023-08-04 21:04:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 21:04:37 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://d79276ca41c5c69c200c580ab517a85d49ea1c46981d9d88ec3289370746f789,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.192,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 21:04:39.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6208" for this suite. @ 08/04/23 21:04:39.218
• [2.066 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 08/04/23 21:04:39.226
  Aug  4 21:04:39.226: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-preemption @ 08/04/23 21:04:39.226
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:04:39.236
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:04:39.238
  Aug  4 21:04:39.248: INFO: Waiting up to 1m0s for all nodes to be ready
  E0804 21:04:39.886548      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:40.886748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:41.887463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:42.887718      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:43.887841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:44.888073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:45.888238      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:46.889022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:47.889835      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:48.890117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:49.890246      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:50.890502      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:51.891606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:52.891827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:53.892129      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:54.892360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:55.892734      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:56.893240      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:57.893863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:58.894092      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:04:59.894594      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:00.894858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:01.895552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:02.895791      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:03.895874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:04.896074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:05.896380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:06.897042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:07.897519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:08.897769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:09.898657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:10.898921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:11.899656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:12.900394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:13.900804      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:14.901036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:15.901460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:16.902010      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:17.903043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:18.904086      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:19.904187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:20.904642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:21.905625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:22.905833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:23.906385      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:24.906595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:25.906967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:26.908042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:27.908179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:28.908390      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:29.908553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:30.908769      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:31.909445      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:32.909697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:33.910162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:34.910417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:35.910962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:36.912050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:37.912643      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:38.912923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:05:39.262: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/04/23 21:05:39.264
  Aug  4 21:05:39.264: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/04/23 21:05:39.264
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:05:39.273
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:05:39.275
  STEP: Finding an available node @ 08/04/23 21:05:39.277
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/04/23 21:05:39.277
  E0804 21:05:39.913058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:40.913303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/04/23 21:05:41.289
  Aug  4 21:05:41.298: INFO: found a healthy node: k8sconformance-m02
  E0804 21:05:41.913722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:42.914113      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:43.915140      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:44.915339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:45.915808      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:46.916066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:05:47.348: INFO: pods created so far: [1 1 1]
  Aug  4 21:05:47.348: INFO: length of pods created so far: 3
  E0804 21:05:47.916067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:48.916981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:05:49.354: INFO: pods created so far: [2 2 1]
  E0804 21:05:49.917625      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:50.917837      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:51.918293      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:52.918511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:53.919489      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:54.920027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:55.920225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:05:56.355: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 21:05:56.376: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-7935" for this suite. @ 08/04/23 21:05:56.402
  STEP: Destroying namespace "sched-preemption-3080" for this suite. @ 08/04/23 21:05:56.406
• [77.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 08/04/23 21:05:56.41
  Aug  4 21:05:56.410: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename statefulset @ 08/04/23 21:05:56.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:05:56.421
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:05:56.423
  STEP: Creating service test in namespace statefulset-40 @ 08/04/23 21:05:56.425
  STEP: Creating statefulset ss in namespace statefulset-40 @ 08/04/23 21:05:56.428
  Aug  4 21:05:56.434: INFO: Found 0 stateful pods, waiting for 1
  E0804 21:05:56.920589      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:57.921500      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:58.921715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:05:59.921936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:00.922203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:01.922527      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:02.922749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:03.922957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:04.923065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:05.924081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:06.438: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 08/04/23 21:06:06.442
  STEP: updating a scale subresource @ 08/04/23 21:06:06.444
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/04/23 21:06:06.449
  STEP: Patch a scale subresource @ 08/04/23 21:06:06.451
  STEP: verifying the statefulset Spec.Replicas was modified @ 08/04/23 21:06:06.457
  Aug  4 21:06:06.460: INFO: Deleting all statefulset in ns statefulset-40
  Aug  4 21:06:06.462: INFO: Scaling statefulset ss to 0
  E0804 21:06:06.924900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:07.925212      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:08.925423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:09.925651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:10.925871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:11.926333      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:12.926471      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:13.926776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:14.926964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:15.927194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:16.474: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 21:06:16.476: INFO: Deleting statefulset ss
  Aug  4 21:06:16.482: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-40" for this suite. @ 08/04/23 21:06:16.484
• [20.079 seconds]
------------------------------
SS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:432
  STEP: Creating a kubernetes client @ 08/04/23 21:06:16.49
  Aug  4 21:06:16.490: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename daemonsets @ 08/04/23 21:06:16.49
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:06:16.497
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:06:16.499
  Aug  4 21:06:16.512: INFO: Create a RollingUpdate DaemonSet
  Aug  4 21:06:16.516: INFO: Check that daemon pods launch on every node of the cluster
  Aug  4 21:06:16.520: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:06:16.520: INFO: Node k8sconformance is running 0 daemon pod, expected 1
  E0804 21:06:16.927861      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:17.525: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 21:06:17.525: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
  E0804 21:06:17.928850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:18.526: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Aug  4 21:06:18.526: INFO: Number of running nodes: 2, number of available pods: 2 in daemonset daemon-set
  Aug  4 21:06:18.526: INFO: Update the DaemonSet to trigger a rollout
  Aug  4 21:06:18.531: INFO: Updating DaemonSet daemon-set
  E0804 21:06:18.928908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:19.929558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:20.541: INFO: Roll back the DaemonSet before rollout is complete
  Aug  4 21:06:20.546: INFO: Updating DaemonSet daemon-set
  Aug  4 21:06:20.546: INFO: Make sure DaemonSet rollback is complete
  Aug  4 21:06:20.548: INFO: Wrong image for pod: daemon-set-vq22g. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Aug  4 21:06:20.548: INFO: Pod daemon-set-vq22g is not available
  E0804 21:06:20.930315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:21.930663      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:22.931454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:23.554: INFO: Pod daemon-set-z4sdm is not available
  STEP: Deleting DaemonSet "daemon-set" @ 08/04/23 21:06:23.56
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5499, will wait for the garbage collector to delete the pods @ 08/04/23 21:06:23.56
  Aug  4 21:06:23.617: INFO: Deleting DaemonSet.extensions daemon-set took: 4.39275ms
  Aug  4 21:06:23.717: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.43153ms
  E0804 21:06:23.931700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:24.931845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:25.020: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:06:25.020: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  4 21:06:25.022: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24045"},"items":null}

  Aug  4 21:06:25.024: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24045"},"items":null}

  Aug  4 21:06:25.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5499" for this suite. @ 08/04/23 21:06:25.031
• [8.545 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 08/04/23 21:06:25.041
  Aug  4 21:06:25.041: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename dns @ 08/04/23 21:06:25.041
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:06:25.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:06:25.054
  STEP: Creating a test headless service @ 08/04/23 21:06:25.056
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3036.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3036.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3036.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3036.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3036.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 141.22.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.22.141_udp@PTR;check="$$(dig +tcp +noall +answer +search 141.22.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.22.141_tcp@PTR;sleep 1; done
   @ 08/04/23 21:06:25.068
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3036.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3036.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3036.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3036.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3036.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3036.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3036.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3036.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 141.22.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.22.141_udp@PTR;check="$$(dig +tcp +noall +answer +search 141.22.107.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.107.22.141_tcp@PTR;sleep 1; done
   @ 08/04/23 21:06:25.068
  STEP: creating a pod to probe DNS @ 08/04/23 21:06:25.068
  STEP: submitting the pod to kubernetes @ 08/04/23 21:06:25.068
  E0804 21:06:25.931994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:26.932682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 21:06:27.084
  STEP: looking for the results for each expected name from probers @ 08/04/23 21:06:27.086
  Aug  4 21:06:27.088: INFO: Unable to read wheezy_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:27.090: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:27.092: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:27.094: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:27.103: INFO: Unable to read jessie_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:27.104: INFO: Unable to read jessie_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:27.106: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:27.108: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:27.114: INFO: Lookups using dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da failed for: [wheezy_udp@dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_udp@dns-test-service.dns-3036.svc.cluster.local jessie_tcp@dns-test-service.dns-3036.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local]

  E0804 21:06:27.933535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:28.933811      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:29.934081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:30.934310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:31.934638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:32.117: INFO: Unable to read wheezy_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:32.119: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:32.121: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:32.123: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:32.132: INFO: Unable to read jessie_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:32.134: INFO: Unable to read jessie_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:32.135: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:32.137: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:32.144: INFO: Lookups using dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da failed for: [wheezy_udp@dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_udp@dns-test-service.dns-3036.svc.cluster.local jessie_tcp@dns-test-service.dns-3036.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local]

  E0804 21:06:32.934976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:33.936074      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:34.936270      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:35.936417      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:36.936989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:37.118: INFO: Unable to read wheezy_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:37.120: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:37.122: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:37.124: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:37.133: INFO: Unable to read jessie_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:37.135: INFO: Unable to read jessie_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:37.137: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:37.139: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:37.145: INFO: Lookups using dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da failed for: [wheezy_udp@dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_udp@dns-test-service.dns-3036.svc.cluster.local jessie_tcp@dns-test-service.dns-3036.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local]

  E0804 21:06:37.937841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:38.937987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:39.938198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:40.938427      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:41.938539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:42.117: INFO: Unable to read wheezy_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:42.119: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:42.121: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:42.123: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:42.131: INFO: Unable to read jessie_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:42.133: INFO: Unable to read jessie_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:42.134: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:42.136: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:42.143: INFO: Lookups using dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da failed for: [wheezy_udp@dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_udp@dns-test-service.dns-3036.svc.cluster.local jessie_tcp@dns-test-service.dns-3036.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local]

  E0804 21:06:42.938616      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:43.938871      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:44.938975      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:45.940043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:46.940222      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:47.117: INFO: Unable to read wheezy_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:47.120: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:47.122: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:47.124: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:47.132: INFO: Unable to read jessie_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:47.134: INFO: Unable to read jessie_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:47.136: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:47.137: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:47.144: INFO: Lookups using dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da failed for: [wheezy_udp@dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_udp@dns-test-service.dns-3036.svc.cluster.local jessie_tcp@dns-test-service.dns-3036.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local]

  E0804 21:06:47.940954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:48.941166      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:49.941279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:50.941290      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:51.941584      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:52.117: INFO: Unable to read wheezy_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:52.119: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:52.121: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:52.123: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:52.131: INFO: Unable to read jessie_udp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:52.132: INFO: Unable to read jessie_tcp@dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:52.134: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:52.136: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local from pod dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da: the server could not find the requested resource (get pods dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da)
  Aug  4 21:06:52.142: INFO: Lookups using dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da failed for: [wheezy_udp@dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@dns-test-service.dns-3036.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_udp@dns-test-service.dns-3036.svc.cluster.local jessie_tcp@dns-test-service.dns-3036.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-3036.svc.cluster.local]

  E0804 21:06:52.941829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:53.941940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:54.942180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:55.942305      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:56.942921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:06:57.143: INFO: DNS probes using dns-3036/dns-test-a04f0c55-afb9-4930-991f-b0ff6458d1da succeeded

  Aug  4 21:06:57.143: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 21:06:57.145
  STEP: deleting the test service @ 08/04/23 21:06:57.155
  STEP: deleting the test headless service @ 08/04/23 21:06:57.174
  STEP: Destroying namespace "dns-3036" for this suite. @ 08/04/23 21:06:57.181
• [32.143 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 08/04/23 21:06:57.186
  Aug  4 21:06:57.186: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename limitrange @ 08/04/23 21:06:57.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:06:57.195
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:06:57.197
  STEP: Creating a LimitRange @ 08/04/23 21:06:57.199
  STEP: Setting up watch @ 08/04/23 21:06:57.199
  STEP: Submitting a LimitRange @ 08/04/23 21:06:57.302
  STEP: Verifying LimitRange creation was observed @ 08/04/23 21:06:57.306
  STEP: Fetching the LimitRange to ensure it has proper values @ 08/04/23 21:06:57.306
  Aug  4 21:06:57.308: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug  4 21:06:57.308: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 08/04/23 21:06:57.308
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 08/04/23 21:06:57.313
  Aug  4 21:06:57.316: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Aug  4 21:06:57.316: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 08/04/23 21:06:57.316
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 08/04/23 21:06:57.32
  Aug  4 21:06:57.323: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Aug  4 21:06:57.323: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 08/04/23 21:06:57.323
  STEP: Failing to create a Pod with more than max resources @ 08/04/23 21:06:57.324
  STEP: Updating a LimitRange @ 08/04/23 21:06:57.327
  STEP: Verifying LimitRange updating is effective @ 08/04/23 21:06:57.331
  E0804 21:06:57.943682      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:06:58.943750      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod with less than former min resources @ 08/04/23 21:06:59.334
  STEP: Failing to create a Pod with more than max resources @ 08/04/23 21:06:59.338
  STEP: Deleting a LimitRange @ 08/04/23 21:06:59.339
  STEP: Verifying the LimitRange was deleted @ 08/04/23 21:06:59.345
  E0804 21:06:59.943933      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:00.944128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:01.944493      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:02.944723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:03.944768      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:07:04.348: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 08/04/23 21:07:04.348
  Aug  4 21:07:04.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-2801" for this suite. @ 08/04/23 21:07:04.356
• [7.175 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 08/04/23 21:07:04.361
  Aug  4 21:07:04.361: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 21:07:04.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:07:04.369
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:07:04.371
  STEP: Creating a ResourceQuota @ 08/04/23 21:07:04.373
  STEP: Getting a ResourceQuota @ 08/04/23 21:07:04.376
  STEP: Listing all ResourceQuotas with LabelSelector @ 08/04/23 21:07:04.378
  STEP: Patching the ResourceQuota @ 08/04/23 21:07:04.38
  STEP: Deleting a Collection of ResourceQuotas @ 08/04/23 21:07:04.384
  STEP: Verifying the deleted ResourceQuota @ 08/04/23 21:07:04.39
  Aug  4 21:07:04.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-3265" for this suite. @ 08/04/23 21:07:04.393
• [0.036 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 08/04/23 21:07:04.397
  Aug  4 21:07:04.397: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 21:07:04.398
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:07:04.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:07:04.406
  STEP: validating cluster-info @ 08/04/23 21:07:04.408
  Aug  4 21:07:04.408: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-2184 cluster-info'
  Aug  4 21:07:04.465: INFO: stderr: ""
  Aug  4 21:07:04.465: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.96.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Aug  4 21:07:04.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2184" for this suite. @ 08/04/23 21:07:04.468
• [0.074 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 08/04/23 21:07:04.472
  Aug  4 21:07:04.472: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-publish-openapi @ 08/04/23 21:07:04.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:07:04.48
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:07:04.482
  STEP: set up a multi version CRD @ 08/04/23 21:07:04.484
  Aug  4 21:07:04.484: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 21:07:04.945839      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:05.945896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:06.946608      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: mark a version not serverd @ 08/04/23 21:07:07.758
  STEP: check the unserved version gets removed @ 08/04/23 21:07:07.774
  E0804 21:07:07.947455      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check the other version is not changed @ 08/04/23 21:07:08.552
  E0804 21:07:08.948189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:09.949160      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:10.949499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:07:11.648: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3356" for this suite. @ 08/04/23 21:07:11.654
• [7.186 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 08/04/23 21:07:11.658
  Aug  4 21:07:11.658: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replicaset @ 08/04/23 21:07:11.659
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:07:11.668
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:07:11.67
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 08/04/23 21:07:11.672
  E0804 21:07:11.950647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:12.951173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When a replicaset with a matching selector is created @ 08/04/23 21:07:13.683
  STEP: Then the orphan pod is adopted @ 08/04/23 21:07:13.687
  E0804 21:07:13.952014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: When the matched label of one of its pods change @ 08/04/23 21:07:14.691
  Aug  4 21:07:14.693: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/04/23 21:07:14.701
  E0804 21:07:14.952563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:07:15.705: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-5428" for this suite. @ 08/04/23 21:07:15.708
• [4.054 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 08/04/23 21:07:15.712
  Aug  4 21:07:15.712: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 21:07:15.712
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:07:15.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:07:15.722
  STEP: Setting up server cert @ 08/04/23 21:07:15.734
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 21:07:15.911
  STEP: Deploying the webhook pod @ 08/04/23 21:07:15.916
  STEP: Wait for the deployment to be ready @ 08/04/23 21:07:15.924
  Aug  4 21:07:15.928: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0804 21:07:15.953536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:16.954060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:07:17.934: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.August, 4, 21, 7, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 7, 15, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 7, 15, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 7, 15, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:07:17.954883      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:18.954951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 21:07:19.937
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 21:07:19.945
  E0804 21:07:19.955515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:07:20.945: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 08/04/23 21:07:20.948
  E0804 21:07:20.956543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 08/04/23 21:07:20.962
  STEP: Creating a configMap that should not be mutated @ 08/04/23 21:07:20.968
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 08/04/23 21:07:20.974
  STEP: Creating a configMap that should be mutated @ 08/04/23 21:07:20.98
  Aug  4 21:07:20.994: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3661" for this suite. @ 08/04/23 21:07:21.022
  STEP: Destroying namespace "webhook-markers-7978" for this suite. @ 08/04/23 21:07:21.029
• [5.325 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 08/04/23 21:07:21.039
  Aug  4 21:07:21.039: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-preemption @ 08/04/23 21:07:21.04
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:07:21.112
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:07:21.114
  Aug  4 21:07:21.124: INFO: Waiting up to 1m0s for all nodes to be ready
  E0804 21:07:21.956659      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:22.957043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:23.957535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:24.957819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:25.958107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:26.958525      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:27.958654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:28.958834      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:29.958993      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:30.960027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:31.960687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:32.961068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:33.961674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:34.961957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:35.962361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:36.962955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:37.964036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:38.964429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:39.965404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:40.965642      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:41.966674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:42.966836      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:43.966954      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:44.968037      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:45.968581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:46.969503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:47.970262      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:48.970442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:49.970882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:50.970955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:51.972056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:52.972089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:53.972511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:54.972829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:55.973125      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:56.973675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:57.974283      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:58.974567      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:07:59.974797      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:00.974941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:01.974985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:02.976045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:03.976303      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:04.976505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:05.976915      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:06.977142      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:07.977540      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:08.977729      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:09.977897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:10.978343      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:11.978483      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:12.978668      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:13.978969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:14.979080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:15.980064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:16.980435      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:17.980819      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:18.981418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:19.982467      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:20.982687      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:08:21.137: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 08/04/23 21:08:21.139
  Aug  4 21:08:21.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-preemption-path @ 08/04/23 21:08:21.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:08:21.148
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:08:21.15
  Aug  4 21:08:21.160: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Aug  4 21:08:21.162: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Aug  4 21:08:21.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 21:08:21.181: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-7616" for this suite. @ 08/04/23 21:08:21.204
  STEP: Destroying namespace "sched-preemption-6838" for this suite. @ 08/04/23 21:08:21.207
• [60.172 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 08/04/23 21:08:21.212
  Aug  4 21:08:21.212: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 21:08:21.212
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:08:21.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:08:21.222
  STEP: Creating secret with name s-test-opt-del-0753d76b-0d6c-4e1a-b607-34cf2c4dd7a9 @ 08/04/23 21:08:21.226
  STEP: Creating secret with name s-test-opt-upd-c1930df8-f458-46b5-a5ed-3fed640cff25 @ 08/04/23 21:08:21.229
  STEP: Creating the pod @ 08/04/23 21:08:21.232
  E0804 21:08:21.982990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:22.984041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-0753d76b-0d6c-4e1a-b607-34cf2c4dd7a9 @ 08/04/23 21:08:23.27
  STEP: Updating secret s-test-opt-upd-c1930df8-f458-46b5-a5ed-3fed640cff25 @ 08/04/23 21:08:23.274
  STEP: Creating secret with name s-test-opt-create-6c7e69ba-fb73-4539-b155-ceca4fec3ff6 @ 08/04/23 21:08:23.277
  STEP: waiting to observe update in volume @ 08/04/23 21:08:23.28
  E0804 21:08:23.984748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:24.984949      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:25.985211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:26.985497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:08:27.309: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-183" for this suite. @ 08/04/23 21:08:27.312
• [6.104 seconds]
------------------------------
SS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 08/04/23 21:08:27.316
  Aug  4 21:08:27.316: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/04/23 21:08:27.316
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:08:27.324
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:08:27.326
  STEP: create the container to handle the HTTPGet hook request. @ 08/04/23 21:08:27.33
  E0804 21:08:27.985972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:28.986019      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/04/23 21:08:29.341
  E0804 21:08:29.986908      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:30.986996      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check poststart hook @ 08/04/23 21:08:31.352
  STEP: delete the pod with lifecycle hook @ 08/04/23 21:08:31.366
  E0804 21:08:31.988098      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:32.988340      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:33.988892      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:34.989112      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:08:35.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2985" for this suite. @ 08/04/23 21:08:35.382
• [8.069 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 08/04/23 21:08:35.385
  Aug  4 21:08:35.385: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 21:08:35.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:08:35.395
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:08:35.397
  STEP: Creating configMap with name configmap-test-volume-map-1f58bbb1-93f0-4e16-a235-1d6aaa00e7eb @ 08/04/23 21:08:35.398
  STEP: Creating a pod to test consume configMaps @ 08/04/23 21:08:35.401
  E0804 21:08:35.989167      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:36.990065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:08:37.411
  Aug  4 21:08:37.412: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-3bb693d4-cd0e-40aa-97a6-56fd9c6ab4b0 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 21:08:37.418
  Aug  4 21:08:37.428: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6121" for this suite. @ 08/04/23 21:08:37.43
• [2.048 seconds]
------------------------------
SS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 08/04/23 21:08:37.433
  Aug  4 21:08:37.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 21:08:37.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:08:37.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:08:37.442
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/04/23 21:08:37.444
  Aug  4 21:08:37.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-2291 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Aug  4 21:08:37.516: INFO: stderr: ""
  Aug  4 21:08:37.516: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 08/04/23 21:08:37.516
  E0804 21:08:37.990253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:38.990478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:39.990670      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:40.990907      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:41.990982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/04/23 21:08:42.567
  Aug  4 21:08:42.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-2291 get pod e2e-test-httpd-pod -o json'
  Aug  4 21:08:42.627: INFO: stderr: ""
  Aug  4 21:08:42.627: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-08-04T21:08:37Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-2291\",\n        \"resourceVersion\": \"24614\",\n        \"uid\": \"279508db-b505-4550-8781-b9d21143baf3\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-j4hr2\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"k8sconformance-m02\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-j4hr2\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-04T21:08:37Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-04T21:08:39Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-04T21:08:39Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-08-04T21:08:37Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"docker://9303f25f53d6347708bdbefe31f3f440f1cafd4ef33d5f36f105b9da6fe201f5\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-08-04T21:08:38Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.49.3\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.244.1.211\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.244.1.211\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-08-04T21:08:37Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 08/04/23 21:08:42.627
  Aug  4 21:08:42.627: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-2291 replace -f -'
  Aug  4 21:08:42.831: INFO: stderr: ""
  Aug  4 21:08:42.831: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 08/04/23 21:08:42.831
  Aug  4 21:08:42.833: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-2291 delete pods e2e-test-httpd-pod'
  E0804 21:08:42.991563      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:43.992117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:08:44.194: INFO: stderr: ""
  Aug  4 21:08:44.195: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug  4 21:08:44.195: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2291" for this suite. @ 08/04/23 21:08:44.198
• [6.768 seconds]
------------------------------
SSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 08/04/23 21:08:44.202
  Aug  4 21:08:44.202: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename dns @ 08/04/23 21:08:44.202
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:08:44.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:08:44.215
  STEP: Creating a test headless service @ 08/04/23 21:08:44.217
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3552 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3552;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3552 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3552;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3552.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-3552.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3552.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-3552.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3552.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3552.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3552.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-3552.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3552.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-3552.svc;check="$$(dig +notcp +noall +answer +search 200.111.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.111.200_udp@PTR;check="$$(dig +tcp +noall +answer +search 200.111.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.111.200_tcp@PTR;sleep 1; done
   @ 08/04/23 21:08:44.231
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3552 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3552;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3552 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3552;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-3552.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-3552.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-3552.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-3552.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-3552.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-3552.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-3552.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-3552.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-3552.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-3552.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-3552.svc;check="$$(dig +notcp +noall +answer +search 200.111.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.111.200_udp@PTR;check="$$(dig +tcp +noall +answer +search 200.111.98.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.98.111.200_tcp@PTR;sleep 1; done
   @ 08/04/23 21:08:44.231
  STEP: creating a pod to probe DNS @ 08/04/23 21:08:44.231
  STEP: submitting the pod to kubernetes @ 08/04/23 21:08:44.231
  E0804 21:08:44.992277      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:45.992551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 21:08:46.246
  STEP: looking for the results for each expected name from probers @ 08/04/23 21:08:46.249
  Aug  4 21:08:46.251: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.253: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.256: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.258: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.260: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.262: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.264: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.266: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.276: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.277: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.279: INFO: Unable to read jessie_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.281: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.283: INFO: Unable to read jessie_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.285: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.287: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.288: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:46.295: INFO: Lookups using dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3552 wheezy_tcp@dns-test-service.dns-3552 wheezy_udp@dns-test-service.dns-3552.svc wheezy_tcp@dns-test-service.dns-3552.svc wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3552 jessie_tcp@dns-test-service.dns-3552 jessie_udp@dns-test-service.dns-3552.svc jessie_tcp@dns-test-service.dns-3552.svc jessie_udp@_http._tcp.dns-test-service.dns-3552.svc jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc]

  E0804 21:08:46.993114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:47.993406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:48.993607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:49.993827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:50.994053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:08:51.299: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.301: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.305: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.307: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.308: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.310: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.312: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.320: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.322: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.323: INFO: Unable to read jessie_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.325: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.327: INFO: Unable to read jessie_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.330: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.331: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:51.338: INFO: Lookups using dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3552 wheezy_tcp@dns-test-service.dns-3552 wheezy_udp@dns-test-service.dns-3552.svc wheezy_tcp@dns-test-service.dns-3552.svc wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3552 jessie_tcp@dns-test-service.dns-3552 jessie_udp@dns-test-service.dns-3552.svc jessie_tcp@dns-test-service.dns-3552.svc jessie_udp@_http._tcp.dns-test-service.dns-3552.svc jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc]

  E0804 21:08:51.994528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:52.994737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:53.994980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:54.996013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:55.996207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:08:56.299: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.301: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.305: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.307: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.308: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.310: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.311: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.319: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.321: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.322: INFO: Unable to read jessie_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.324: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.326: INFO: Unable to read jessie_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.327: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.329: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.331: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:08:56.337: INFO: Lookups using dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3552 wheezy_tcp@dns-test-service.dns-3552 wheezy_udp@dns-test-service.dns-3552.svc wheezy_tcp@dns-test-service.dns-3552.svc wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3552 jessie_tcp@dns-test-service.dns-3552 jessie_udp@dns-test-service.dns-3552.svc jessie_tcp@dns-test-service.dns-3552.svc jessie_udp@_http._tcp.dns-test-service.dns-3552.svc jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc]

  E0804 21:08:56.996324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:57.996628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:58.996889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:08:59.997127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:00.997359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:01.299: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.301: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.303: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.305: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.306: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.308: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.310: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.311: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.320: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.322: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.323: INFO: Unable to read jessie_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.325: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.326: INFO: Unable to read jessie_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.330: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.331: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:01.338: INFO: Lookups using dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3552 wheezy_tcp@dns-test-service.dns-3552 wheezy_udp@dns-test-service.dns-3552.svc wheezy_tcp@dns-test-service.dns-3552.svc wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3552 jessie_tcp@dns-test-service.dns-3552 jessie_udp@dns-test-service.dns-3552.svc jessie_tcp@dns-test-service.dns-3552.svc jessie_udp@_http._tcp.dns-test-service.dns-3552.svc jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc]

  E0804 21:09:01.997661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:02.997879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:03.998109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:04.998358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:05.999376      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:06.299: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.302: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.304: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.306: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.308: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.310: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.312: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.314: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.323: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.325: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.327: INFO: Unable to read jessie_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.330: INFO: Unable to read jessie_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.332: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.334: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.335: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:06.342: INFO: Lookups using dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3552 wheezy_tcp@dns-test-service.dns-3552 wheezy_udp@dns-test-service.dns-3552.svc wheezy_tcp@dns-test-service.dns-3552.svc wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3552 jessie_tcp@dns-test-service.dns-3552 jessie_udp@dns-test-service.dns-3552.svc jessie_tcp@dns-test-service.dns-3552.svc jessie_udp@_http._tcp.dns-test-service.dns-3552.svc jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc]

  E0804 21:09:07.000103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:08.000329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:09.000451      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:10.000714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:11.000829      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:11.298: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.300: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.302: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.304: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.306: INFO: Unable to read wheezy_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.308: INFO: Unable to read wheezy_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.309: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.311: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.320: INFO: Unable to read jessie_udp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.321: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.323: INFO: Unable to read jessie_udp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.325: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552 from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.326: INFO: Unable to read jessie_udp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.328: INFO: Unable to read jessie_tcp@dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.330: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.331: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc from pod dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7: the server could not find the requested resource (get pods dns-test-498515d6-0043-4c04-a390-627c134d7ab7)
  Aug  4 21:09:11.338: INFO: Lookups using dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7 failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-3552 wheezy_tcp@dns-test-service.dns-3552 wheezy_udp@dns-test-service.dns-3552.svc wheezy_tcp@dns-test-service.dns-3552.svc wheezy_udp@_http._tcp.dns-test-service.dns-3552.svc wheezy_tcp@_http._tcp.dns-test-service.dns-3552.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-3552 jessie_tcp@dns-test-service.dns-3552 jessie_udp@dns-test-service.dns-3552.svc jessie_tcp@dns-test-service.dns-3552.svc jessie_udp@_http._tcp.dns-test-service.dns-3552.svc jessie_tcp@_http._tcp.dns-test-service.dns-3552.svc]

  E0804 21:09:12.001650      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:13.001889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:14.002109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:15.002512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:16.002717      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:16.336: INFO: DNS probes using dns-3552/dns-test-498515d6-0043-4c04-a390-627c134d7ab7 succeeded

  Aug  4 21:09:16.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 21:09:16.338
  STEP: deleting the test service @ 08/04/23 21:09:16.349
  STEP: deleting the test headless service @ 08/04/23 21:09:16.367
  STEP: Destroying namespace "dns-3552" for this suite. @ 08/04/23 21:09:16.373
• [32.175 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:194
  STEP: Creating a kubernetes client @ 08/04/23 21:09:16.377
  Aug  4 21:09:16.377: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename daemonsets @ 08/04/23 21:09:16.378
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:09:16.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:09:16.389
  Aug  4 21:09:16.408: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 08/04/23 21:09:16.413
  Aug  4 21:09:16.415: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:09:16.415: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 08/04/23 21:09:16.415
  Aug  4 21:09:16.428: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:09:16.428: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
  E0804 21:09:17.003793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:17.430: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:09:17.430: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
  E0804 21:09:18.004082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:18.431: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 21:09:18.431: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 08/04/23 21:09:18.433
  Aug  4 21:09:18.445: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 21:09:18.445: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  E0804 21:09:19.005028      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:19.448: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:09:19.448: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 08/04/23 21:09:19.448
  Aug  4 21:09:19.457: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:09:19.457: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
  E0804 21:09:20.005168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:20.461: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:09:20.461: INFO: Node k8sconformance-m02 is running 0 daemon pod, expected 1
  E0804 21:09:21.006093      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:21.460: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Aug  4 21:09:21.460: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 08/04/23 21:09:21.464
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-4394, will wait for the garbage collector to delete the pods @ 08/04/23 21:09:21.464
  Aug  4 21:09:21.519: INFO: Deleting DaemonSet.extensions daemon-set took: 3.435593ms
  Aug  4 21:09:21.620: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.071016ms
  E0804 21:09:22.007052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:22.523: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Aug  4 21:09:22.523: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Aug  4 21:09:22.524: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"24819"},"items":null}

  Aug  4 21:09:22.526: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"24819"},"items":null}

  Aug  4 21:09:22.536: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-4394" for this suite. @ 08/04/23 21:09:22.539
• [6.167 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 08/04/23 21:09:22.544
  Aug  4 21:09:22.544: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 21:09:22.545
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:09:22.553
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:09:22.555
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 21:09:22.557
  E0804 21:09:23.008096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:24.008386      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:25.009095      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:26.009347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:09:26.572
  Aug  4 21:09:26.573: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-54cd0522-8d8d-40d1-be21-eb516d1e012f container client-container: <nil>
  STEP: delete the pod @ 08/04/23 21:09:26.579
  Aug  4 21:09:26.588: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7891" for this suite. @ 08/04/23 21:09:26.59
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 08/04/23 21:09:26.596
  Aug  4 21:09:26.596: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubelet-test @ 08/04/23 21:09:26.597
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:09:26.604
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:09:26.606
  E0804 21:09:27.010351      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:28.010569      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:29.010831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:30.011064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:09:30.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-7670" for this suite. @ 08/04/23 21:09:30.62
• [4.027 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 08/04/23 21:09:30.624
  Aug  4 21:09:30.624: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-preemption @ 08/04/23 21:09:30.625
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:09:30.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:09:30.633
  Aug  4 21:09:30.642: INFO: Waiting up to 1m0s for all nodes to be ready
  E0804 21:09:31.012141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:32.012748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:33.012924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:34.013347      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:35.013519      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:36.014274      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:37.014516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:38.014762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:39.014850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:40.014962      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:41.014997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:42.015370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:43.015665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:44.016022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:45.016665      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:46.016862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:47.017143      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:48.017313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:49.018137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:50.018280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:51.018622      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:52.019017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:53.020027      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:54.020259      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:55.020633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:56.020882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:57.021484      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:58.021700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:09:59.021796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:00.022046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:01.022172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:02.022599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:03.022948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:04.022983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:05.023783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:06.023956      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:07.024607      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:08.024822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:09.025035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:10.025213      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:11.025342      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:12.025654      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:13.025818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:14.026051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:15.026121      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:16.026676      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:17.027203      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:18.027433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:19.027595      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:20.028090      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:21.029026      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:22.029499      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:23.029833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:24.030046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:25.030176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:26.030382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:27.031253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:28.031457      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:29.032328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:30.032530      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:10:30.655: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 08/04/23 21:10:30.656
  Aug  4 21:10:30.671: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Aug  4 21:10:30.675: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Aug  4 21:10:30.688: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Aug  4 21:10:30.695: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 08/04/23 21:10:30.695
  E0804 21:10:31.032727      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:32.033245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 08/04/23 21:10:32.707
  E0804 21:10:33.033626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:34.033830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:35.034244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:36.034378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:10:36.725: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-3035" for this suite. @ 08/04/23 21:10:36.746
• [66.126 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 08/04/23 21:10:36.751
  Aug  4 21:10:36.751: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pods @ 08/04/23 21:10:36.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:10:36.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:10:36.762
  STEP: creating pod @ 08/04/23 21:10:36.764
  E0804 21:10:37.034831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:38.034951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:10:38.778: INFO: Pod pod-hostip-2845e963-e30b-411c-ac2a-02974d7d040c has hostIP: 192.168.49.3
  Aug  4 21:10:38.778: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-458" for this suite. @ 08/04/23 21:10:38.78
• [2.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 08/04/23 21:10:38.785
  Aug  4 21:10:38.785: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename security-context-test @ 08/04/23 21:10:38.786
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:10:38.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:10:38.796
  E0804 21:10:39.035374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:40.035640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:41.036664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:42.036854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:10:42.818: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6240" for this suite. @ 08/04/23 21:10:42.82
• [4.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 08/04/23 21:10:42.826
  Aug  4 21:10:42.826: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 21:10:42.827
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:10:42.835
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:10:42.837
  STEP: Creating a ResourceQuota with best effort scope @ 08/04/23 21:10:42.839
  STEP: Ensuring ResourceQuota status is calculated @ 08/04/23 21:10:42.842
  E0804 21:10:43.037244      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:44.037478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 08/04/23 21:10:44.846
  STEP: Ensuring ResourceQuota status is calculated @ 08/04/23 21:10:44.851
  E0804 21:10:45.037838      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:46.038071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 08/04/23 21:10:46.854
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 08/04/23 21:10:46.865
  E0804 21:10:47.038184      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:48.038419      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 08/04/23 21:10:48.868
  E0804 21:10:49.039015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:50.040063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/04/23 21:10:50.871
  STEP: Ensuring resource quota status released the pod usage @ 08/04/23 21:10:50.877
  E0804 21:10:51.040176      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:52.040664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 08/04/23 21:10:52.88
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 08/04/23 21:10:52.889
  E0804 21:10:53.041736      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:54.041969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 08/04/23 21:10:54.892
  E0804 21:10:55.042165      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:56.042375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 08/04/23 21:10:56.894
  STEP: Ensuring resource quota status released the pod usage @ 08/04/23 21:10:56.899
  E0804 21:10:57.043099      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:10:58.044041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:10:58.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6988" for this suite. @ 08/04/23 21:10:58.904
• [16.081 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 08/04/23 21:10:58.907
  Aug  4 21:10:58.907: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 21:10:58.908
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:10:58.917
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:10:58.919
  STEP: creating a collection of services @ 08/04/23 21:10:58.921
  Aug  4 21:10:58.921: INFO: Creating e2e-svc-a-b55wm
  Aug  4 21:10:58.929: INFO: Creating e2e-svc-b-pmhpw
  Aug  4 21:10:58.937: INFO: Creating e2e-svc-c-pkqtk
  STEP: deleting service collection @ 08/04/23 21:10:59.011
  Aug  4 21:10:59.029: INFO: Collection of services has been deleted
  Aug  4 21:10:59.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5962" for this suite. @ 08/04/23 21:10:59.032
• [0.128 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 08/04/23 21:10:59.036
  Aug  4 21:10:59.036: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename events @ 08/04/23 21:10:59.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:10:59.043
  E0804 21:10:59.044686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:10:59.045
  STEP: creating a test event @ 08/04/23 21:10:59.047
  STEP: listing all events in all namespaces @ 08/04/23 21:10:59.05
  STEP: patching the test event @ 08/04/23 21:10:59.052
  STEP: fetching the test event @ 08/04/23 21:10:59.055
  STEP: updating the test event @ 08/04/23 21:10:59.057
  STEP: getting the test event @ 08/04/23 21:10:59.061
  STEP: deleting the test event @ 08/04/23 21:10:59.063
  STEP: listing all events in all namespaces @ 08/04/23 21:10:59.066
  Aug  4 21:10:59.068: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5933" for this suite. @ 08/04/23 21:10:59.07
• [0.039 seconds]
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 08/04/23 21:10:59.075
  Aug  4 21:10:59.075: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename sched-pred @ 08/04/23 21:10:59.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:10:59.082
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:10:59.084
  Aug  4 21:10:59.086: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Aug  4 21:10:59.107: INFO: Waiting for terminating namespaces to be deleted...
  Aug  4 21:10:59.112: INFO: 
  Logging pods the apiserver thinks is on node k8sconformance before test
  Aug  4 21:10:59.115: INFO: coredns-5d78c9869d-zl979 from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container coredns ready: true, restart count 1
  Aug  4 21:10:59.115: INFO: etcd-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container etcd ready: true, restart count 0
  Aug  4 21:10:59.115: INFO: kindnet-5zl6d from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container kindnet-cni ready: true, restart count 0
  Aug  4 21:10:59.115: INFO: kube-apiserver-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container kube-apiserver ready: true, restart count 0
  Aug  4 21:10:59.115: INFO: kube-controller-manager-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Aug  4 21:10:59.115: INFO: kube-proxy-mslwk from kube-system started at 2023-08-04 19:46:58 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug  4 21:10:59.115: INFO: kube-scheduler-k8sconformance from kube-system started at 2023-08-04 19:46:46 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container kube-scheduler ready: true, restart count 0
  Aug  4 21:10:59.115: INFO: storage-provisioner from kube-system started at 2023-08-04 19:46:59 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container storage-provisioner ready: true, restart count 1
  Aug  4 21:10:59.115: INFO: sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-fdn76 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 21:10:59.115: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 21:10:59.115: INFO: 	Container systemd-logs ready: true, restart count 0
  Aug  4 21:10:59.115: INFO: 
  Logging pods the apiserver thinks is on node k8sconformance-m02 before test
  Aug  4 21:10:59.118: INFO: kindnet-grl89 from kube-system started at 2023-08-04 20:09:19 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.119: INFO: 	Container kindnet-cni ready: true, restart count 0
  Aug  4 21:10:59.119: INFO: kube-proxy-nn2ml from kube-system started at 2023-08-04 19:47:12 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.119: INFO: 	Container kube-proxy ready: true, restart count 0
  Aug  4 21:10:59.119: INFO: sonobuoy from sonobuoy started at 2023-08-04 19:47:19 +0000 UTC (1 container statuses recorded)
  Aug  4 21:10:59.119: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Aug  4 21:10:59.119: INFO: sonobuoy-e2e-job-e2b8ac8d10f74a23 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 21:10:59.119: INFO: 	Container e2e ready: true, restart count 0
  Aug  4 21:10:59.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 21:10:59.119: INFO: sonobuoy-systemd-logs-daemon-set-5e1eef00e833487d-plz82 from sonobuoy started at 2023-08-04 19:47:23 +0000 UTC (2 container statuses recorded)
  Aug  4 21:10:59.119: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Aug  4 21:10:59.119: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 08/04/23 21:10:59.119
  E0804 21:11:00.045358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:01.045725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Explicitly delete pod here to free the resource it takes. @ 08/04/23 21:11:01.132
  STEP: Trying to apply a random label on the found node. @ 08/04/23 21:11:01.141
  STEP: verifying the node has the label kubernetes.io/e2e-75e1dc58-4a56-4ffa-8ae6-7b7cea3afb3f 42 @ 08/04/23 21:11:01.148
  STEP: Trying to relaunch the pod, now with labels. @ 08/04/23 21:11:01.15
  E0804 21:11:02.046725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:03.046973      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: removing the label kubernetes.io/e2e-75e1dc58-4a56-4ffa-8ae6-7b7cea3afb3f off the node k8sconformance-m02 @ 08/04/23 21:11:03.163
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-75e1dc58-4a56-4ffa-8ae6-7b7cea3afb3f @ 08/04/23 21:11:03.171
  Aug  4 21:11:03.173: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9443" for this suite. @ 08/04/23 21:11:03.175
• [4.105 seconds]
------------------------------
S
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 08/04/23 21:11:03.18
  Aug  4 21:11:03.180: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 21:11:03.18
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:11:03.189
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:11:03.192
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 08/04/23 21:11:03.194
  Aug  4 21:11:03.194: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7342 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Aug  4 21:11:03.258: INFO: stderr: ""
  Aug  4 21:11:03.258: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 08/04/23 21:11:03.258
  Aug  4 21:11:03.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-7342 delete pods e2e-test-httpd-pod'
  E0804 21:11:04.046997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:05.047214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:11:05.234: INFO: stderr: ""
  Aug  4 21:11:05.234: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Aug  4 21:11:05.234: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7342" for this suite. @ 08/04/23 21:11:05.237
• [2.060 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 08/04/23 21:11:05.241
  Aug  4 21:11:05.241: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename crd-webhook @ 08/04/23 21:11:05.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:11:05.251
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:11:05.253
  STEP: Setting up server cert @ 08/04/23 21:11:05.254
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 08/04/23 21:11:05.789
  STEP: Deploying the custom resource conversion webhook pod @ 08/04/23 21:11:05.794
  STEP: Wait for the deployment to be ready @ 08/04/23 21:11:05.802
  Aug  4 21:11:05.805: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  E0804 21:11:06.048088      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:07.048542      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 21:11:07.812
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 21:11:07.819
  E0804 21:11:08.049071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:11:08.819: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Aug  4 21:11:08.821: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 21:11:09.050068      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:10.051054      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:11.052108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a v1 custom resource @ 08/04/23 21:11:11.353
  STEP: v2 custom resource should be converted @ 08/04/23 21:11:11.357
  Aug  4 21:11:11.360: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-7520" for this suite. @ 08/04/23 21:11:11.894
• [6.657 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 08/04/23 21:11:11.899
  Aug  4 21:11:11.899: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename pod-network-test @ 08/04/23 21:11:11.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:11:11.909
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:11:11.911
  STEP: Performing setup for networking test in namespace pod-network-test-7757 @ 08/04/23 21:11:11.913
  STEP: creating a selector @ 08/04/23 21:11:11.913
  STEP: Creating the service pods in kubernetes @ 08/04/23 21:11:11.913
  Aug  4 21:11:11.913: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  E0804 21:11:12.052582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:13.052944      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:14.053675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:15.053902      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:16.054282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:17.054865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:18.055505      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:19.056049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:20.056863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:21.056966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:22.057617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:23.057828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating test pods @ 08/04/23 21:11:23.954
  E0804 21:11:24.058574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:25.058785      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:11:25.974: INFO: Setting MaxTries for pod polling to 34 for networking test based on endpoint count 2
  Aug  4 21:11:25.974: INFO: Going to poll 10.244.0.235 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Aug  4 21:11:25.976: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.0.235 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7757 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:11:25.976: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:11:25.976: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:11:25.976: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7757/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.0.235+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0804 21:11:26.059469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:11:27.052: INFO: Found all 1 expected endpoints: [netserver-0]
  Aug  4 21:11:27.052: INFO: Going to poll 10.244.1.225 on port 8081 at least 0 times, with a maximum of 34 tries before failing
  Aug  4 21:11:27.054: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.244.1.225 8081 | grep -v '^\s*$'] Namespace:pod-network-test-7757 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:11:27.054: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:11:27.055: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:11:27.055: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/pod-network-test-7757/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.244.1.225+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  E0804 21:11:27.059815      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:28.059990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:11:28.126: INFO: Found all 1 expected endpoints: [netserver-1]
  Aug  4 21:11:28.126: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7757" for this suite. @ 08/04/23 21:11:28.128
• [16.234 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:465
  STEP: Creating a kubernetes client @ 08/04/23 21:11:28.133
  Aug  4 21:11:28.133: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename field-validation @ 08/04/23 21:11:28.133
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:11:28.143
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:11:28.145
  Aug  4 21:11:28.147: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 21:11:29.060382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:30.060639      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  W0804 21:11:30.680045      22 warnings.go:70] unknown field "alpha"
  W0804 21:11:30.680066      22 warnings.go:70] unknown field "beta"
  W0804 21:11:30.680070      22 warnings.go:70] unknown field "delta"
  W0804 21:11:30.680073      22 warnings.go:70] unknown field "epsilon"
  W0804 21:11:30.680076      22 warnings.go:70] unknown field "gamma"
  Aug  4 21:11:30.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-9098" for this suite. @ 08/04/23 21:11:30.696
• [2.567 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 08/04/23 21:11:30.7
  Aug  4 21:11:30.700: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 21:11:30.7
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:11:30.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:11:30.71
  STEP: Creating secret with name s-test-opt-del-084780a7-1423-4c43-9a58-2caf3c466fdd @ 08/04/23 21:11:30.723
  STEP: Creating secret with name s-test-opt-upd-94f93f81-858f-4927-8c8f-d8d931caf295 @ 08/04/23 21:11:30.727
  STEP: Creating the pod @ 08/04/23 21:11:30.73
  E0804 21:11:31.061720      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:32.062011      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-084780a7-1423-4c43-9a58-2caf3c466fdd @ 08/04/23 21:11:32.761
  STEP: Updating secret s-test-opt-upd-94f93f81-858f-4927-8c8f-d8d931caf295 @ 08/04/23 21:11:32.765
  STEP: Creating secret with name s-test-opt-create-a7b12fe6-370b-4c8d-8f94-aa9362a8664b @ 08/04/23 21:11:32.768
  STEP: waiting to observe update in volume @ 08/04/23 21:11:32.773
  E0804 21:11:33.062961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:34.063091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:35.063803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:36.064043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:37.064188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:38.064573      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:39.065578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:40.065916      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:41.065976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:42.066401      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:43.066476      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:44.067273      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:45.067976      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:46.068232      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:47.068403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:48.068610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:49.069513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:50.069709      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:51.070198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:52.070631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:53.070688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:54.070972      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:55.071978      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:56.072207      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:57.072488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:58.072938      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:11:59.073053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:00.073247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:01.074292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:02.075362      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:03.075689      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:04.076050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:05.076406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:06.076648      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:07.077119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:08.077755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:09.077854      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:10.078109      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:11.079188      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:12.079810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:13.080893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:14.081137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:15.081966      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:16.082194      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:17.082559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:18.082826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:19.083267      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:20.084091      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:21.084960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:22.085106      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:23.085590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:24.086456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:25.086941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:26.086989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:27.087490      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:28.087738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:29.088671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:30.089316      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:31.090430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:32.090861      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:33.091503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:34.091645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:35.092275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:36.092513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:37.093198      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:38.093437      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:39.094439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:40.094940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:41.095777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:42.096211      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:12:43.072: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-943" for this suite. @ 08/04/23 21:12:43.074
• [72.378 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 08/04/23 21:12:43.078
  Aug  4 21:12:43.078: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename deployment @ 08/04/23 21:12:43.079
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:12:43.087
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:12:43.089
  Aug  4 21:12:43.096: INFO: Pod name rollover-pod: Found 0 pods out of 1
  E0804 21:12:43.096624      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:44.097248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:45.097440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:46.097690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:47.098189      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:48.098711      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:12:48.099: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 08/04/23 21:12:48.099
  Aug  4 21:12:48.099: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  E0804 21:12:49.098947      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:50.098990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:12:50.102: INFO: Creating deployment "test-rollover-deployment"
  Aug  4 21:12:50.107: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  E0804 21:12:51.099738      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:52.100358      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:12:52.111: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Aug  4 21:12:52.115: INFO: Ensure that both replica sets have 1 created replica
  Aug  4 21:12:52.118: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Aug  4 21:12:52.124: INFO: Updating deployment test-rollover-deployment
  Aug  4 21:12:52.124: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  E0804 21:12:53.100469      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:54.100693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:12:54.128: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Aug  4 21:12:54.132: INFO: Make sure deployment "test-rollover-deployment" is complete
  Aug  4 21:12:54.135: INFO: all replica sets need to contain the pod-template-hash label
  Aug  4 21:12:54.135: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:12:55.101491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:56.101706      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:12:56.140: INFO: all replica sets need to contain the pod-template-hash label
  Aug  4 21:12:56.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:12:57.102295      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:12:58.102509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:12:58.141: INFO: all replica sets need to contain the pod-template-hash label
  Aug  4 21:12:58.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:12:59.102656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:00.102986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:00.140: INFO: all replica sets need to contain the pod-template-hash label
  Aug  4 21:13:00.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:13:01.104237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:02.104697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:02.140: INFO: all replica sets need to contain the pod-template-hash label
  Aug  4 21:13:02.140: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 12, 53, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 12, 50, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  E0804 21:13:03.105056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:04.105309      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:04.141: INFO: 
  Aug  4 21:13:04.141: INFO: Ensure that both old replica sets have no replicas
  Aug  4 21:13:04.145: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-804  543de9d5-e022-4e15-a0f8-1a28e3b82d66 25660 2 2023-08-04 21:12:50 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-04 21:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:13:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004d89ec8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-08-04 21:12:50 +0000 UTC,LastTransitionTime:2023-08-04 21:12:50 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-08-04 21:13:03 +0000 UTC,LastTransitionTime:2023-08-04 21:12:50 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug  4 21:13:04.147: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-804  d227e10a-7149-4d43-99c7-bea7aaac5558 25650 2 2023-08-04 21:12:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment 543de9d5-e022-4e15-a0f8-1a28e3b82d66 0xc003e48d77 0xc003e48d78}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"543de9d5-e022-4e15-a0f8-1a28e3b82d66\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:13:03 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e49008 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:13:04.147: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Aug  4 21:13:04.147: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-804  5f8345dc-46e0-49c8-96e0-224ced8c3b74 25659 2 2023-08-04 21:12:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment 543de9d5-e022-4e15-a0f8-1a28e3b82d66 0xc000ce7ff7 0xc000ce7ff8}] [] [{e2e.test Update apps/v1 2023-08-04 21:12:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:13:03 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"543de9d5-e022-4e15-a0f8-1a28e3b82d66\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:13:03 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc003e48c08 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:13:04.148: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-804  ca7d396a-1b58-4e30-bb96-1be5ffafc624 25620 2 2023-08-04 21:12:50 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment 543de9d5-e022-4e15-a0f8-1a28e3b82d66 0xc003e490f7 0xc003e490f8}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"543de9d5-e022-4e15-a0f8-1a28e3b82d66\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:12:52 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e49358 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:13:04.149: INFO: Pod "test-rollover-deployment-57777854c9-tlbkj" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-tlbkj test-rollover-deployment-57777854c9- deployment-804  c69b03dd-f93e-42fe-ba14-9bd3e9abbb1e 25636 0 2023-08-04 21:12:52 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 d227e10a-7149-4d43-99c7-bea7aaac5558 0xc00336d217 0xc00336d218}] [] [{kube-controller-manager Update v1 2023-08-04 21:12:52 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d227e10a-7149-4d43-99c7-bea7aaac5558\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:12:53 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.230\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-wlc2g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-wlc2g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:12:52 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:12:53 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:12:52 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.230,StartTime:2023-08-04 21:12:52 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 21:12:52 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:docker://b57ea6dbde4acbd25389ad27007d7e83034598554d0d4bc9bfc586c44661f9ea,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.230,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 21:13:04.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-804" for this suite. @ 08/04/23 21:13:04.152
• [21.077 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 08/04/23 21:13:04.156
  Aug  4 21:13:04.156: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename deployment @ 08/04/23 21:13:04.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:13:04.165
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:13:04.167
  STEP: creating a Deployment @ 08/04/23 21:13:04.172
  STEP: waiting for Deployment to be created @ 08/04/23 21:13:04.175
  STEP: waiting for all Replicas to be Ready @ 08/04/23 21:13:04.177
  Aug  4 21:13:04.178: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  4 21:13:04.178: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  4 21:13:04.184: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  4 21:13:04.184: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  4 21:13:04.192: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  4 21:13:04.192: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  4 21:13:04.207: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Aug  4 21:13:04.207: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  E0804 21:13:05.106116      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:05.211: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug  4 21:13:05.211: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Aug  4 21:13:05.282: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 08/04/23 21:13:05.282
  W0804 21:13:05.288149      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug  4 21:13:05.289: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 08/04/23 21:13:05.289
  Aug  4 21:13:05.290: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0
  Aug  4 21:13:05.290: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 0
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:05.291: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:05.299: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:05.299: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:05.316: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:05.316: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:05.329: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:05.329: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:05.335: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:05.335: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  E0804 21:13:06.106960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:06.305: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:06.305: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:06.323: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  STEP: listing Deployments @ 08/04/23 21:13:06.323
  Aug  4 21:13:06.325: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 08/04/23 21:13:06.325
  Aug  4 21:13:06.336: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 08/04/23 21:13:06.336
  Aug  4 21:13:06.341: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  4 21:13:06.347: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  4 21:13:06.359: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  4 21:13:06.372: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  4 21:13:06.410: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  E0804 21:13:07.107000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:07.235: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  4 21:13:07.327: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  4 21:13:07.355: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Aug  4 21:13:07.362: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  E0804 21:13:08.108061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:08.257: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 08/04/23 21:13:08.271
  STEP: fetching the DeploymentStatus @ 08/04/23 21:13:08.276
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 1
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 3
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 2
  Aug  4 21:13:08.280: INFO: observed Deployment test-deployment in namespace deployment-962 with ReadyReplicas 3
  STEP: deleting the Deployment @ 08/04/23 21:13:08.28
  Aug  4 21:13:08.286: INFO: observed event type MODIFIED
  Aug  4 21:13:08.286: INFO: observed event type MODIFIED
  Aug  4 21:13:08.286: INFO: observed event type MODIFIED
  Aug  4 21:13:08.286: INFO: observed event type MODIFIED
  Aug  4 21:13:08.286: INFO: observed event type MODIFIED
  Aug  4 21:13:08.287: INFO: observed event type MODIFIED
  Aug  4 21:13:08.287: INFO: observed event type MODIFIED
  Aug  4 21:13:08.287: INFO: observed event type MODIFIED
  Aug  4 21:13:08.287: INFO: observed event type MODIFIED
  Aug  4 21:13:08.287: INFO: observed event type MODIFIED
  Aug  4 21:13:08.287: INFO: observed event type MODIFIED
  Aug  4 21:13:08.289: INFO: Log out all the ReplicaSets if there is no deployment created
  Aug  4 21:13:08.291: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-962  1a9d1617-53ad-454c-aa36-7dcc108b094d 25732 3 2023-08-04 21:13:04 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment afe94991-0260-471d-bb9b-5a32e94764a0 0xc005628c67 0xc005628c68}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afe94991-0260-471d-bb9b-5a32e94764a0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:13:06 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005628cf0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Aug  4 21:13:08.293: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-962  19d5cf9d-123e-4247-840c-a5f39dc0d1c5 25809 4 2023-08-04 21:13:05 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment afe94991-0260-471d-bb9b-5a32e94764a0 0xc005628d57 0xc005628d58}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:13:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afe94991-0260-471d-bb9b-5a32e94764a0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:13:08 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005628de0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Aug  4 21:13:08.309: INFO: pod: "test-deployment-5b5dcbcd95-2hcjw":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-2hcjw test-deployment-5b5dcbcd95- deployment-962  0290ad60-0041-4244-9095-10e2da9dea49 25804 0 2023-08-04 21:13:06 +0000 UTC 2023-08-04 21:13:09 +0000 UTC 0xc003795278 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 19d5cf9d-123e-4247-840c-a5f39dc0d1c5 0xc0037952a7 0xc0037952a8}] [] [{kube-controller-manager Update v1 2023-08-04 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19d5cf9d-123e-4247-840c-a5f39dc0d1c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:13:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.237\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-rsjcq,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-rsjcq,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.237,StartTime:2023-08-04 21:13:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 21:13:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:docker-pullable://registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:docker://81188ece217fd12f53930747d99bdf7d3f936ac9561f9ed361490ab7e41493d7,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.237,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug  4 21:13:08.309: INFO: pod: "test-deployment-5b5dcbcd95-478hh":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-478hh test-deployment-5b5dcbcd95- deployment-962  8ac24b89-6814-4ac6-9706-58888b91e793 25796 0 2023-08-04 21:13:05 +0000 UTC 2023-08-04 21:13:08 +0000 UTC 0xc003795470 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 19d5cf9d-123e-4247-840c-a5f39dc0d1c5 0xc0037954a7 0xc0037954a8}] [] [{kube-controller-manager Update v1 2023-08-04 21:13:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"19d5cf9d-123e-4247-840c-a5f39dc0d1c5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:13:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-phrt8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-phrt8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Succeeded,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:05 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:07 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:07 +0000 UTC,Reason:PodCompleted,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 21:13:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:nil,Terminated:&ContainerStateTerminated{ExitCode:0,Signal:0,Reason:Completed,Message:,StartedAt:2023-08-04 21:13:05 +0000 UTC,FinishedAt:2023-08-04 21:13:07 +0000 UTC,ContainerID:docker://d4cb174fe62bab78212658d2d67878912b9eb7929787def1afa7ce1f966120a3,},},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:docker-pullable://registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:docker://d4cb174fe62bab78212658d2d67878912b9eb7929787def1afa7ce1f966120a3,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug  4 21:13:08.309: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-962  e92b2f16-d891-4224-990c-490cf2068018 25801 2 2023-08-04 21:13:06 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment afe94991-0260-471d-bb9b-5a32e94764a0 0xc005628e47 0xc005628e48}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:13:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"afe94991-0260-471d-bb9b-5a32e94764a0\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:13:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc005628ed0 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Aug  4 21:13:08.313: INFO: pod: "test-deployment-6fc78d85c6-glzk8":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-glzk8 test-deployment-6fc78d85c6- deployment-962  e86297b6-bc78-4b92-aa61-5a9ec00f2b8a 25800 0 2023-08-04 21:13:07 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 e92b2f16-d891-4224-990c-490cf2068018 0xc00518dd57 0xc00518dd58}] [] [{kube-controller-manager Update v1 2023-08-04 21:13:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e92b2f16-d891-4224-990c-490cf2068018\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:13:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.0.238\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bs7h7,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bs7h7,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.2,PodIP:10.244.0.238,StartTime:2023-08-04 21:13:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 21:13:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://049af9fa680d1ee7dce70fd1423b3f74afbdde9c53c58c8db1da4c779a41a429,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.0.238,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug  4 21:13:08.314: INFO: pod: "test-deployment-6fc78d85c6-n64sn":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-n64sn test-deployment-6fc78d85c6- deployment-962  d4d9f327-5824-4dbd-ac0c-7976db5db1e8 25773 0 2023-08-04 21:13:06 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 e92b2f16-d891-4224-990c-490cf2068018 0xc00518df47 0xc00518df48}] [] [{kube-controller-manager Update v1 2023-08-04 21:13:06 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"e92b2f16-d891-4224-990c-490cf2068018\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:13:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.233\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qtqzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qtqzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:06 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:13:06 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.233,StartTime:2023-08-04 21:13:06 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 21:13:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://431aac8d7589cd4632bd4574f6a54b16821ad89a0b035eebef566564d341a013,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.233,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Aug  4 21:13:08.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-962" for this suite. @ 08/04/23 21:13:08.317
• [4.166 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 08/04/23 21:13:08.323
  Aug  4 21:13:08.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 21:13:08.324
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:13:08.335
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:13:08.337
  STEP: Counting existing ResourceQuota @ 08/04/23 21:13:08.34
  E0804 21:13:09.109020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:10.109158      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:11.109494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:12.110041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:13.110296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/04/23 21:13:13.342
  STEP: Ensuring resource quota status is calculated @ 08/04/23 21:13:13.345
  E0804 21:13:14.110420      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:15.110697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Service @ 08/04/23 21:13:15.348
  STEP: Creating a NodePort Service @ 08/04/23 21:13:15.361
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 08/04/23 21:13:15.376
  STEP: Ensuring resource quota status captures service creation @ 08/04/23 21:13:15.393
  E0804 21:13:16.110850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:17.110971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting Services @ 08/04/23 21:13:17.396
  STEP: Ensuring resource quota status released usage @ 08/04/23 21:13:17.423
  E0804 21:13:18.112030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:19.112229      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:19.427: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7706" for this suite. @ 08/04/23 21:13:19.429
• [11.109 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 08/04/23 21:13:19.433
  Aug  4 21:13:19.433: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 21:13:19.434
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:13:19.441
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:13:19.443
  STEP: Setting up server cert @ 08/04/23 21:13:19.455
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 21:13:19.723
  STEP: Deploying the webhook pod @ 08/04/23 21:13:19.728
  STEP: Wait for the deployment to be ready @ 08/04/23 21:13:19.736
  Aug  4 21:13:19.739: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0804 21:13:20.113236      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:21.113470      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 21:13:21.747
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 21:13:21.756
  E0804 21:13:22.113782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:22.756: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 08/04/23 21:13:22.758
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/04/23 21:13:22.758
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 08/04/23 21:13:22.771
  E0804 21:13:23.114546      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 08/04/23 21:13:23.777
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/04/23 21:13:23.777
  E0804 21:13:24.114656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is longer than webhook latency @ 08/04/23 21:13:24.798
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/04/23 21:13:24.798
  E0804 21:13:25.115535      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:26.116399      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:27.117063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:28.118096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:29.118313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 08/04/23 21:13:29.819
  STEP: Registering slow webhook via the AdmissionRegistration API @ 08/04/23 21:13:29.819
  E0804 21:13:30.118792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:31.119361      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:32.119749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:33.119858      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:34.120409      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:34.838: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-9177" for this suite. @ 08/04/23 21:13:34.872
  STEP: Destroying namespace "webhook-markers-4052" for this suite. @ 08/04/23 21:13:34.875
• [15.445 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 08/04/23 21:13:34.879
  Aug  4 21:13:34.879: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename statefulset @ 08/04/23 21:13:34.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:13:34.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:13:34.889
  STEP: Creating service test in namespace statefulset-1425 @ 08/04/23 21:13:34.891
  Aug  4 21:13:34.901: INFO: Found 0 stateful pods, waiting for 1
  E0804 21:13:35.121261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:36.121454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:37.121886      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:38.122782      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:39.123023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:40.124015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:41.124168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:42.124541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:43.124774      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:44.124974      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:44.905: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 08/04/23 21:13:44.909
  W0804 21:13:44.915281      22 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Aug  4 21:13:44.918: INFO: Found 1 stateful pods, waiting for 2
  E0804 21:13:45.125796      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:46.126024      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:47.126539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:48.126910      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:49.126960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:50.128022      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:51.128248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:52.128593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:53.128787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:54.128982      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:13:54.921: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 21:13:54.921: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 08/04/23 21:13:54.925
  STEP: Delete all of the StatefulSets @ 08/04/23 21:13:54.926
  STEP: Verify that StatefulSets have been deleted @ 08/04/23 21:13:54.93
  Aug  4 21:13:54.932: INFO: Deleting all statefulset in ns statefulset-1425
  Aug  4 21:13:54.937: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1425" for this suite. @ 08/04/23 21:13:54.94
• [20.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 08/04/23 21:13:54.945
  Aug  4 21:13:54.945: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 21:13:54.946
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:13:54.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:13:54.962
  STEP: Creating configMap with name projected-configmap-test-volume-1a652fce-54ae-440c-bf7c-aed6800d923e @ 08/04/23 21:13:54.965
  STEP: Creating a pod to test consume configMaps @ 08/04/23 21:13:54.968
  E0804 21:13:55.129404      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:56.129618      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:13:56.979
  Aug  4 21:13:56.981: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-c19bfb54-e9c7-46ce-a4d3-ad095b931fdb container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 21:13:56.988
  Aug  4 21:13:56.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5035" for this suite. @ 08/04/23 21:13:57.001
• [2.059 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 08/04/23 21:13:57.004
  Aug  4 21:13:57.004: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 21:13:57.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:13:57.012
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:13:57.013
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 08/04/23 21:13:57.015
  E0804 21:13:57.130619      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:13:58.130863      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:13:59.025
  Aug  4 21:13:59.026: INFO: Trying to get logs from node k8sconformance-m02 pod pod-39114f59-c143-4796-b41c-e74c326333dd container test-container: <nil>
  STEP: delete the pod @ 08/04/23 21:13:59.032
  Aug  4 21:13:59.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9908" for this suite. @ 08/04/23 21:13:59.044
• [2.042 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 08/04/23 21:13:59.047
  Aug  4 21:13:59.047: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename disruption @ 08/04/23 21:13:59.047
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:13:59.055
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:13:59.057
  STEP: creating the pdb @ 08/04/23 21:13:59.059
  STEP: Waiting for the pdb to be processed @ 08/04/23 21:13:59.062
  E0804 21:13:59.130921      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:00.130989      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: updating the pdb @ 08/04/23 21:14:01.066
  STEP: Waiting for the pdb to be processed @ 08/04/23 21:14:01.071
  E0804 21:14:01.131964      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:02.132356      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching the pdb @ 08/04/23 21:14:03.075
  STEP: Waiting for the pdb to be processed @ 08/04/23 21:14:03.08
  E0804 21:14:03.132818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:04.133008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for the pdb to be deleted @ 08/04/23 21:14:05.089
  Aug  4 21:14:05.090: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-4665" for this suite. @ 08/04/23 21:14:05.092
• [6.049 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 08/04/23 21:14:05.096
  Aug  4 21:14:05.096: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-runtime @ 08/04/23 21:14:05.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:14:05.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:14:05.108
  STEP: create the container @ 08/04/23 21:14:05.11
  W0804 21:14:05.114541      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/04/23 21:14:05.114
  E0804 21:14:05.133853      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:06.134930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/04/23 21:14:07.123
  STEP: the container should be terminated @ 08/04/23 21:14:07.125
  STEP: the termination message should be set @ 08/04/23 21:14:07.125
  Aug  4 21:14:07.125: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 08/04/23 21:14:07.125
  Aug  4 21:14:07.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0804 21:14:07.135756      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "container-runtime-1082" for this suite. @ 08/04/23 21:14:07.135
• [2.043 seconds]
------------------------------
SSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 08/04/23 21:14:07.139
  Aug  4 21:14:07.139: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubelet-test @ 08/04/23 21:14:07.14
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:14:07.151
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:14:07.153
  STEP: Waiting for pod completion @ 08/04/23 21:14:07.159
  E0804 21:14:08.135961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:09.136212      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:14:09.171: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-3550" for this suite. @ 08/04/23 21:14:09.174
• [2.039 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 08/04/23 21:14:09.178
  Aug  4 21:14:09.178: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 21:14:09.179
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:14:09.187
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:14:09.189
  STEP: Creating a ResourceQuota @ 08/04/23 21:14:09.191
  STEP: Getting a ResourceQuota @ 08/04/23 21:14:09.194
  STEP: Updating a ResourceQuota @ 08/04/23 21:14:09.195
  STEP: Verifying a ResourceQuota was modified @ 08/04/23 21:14:09.201
  STEP: Deleting a ResourceQuota @ 08/04/23 21:14:09.204
  STEP: Verifying the deleted ResourceQuota @ 08/04/23 21:14:09.207
  Aug  4 21:14:09.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5856" for this suite. @ 08/04/23 21:14:09.211
• [0.036 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 08/04/23 21:14:09.215
  Aug  4 21:14:09.215: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename statefulset @ 08/04/23 21:14:09.216
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:14:09.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:14:09.228
  STEP: Creating service test in namespace statefulset-774 @ 08/04/23 21:14:09.23
  STEP: Creating a new StatefulSet @ 08/04/23 21:14:09.233
  Aug  4 21:14:09.240: INFO: Found 0 stateful pods, waiting for 3
  E0804 21:14:10.137153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:11.137694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:12.138118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:13.138379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:14.138562      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:15.138841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:16.139418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:17.140015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:18.140217      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:19.140430      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:14:19.243: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 21:14:19.243: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 21:14:19.243: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Aug  4 21:14:19.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-774 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 21:14:19.383: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 21:14:19.383: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 21:14:19.383: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0804 21:14:20.141382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:21.141598      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:22.141745      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:23.142860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:24.142967      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:25.143231      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:26.144041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:27.144726      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:28.144942      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:29.145179      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 08/04/23 21:14:29.393
  Aug  4 21:14:29.411: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 08/04/23 21:14:29.411
  E0804 21:14:30.145411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:31.145636      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:32.146020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:33.146225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:34.146438      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:35.146656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:36.146900      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:37.147412      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:38.147621      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:39.147821      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating Pods in reverse ordinal order @ 08/04/23 21:14:39.421
  Aug  4 21:14:39.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-774 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  4 21:14:39.544: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  4 21:14:39.544: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 21:14:39.544: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0804 21:14:40.148286      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:41.148494      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:42.148841      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:43.149070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:44.149330      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:45.149422      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:46.149641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:47.150249      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:48.150443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:49.150790      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back to a previous revision @ 08/04/23 21:14:49.558
  Aug  4 21:14:49.558: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-774 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Aug  4 21:14:49.699: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Aug  4 21:14:49.699: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Aug  4 21:14:49.699: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  E0804 21:14:50.150990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:51.151175      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:52.151558      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:53.151767      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:54.152023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:55.152498      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:56.152733      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:57.153304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:58.153529      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:14:59.153759      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:14:59.726: INFO: Updating stateful set ss2
  E0804 21:15:00.154059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:01.154241      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:02.154722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:03.154979      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:04.155108      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:05.156052      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:06.156291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:07.156792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:08.156986      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:09.157209      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Rolling back update in reverse ordinal order @ 08/04/23 21:15:09.737
  Aug  4 21:15:09.739: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=statefulset-774 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Aug  4 21:15:09.857: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Aug  4 21:15:09.857: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Aug  4 21:15:09.857: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  E0804 21:15:10.158007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:11.158255      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:12.158688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:13.158948      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:14.159007      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:15.160036      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:16.160197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:17.160708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:18.160898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:19.161094      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:15:19.870: INFO: Deleting all statefulset in ns statefulset-774
  Aug  4 21:15:19.872: INFO: Scaling statefulset ss2 to 0
  E0804 21:15:20.161809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:21.162300      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:22.162664      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:23.162960      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:24.162985      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:25.163174      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:26.164041      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:27.164603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:28.164809      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:29.165394      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:15:29.882: INFO: Waiting for statefulset status.replicas updated to 0
  Aug  4 21:15:29.884: INFO: Deleting statefulset ss2
  Aug  4 21:15:29.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-774" for this suite. @ 08/04/23 21:15:29.893
• [80.681 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 08/04/23 21:15:29.897
  Aug  4 21:15:29.897: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 21:15:29.898
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:15:29.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:15:29.909
  STEP: Creating configMap with name configmap-test-volume-ef5c2bdd-476c-4bfb-8986-02b30f904cea @ 08/04/23 21:15:29.91
  STEP: Creating a pod to test consume configMaps @ 08/04/23 21:15:29.913
  E0804 21:15:30.165582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:31.166066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:32.166148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:33.166348      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:15:33.926
  Aug  4 21:15:33.928: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-67961df2-2f31-4003-849c-c153fce18db2 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 21:15:33.933
  Aug  4 21:15:33.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4147" for this suite. @ 08/04/23 21:15:33.945
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 08/04/23 21:15:33.95
  Aug  4 21:15:33.950: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 21:15:33.95
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:15:33.959
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:15:33.961
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 21:15:33.963
  E0804 21:15:34.167250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:35.167402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:36.167950      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:37.168740      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:15:37.978
  Aug  4 21:15:37.980: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-4b597ced-1695-4e0b-bc8d-82c5cb9f3f0b container client-container: <nil>
  STEP: delete the pod @ 08/04/23 21:15:37.985
  Aug  4 21:15:37.995: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-5089" for this suite. @ 08/04/23 21:15:37.997
• [4.050 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 08/04/23 21:15:38.001
  Aug  4 21:15:38.001: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename prestop @ 08/04/23 21:15:38.001
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:15:38.008
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:15:38.01
  STEP: Creating server pod server in namespace prestop-7805 @ 08/04/23 21:15:38.012
  STEP: Waiting for pods to come up. @ 08/04/23 21:15:38.016
  E0804 21:15:38.169531      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:39.170559      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating tester pod tester in namespace prestop-7805 @ 08/04/23 21:15:40.023
  E0804 21:15:40.170661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:41.170795      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting pre-stop pod @ 08/04/23 21:15:42.033
  E0804 21:15:42.171672      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:43.172039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:44.172250      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:45.172704      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:46.172941      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:15:47.041: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Aug  4 21:15:47.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 08/04/23 21:15:47.044
  STEP: Destroying namespace "prestop-7805" for this suite. @ 08/04/23 21:15:47.054
• [9.057 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 08/04/23 21:15:47.058
  Aug  4 21:15:47.058: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 21:15:47.059
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:15:47.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:15:47.072
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 21:15:47.074
  E0804 21:15:47.173700      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:48.173724      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:15:49.085
  Aug  4 21:15:49.087: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-95dcb0bf-ccfc-48a7-80b8-155df025957d container client-container: <nil>
  STEP: delete the pod @ 08/04/23 21:15:49.092
  Aug  4 21:15:49.103: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5222" for this suite. @ 08/04/23 21:15:49.105
• [2.050 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 08/04/23 21:15:49.109
  Aug  4 21:15:49.109: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename dns @ 08/04/23 21:15:49.11
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:15:49.117
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:15:49.119
  STEP: Creating a test externalName service @ 08/04/23 21:15:49.121
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3174.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local; sleep 1; done
   @ 08/04/23 21:15:49.124
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3174.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local; sleep 1; done
   @ 08/04/23 21:15:49.124
  STEP: creating a pod to probe DNS @ 08/04/23 21:15:49.124
  STEP: submitting the pod to kubernetes @ 08/04/23 21:15:49.124
  E0804 21:15:49.173787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:50.173971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 21:15:51.136
  STEP: looking for the results for each expected name from probers @ 08/04/23 21:15:51.138
  Aug  4 21:15:51.143: INFO: DNS probes using dns-test-62a94e16-2b53-494e-8ccd-21b844a420ea succeeded

  STEP: changing the externalName to bar.example.com @ 08/04/23 21:15:51.143
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3174.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local; sleep 1; done
   @ 08/04/23 21:15:51.148
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3174.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local; sleep 1; done
   @ 08/04/23 21:15:51.148
  STEP: creating a second pod to probe DNS @ 08/04/23 21:15:51.148
  STEP: submitting the pod to kubernetes @ 08/04/23 21:15:51.148
  E0804 21:15:51.174497      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:52.174857      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 21:15:53.16
  STEP: looking for the results for each expected name from probers @ 08/04/23 21:15:53.162
  Aug  4 21:15:53.165: INFO: File wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:15:53.167: INFO: File jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:15:53.167: INFO: Lookups using dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd failed for: [wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local]

  E0804 21:15:53.175324      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:54.175479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:55.176061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:56.176291      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:57.177057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:15:58.170: INFO: File wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:15:58.173: INFO: File jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:15:58.173: INFO: Lookups using dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd failed for: [wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local]

  E0804 21:15:58.177265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:15:59.177496      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:00.177744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:01.178122      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:02.178552      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:16:03.170: INFO: File wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:16:03.172: INFO: File jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:16:03.172: INFO: Lookups using dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd failed for: [wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local]

  E0804 21:16:03.178914      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:04.178981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:05.179214      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:06.180078      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:07.180564      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:16:08.170: INFO: File wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:16:08.172: INFO: File jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:16:08.172: INFO: Lookups using dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd failed for: [wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local]

  E0804 21:16:08.181266      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:09.181460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:10.181657      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:11.181873      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:12.182379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:16:13.173: INFO: File wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:16:13.175: INFO: File jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:16:13.175: INFO: Lookups using dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd failed for: [wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local]

  E0804 21:16:13.182951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:14.182970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:15.184067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:16.184272      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:17.184647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:16:18.170: INFO: File wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:16:18.172: INFO: File jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local from pod  dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Aug  4 21:16:18.172: INFO: Lookups using dns-3174/dns-test-551be954-3d84-4f05-893d-73b3376922fd failed for: [wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local]

  E0804 21:16:18.185746      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:19.185955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:20.186153      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:21.186450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:22.186776      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:16:23.174: INFO: DNS probes using dns-test-551be954-3d84-4f05-893d-73b3376922fd succeeded

  STEP: changing the service to type=ClusterIP @ 08/04/23 21:16:23.174
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3174.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-3174.svc.cluster.local; sleep 1; done
   @ 08/04/23 21:16:23.186
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-3174.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-3174.svc.cluster.local; sleep 1; done
   @ 08/04/23 21:16:23.186
  STEP: creating a third pod to probe DNS @ 08/04/23 21:16:23.186
  E0804 21:16:23.187268      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: submitting the pod to kubernetes @ 08/04/23 21:16:23.188
  E0804 21:16:24.187522      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:25.188107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 21:16:25.199
  STEP: looking for the results for each expected name from probers @ 08/04/23 21:16:25.201
  Aug  4 21:16:25.205: INFO: DNS probes using dns-test-c05bf208-9104-4554-95f3-14ec88375ab8 succeeded

  Aug  4 21:16:25.205: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 21:16:25.207
  STEP: deleting the pod @ 08/04/23 21:16:25.216
  STEP: deleting the pod @ 08/04/23 21:16:25.231
  STEP: deleting the test externalName service @ 08/04/23 21:16:25.239
  STEP: Destroying namespace "dns-3174" for this suite. @ 08/04/23 21:16:25.253
• [36.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 08/04/23 21:16:25.26
  Aug  4 21:16:25.260: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-runtime @ 08/04/23 21:16:25.261
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:16:25.305
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:16:25.308
  STEP: create the container @ 08/04/23 21:16:25.311
  W0804 21:16:25.319378      22 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 08/04/23 21:16:25.319
  E0804 21:16:26.188953      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:27.189585      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:28.190433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: get the container status @ 08/04/23 21:16:28.331
  STEP: the container should be terminated @ 08/04/23 21:16:28.332
  STEP: the termination message should be set @ 08/04/23 21:16:28.332
  Aug  4 21:16:28.332: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 08/04/23 21:16:28.332
  Aug  4 21:16:28.339: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-1589" for this suite. @ 08/04/23 21:16:28.343
• [3.085 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 08/04/23 21:16:28.346
  Aug  4 21:16:28.346: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename webhook @ 08/04/23 21:16:28.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:16:28.354
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:16:28.356
  STEP: Setting up server cert @ 08/04/23 21:16:28.367
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 08/04/23 21:16:28.847
  STEP: Deploying the webhook pod @ 08/04/23 21:16:28.852
  STEP: Wait for the deployment to be ready @ 08/04/23 21:16:28.862
  Aug  4 21:16:28.866: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0804 21:16:29.191047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:30.192065      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 08/04/23 21:16:30.873
  STEP: Verifying the service has paired with the endpoint @ 08/04/23 21:16:30.881
  E0804 21:16:31.193096      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:16:31.882: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Aug  4 21:16:31.885: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 21:16:32.193693      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 08/04/23 21:16:32.392
  STEP: Creating a custom resource that should be denied by the webhook @ 08/04/23 21:16:32.404
  E0804 21:16:33.193787      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:34.193999      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 08/04/23 21:16:34.42
  STEP: Updating the custom resource with disallowed data should be denied @ 08/04/23 21:16:34.427
  STEP: Deleting the custom resource should be denied @ 08/04/23 21:16:34.434
  STEP: Remove the offending key and value from the custom resource data @ 08/04/23 21:16:34.439
  STEP: Deleting the updated custom resource should be successful @ 08/04/23 21:16:34.445
  Aug  4 21:16:34.450: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8331" for this suite. @ 08/04/23 21:16:34.988
  STEP: Destroying namespace "webhook-markers-7322" for this suite. @ 08/04/23 21:16:34.992
• [6.652 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 08/04/23 21:16:34.998
  Aug  4 21:16:34.998: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 21:16:34.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:16:35.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:16:35.009
  STEP: Counting existing ResourceQuota @ 08/04/23 21:16:35.011
  E0804 21:16:35.195126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:36.195864      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:37.195899      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:38.196516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:39.197149      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/04/23 21:16:40.013
  STEP: Ensuring resource quota status is calculated @ 08/04/23 21:16:40.017
  E0804 21:16:40.197688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:41.198051      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 08/04/23 21:16:42.021
  STEP: Ensuring resource quota status captures replicaset creation @ 08/04/23 21:16:42.03
  E0804 21:16:42.198154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:43.198582      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 08/04/23 21:16:44.034
  STEP: Ensuring resource quota status released usage @ 08/04/23 21:16:44.04
  E0804 21:16:44.199442      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:45.200047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:16:46.043: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8828" for this suite. @ 08/04/23 21:16:46.045
• [11.051 seconds]
------------------------------
S
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 08/04/23 21:16:46.049
  Aug  4 21:16:46.049: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 21:16:46.05
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:16:46.059
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:16:46.061
  STEP: Creating configMap with name configmap-test-volume-55c62d0c-57eb-46e2-ba5a-3a5f145ec797 @ 08/04/23 21:16:46.063
  STEP: Creating a pod to test consume configMaps @ 08/04/23 21:16:46.066
  E0804 21:16:46.200504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:47.201127      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:48.201576      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:49.201825      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:16:50.08
  Aug  4 21:16:50.082: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-3818a14b-136d-443f-aac8-bd2260c05750 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 21:16:50.088
  Aug  4 21:16:50.096: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-2414" for this suite. @ 08/04/23 21:16:50.098
• [4.052 seconds]
------------------------------
SS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 08/04/23 21:16:50.101
  Aug  4 21:16:50.102: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 21:16:50.102
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:16:50.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:16:50.113
  STEP: Creating configMap with name configmap-test-volume-map-46e69f5c-d720-4c86-9ef4-be2b6c175e20 @ 08/04/23 21:16:50.114
  STEP: Creating a pod to test consume configMaps @ 08/04/23 21:16:50.119
  E0804 21:16:50.202722      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:51.203080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:52.203919      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:53.204640      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:16:54.133
  Aug  4 21:16:54.135: INFO: Trying to get logs from node k8sconformance-m02 pod pod-configmaps-6515a37d-68a0-42b9-a820-0c6537f4b5e5 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 21:16:54.141
  Aug  4 21:16:54.149: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4853" for this suite. @ 08/04/23 21:16:54.153
  E0804 21:16:54.204656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
• [4.104 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 08/04/23 21:16:54.206
  Aug  4 21:16:54.206: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename services @ 08/04/23 21:16:54.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:16:54.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:16:54.218
  STEP: creating service in namespace services-5862 @ 08/04/23 21:16:54.22
  STEP: creating service affinity-clusterip in namespace services-5862 @ 08/04/23 21:16:54.22
  STEP: creating replication controller affinity-clusterip in namespace services-5862 @ 08/04/23 21:16:54.228
  I0804 21:16:54.232681      22 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-5862, replica count: 3
  E0804 21:16:55.205137      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:56.205429      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:57.206278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0804 21:16:57.283530      22 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Aug  4 21:16:57.287: INFO: Creating new exec pod
  E0804 21:16:58.206560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:16:59.207030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:00.207897      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:17:00.296: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-5862 exec execpod-affinity98h5b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Aug  4 21:17:00.422: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Aug  4 21:17:00.422: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 21:17:00.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-5862 exec execpod-affinity98h5b -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.107.48.172 80'
  Aug  4 21:17:00.556: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.107.48.172 80\nConnection to 10.107.48.172 80 port [tcp/http] succeeded!\n"
  Aug  4 21:17:00.556: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Aug  4 21:17:00.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=services-5862 exec execpod-affinity98h5b -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.107.48.172:80/ ; done'
  Aug  4 21:17:00.733: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.107.48.172:80/\n"
  Aug  4 21:17:00.733: INFO: stdout: "\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj\naffinity-clusterip-sq6sj"
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Received response from host: affinity-clusterip-sq6sj
  Aug  4 21:17:00.733: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 21:17:00.736: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-5862, will wait for the garbage collector to delete the pods @ 08/04/23 21:17:00.743
  Aug  4 21:17:00.801: INFO: Deleting ReplicationController affinity-clusterip took: 4.382343ms
  Aug  4 21:17:00.901: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.152645ms
  E0804 21:17:01.208626      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:02.209157      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "services-5862" for this suite. @ 08/04/23 21:17:03.111
• [8.910 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 08/04/23 21:17:03.116
  Aug  4 21:17:03.116: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename emptydir @ 08/04/23 21:17:03.117
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:03.125
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:03.127
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 08/04/23 21:17:03.128
  E0804 21:17:03.209304      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:04.209553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:17:05.139
  Aug  4 21:17:05.141: INFO: Trying to get logs from node k8sconformance-m02 pod pod-9e9cffc2-b0d8-4b3a-abd2-bdd32da6b1e1 container test-container: <nil>
  STEP: delete the pod @ 08/04/23 21:17:05.146
  Aug  4 21:17:05.154: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8019" for this suite. @ 08/04/23 21:17:05.156
• [2.043 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 08/04/23 21:17:05.16
  Aug  4 21:17:05.160: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename secrets @ 08/04/23 21:17:05.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:05.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:05.169
  STEP: Creating secret with name secret-test-map-29edf241-b4fc-42c6-a7e1-7e798e791bb9 @ 08/04/23 21:17:05.171
  STEP: Creating a pod to test consume secrets @ 08/04/23 21:17:05.174
  E0804 21:17:05.210424      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:06.210517      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:17:07.185
  Aug  4 21:17:07.186: INFO: Trying to get logs from node k8sconformance-m02 pod pod-secrets-8dabdcd7-b91d-4f6d-a4c2-ff3a8eb62737 container secret-volume-test: <nil>
  STEP: delete the pod @ 08/04/23 21:17:07.192
  Aug  4 21:17:07.202: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-5690" for this suite. @ 08/04/23 21:17:07.204
• [2.048 seconds]
------------------------------
SS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 08/04/23 21:17:07.207
  Aug  4 21:17:07.207: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename deployment @ 08/04/23 21:17:07.208
  E0804 21:17:07.210777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:07.215
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:07.217
  Aug  4 21:17:07.219: INFO: Creating deployment "test-recreate-deployment"
  Aug  4 21:17:07.223: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Aug  4 21:17:07.227: INFO: deployment "test-recreate-deployment" doesn't have the required revision set
  E0804 21:17:08.211744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:09.212161      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:17:09.232: INFO: Waiting deployment "test-recreate-deployment" to complete
  Aug  4 21:17:09.234: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Aug  4 21:17:09.239: INFO: Updating deployment test-recreate-deployment
  Aug  4 21:17:09.239: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Aug  4 21:17:09.292: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-3122  c49d72d4-16c7-409a-b997-c900ab5a7869 27460 2 2023-08-04 21:17:07 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-08-04 21:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003de7f48 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-08-04 21:17:09 +0000 UTC,LastTransitionTime:2023-08-04 21:17:09 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-08-04 21:17:09 +0000 UTC,LastTransitionTime:2023-08-04 21:17:07 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Aug  4 21:17:09.294: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-3122  a7e52ab6-6eae-4bbe-903f-72e48b0cee7d 27456 1 2023-08-04 21:17:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment c49d72d4-16c7-409a-b997-c900ab5a7869 0xc003e78c07 0xc003e78c08}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c49d72d4-16c7-409a-b997-c900ab5a7869\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:17:09 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e78ca8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:17:09.294: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Aug  4 21:17:09.295: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-3122  e8c42238-fd38-4f6e-b522-5a2ac9234fc9 27447 2 2023-08-04 21:17:07 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment c49d72d4-16c7-409a-b997-c900ab5a7869 0xc003e78d17 0xc003e78d18}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"c49d72d4-16c7-409a-b997-c900ab5a7869\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:17:09 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc003e78dc8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:17:09.297: INFO: Pod "test-recreate-deployment-54757ffd6c-r4hxh" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-r4hxh test-recreate-deployment-54757ffd6c- deployment-3122  04c17a30-6e16-4e0c-afd4-e7f7960b20d9 27459 0 2023-08-04 21:17:09 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c a7e52ab6-6eae-4bbe-903f-72e48b0cee7d 0xc003e79247 0xc003e79248}] [] [{kube-controller-manager Update v1 2023-08-04 21:17:09 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a7e52ab6-6eae-4bbe-903f-72e48b0cee7d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:17:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sns9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sns9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:17:09 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:17:09 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:,StartTime:2023-08-04 21:17:09 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 21:17:09.297: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-3122" for this suite. @ 08/04/23 21:17:09.299
• [2.096 seconds]
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 08/04/23 21:17:09.303
  Aug  4 21:17:09.303: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubectl @ 08/04/23 21:17:09.304
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:09.315
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:09.317
  STEP: creating Agnhost RC @ 08/04/23 21:17:09.319
  Aug  4 21:17:09.319: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-1762 create -f -'
  Aug  4 21:17:09.944: INFO: stderr: ""
  Aug  4 21:17:09.944: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 08/04/23 21:17:09.944
  E0804 21:17:10.212721      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:17:10.947: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 21:17:10.947: INFO: Found 0 / 1
  E0804 21:17:11.213508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:17:11.947: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 21:17:11.947: INFO: Found 1 / 1
  Aug  4 21:17:11.947: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 08/04/23 21:17:11.947
  Aug  4 21:17:11.949: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 21:17:11.949: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug  4 21:17:11.949: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-1211387682 --namespace=kubectl-1762 patch pod agnhost-primary-99sb4 -p {"metadata":{"annotations":{"x":"y"}}}'
  Aug  4 21:17:12.011: INFO: stderr: ""
  Aug  4 21:17:12.011: INFO: stdout: "pod/agnhost-primary-99sb4 patched\n"
  STEP: checking annotations @ 08/04/23 21:17:12.011
  Aug  4 21:17:12.013: INFO: Selector matched 1 pods for map[app:agnhost]
  Aug  4 21:17:12.013: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Aug  4 21:17:12.013: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1762" for this suite. @ 08/04/23 21:17:12.015
• [2.717 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 08/04/23 21:17:12.021
  Aug  4 21:17:12.021: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 08/04/23 21:17:12.021
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:12.029
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:12.031
  STEP: create the container to handle the HTTPGet hook request. @ 08/04/23 21:17:12.034
  E0804 21:17:12.213816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:13.214191      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: create the pod with lifecycle hook @ 08/04/23 21:17:14.048
  E0804 21:17:14.215145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:15.216081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete the pod with lifecycle hook @ 08/04/23 21:17:16.058
  E0804 21:17:16.216162      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:17.216237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: check prestop hook @ 08/04/23 21:17:18.069
  Aug  4 21:17:18.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-5581" for this suite. @ 08/04/23 21:17:18.077
• [6.060 seconds]
------------------------------
SSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 08/04/23 21:17:18.081
  Aug  4 21:17:18.081: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename init-container @ 08/04/23 21:17:18.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:18.092
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:18.093
  STEP: creating the pod @ 08/04/23 21:17:18.095
  Aug  4 21:17:18.095: INFO: PodSpec: initContainers in spec.initContainers
  E0804 21:17:18.216275      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:19.216374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:20.216586      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:21.216926      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:17:21.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-483" for this suite. @ 08/04/23 21:17:21.359
• [3.283 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 08/04/23 21:17:21.366
  Aug  4 21:17:21.366: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 21:17:21.367
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:21.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:21.378
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 21:17:21.38
  E0804 21:17:22.216994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:23.217278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:24.217879      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:25.218118      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:17:25.393
  Aug  4 21:17:25.395: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-183708a9-01e5-4f37-b7a2-3135cf349fbb container client-container: <nil>
  STEP: delete the pod @ 08/04/23 21:17:25.401
  Aug  4 21:17:25.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6608" for this suite. @ 08/04/23 21:17:25.414
• [4.051 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 08/04/23 21:17:25.418
  Aug  4 21:17:25.418: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 21:17:25.419
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:25.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:25.429
  STEP: Creating a pod to test downward API volume plugin @ 08/04/23 21:17:25.431
  E0804 21:17:26.218426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:27.218995      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:28.219111      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:29.220048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:17:29.445
  Aug  4 21:17:29.447: INFO: Trying to get logs from node k8sconformance-m02 pod downwardapi-volume-1cf62e65-a3b8-4828-aa23-0b33ba769230 container client-container: <nil>
  STEP: delete the pod @ 08/04/23 21:17:29.453
  Aug  4 21:17:29.465: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8508" for this suite. @ 08/04/23 21:17:29.468
• [4.053 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 08/04/23 21:17:29.473
  Aug  4 21:17:29.473: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename projected @ 08/04/23 21:17:29.474
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:29.488
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:29.491
  STEP: Creating configMap with name projected-configmap-test-volume-map-896585f8-162b-4bb7-999c-dcb73ec302ed @ 08/04/23 21:17:29.493
  STEP: Creating a pod to test consume configMaps @ 08/04/23 21:17:29.508
  E0804 21:17:30.220929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:31.221454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:17:31.521
  Aug  4 21:17:31.523: INFO: Trying to get logs from node k8sconformance-m02 pod pod-projected-configmaps-b57bd2e1-ee64-47af-9281-e8ddfbb6c0a8 container agnhost-container: <nil>
  STEP: delete the pod @ 08/04/23 21:17:31.529
  Aug  4 21:17:31.540: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5295" for this suite. @ 08/04/23 21:17:31.543
• [2.074 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 08/04/23 21:17:31.547
  Aug  4 21:17:31.547: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename kubelet-test @ 08/04/23 21:17:31.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:31.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:31.559
  E0804 21:17:32.221793      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:33.222057      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:17:33.579: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2228" for this suite. @ 08/04/23 21:17:33.582
• [2.039 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 08/04/23 21:17:33.586
  Aug  4 21:17:33.586: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename job @ 08/04/23 21:17:33.587
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:17:33.595
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:17:33.597
  STEP: Creating a job @ 08/04/23 21:17:33.599
  STEP: Ensuring active pods == parallelism @ 08/04/23 21:17:33.603
  E0804 21:17:34.222403      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:35.222617      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: delete a job @ 08/04/23 21:17:35.606
  STEP: deleting Job.batch foo in namespace job-4664, will wait for the garbage collector to delete the pods @ 08/04/23 21:17:35.606
  Aug  4 21:17:35.663: INFO: Deleting Job.batch foo took: 4.426009ms
  Aug  4 21:17:35.764: INFO: Terminating Job.batch foo pods took: 100.749987ms
  E0804 21:17:36.223379      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:37.224060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:38.225066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:39.225894      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:40.226889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:41.227715      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:42.228803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:43.229789      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:44.230878      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:45.231816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:46.232488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:47.232723      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:48.232862      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:49.233341      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:50.233599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:51.233880      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:52.234216      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:53.234610      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:54.235615      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:55.236509      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:56.237541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:57.238225      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:58.238264      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:17:59.238378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:00.239443      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:01.240374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:02.241434      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:03.241868      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:04.242822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:05.243114      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:06.244039      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring job was deleted @ 08/04/23 21:18:06.764
  Aug  4 21:18:06.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4664" for this suite. @ 08/04/23 21:18:06.769
• [33.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 08/04/23 21:18:06.773
  Aug  4 21:18:06.773: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename custom-resource-definition @ 08/04/23 21:18:06.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:06.783
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:06.785
  Aug  4 21:18:06.787: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  E0804 21:18:07.244681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:18:07.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-2544" for this suite. @ 08/04/23 21:18:07.317
• [0.547 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 08/04/23 21:18:07.323
  Aug  4 21:18:07.323: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename deployment @ 08/04/23 21:18:07.323
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:07.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:07.333
  STEP: creating a Deployment @ 08/04/23 21:18:07.336
  Aug  4 21:18:07.336: INFO: Creating simple deployment test-deployment-dd4b8
  Aug  4 21:18:07.342: INFO: new replicaset for deployment "test-deployment-dd4b8" is yet to be created
  E0804 21:18:08.245375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:09.245623      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 08/04/23 21:18:09.35
  Aug  4 21:18:09.352: INFO: Deployment test-deployment-dd4b8 has Conditions: [{Available True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dd4b8-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 08/04/23 21:18:09.352
  Aug  4 21:18:09.358: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 18, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 18, 8, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.August, 4, 21, 18, 8, 0, time.Local), LastTransitionTime:time.Date(2023, time.August, 4, 21, 18, 7, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-dd4b8-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 08/04/23 21:18:09.358
  Aug  4 21:18:09.359: INFO: Observed &Deployment event: ADDED
  Aug  4 21:18:09.359: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dd4b8-5994cf9475"}
  Aug  4 21:18:09.360: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.360: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dd4b8-5994cf9475"}
  Aug  4 21:18:09.360: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug  4 21:18:09.360: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.360: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug  4 21:18:09.360: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dd4b8-5994cf9475" is progressing.}
  Aug  4 21:18:09.360: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.360: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug  4 21:18:09.360: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dd4b8-5994cf9475" has successfully progressed.}
  Aug  4 21:18:09.360: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.360: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug  4 21:18:09.360: INFO: Observed Deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dd4b8-5994cf9475" has successfully progressed.}
  Aug  4 21:18:09.360: INFO: Found Deployment test-deployment-dd4b8 in namespace deployment-4855 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  4 21:18:09.360: INFO: Deployment test-deployment-dd4b8 has an updated status
  STEP: patching the Statefulset Status @ 08/04/23 21:18:09.36
  Aug  4 21:18:09.360: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Aug  4 21:18:09.366: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 08/04/23 21:18:09.366
  Aug  4 21:18:09.367: INFO: Observed &Deployment event: ADDED
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dd4b8-5994cf9475"}
  Aug  4 21:18:09.368: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-dd4b8-5994cf9475"}
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug  4 21:18:09.368: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:07 +0000 UTC 2023-08-04 21:18:07 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-dd4b8-5994cf9475" is progressing.}
  Aug  4 21:18:09.368: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dd4b8-5994cf9475" has successfully progressed.}
  Aug  4 21:18:09.368: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:08 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-08-04 21:18:08 +0000 UTC 2023-08-04 21:18:07 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-dd4b8-5994cf9475" has successfully progressed.}
  Aug  4 21:18:09.368: INFO: Observed deployment test-deployment-dd4b8 in namespace deployment-4855 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Aug  4 21:18:09.368: INFO: Observed &Deployment event: MODIFIED
  Aug  4 21:18:09.368: INFO: Found deployment test-deployment-dd4b8 in namespace deployment-4855 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Aug  4 21:18:09.368: INFO: Deployment test-deployment-dd4b8 has a patched status
  Aug  4 21:18:09.370: INFO: Deployment "test-deployment-dd4b8":
  &Deployment{ObjectMeta:{test-deployment-dd4b8  deployment-4855  3c01acb2-720f-4fd1-887e-4ed0b06d47a4 27835 1 2023-08-04 21:18:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-08-04 21:18:07 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:18:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status} {e2e.test Update apps/v1 2023-08-04 21:18:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ef3b38 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Aug  4 21:18:09.372: INFO: New ReplicaSet "test-deployment-dd4b8-5994cf9475" of Deployment "test-deployment-dd4b8":
  &ReplicaSet{ObjectMeta:{test-deployment-dd4b8-5994cf9475  deployment-4855  aef85d06-6b5c-49c5-ae0b-3f1ed5a90533 27831 1 2023-08-04 21:18:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-dd4b8 3c01acb2-720f-4fd1-887e-4ed0b06d47a4 0xc006ef3f00 0xc006ef3f01}] [] [{kube-controller-manager Update apps/v1 2023-08-04 21:18:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3c01acb2-720f-4fd1-887e-4ed0b06d47a4\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-08-04 21:18:08 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc006ef3fa8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Aug  4 21:18:09.374: INFO: Pod "test-deployment-dd4b8-5994cf9475-kmmnq" is available:
  &Pod{ObjectMeta:{test-deployment-dd4b8-5994cf9475-kmmnq test-deployment-dd4b8-5994cf9475- deployment-4855  03fa0c48-6463-47e2-bf3d-c597f3125b1c 27830 0 2023-08-04 21:18:07 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-dd4b8-5994cf9475 aef85d06-6b5c-49c5-ae0b-3f1ed5a90533 0xc004ddd4c0 0xc004ddd4c1}] [] [{kube-controller-manager Update v1 2023-08-04 21:18:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"aef85d06-6b5c-49c5-ae0b-3f1ed5a90533\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-08-04 21:18:08 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.244.1.24\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-d2kpd,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-d2kpd,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:k8sconformance-m02,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:18:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:18:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:18:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-08-04 21:18:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.49.3,PodIP:10.244.1.24,StartTime:2023-08-04 21:18:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-08-04 21:18:07 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:docker-pullable://registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:docker://df2abf5570d85a936d3817f5d9bd9b87014f12f8b238706a3096fb59863d12f2,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.244.1.24,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Aug  4 21:18:09.374: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4855" for this suite. @ 08/04/23 21:18:09.378
• [2.060 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 08/04/23 21:18:09.383
  Aug  4 21:18:09.383: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename dns @ 08/04/23 21:18:09.384
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:09.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:09.393
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 08/04/23 21:18:09.395
  Aug  4 21:18:09.400: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-9175  8c8d0bf3-1e29-4e27-8fc6-7d3f2bf9072a 27842 0 2023-08-04 21:18:09 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-08-04 21:18:09 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vpnqz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vpnqz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  E0804 21:18:10.246124      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:11.246370      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Verifying customized DNS suffix list is configured on pod... @ 08/04/23 21:18:11.405
  Aug  4 21:18:11.405: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-9175 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:18:11.405: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:18:11.405: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:18:11.405: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9175/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 08/04/23 21:18:11.492
  Aug  4 21:18:11.492: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-9175 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Aug  4 21:18:11.492: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  Aug  4 21:18:11.493: INFO: ExecWithOptions: Clientset creation
  Aug  4 21:18:11.493: INFO: ExecWithOptions: execute(POST https://10.96.0.1:443/api/v1/namespaces/dns-9175/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Aug  4 21:18:11.574: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Aug  4 21:18:11.577: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-9175" for this suite. @ 08/04/23 21:18:11.584
• [2.204 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 08/04/23 21:18:11.588
  Aug  4 21:18:11.588: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename security-context-test @ 08/04/23 21:18:11.588
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:11.596
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:11.598
  E0804 21:18:12.246981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:13.248060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:14.248578      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:15.248830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:18:15.619: INFO: Got logs for pod "busybox-privileged-false-37cb3d55-96e7-41ea-9fe7-8a96ca547983": "ip: RTNETLINK answers: Operation not permitted\n"
  Aug  4 21:18:15.619: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4232" for this suite. @ 08/04/23 21:18:15.621
• [4.037 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 08/04/23 21:18:15.626
  Aug  4 21:18:15.626: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename namespaces @ 08/04/23 21:18:15.626
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:15.633
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:15.635
  STEP: Creating namespace "e2e-ns-j5wbh" @ 08/04/23 21:18:15.637
  Aug  4 21:18:15.644: INFO: Namespace "e2e-ns-j5wbh-1352" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-j5wbh-1352" @ 08/04/23 21:18:15.644
  Aug  4 21:18:15.650: INFO: Namespace "e2e-ns-j5wbh-1352" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-j5wbh-1352" @ 08/04/23 21:18:15.65
  Aug  4 21:18:15.654: INFO: Namespace "e2e-ns-j5wbh-1352" has []v1.FinalizerName{"kubernetes"}
  Aug  4 21:18:15.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7221" for this suite. @ 08/04/23 21:18:15.657
  STEP: Destroying namespace "e2e-ns-j5wbh-1352" for this suite. @ 08/04/23 21:18:15.66
• [0.037 seconds]
------------------------------
S
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 08/04/23 21:18:15.663
  Aug  4 21:18:15.663: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename var-expansion @ 08/04/23 21:18:15.663
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:15.671
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:15.672
  STEP: Creating a pod to test substitution in container's args @ 08/04/23 21:18:15.674
  E0804 21:18:16.249593      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:17.250239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:18.250328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:19.250440      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:18:19.687
  Aug  4 21:18:19.689: INFO: Trying to get logs from node k8sconformance-m02 pod var-expansion-b972926a-34bc-4eea-9de1-36f7e443a7ba container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 21:18:19.693
  Aug  4 21:18:19.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9047" for this suite. @ 08/04/23 21:18:19.706
• [4.046 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 08/04/23 21:18:19.709
  Aug  4 21:18:19.709: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename subpath @ 08/04/23 21:18:19.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:19.718
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:19.72
  STEP: Setting up data @ 08/04/23 21:18:19.722
  STEP: Creating pod pod-subpath-test-secret-4v97 @ 08/04/23 21:18:19.726
  STEP: Creating a pod to test atomic-volume-subpath @ 08/04/23 21:18:19.726
  E0804 21:18:20.251014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:21.252038      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:22.252583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:23.252799      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:24.253228      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:25.253460      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:26.253823      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:27.254491      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:28.255456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:29.256059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:30.256605      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:31.256828      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:32.257830      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:33.258021      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:34.258539      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:35.258884      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:36.259551      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:37.260181      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:38.260326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:39.260518      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:40.261043      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:41.261173      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:42.261512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:43.261697      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:18:43.769
  Aug  4 21:18:43.771: INFO: Trying to get logs from node k8sconformance-m02 pod pod-subpath-test-secret-4v97 container test-container-subpath-secret-4v97: <nil>
  STEP: delete the pod @ 08/04/23 21:18:43.778
  STEP: Deleting pod pod-subpath-test-secret-4v97 @ 08/04/23 21:18:43.787
  Aug  4 21:18:43.787: INFO: Deleting pod "pod-subpath-test-secret-4v97" in namespace "subpath-1231"
  Aug  4 21:18:43.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-1231" for this suite. @ 08/04/23 21:18:43.792
• [24.087 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 08/04/23 21:18:43.796
  Aug  4 21:18:43.796: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename replication-controller @ 08/04/23 21:18:43.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:43.805
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:43.807
  STEP: Given a ReplicationController is created @ 08/04/23 21:18:43.809
  STEP: When the matched label of one of its pods change @ 08/04/23 21:18:43.813
  Aug  4 21:18:43.814: INFO: Pod name pod-release: Found 0 pods out of 1
  E0804 21:18:44.262583      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:45.262955      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:46.263013      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:47.263690      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:48.264015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:18:48.817: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 08/04/23 21:18:48.825
  E0804 21:18:49.264818      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:18:49.829: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2475" for this suite. @ 08/04/23 21:18:49.832
• [6.039 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 08/04/23 21:18:49.836
  Aug  4 21:18:49.836: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 21:18:49.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:49.846
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:49.847
  STEP: Creating a pod to test downward api env vars @ 08/04/23 21:18:49.849
  E0804 21:18:50.265571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:51.265581      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:52.266230      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:53.266459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:18:53.862
  Aug  4 21:18:53.864: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-c83f88dd-b533-46bd-b4e5-fc06b544cd96 container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 21:18:53.869
  Aug  4 21:18:53.879: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-9747" for this suite. @ 08/04/23 21:18:53.881
• [4.051 seconds]
------------------------------
SS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 08/04/23 21:18:53.887
  Aug  4 21:18:53.887: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename dns @ 08/04/23 21:18:53.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:53.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:53.898
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2226.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-2226.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 08/04/23 21:18:53.9
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-2226.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-2226.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 08/04/23 21:18:53.9
  STEP: creating a pod to probe /etc/hosts @ 08/04/23 21:18:53.9
  STEP: submitting the pod to kubernetes @ 08/04/23 21:18:53.9
  E0804 21:18:54.267168      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:55.267893      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: retrieving the pod @ 08/04/23 21:18:55.911
  STEP: looking for the results for each expected name from probers @ 08/04/23 21:18:55.913
  Aug  4 21:18:55.921: INFO: DNS probes using dns-2226/dns-test-79bd7376-e4dd-486a-b612-1e6480d1139b succeeded

  Aug  4 21:18:55.921: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 08/04/23 21:18:55.923
  STEP: Destroying namespace "dns-2226" for this suite. @ 08/04/23 21:18:55.93
• [2.047 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 08/04/23 21:18:55.935
  Aug  4 21:18:55.935: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename resourcequota @ 08/04/23 21:18:55.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:18:55.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:18:55.946
  STEP: Counting existing ResourceQuota @ 08/04/23 21:18:55.948
  E0804 21:18:56.268957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:57.269984      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:58.270223      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:18:59.270439      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:00.270587      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 08/04/23 21:19:00.95
  STEP: Ensuring resource quota status is calculated @ 08/04/23 21:19:00.955
  E0804 21:19:01.271606      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:02.272001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicationController @ 08/04/23 21:19:02.958
  STEP: Ensuring resource quota status captures replication controller creation @ 08/04/23 21:19:02.966
  E0804 21:19:03.272187      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:04.272792      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicationController @ 08/04/23 21:19:04.969
  STEP: Ensuring resource quota status released usage @ 08/04/23 21:19:04.973
  E0804 21:19:05.273204      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:06.273418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  Aug  4 21:19:06.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6311" for this suite. @ 08/04/23 21:19:06.978
• [11.046 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 08/04/23 21:19:06.982
  Aug  4 21:19:06.982: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename cronjob @ 08/04/23 21:19:06.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:19:06.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:19:06.996
  STEP: Creating a ForbidConcurrent cronjob @ 08/04/23 21:19:06.998
  STEP: Ensuring a job is scheduled @ 08/04/23 21:19:07.002
  E0804 21:19:07.273866      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:08.274770      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:09.275845      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:10.276067      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:11.276419      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:12.276860      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:13.277528      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:14.277831      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:15.278145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:16.278388      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:17.279365      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:18.280064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:19.280512      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:20.280728      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:21.280744      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:22.281120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:23.281504      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:24.281747      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:25.282488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:26.282714      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:27.283481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:28.284056      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:29.284695      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:30.284920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:31.285380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:32.285889      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:33.286783      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:34.287063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:35.287752      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:36.288080      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:37.289101      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:38.289359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:39.289393      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:40.289590      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:41.290431      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:42.290851      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:43.291490      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:44.291896      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:45.292144      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:46.292400      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:47.292474      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:48.292726      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:49.292997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:50.293411      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:51.293481      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:52.293827      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:53.294178      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:54.294549      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:55.295239      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:56.296035      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:57.296541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:58.296780      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:19:59.297450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:00.297688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 08/04/23 21:20:01.005
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 08/04/23 21:20:01.007
  STEP: Ensuring no more jobs are scheduled @ 08/04/23 21:20:01.009
  E0804 21:20:01.297801      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:02.298177      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:03.298777      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:04.299001      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:05.299134      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:06.299329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:07.299934      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:08.300180      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:09.300761      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:10.300936      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:11.301447      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:12.301833      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:13.302753      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:14.303000      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:15.303865      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:16.304708      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:17.305603      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:18.305806      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:19.306237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:20.306433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:21.306937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:22.308015      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:23.308278      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:24.308478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:25.309382      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:26.309760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:27.310421      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:28.310638      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:29.311253      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:30.311537      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:31.312009      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:32.312565      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:33.313128      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:34.313375      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:35.313692      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:36.313920      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:37.314757      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:38.315245      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:39.315694      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:40.316030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:41.316030      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:42.316196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:43.316822      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:44.317029      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:45.317574      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:46.317816      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:47.318296      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:48.318536      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:49.319089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:50.320071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:51.320698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:52.321077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:53.321633      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:54.322599      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:55.323117      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:56.324066      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:57.324990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:58.325276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:20:59.326298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:00.326425      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:01.327141      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:02.327560      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:03.328075      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:04.328315      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:05.329016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:06.329233      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:07.329335      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:08.329636      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:09.329850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:10.330454      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:11.331152      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:12.332071      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:13.332237      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:14.332459      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:15.333107      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:16.333321      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:17.334265      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:18.334479      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:19.335146      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:20.335186      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:21.336014      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:22.336495      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:23.336983      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:24.337205      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:25.338084      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:26.338247      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:27.338543      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:28.339119      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:29.339691      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:30.340317      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:31.340803      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:32.341874      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:33.342276      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:34.342541      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:35.343325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:36.344061      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:37.344325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:38.344547      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:39.345271      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:40.345488      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:41.345981      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:42.346416      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:43.346477      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:44.346842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:45.346965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:46.348033      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:47.349064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:48.349282      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:49.349961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:50.350224      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:51.350755      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:52.350992      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:53.352045      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:54.352456      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:55.353449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:56.353681      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:57.354159      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:58.354377      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:21:59.354924      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:00.354969      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:01.354987      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:02.356060      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:03.356912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:04.357126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:05.357760      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:06.358357      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:07.359329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:08.359532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:09.359930      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:10.360139      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:11.360410      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:12.360951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:13.361561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:14.361661      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:15.362310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:16.362508      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:17.362844      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:18.362970      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:19.363372      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:20.364020      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:21.364994      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:22.365426      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:23.365940      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:24.366154      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:25.366739      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:26.366963      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:27.367503      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:28.368053      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:29.368381      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:30.368684      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:31.369059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:32.369402      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:33.369651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:34.369882      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:35.370104      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:36.370423      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:37.371218      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:38.371461      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:39.372515      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:40.372749      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:41.373325      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:42.373737      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:43.374345      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:44.374554      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:45.374923      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:46.375339      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:47.376023      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:48.376298      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:49.376951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:50.377145      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:51.378063      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:52.378463      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:53.379008      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:54.379172      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:55.379917      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:56.380380      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:57.381138      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:58.381371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:22:59.381912      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:00.382826      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:01.382850      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:02.382965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:03.383651      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:04.384064      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:05.384647      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:06.385089      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:07.385971      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:08.387046      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:09.387689      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:10.388641      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:11.389406      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:12.389674      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:13.390383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:14.390553      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:15.391327      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:16.391980      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:17.392511      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:18.392741      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:19.393513      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:20.393580      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:21.394516      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:22.394645      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:23.395197      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:24.396055      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:25.396620      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:26.397263      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:27.397867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:28.398073      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:29.398735      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:30.398951      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:31.399571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:32.399671      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:33.399810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:34.400025      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:35.400631      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:36.401313      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:37.402070      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:38.402280      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:39.402968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:40.402977      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:41.403100      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:42.404042      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:43.404383      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:44.404675      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:45.405450      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:46.405997      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:47.406126      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:48.406328      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:49.406957      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:50.408016      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:51.408279      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:52.408532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:53.409148      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:54.409374      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:55.409433      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:56.409961      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:57.410081      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:58.410378      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:23:59.410500      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:00.410688      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:01.411135      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:02.412017      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:03.412050      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:04.412248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:05.412391      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:06.412990      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:07.413418      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:08.413628      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:09.413686      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:10.413876      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:11.414762      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:12.414965      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:13.416059      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:14.416261      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:15.416395      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:16.416903      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:17.417083      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:18.417329      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:19.417534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:20.417725      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:21.418656      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:22.418898      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:23.419226      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:24.419449      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:25.420047      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:26.420561      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:27.421360      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:28.421544      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:29.422326      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:30.422532      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:31.423600      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:32.424048      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:33.424698      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:34.424842      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:35.425473      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:36.426058      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:37.426248      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:38.426446      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:39.427371      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:40.427597      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:41.428292      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:42.428478      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:43.429103      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:44.429337      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:45.429432      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:46.429937      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:47.430349      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:48.430534      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:49.431196      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:50.432049      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:51.432968      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:52.433156      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:53.433363      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:54.434302      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:55.434929      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:56.435959      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:57.436649      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:58.436867      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:24:59.437359      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:25:00.437571      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 08/04/23 21:25:01.015
  Aug  4 21:25:01.019: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-3872" for this suite. @ 08/04/23 21:25:01.021
• [354.043 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 08/04/23 21:25:01.026
  Aug  4 21:25:01.026: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename runtimeclass @ 08/04/23 21:25:01.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:25:01.046
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:25:01.048
  Aug  4 21:25:01.054: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5339" for this suite. @ 08/04/23 21:25:01.056
• [0.033 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 08/04/23 21:25:01.061
  Aug  4 21:25:01.061: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename configmap @ 08/04/23 21:25:01.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:25:01.072
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:25:01.074
  STEP: creating a ConfigMap @ 08/04/23 21:25:01.076
  STEP: fetching the ConfigMap @ 08/04/23 21:25:01.079
  STEP: patching the ConfigMap @ 08/04/23 21:25:01.08
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 08/04/23 21:25:01.083
  STEP: deleting the ConfigMap by collection with a label selector @ 08/04/23 21:25:01.085
  STEP: listing all ConfigMaps in test namespace @ 08/04/23 21:25:01.089
  Aug  4 21:25:01.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3770" for this suite. @ 08/04/23 21:25:01.093
• [0.034 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 08/04/23 21:25:01.097
  Aug  4 21:25:01.097: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename downward-api @ 08/04/23 21:25:01.097
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:25:01.106
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:25:01.108
  STEP: Creating a pod to test downward api env vars @ 08/04/23 21:25:01.11
  E0804 21:25:01.438310      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:25:02.439190      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 08/04/23 21:25:03.121
  Aug  4 21:25:03.122: INFO: Trying to get logs from node k8sconformance-m02 pod downward-api-5d8ad67a-aa5e-478a-b393-88b6be248c2e container dapi-container: <nil>
  STEP: delete the pod @ 08/04/23 21:25:03.136
  Aug  4 21:25:03.147: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4037" for this suite. @ 08/04/23 21:25:03.149
• [2.057 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 08/04/23 21:25:03.154
  Aug  4 21:25:03.154: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename disruption @ 08/04/23 21:25:03.155
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:25:03.163
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:25:03.165
  STEP: Creating a pdb that targets all three pods in a test replica set @ 08/04/23 21:25:03.167
  STEP: Waiting for the pdb to be processed @ 08/04/23 21:25:03.17
  E0804 21:25:03.439307      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:25:04.440077      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: First trying to evict a pod which shouldn't be evictable @ 08/04/23 21:25:05.18
  STEP: Waiting for all pods to be running @ 08/04/23 21:25:05.18
  Aug  4 21:25:05.182: INFO: pods: 0 < 3
  E0804 21:25:05.440748      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:25:06.440943      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: locating a running pod @ 08/04/23 21:25:07.185
  STEP: Updating the pdb to allow a pod to be evicted @ 08/04/23 21:25:07.192
  STEP: Waiting for the pdb to be processed @ 08/04/23 21:25:07.197
  E0804 21:25:07.441120      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:25:08.441810      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/04/23 21:25:09.203
  STEP: Waiting for all pods to be running @ 08/04/23 21:25:09.203
  STEP: Waiting for the pdb to observed all healthy pods @ 08/04/23 21:25:09.205
  STEP: Patching the pdb to disallow a pod to be evicted @ 08/04/23 21:25:09.22
  STEP: Waiting for the pdb to be processed @ 08/04/23 21:25:09.239
  E0804 21:25:09.441856      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0804 21:25:10.442082      22 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for all pods to be running @ 08/04/23 21:25:11.244
  STEP: locating a running pod @ 08/04/23 21:25:11.246
  STEP: Deleting the pdb to allow a pod to be evicted @ 08/04/23 21:25:11.253
  STEP: Waiting for the pdb to be deleted @ 08/04/23 21:25:11.257
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 08/04/23 21:25:11.258
  STEP: Waiting for all pods to be running @ 08/04/23 21:25:11.258
  Aug  4 21:25:11.269: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5034" for this suite. @ 08/04/23 21:25:11.272
• [8.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 08/04/23 21:25:11.277
  Aug  4 21:25:11.277: INFO: >>> kubeConfig: /tmp/kubeconfig-1211387682
  STEP: Building a namespace api object, basename lease-test @ 08/04/23 21:25:11.278
  STEP: Waiting for a default service account to be provisioned in namespace @ 08/04/23 21:25:11.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 08/04/23 21:25:11.29
  Aug  4 21:25:11.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-6197" for this suite. @ 08/04/23 21:25:11.329
• [0.056 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Aug  4 21:25:11.335: INFO: Running AfterSuite actions on node 1
  Aug  4 21:25:11.335: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.048 seconds]
------------------------------

Ran 378 of 7207 Specs in 5854.771 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h37m35.094569309s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

