  I0729 15:30:11.696347      13 e2e.go:117] Starting e2e run "17f5218d-1ccd-4f9b-86d8-e116b9c61702" on Ginkgo node 1
  Jul 29 15:30:11.757: INFO: Enabling in-tree volume drivers
Running Suite: Kubernetes e2e suite - /usr/local/bin
====================================================
Random Seed: 1690644611 - will randomize all specs

Will run 378 of 7207 specs
------------------------------
[ReportBeforeSuite] 
test/e2e/e2e_test.go:148
[ReportBeforeSuite] PASSED [0.000 seconds]
------------------------------
[SynchronizedBeforeSuite] 
test/e2e/e2e.go:77
  Jul 29 15:30:12.164: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 15:30:12.172: INFO: Waiting up to 30m0s for all (but 0) nodes to be schedulable
  Jul 29 15:30:12.261: INFO: Waiting up to 5m0s for all daemonsets in namespace 'kube-system' to start
  Jul 29 15:30:12.275: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium' (0 seconds elapsed)
  Jul 29 15:30:12.275: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'cilium-node-init' (0 seconds elapsed)
  Jul 29 15:30:12.276: INFO: 3 / 3 pods ready in namespace 'kube-system' in daemonset 'kube-proxy' (0 seconds elapsed)
  Jul 29 15:30:12.276: INFO: e2e test version: v1.27.4
  Jul 29 15:30:12.279: INFO: kube-apiserver version: v1.27.4
  Jul 29 15:30:12.279: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 15:30:12.288: INFO: Cluster IP family: ipv4
[SynchronizedBeforeSuite] PASSED [0.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with pruning [Conformance]
test/e2e/apimachinery/webhook.go:331
  STEP: Creating a kubernetes client @ 07/29/23 15:30:12.796
  Jul 29 15:30:12.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 15:30:12.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:30:12.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:30:12.843
  STEP: Setting up server cert @ 07/29/23 15:30:12.893
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 15:30:13.547
  STEP: Deploying the webhook pod @ 07/29/23 15:30:13.565
  STEP: Wait for the deployment to be ready @ 07/29/23 15:30:13.586
  Jul 29 15:30:13.597: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  Jul 29 15:30:15.623: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:30:17.632: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:30:19.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:30:21.633: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:30:23.631: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:30:25.630: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 30, 13, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/29/23 15:30:27.632
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 15:30:27.654
  Jul 29 15:30:28.655: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 15:30:28.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-1774-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 15:30:29.203
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/29/23 15:30:29.241
  Jul 29 15:30:31.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5495" for this suite. @ 07/29/23 15:30:32.135
  STEP: Destroying namespace "webhook-markers-7723" for this suite. @ 07/29/23 15:30:32.145
• [19.362 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:197
  STEP: Creating a kubernetes client @ 07/29/23 15:30:32.165
  Jul 29 15:30:32.165: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 15:30:32.168
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:30:32.198
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:30:32.203
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/29/23 15:30:32.208
  STEP: Saw pod success @ 07/29/23 15:30:36.255
  Jul 29 15:30:36.262: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-20a0933b-19c1-41da-8343-700ebc623617 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 15:30:36.299
  Jul 29 15:30:36.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4780" for this suite. @ 07/29/23 15:30:36.336
• [4.186 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Pods should support retrieving logs from the container over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:619
  STEP: Creating a kubernetes client @ 07/29/23 15:30:36.355
  Jul 29 15:30:36.355: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 15:30:36.358
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:30:36.392
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:30:36.395
  Jul 29 15:30:36.399: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: creating the pod @ 07/29/23 15:30:36.401
  STEP: submitting the pod to kubernetes @ 07/29/23 15:30:36.402
  Jul 29 15:30:40.472: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-5739" for this suite. @ 07/29/23 15:30:40.482
• [4.140 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide /etc/hosts entries for the cluster [Conformance]
test/e2e/network/dns.go:117
  STEP: Creating a kubernetes client @ 07/29/23 15:30:40.496
  Jul 29 15:30:40.496: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename dns @ 07/29/23 15:30:40.498
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:30:40.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:30:40.534
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8074.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-1.dns-test-service.dns-8074.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/wheezy_hosts@dns-querier-1;sleep 1; done
   @ 07/29/23 15:30:40.54
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-1.dns-test-service.dns-8074.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-1.dns-test-service.dns-8074.svc.cluster.local;test -n "$$(getent hosts dns-querier-1)" && echo OK > /results/jessie_hosts@dns-querier-1;sleep 1; done
   @ 07/29/23 15:30:40.54
  STEP: creating a pod to probe /etc/hosts @ 07/29/23 15:30:40.54
  STEP: submitting the pod to kubernetes @ 07/29/23 15:30:40.54
  STEP: retrieving the pod @ 07/29/23 15:31:00.647
  STEP: looking for the results for each expected name from probers @ 07/29/23 15:31:00.653
  Jul 29 15:31:00.691: INFO: DNS probes using dns-8074/dns-test-412897bb-8da2-4109-86e8-414c9a035af3 succeeded

  Jul 29 15:31:00.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 15:31:00.7
  STEP: Destroying namespace "dns-8074" for this suite. @ 07/29/23 15:31:00.726
• [20.242 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should patch a secret [Conformance]
test/e2e/common/node/secrets.go:154
  STEP: Creating a kubernetes client @ 07/29/23 15:31:00.748
  Jul 29 15:31:00.748: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 15:31:00.75
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:31:00.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:31:00.787
  STEP: creating a secret @ 07/29/23 15:31:00.792
  STEP: listing secrets in all namespaces to ensure that there are more than zero @ 07/29/23 15:31:00.799
  STEP: patching the secret @ 07/29/23 15:31:00.809
  STEP: deleting the secret using a LabelSelector @ 07/29/23 15:31:00.822
  STEP: listing secrets in all namespaces, searching for label name and value in patch @ 07/29/23 15:31:00.837
  Jul 29 15:31:00.843: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7913" for this suite. @ 07/29/23 15:31:00.849
• [0.111 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:207
  STEP: Creating a kubernetes client @ 07/29/23 15:31:00.861
  Jul 29 15:31:00.861: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 15:31:00.863
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:31:00.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:31:00.895
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/29/23 15:31:00.9
  STEP: Saw pod success @ 07/29/23 15:31:04.937
  Jul 29 15:31:04.943: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-1f80cdf7-d2c2-4aa5-ba3c-8a85b1f3227f container test-container: <nil>
  STEP: delete the pod @ 07/29/23 15:31:04.955
  Jul 29 15:31:04.982: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-2122" for this suite. @ 07/29/23 15:31:04.989
• [4.136 seconds]
------------------------------
S
------------------------------
[sig-apps] Job should create pods for an Indexed job with completion indexes and specified hostname [Conformance]
test/e2e/apps/job.go:370
  STEP: Creating a kubernetes client @ 07/29/23 15:31:04.998
  Jul 29 15:31:04.998: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename job @ 07/29/23 15:31:05
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:31:05.026
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:31:05.031
  STEP: Creating Indexed job @ 07/29/23 15:31:05.035
  STEP: Ensuring job reaches completions @ 07/29/23 15:31:05.046
  STEP: Ensuring pods with index for job exist @ 07/29/23 15:31:13.055
  Jul 29 15:31:13.063: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-4553" for this suite. @ 07/29/23 15:31:13.071
• [8.088 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:99
  STEP: Creating a kubernetes client @ 07/29/23 15:31:13.088
  Jul 29 15:31:13.088: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:31:13.09
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:31:13.124
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:31:13.129
  STEP: Creating configMap with name projected-configmap-test-volume-map-ce8085d6-e29b-4228-90d1-a64d4b8ab153 @ 07/29/23 15:31:13.133
  STEP: Creating a pod to test consume configMaps @ 07/29/23 15:31:13.143
  STEP: Saw pod success @ 07/29/23 15:31:17.183
  Jul 29 15:31:17.189: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-configmaps-f152cf0e-5006-4ff0-9cc4-d35006a9fec5 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 15:31:17.204
  Jul 29 15:31:17.230: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4794" for this suite. @ 07/29/23 15:31:17.239
• [4.162 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts ServiceAccountIssuerDiscovery should support OIDC discovery of service account issuer [Conformance]
test/e2e/auth/service_accounts.go:529
  STEP: Creating a kubernetes client @ 07/29/23 15:31:17.255
  Jul 29 15:31:17.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 15:31:17.258
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:31:17.284
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:31:17.289
  Jul 29 15:31:17.315: INFO: created pod
  STEP: Saw pod success @ 07/29/23 15:31:21.343
  Jul 29 15:31:51.346: INFO: polling logs
  Jul 29 15:31:51.367: INFO: Pod logs: 
  I0729 15:31:18.273171       1 log.go:198] OK: Got token
  I0729 15:31:18.273330       1 log.go:198] validating with in-cluster discovery
  I0729 15:31:18.274409       1 log.go:198] OK: got issuer https://kubernetes.default.svc.cluster.local
  I0729 15:31:18.274451       1 log.go:198] Full, not-validated claims: 
  openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7093:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690645277, NotBefore:1690644677, IssuedAt:1690644677, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7093", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"28ea9dbe-c2f1-43be-b48b-df94d97ad4a0"}}}
  I0729 15:31:18.299647       1 log.go:198] OK: Constructed OIDC provider for issuer https://kubernetes.default.svc.cluster.local
  I0729 15:31:18.313600       1 log.go:198] OK: Validated signature on JWT
  I0729 15:31:18.313749       1 log.go:198] OK: Got valid claims from token!
  I0729 15:31:18.313789       1 log.go:198] Full, validated claims: 
  &openidmetadata.claims{Claims:jwt.Claims{Issuer:"https://kubernetes.default.svc.cluster.local", Subject:"system:serviceaccount:svcaccounts-7093:default", Audience:jwt.Audience{"oidc-discovery-test"}, Expiry:1690645277, NotBefore:1690644677, IssuedAt:1690644677, ID:""}, Kubernetes:openidmetadata.kubeClaims{Namespace:"svcaccounts-7093", ServiceAccount:openidmetadata.kubeName{Name:"default", UID:"28ea9dbe-c2f1-43be-b48b-df94d97ad4a0"}}}

  Jul 29 15:31:51.368: INFO: completed pod
  Jul 29 15:31:51.378: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7093" for this suite. @ 07/29/23 15:31:51.386
• [34.141 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:255
  STEP: Creating a kubernetes client @ 07/29/23 15:31:51.4
  Jul 29 15:31:51.400: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename init-container @ 07/29/23 15:31:51.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:31:51.438
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:31:51.444
  STEP: creating the pod @ 07/29/23 15:31:51.449
  Jul 29 15:31:51.449: INFO: PodSpec: initContainers in spec.initContainers
  Jul 29 15:31:54.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-2951" for this suite. @ 07/29/23 15:31:54.903
• [3.517 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] Services should have session affinity work for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2202
  STEP: Creating a kubernetes client @ 07/29/23 15:31:54.92
  Jul 29 15:31:54.920: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 15:31:54.923
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:31:54.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:31:54.988
  STEP: creating service in namespace services-4065 @ 07/29/23 15:31:54.994
  STEP: creating service affinity-nodeport in namespace services-4065 @ 07/29/23 15:31:54.994
  STEP: creating replication controller affinity-nodeport in namespace services-4065 @ 07/29/23 15:31:55.02
  I0729 15:31:55.035457      13 runners.go:194] Created replication controller with name: affinity-nodeport, namespace: services-4065, replica count: 3
  I0729 15:31:58.092868      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 15:32:01.095146      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 15:32:04.095636      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 15:32:07.096146      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 1 running, 2 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 15:32:10.096580      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 2 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 15:32:13.100444      13 runners.go:194] affinity-nodeport Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 15:32:13.131: INFO: Creating new exec pod
  Jul 29 15:32:16.177: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4065 exec execpod-affinity8k8rr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport 80'
  Jul 29 15:32:16.637: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport 80\nConnection to affinity-nodeport 80 port [tcp/http] succeeded!\n"
  Jul 29 15:32:16.637: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:32:16.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4065 exec execpod-affinity8k8rr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.48.184 80'
  Jul 29 15:32:16.868: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.48.184 80\nConnection to 10.233.48.184 80 port [tcp/http] succeeded!\n"
  Jul 29 15:32:16.868: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:32:16.870: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4065 exec execpod-affinity8k8rr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.49 30335'
  Jul 29 15:32:17.116: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.49 30335\nConnection to 192.168.121.49 30335 port [tcp/*] succeeded!\n"
  Jul 29 15:32:17.116: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:32:17.117: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4065 exec execpod-affinity8k8rr -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.144 30335'
  Jul 29 15:32:17.336: INFO: stderr: "+ + nc -v -t -w 2echo 192.168.121.144 hostName 30335\n\nConnection to 192.168.121.144 30335 port [tcp/*] succeeded!\n"
  Jul 29 15:32:17.336: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:32:17.336: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4065 exec execpod-affinity8k8rr -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.39:30335/ ; done'
  Jul 29 15:32:17.737: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30335/\n"
  Jul 29 15:32:17.737: INFO: stdout: "\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx\naffinity-nodeport-8h4qx"
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.737: INFO: Received response from host: affinity-nodeport-8h4qx
  Jul 29 15:32:17.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 15:32:17.749: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport in namespace services-4065, will wait for the garbage collector to delete the pods @ 07/29/23 15:32:17.777
  Jul 29 15:32:17.855: INFO: Deleting ReplicationController affinity-nodeport took: 11.26746ms
  Jul 29 15:32:17.955: INFO: Terminating ReplicationController affinity-nodeport pods took: 100.828159ms
  STEP: Destroying namespace "services-4065" for this suite. @ 07/29/23 15:32:20.204
• [25.298 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields in both the root and embedded object of a CR [Conformance]
test/e2e/apimachinery/field_validation.go:474
  STEP: Creating a kubernetes client @ 07/29/23 15:32:20.218
  Jul 29 15:32:20.218: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 15:32:20.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:20.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:20.258
  Jul 29 15:32:20.263: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  W0729 15:32:23.009552      13 warnings.go:70] unknown field "alpha"
  W0729 15:32:23.009615      13 warnings.go:70] unknown field "beta"
  W0729 15:32:23.009624      13 warnings.go:70] unknown field "delta"
  W0729 15:32:23.009632      13 warnings.go:70] unknown field "epsilon"
  W0729 15:32:23.009662      13 warnings.go:70] unknown field "gamma"
  Jul 29 15:32:23.570: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-2554" for this suite. @ 07/29/23 15:32:23.6
• [3.392 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Services should complete a service status lifecycle [Conformance]
test/e2e/network/service.go:3322
  STEP: Creating a kubernetes client @ 07/29/23 15:32:23.612
  Jul 29 15:32:23.612: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 15:32:23.614
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:23.649
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:23.654
  STEP: creating a Service @ 07/29/23 15:32:23.666
  STEP: watching for the Service to be added @ 07/29/23 15:32:23.689
  Jul 29 15:32:23.692: INFO: Found Service test-service-pvjn9 in namespace services-6281 with labels: map[test-service-static:true] & ports [{http TCP <nil> 80 {0 80 } 0}]
  Jul 29 15:32:23.693: INFO: Service test-service-pvjn9 created
  STEP: Getting /status @ 07/29/23 15:32:23.694
  Jul 29 15:32:23.701: INFO: Service test-service-pvjn9 has LoadBalancer: {[]}
  STEP: patching the ServiceStatus @ 07/29/23 15:32:23.701
  STEP: watching for the Service to be patched @ 07/29/23 15:32:23.715
  Jul 29 15:32:23.718: INFO: observed Service test-service-pvjn9 in namespace services-6281 with annotations: map[] & LoadBalancer: {[]}
  Jul 29 15:32:23.718: INFO: Found Service test-service-pvjn9 in namespace services-6281 with annotations: map[patchedstatus:true] & LoadBalancer: {[{203.0.113.1  []}]}
  Jul 29 15:32:23.718: INFO: Service test-service-pvjn9 has service status patched
  STEP: updating the ServiceStatus @ 07/29/23 15:32:23.718
  Jul 29 15:32:23.740: INFO: updatedStatus.Conditions: []v1.Condition{v1.Condition{Type:"StatusUpdate", Status:"True", ObservedGeneration:0, LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Service to be updated @ 07/29/23 15:32:23.741
  Jul 29 15:32:23.744: INFO: Observed Service test-service-pvjn9 in namespace services-6281 with annotations: map[] & Conditions: {[]}
  Jul 29 15:32:23.744: INFO: Observed event: &Service{ObjectMeta:{test-service-pvjn9  services-6281  ea90adf2-be5c-474d-b9f8-6e66c8acd064 4159 0 2023-07-29 15:32:23 +0000 UTC <nil> <nil> map[test-service-static:true] map[patchedstatus:true] [] [] [{e2e.test Update v1 2023-07-29 15:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:test-service-static":{}}},"f:spec":{"f:internalTrafficPolicy":{},"f:ports":{".":{},"k:{\"port\":80,\"protocol\":\"TCP\"}":{".":{},"f:name":{},"f:port":{},"f:protocol":{},"f:targetPort":{}}},"f:sessionAffinity":{},"f:type":{}}} } {e2e.test Update v1 2023-07-29 15:32:23 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:patchedstatus":{}}},"f:status":{"f:loadBalancer":{"f:ingress":{}}}} status}]},Spec:ServiceSpec{Ports:[]ServicePort{ServicePort{Name:http,Protocol:TCP,Port:80,TargetPort:{0 80 },NodePort:0,AppProtocol:nil,},},Selector:map[string]string{},ClusterIP:10.233.38.4,Type:ClusterIP,ExternalIPs:[],SessionAffinity:None,LoadBalancerIP:,LoadBalancerSourceRanges:[],ExternalName:,ExternalTrafficPolicy:,HealthCheckNodePort:0,PublishNotReadyAddresses:false,SessionAffinityConfig:nil,IPFamilyPolicy:*SingleStack,ClusterIPs:[10.233.38.4],IPFamilies:[IPv4],AllocateLoadBalancerNodePorts:nil,LoadBalancerClass:nil,InternalTrafficPolicy:*Cluster,},Status:ServiceStatus{LoadBalancer:LoadBalancerStatus{Ingress:[]LoadBalancerIngress{LoadBalancerIngress{IP:203.0.113.1,Hostname:,Ports:[]PortStatus{},},},},Conditions:[]Condition{},},}
  Jul 29 15:32:23.745: INFO: Found Service test-service-pvjn9 in namespace services-6281 with annotations: map[patchedstatus:true] & Conditions: [{StatusUpdate True 0 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 15:32:23.746: INFO: Service test-service-pvjn9 has service status updated
  STEP: patching the service @ 07/29/23 15:32:23.747
  STEP: watching for the Service to be patched @ 07/29/23 15:32:23.766
  Jul 29 15:32:23.771: INFO: observed Service test-service-pvjn9 in namespace services-6281 with labels: map[test-service-static:true]
  Jul 29 15:32:23.771: INFO: observed Service test-service-pvjn9 in namespace services-6281 with labels: map[test-service-static:true]
  Jul 29 15:32:23.771: INFO: observed Service test-service-pvjn9 in namespace services-6281 with labels: map[test-service-static:true]
  Jul 29 15:32:23.771: INFO: Found Service test-service-pvjn9 in namespace services-6281 with labels: map[test-service:patched test-service-static:true]
  Jul 29 15:32:23.771: INFO: Service test-service-pvjn9 patched
  STEP: deleting the service @ 07/29/23 15:32:23.771
  STEP: watching for the Service to be deleted @ 07/29/23 15:32:23.797
  Jul 29 15:32:23.800: INFO: Observed event: ADDED
  Jul 29 15:32:23.800: INFO: Observed event: MODIFIED
  Jul 29 15:32:23.801: INFO: Observed event: MODIFIED
  Jul 29 15:32:23.801: INFO: Observed event: MODIFIED
  Jul 29 15:32:23.801: INFO: Found Service test-service-pvjn9 in namespace services-6281 with labels: map[test-service:patched test-service-static:true] & annotations: map[patchedstatus:true]
  Jul 29 15:32:23.802: INFO: Service test-service-pvjn9 deleted
  Jul 29 15:32:23.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-6281" for this suite. @ 07/29/23 15:32:23.811
• [0.211 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should patch a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:272
  STEP: Creating a kubernetes client @ 07/29/23 15:32:23.84
  Jul 29 15:32:23.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 15:32:23.843
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:23.876
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:23.881
  STEP: creating a Namespace @ 07/29/23 15:32:23.885
  STEP: patching the Namespace @ 07/29/23 15:32:23.914
  STEP: get the Namespace and ensuring it has the label @ 07/29/23 15:32:23.923
  Jul 29 15:32:23.929: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8302" for this suite. @ 07/29/23 15:32:23.936
  STEP: Destroying namespace "nspatchtest-e3a0dbff-72eb-48dc-b904-334a91053824-1282" for this suite. @ 07/29/23 15:32:23.946
• [0.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a non-existent RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:55
  STEP: Creating a kubernetes client @ 07/29/23 15:32:23.968
  Jul 29 15:32:23.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 15:32:23.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:24.004
  Jul 29 15:32:24.022: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-6320" for this suite. @ 07/29/23 15:32:24.029
• [0.075 seconds]
------------------------------
[sig-storage] Downward API volume should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:194
  STEP: Creating a kubernetes client @ 07/29/23 15:32:24.044
  Jul 29 15:32:24.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 15:32:24.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:24.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:24.084
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 15:32:24.091
  STEP: Saw pod success @ 07/29/23 15:32:28.134
  Jul 29 15:32:28.142: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-e484971f-1fe1-4178-91b2-a821cf63bd90 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 15:32:28.154
  Jul 29 15:32:28.183: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6848" for this suite. @ 07/29/23 15:32:28.191
• [4.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Scaling should happen in predictable order and halt if any stateful pod is unhealthy [Slow] [Conformance]
test/e2e/apps/statefulset.go:591
  STEP: Creating a kubernetes client @ 07/29/23 15:32:28.207
  Jul 29 15:32:28.208: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 15:32:28.21
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:32:28.24
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:32:28.244
  STEP: Creating service test in namespace statefulset-5744 @ 07/29/23 15:32:28.248
  STEP: Initializing watcher for selector baz=blah,foo=bar @ 07/29/23 15:32:28.258
  STEP: Creating stateful set ss in namespace statefulset-5744 @ 07/29/23 15:32:28.266
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-5744 @ 07/29/23 15:32:28.277
  Jul 29 15:32:28.281: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 15:32:38.297: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Pending - Ready=false
  Jul 29 15:32:48.290: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will halt with unhealthy stateful pod @ 07/29/23 15:32:48.291
  Jul 29 15:32:48.301: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 15:32:48.589: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 15:32:48.589: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 15:32:48.589: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 15:32:48.597: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jul 29 15:32:58.610: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 15:32:58.610: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 15:32:58.636: INFO: Verifying statefulset ss doesn't scale past 1 for another 9.999999651s
  Jul 29 15:32:59.645: INFO: Verifying statefulset ss doesn't scale past 1 for another 8.993983934s
  Jul 29 15:33:00.652: INFO: Verifying statefulset ss doesn't scale past 1 for another 7.98534167s
  Jul 29 15:33:01.658: INFO: Verifying statefulset ss doesn't scale past 1 for another 6.978078721s
  Jul 29 15:33:02.668: INFO: Verifying statefulset ss doesn't scale past 1 for another 5.972060467s
  Jul 29 15:33:03.677: INFO: Verifying statefulset ss doesn't scale past 1 for another 4.962367457s
  Jul 29 15:33:04.682: INFO: Verifying statefulset ss doesn't scale past 1 for another 3.952985044s
  Jul 29 15:33:05.691: INFO: Verifying statefulset ss doesn't scale past 1 for another 2.947939102s
  Jul 29 15:33:06.697: INFO: Verifying statefulset ss doesn't scale past 1 for another 1.939916351s
  Jul 29 15:33:07.703: INFO: Verifying statefulset ss doesn't scale past 1 for another 933.859025ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-5744 @ 07/29/23 15:33:08.704
  Jul 29 15:33:08.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:33:08.984: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 15:33:08.984: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 15:33:08.984: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 15:33:08.991: INFO: Found 1 stateful pods, waiting for 3
  Jul 29 15:33:19.000: INFO: Found 2 stateful pods, waiting for 3
  Jul 29 15:33:29.005: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 15:33:29.005: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 15:33:29.005: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Verifying that stateful set ss was scaled up in order @ 07/29/23 15:33:29.005
  STEP: Scale down will halt with unhealthy stateful pod @ 07/29/23 15:33:29.005
  Jul 29 15:33:29.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 15:33:29.280: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 15:33:29.280: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 15:33:29.280: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 15:33:29.280: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 15:33:29.590: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 15:33:29.590: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 15:33:29.590: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 15:33:29.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 15:33:29.891: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 15:33:29.891: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 15:33:29.891: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 15:33:29.891: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 15:33:29.899: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 2
  Jul 29 15:33:39.922: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 15:33:39.922: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 15:33:39.922: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 15:33:39.945: INFO: Verifying statefulset ss doesn't scale past 3 for another 9.9999997s
  Jul 29 15:33:40.956: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993890577s
  Jul 29 15:33:41.968: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.982194125s
  Jul 29 15:33:42.977: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.970519691s
  Jul 29 15:33:43.988: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.961376279s
  Jul 29 15:33:44.998: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.950298566s
  Jul 29 15:33:46.008: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.940167103s
  Jul 29 15:33:47.018: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.930831698s
  Jul 29 15:33:48.028: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.919964135s
  Jul 29 15:33:49.038: INFO: Verifying statefulset ss doesn't scale past 3 for another 910.650837ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-5744 @ 07/29/23 15:33:50.039
  Jul 29 15:33:50.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:33:50.389: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 15:33:50.390: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 15:33:50.390: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 15:33:50.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:33:50.702: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 15:33:50.702: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 15:33:50.702: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 15:33:50.702: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:33:51.142: INFO: rc: 1
  Jul 29 15:33:51.142: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  error: Internal error occurred: error executing command in container: container is not created or running

  error:
  exit status 1
  Jul 29 15:34:01.144: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:34:01.283: INFO: rc: 1
  Jul 29 15:34:01.283: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:34:11.284: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:34:11.439: INFO: rc: 1
  Jul 29 15:34:11.439: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:34:21.440: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:34:21.583: INFO: rc: 1
  Jul 29 15:34:21.583: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:34:31.587: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:34:31.717: INFO: rc: 1
  Jul 29 15:34:31.717: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:34:41.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:34:41.870: INFO: rc: 1
  Jul 29 15:34:41.870: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:34:51.872: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:34:52.020: INFO: rc: 1
  Jul 29 15:34:52.020: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:35:02.021: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:35:02.201: INFO: rc: 1
  Jul 29 15:35:02.201: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:35:12.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:35:12.414: INFO: rc: 1
  Jul 29 15:35:12.414: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:35:22.415: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:35:22.589: INFO: rc: 1
  Jul 29 15:35:22.589: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:35:32.591: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:35:32.730: INFO: rc: 1
  Jul 29 15:35:32.730: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:35:42.731: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:35:42.873: INFO: rc: 1
  Jul 29 15:35:42.873: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:35:52.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:35:53.042: INFO: rc: 1
  Jul 29 15:35:53.043: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:36:03.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:36:03.186: INFO: rc: 1
  Jul 29 15:36:03.186: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:36:13.188: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:36:13.359: INFO: rc: 1
  Jul 29 15:36:13.359: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:36:23.360: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:36:23.495: INFO: rc: 1
  Jul 29 15:36:23.496: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:36:33.497: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:36:33.661: INFO: rc: 1
  Jul 29 15:36:33.662: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:36:43.663: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:36:43.810: INFO: rc: 1
  Jul 29 15:36:43.811: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:36:53.812: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:36:53.951: INFO: rc: 1
  Jul 29 15:36:53.951: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:37:03.952: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:37:04.126: INFO: rc: 1
  Jul 29 15:37:04.126: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:37:14.133: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:37:14.319: INFO: rc: 1
  Jul 29 15:37:14.319: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:37:24.321: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:37:24.462: INFO: rc: 1
  Jul 29 15:37:24.462: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:37:34.466: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:37:34.619: INFO: rc: 1
  Jul 29 15:37:34.619: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:37:44.622: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:37:44.793: INFO: rc: 1
  Jul 29 15:37:44.793: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:37:54.795: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:37:55.017: INFO: rc: 1
  Jul 29 15:37:55.017: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:38:05.020: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:38:05.188: INFO: rc: 1
  Jul 29 15:38:05.189: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:38:15.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:38:15.354: INFO: rc: 1
  Jul 29 15:38:15.354: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:38:25.356: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:38:25.502: INFO: rc: 1
  Jul 29 15:38:25.502: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:38:35.508: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:38:35.681: INFO: rc: 1
  Jul 29 15:38:35.681: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:38:45.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:38:45.839: INFO: rc: 1
  Jul 29 15:38:45.840: INFO: Waiting 10s to retry failed RunHostCmd: error running /usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true:
  Command stdout:

  stderr:
  Error from server (NotFound): pods "ss-2" not found

  error:
  exit status 1
  Jul 29 15:38:55.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-5744 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 15:38:55.986: INFO: rc: 1
  Jul 29 15:38:55.986: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: 
  Jul 29 15:38:55.986: INFO: Scaling statefulset ss to 0
  STEP: Verifying that stateful set ss was scaled down in reverse order @ 07/29/23 15:38:56.017
  Jul 29 15:38:56.018: INFO: Deleting all statefulset in ns statefulset-5744
  Jul 29 15:38:56.023: INFO: Scaling statefulset ss to 0
  Jul 29 15:38:56.039: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 15:38:56.044: INFO: Deleting statefulset ss
  Jul 29 15:38:56.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5744" for this suite. @ 07/29/23 15:38:56.074
• [387.876 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with projected pod [Conformance]
test/e2e/storage/subpath.go:106
  STEP: Creating a kubernetes client @ 07/29/23 15:38:56.092
  Jul 29 15:38:56.093: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename subpath @ 07/29/23 15:38:56.096
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:38:56.126
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:38:56.131
  STEP: Setting up data @ 07/29/23 15:38:56.135
  STEP: Creating pod pod-subpath-test-projected-gdsv @ 07/29/23 15:38:56.149
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 15:38:56.15
  STEP: Saw pod success @ 07/29/23 15:39:20.289
  Jul 29 15:39:20.300: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-subpath-test-projected-gdsv container test-container-subpath-projected-gdsv: <nil>
  STEP: delete the pod @ 07/29/23 15:39:20.343
  STEP: Deleting pod pod-subpath-test-projected-gdsv @ 07/29/23 15:39:20.372
  Jul 29 15:39:20.372: INFO: Deleting pod "pod-subpath-test-projected-gdsv" in namespace "subpath-8632"
  Jul 29 15:39:20.379: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8632" for this suite. @ 07/29/23 15:39:20.392
• [24.312 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default arguments (container cmd) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:61
  STEP: Creating a kubernetes client @ 07/29/23 15:39:20.411
  Jul 29 15:39:20.411: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename containers @ 07/29/23 15:39:20.413
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:39:20.446
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:39:20.451
  STEP: Creating a pod to test override arguments @ 07/29/23 15:39:20.457
  STEP: Saw pod success @ 07/29/23 15:39:24.508
  Jul 29 15:39:24.516: INFO: Trying to get logs from node ci9axai7aiv7-3 pod client-containers-ec5b180f-c268-4286-a980-adad15324620 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 15:39:24.533
  Jul 29 15:39:24.558: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-6941" for this suite. @ 07/29/23 15:39:24.567
• [4.169 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:54
  STEP: Creating a kubernetes client @ 07/29/23 15:39:24.581
  Jul 29 15:39:24.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 15:39:24.585
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:39:24.616
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:39:24.621
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 15:39:24.626
  STEP: Saw pod success @ 07/29/23 15:39:28.671
  Jul 29 15:39:28.677: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-d81fe04e-bd0b-4458-9453-04af4f8db400 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 15:39:28.689
  Jul 29 15:39:28.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8179" for this suite. @ 07/29/23 15:39:28.724
• [4.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should create Endpoints and EndpointSlices for Pods matching a Service [Conformance]
test/e2e/network/endpointslice.go:207
  STEP: Creating a kubernetes client @ 07/29/23 15:39:28.74
  Jul 29 15:39:28.741: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 15:39:28.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:39:28.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:39:28.777
  STEP: referencing a single matching pod @ 07/29/23 15:39:33.925
  STEP: referencing matching pods with named port @ 07/29/23 15:39:38.946
  STEP: creating empty Endpoints and EndpointSlices for no matching Pods @ 07/29/23 15:39:43.969
  STEP: recreating EndpointSlices after they've been deleted @ 07/29/23 15:39:48.987
  Jul 29 15:39:49.034: INFO: EndpointSlice for Service endpointslice-5826/example-named-port not found
  Jul 29 15:39:59.077: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5826" for this suite. @ 07/29/23 15:39:59.088
• [30.375 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:240
  STEP: Creating a kubernetes client @ 07/29/23 15:39:59.12
  Jul 29 15:39:59.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 15:39:59.123
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:39:59.158
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:39:59.162
  STEP: Creating configMap with name cm-test-opt-del-8e9c8ce6-46d0-49e8-8a38-9617bd2cdb0c @ 07/29/23 15:39:59.201
  STEP: Creating configMap with name cm-test-opt-upd-94968665-15ad-45e0-a764-d18f88c3a5f5 @ 07/29/23 15:39:59.209
  STEP: Creating the pod @ 07/29/23 15:39:59.224
  STEP: Deleting configmap cm-test-opt-del-8e9c8ce6-46d0-49e8-8a38-9617bd2cdb0c @ 07/29/23 15:40:01.297
  STEP: Updating configmap cm-test-opt-upd-94968665-15ad-45e0-a764-d18f88c3a5f5 @ 07/29/23 15:40:01.306
  STEP: Creating configMap with name cm-test-opt-create-b4ea2cf2-6872-4a35-b348-869092423ecc @ 07/29/23 15:40:01.315
  STEP: waiting to observe update in volume @ 07/29/23 15:40:01.322
  Jul 29 15:41:14.138: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4907" for this suite. @ 07/29/23 15:41:14.186
• [75.079 seconds]
------------------------------
SSSS
------------------------------
[sig-node] PodTemplates should run the lifecycle of PodTemplates [Conformance]
test/e2e/common/node/podtemplates.go:53
  STEP: Creating a kubernetes client @ 07/29/23 15:41:14.2
  Jul 29 15:41:14.200: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 15:41:14.203
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:41:14.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:41:14.238
  Jul 29 15:41:14.300: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-28" for this suite. @ 07/29/23 15:41:14.309
• [0.122 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for services  [Conformance]
test/e2e/network/dns.go:137
  STEP: Creating a kubernetes client @ 07/29/23 15:41:14.325
  Jul 29 15:41:14.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename dns @ 07/29/23 15:41:14.327
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:41:14.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:41:14.362
  STEP: Creating a test headless service @ 07/29/23 15:41:14.367
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-5510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-5510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-5510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.49.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.49.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.49.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.49.77_tcp@PTR;sleep 1; done
   @ 07/29/23 15:41:14.407
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service.dns-5510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-5510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-5510.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-5510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-5510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-5510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-5510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-5510.svc.cluster.local;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-5510.svc.cluster.local SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-5510.svc.cluster.local;check="$$(dig +notcp +noall +answer +search 77.49.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.49.77_udp@PTR;check="$$(dig +tcp +noall +answer +search 77.49.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.49.77_tcp@PTR;sleep 1; done
   @ 07/29/23 15:41:14.407
  STEP: creating a pod to probe DNS @ 07/29/23 15:41:14.407
  STEP: submitting the pod to kubernetes @ 07/29/23 15:41:14.408
  STEP: retrieving the pod @ 07/29/23 15:41:16.466
  STEP: looking for the results for each expected name from probers @ 07/29/23 15:41:16.471
  Jul 29 15:41:16.484: INFO: Unable to read wheezy_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:16.491: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:16.496: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:16.501: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:16.528: INFO: Unable to read jessie_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:16.533: INFO: Unable to read jessie_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:16.543: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:16.553: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:16.580: INFO: Lookups using dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674 failed for: [wheezy_udp@dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_udp@dns-test-service.dns-5510.svc.cluster.local jessie_tcp@dns-test-service.dns-5510.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local]

  Jul 29 15:41:21.599: INFO: Unable to read wheezy_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:21.606: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:21.611: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:21.616: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:21.643: INFO: Unable to read jessie_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:21.648: INFO: Unable to read jessie_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:21.654: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:21.659: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:21.682: INFO: Lookups using dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674 failed for: [wheezy_udp@dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_udp@dns-test-service.dns-5510.svc.cluster.local jessie_tcp@dns-test-service.dns-5510.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local]

  Jul 29 15:41:26.603: INFO: Unable to read wheezy_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:26.610: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:26.617: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:26.621: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:26.649: INFO: Unable to read jessie_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:26.656: INFO: Unable to read jessie_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:26.662: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:26.668: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:26.693: INFO: Lookups using dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674 failed for: [wheezy_udp@dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_udp@dns-test-service.dns-5510.svc.cluster.local jessie_tcp@dns-test-service.dns-5510.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local]

  Jul 29 15:41:31.593: INFO: Unable to read wheezy_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:31.601: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:31.610: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:31.615: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:31.643: INFO: Unable to read jessie_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:31.647: INFO: Unable to read jessie_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:31.657: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:31.664: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:31.693: INFO: Lookups using dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674 failed for: [wheezy_udp@dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_udp@dns-test-service.dns-5510.svc.cluster.local jessie_tcp@dns-test-service.dns-5510.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local]

  Jul 29 15:41:36.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:36.602: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:36.612: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:36.621: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:36.659: INFO: Unable to read jessie_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:36.665: INFO: Unable to read jessie_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:36.671: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:36.678: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:36.707: INFO: Lookups using dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674 failed for: [wheezy_udp@dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_udp@dns-test-service.dns-5510.svc.cluster.local jessie_tcp@dns-test-service.dns-5510.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local]

  Jul 29 15:41:41.592: INFO: Unable to read wheezy_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:41.599: INFO: Unable to read wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:41.606: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:41.614: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:41.643: INFO: Unable to read jessie_udp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:41.650: INFO: Unable to read jessie_tcp@dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:41.655: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:41.662: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local from pod dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674: the server could not find the requested resource (get pods dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674)
  Jul 29 15:41:41.692: INFO: Lookups using dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674 failed for: [wheezy_udp@dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@dns-test-service.dns-5510.svc.cluster.local wheezy_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local wheezy_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_udp@dns-test-service.dns-5510.svc.cluster.local jessie_tcp@dns-test-service.dns-5510.svc.cluster.local jessie_udp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local jessie_tcp@_http._tcp.dns-test-service.dns-5510.svc.cluster.local]

  Jul 29 15:41:46.695: INFO: DNS probes using dns-5510/dns-test-a804c2ff-e12d-4ce9-842e-3c437e1f7674 succeeded

  Jul 29 15:41:46.696: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 15:41:46.705
  STEP: deleting the test service @ 07/29/23 15:41:46.769
  STEP: deleting the test headless service @ 07/29/23 15:41:46.827
  STEP: Destroying namespace "dns-5510" for this suite. @ 07/29/23 15:41:46.855
• [32.550 seconds]
------------------------------
[sig-network] Services should provide secure master service  [Conformance]
test/e2e/network/service.go:775
  STEP: Creating a kubernetes client @ 07/29/23 15:41:46.876
  Jul 29 15:41:46.876: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 15:41:46.88
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:41:46.926
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:41:46.931
  Jul 29 15:41:46.943: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3384" for this suite. @ 07/29/23 15:41:46.955
• [0.093 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events should manage the lifecycle of an event [Conformance]
test/e2e/instrumentation/core_events.go:57
  STEP: Creating a kubernetes client @ 07/29/23 15:41:46.973
  Jul 29 15:41:46.973: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename events @ 07/29/23 15:41:46.976
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:41:47.007
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:41:47.013
  STEP: creating a test event @ 07/29/23 15:41:47.021
  STEP: listing all events in all namespaces @ 07/29/23 15:41:47.034
  STEP: patching the test event @ 07/29/23 15:41:47.063
  STEP: fetching the test event @ 07/29/23 15:41:47.073
  STEP: updating the test event @ 07/29/23 15:41:47.079
  STEP: getting the test event @ 07/29/23 15:41:47.099
  STEP: deleting the test event @ 07/29/23 15:41:47.103
  STEP: listing all events in all namespaces @ 07/29/23 15:41:47.113
  Jul 29 15:41:47.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-4761" for this suite. @ 07/29/23 15:41:47.178
• [0.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates basic preemption works [Conformance]
test/e2e/scheduling/preemption.go:130
  STEP: Creating a kubernetes client @ 07/29/23 15:41:47.204
  Jul 29 15:41:47.205: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 15:41:47.207
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:41:47.243
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:41:47.247
  Jul 29 15:41:47.273: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 15:42:47.328: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/29/23 15:42:47.337
  Jul 29 15:42:47.379: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 29 15:42:47.388: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 29 15:42:47.426: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 29 15:42:47.437: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 29 15:42:47.507: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 29 15:42:47.541: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/29/23 15:42:47.541
  STEP: Run a high priority pod that has same requirements as that of lower priority pod @ 07/29/23 15:42:49.607
  Jul 29 15:42:53.685: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-5702" for this suite. @ 07/29/23 15:42:53.783
• [66.590 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Lease lease API should be available [Conformance]
test/e2e/common/node/lease.go:72
  STEP: Creating a kubernetes client @ 07/29/23 15:42:53.795
  Jul 29 15:42:53.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename lease-test @ 07/29/23 15:42:53.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:42:53.85
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:42:53.855
  Jul 29 15:42:53.949: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "lease-test-1518" for this suite. @ 07/29/23 15:42:53.959
• [0.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should not schedule jobs when suspended [Slow] [Conformance]
test/e2e/apps/cronjob.go:97
  STEP: Creating a kubernetes client @ 07/29/23 15:42:53.98
  Jul 29 15:42:53.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 15:42:53.982
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:42:54.014
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:42:54.018
  STEP: Creating a suspended cronjob @ 07/29/23 15:42:54.023
  STEP: Ensuring no jobs are scheduled @ 07/29/23 15:42:54.037
  STEP: Ensuring no job exists by listing jobs explicitly @ 07/29/23 15:47:54.056
  STEP: Removing cronjob @ 07/29/23 15:47:54.065
  Jul 29 15:47:54.076: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-5550" for this suite. @ 07/29/23 15:47:54.084
• [300.131 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a tcp:8080 liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:183
  STEP: Creating a kubernetes client @ 07/29/23 15:47:54.114
  Jul 29 15:47:54.114: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 15:47:54.118
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:47:54.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:47:54.188
  STEP: Creating pod liveness-6a856dfa-b022-4071-a678-13bfc2b197eb in namespace container-probe-4379 @ 07/29/23 15:47:54.194
  Jul 29 15:47:56.234: INFO: Started pod liveness-6a856dfa-b022-4071-a678-13bfc2b197eb in namespace container-probe-4379
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 15:47:56.235
  Jul 29 15:47:56.243: INFO: Initial restart count of pod liveness-6a856dfa-b022-4071-a678-13bfc2b197eb is 0
  Jul 29 15:51:57.488: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 15:51:57.5
  STEP: Destroying namespace "container-probe-4379" for this suite. @ 07/29/23 15:51:57.533
• [243.442 seconds]
------------------------------
SS
------------------------------
[sig-apps] ReplicationController should adopt matching pods on creation [Conformance]
test/e2e/apps/rc.go:94
  STEP: Creating a kubernetes client @ 07/29/23 15:51:57.556
  Jul 29 15:51:57.556: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 15:51:57.561
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:51:57.612
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:51:57.616
  STEP: Given a Pod with a 'name' label pod-adoption is created @ 07/29/23 15:51:57.62
  STEP: When a replication controller with a matching selector is created @ 07/29/23 15:51:59.664
  STEP: Then the orphan pod is adopted @ 07/29/23 15:51:59.675
  Jul 29 15:52:00.689: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-520" for this suite. @ 07/29/23 15:52:00.696
• [3.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve a basic endpoint from pods  [Conformance]
test/e2e/network/service.go:785
  STEP: Creating a kubernetes client @ 07/29/23 15:52:00.715
  Jul 29 15:52:00.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 15:52:00.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:52:00.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:52:00.749
  STEP: creating service endpoint-test2 in namespace services-3282 @ 07/29/23 15:52:00.753
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3282 to expose endpoints map[] @ 07/29/23 15:52:00.772
  Jul 29 15:52:00.779: INFO: Failed go get Endpoints object: endpoints "endpoint-test2" not found
  Jul 29 15:52:01.793: INFO: successfully validated that service endpoint-test2 in namespace services-3282 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-3282 @ 07/29/23 15:52:01.794
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3282 to expose endpoints map[pod1:[80]] @ 07/29/23 15:52:03.837
  Jul 29 15:52:03.863: INFO: successfully validated that service endpoint-test2 in namespace services-3282 exposes endpoints map[pod1:[80]]
  STEP: Checking if the Service forwards traffic to pod1 @ 07/29/23 15:52:03.864
  Jul 29 15:52:03.864: INFO: Creating new exec pod
  Jul 29 15:52:06.893: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3282 exec execpodzd7wk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 15:52:07.220: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 15:52:07.220: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:52:07.220: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3282 exec execpodzd7wk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.175 80'
  Jul 29 15:52:07.488: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.175 80\nConnection to 10.233.25.175 80 port [tcp/http] succeeded!\n"
  Jul 29 15:52:07.488: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Creating pod pod2 in namespace services-3282 @ 07/29/23 15:52:07.488
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3282 to expose endpoints map[pod1:[80] pod2:[80]] @ 07/29/23 15:52:09.524
  Jul 29 15:52:09.553: INFO: successfully validated that service endpoint-test2 in namespace services-3282 exposes endpoints map[pod1:[80] pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod1 and pod2 @ 07/29/23 15:52:09.554
  Jul 29 15:52:10.556: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3282 exec execpodzd7wk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 15:52:10.817: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 15:52:10.817: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:52:10.818: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3282 exec execpodzd7wk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.175 80'
  Jul 29 15:52:11.074: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.175 80\nConnection to 10.233.25.175 80 port [tcp/http] succeeded!\n"
  Jul 29 15:52:11.075: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-3282 @ 07/29/23 15:52:11.075
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3282 to expose endpoints map[pod2:[80]] @ 07/29/23 15:52:11.097
  Jul 29 15:52:12.171: INFO: successfully validated that service endpoint-test2 in namespace services-3282 exposes endpoints map[pod2:[80]]
  STEP: Checking if the Service forwards traffic to pod2 @ 07/29/23 15:52:12.171
  Jul 29 15:52:13.173: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3282 exec execpodzd7wk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 endpoint-test2 80'
  Jul 29 15:52:13.422: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 endpoint-test2 80\nConnection to endpoint-test2 80 port [tcp/http] succeeded!\n"
  Jul 29 15:52:13.422: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 15:52:13.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3282 exec execpodzd7wk -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.25.175 80'
  Jul 29 15:52:13.663: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.25.175 80\nConnection to 10.233.25.175 80 port [tcp/http] succeeded!\n"
  Jul 29 15:52:13.663: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod2 in namespace services-3282 @ 07/29/23 15:52:13.663
  STEP: waiting up to 3m0s for service endpoint-test2 in namespace services-3282 to expose endpoints map[] @ 07/29/23 15:52:13.687
  Jul 29 15:52:14.724: INFO: successfully validated that service endpoint-test2 in namespace services-3282 exposes endpoints map[]
  Jul 29 15:52:14.724: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3282" for this suite. @ 07/29/23 15:52:14.767
• [14.066 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should have Endpoints and EndpointSlices pointing to API Server [Conformance]
test/e2e/network/endpointslice.go:68
  STEP: Creating a kubernetes client @ 07/29/23 15:52:14.784
  Jul 29 15:52:14.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 15:52:14.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:52:14.823
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:52:14.827
  Jul 29 15:52:14.844: INFO: Endpoints addresses: [192.168.121.39 192.168.121.49] , ports: [6443]
  Jul 29 15:52:14.844: INFO: EndpointSlices addresses: [192.168.121.39 192.168.121.49] , ports: [6443]
  Jul 29 15:52:14.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-5132" for this suite. @ 07/29/23 15:52:14.85
• [0.076 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via environment variable [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:45
  STEP: Creating a kubernetes client @ 07/29/23 15:52:14.868
  Jul 29 15:52:14.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 15:52:14.87
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:52:14.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:52:14.908
  STEP: Creating configMap configmap-6647/configmap-test-4beba0c8-972d-4131-86d0-52c1a19a3766 @ 07/29/23 15:52:14.913
  STEP: Creating a pod to test consume configMaps @ 07/29/23 15:52:14.919
  STEP: Saw pod success @ 07/29/23 15:52:18.958
  Jul 29 15:52:18.964: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-197f70f2-b6d9-418a-b62f-710c8ee214b1 container env-test: <nil>
  STEP: delete the pod @ 07/29/23 15:52:18.994
  Jul 29 15:52:19.017: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-6647" for this suite. @ 07/29/23 15:52:19.024
• [4.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should test the lifecycle of an Endpoint [Conformance]
test/e2e/network/service.go:3138
  STEP: Creating a kubernetes client @ 07/29/23 15:52:19.035
  Jul 29 15:52:19.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 15:52:19.037
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:52:19.066
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:52:19.069
  STEP: creating an Endpoint @ 07/29/23 15:52:19.079
  STEP: waiting for available Endpoint @ 07/29/23 15:52:19.091
  STEP: listing all Endpoints @ 07/29/23 15:52:19.093
  STEP: updating the Endpoint @ 07/29/23 15:52:19.098
  STEP: fetching the Endpoint @ 07/29/23 15:52:19.109
  STEP: patching the Endpoint @ 07/29/23 15:52:19.113
  STEP: fetching the Endpoint @ 07/29/23 15:52:19.127
  STEP: deleting the Endpoint by Collection @ 07/29/23 15:52:19.132
  STEP: waiting for Endpoint deletion @ 07/29/23 15:52:19.143
  STEP: fetching the Endpoint @ 07/29/23 15:52:19.145
  Jul 29 15:52:19.150: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-4996" for this suite. @ 07/29/23 15:52:19.157
• [0.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl expose should create services for rc  [Conformance]
test/e2e/kubectl/kubectl.go:1480
  STEP: Creating a kubernetes client @ 07/29/23 15:52:19.173
  Jul 29 15:52:19.173: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 15:52:19.175
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:52:19.217
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:52:19.224
  STEP: creating Agnhost RC @ 07/29/23 15:52:19.229
  Jul 29 15:52:19.229: INFO: namespace kubectl-4338
  Jul 29 15:52:19.230: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-4338 create -f -'
  Jul 29 15:52:20.014: INFO: stderr: ""
  Jul 29 15:52:20.014: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 15:52:20.014
  Jul 29 15:52:21.022: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 15:52:21.022: INFO: Found 0 / 1
  Jul 29 15:52:22.023: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 15:52:22.023: INFO: Found 1 / 1
  Jul 29 15:52:22.023: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 29 15:52:22.029: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 15:52:22.029: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 15:52:22.029: INFO: wait on agnhost-primary startup in kubectl-4338 
  Jul 29 15:52:22.029: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-4338 logs agnhost-primary-sdf59 agnhost-primary'
  Jul 29 15:52:22.235: INFO: stderr: ""
  Jul 29 15:52:22.235: INFO: stdout: "Paused\n"
  STEP: exposing RC @ 07/29/23 15:52:22.235
  Jul 29 15:52:22.236: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-4338 expose rc agnhost-primary --name=rm2 --port=1234 --target-port=6379'
  Jul 29 15:52:22.406: INFO: stderr: ""
  Jul 29 15:52:22.406: INFO: stdout: "service/rm2 exposed\n"
  Jul 29 15:52:22.414: INFO: Service rm2 in namespace kubectl-4338 found.
  STEP: exposing service @ 07/29/23 15:52:24.425
  Jul 29 15:52:24.426: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-4338 expose service rm2 --name=rm3 --port=2345 --target-port=6379'
  Jul 29 15:52:24.591: INFO: stderr: ""
  Jul 29 15:52:24.591: INFO: stdout: "service/rm3 exposed\n"
  Jul 29 15:52:24.605: INFO: Service rm3 in namespace kubectl-4338 found.
  Jul 29 15:52:26.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-4338" for this suite. @ 07/29/23 15:52:26.628
• [7.465 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support proxy with --port 0  [Conformance]
test/e2e/kubectl/kubectl.go:1775
  STEP: Creating a kubernetes client @ 07/29/23 15:52:26.651
  Jul 29 15:52:26.651: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 15:52:26.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:52:26.691
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:52:26.695
  STEP: starting the proxy server @ 07/29/23 15:52:26.701
  Jul 29 15:52:26.701: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1105 proxy -p 0 --disable-filter'
  STEP: curling proxy /api/ output @ 07/29/23 15:52:26.8
  Jul 29 15:52:26.822: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1105" for this suite. @ 07/29/23 15:52:26.83
• [0.193 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Aggregator Should be able to support the 1.17 Sample API Server using the current Aggregator [Conformance]
test/e2e/apimachinery/aggregator.go:92
  STEP: Creating a kubernetes client @ 07/29/23 15:52:26.851
  Jul 29 15:52:26.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename aggregator @ 07/29/23 15:52:26.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:52:26.886
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:52:26.892
  Jul 29 15:52:26.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Registering the sample API server. @ 07/29/23 15:52:26.9
  Jul 29 15:52:27.944: INFO: Found ClusterRoles; assuming RBAC is enabled.
  Jul 29 15:52:27.994: INFO: new replicaset for deployment "sample-apiserver-deployment" is yet to be created
  Jul 29 15:52:30.074: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:32.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:34.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:36.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:38.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:40.085: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:42.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:44.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:46.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:48.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:50.080: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:52.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:54.101: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:56.086: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:52:58.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:00.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:02.081: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:04.082: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:06.088: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:08.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:10.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:12.087: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:14.083: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:16.087: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:18.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:20.084: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 15, 52, 28, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-apiserver-deployment-6dfd6dfd5b\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 15:53:22.225: INFO: Waited 128.998554ms for the sample-apiserver to be ready to handle requests.
  STEP: Read Status for v1alpha1.wardle.example.com @ 07/29/23 15:53:22.307
  STEP: kubectl patch apiservice v1alpha1.wardle.example.com -p '{"spec":{"versionPriority": 400}}' @ 07/29/23 15:53:22.313
  STEP: List APIServices @ 07/29/23 15:53:22.337
  Jul 29 15:53:22.354: INFO: Found v1alpha1.wardle.example.com in APIServiceList
  STEP: Adding a label to the APIService @ 07/29/23 15:53:22.354
  Jul 29 15:53:22.377: INFO: APIService labels: map[e2e-apiservice:patched]
  STEP: Updating APIService Status @ 07/29/23 15:53:22.377
  Jul 29 15:53:22.402: INFO: updatedStatus.Conditions: []v1.APIServiceCondition{v1.APIServiceCondition{Type:"Available", Status:"True", LastTransitionTime:time.Date(2023, time.July, 29, 15, 53, 22, 0, time.Local), Reason:"Passed", Message:"all checks passed"}, v1.APIServiceCondition{Type:"StatusUpdated", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: Confirm that v1alpha1.wardle.example.com /status was updated @ 07/29/23 15:53:22.402
  Jul 29 15:53:22.409: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {Available True 2023-07-29 15:53:22 +0000 UTC Passed all checks passed}
  Jul 29 15:53:22.409: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[e2e-apiservice:patched] & Condition: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 15:53:22.409: INFO: Found updated status condition for v1alpha1.wardle.example.com
  STEP: Replace APIService v1alpha1.wardle.example.com @ 07/29/23 15:53:22.409
  Jul 29 15:53:22.426: INFO: Found updated apiService label for "v1alpha1.wardle.example.com"
  STEP: Delete APIService "dynamic-flunder-1422069486" @ 07/29/23 15:53:22.426
  STEP: Recreating test-flunder before removing endpoint via deleteCollection @ 07/29/23 15:53:22.449
  STEP: Read v1alpha1.wardle.example.com /status before patching it @ 07/29/23 15:53:22.463
  STEP: Patch APIService Status @ 07/29/23 15:53:22.47
  STEP: Confirm that v1alpha1.wardle.example.com /status was patched @ 07/29/23 15:53:22.494
  Jul 29 15:53:22.502: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {Available True 2023-07-29 15:53:22 +0000 UTC Passed all checks passed}
  Jul 29 15:53:22.502: INFO: Observed APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusUpdated True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 15:53:22.502: INFO: Found APIService v1alpha1.wardle.example.com with Labels: map[v1alpha1.wardle.example.com:updated] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC E2E Set by e2e test}
  Jul 29 15:53:22.502: INFO: Found patched status condition for v1alpha1.wardle.example.com
  STEP: APIService deleteCollection with labelSelector: "e2e-apiservice=patched" @ 07/29/23 15:53:22.502
  STEP: Confirm that the generated APIService has been deleted @ 07/29/23 15:53:22.516
  Jul 29 15:53:22.516: INFO: Requesting list of APIServices to confirm quantity
  Jul 29 15:53:22.529: INFO: Found 0 APIService with label "e2e-apiservice=patched"
  Jul 29 15:53:22.529: INFO: APIService v1alpha1.wardle.example.com has been deleted.
  Jul 29 15:53:22.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "aggregator-791" for this suite. @ 07/29/23 15:53:22.713
• [55.873 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should create a PodDisruptionBudget [Conformance]
test/e2e/apps/disruption.go:108
  STEP: Creating a kubernetes client @ 07/29/23 15:53:22.737
  Jul 29 15:53:22.738: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename disruption @ 07/29/23 15:53:22.742
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:53:22.775
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:53:22.787
  STEP: creating the pdb @ 07/29/23 15:53:22.794
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:53:22.802
  STEP: updating the pdb @ 07/29/23 15:53:24.814
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:53:24.831
  STEP: patching the pdb @ 07/29/23 15:53:26.846
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:53:26.865
  STEP: Waiting for the pdb to be deleted @ 07/29/23 15:53:28.898
  Jul 29 15:53:28.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-5679" for this suite. @ 07/29/23 15:53:28.915
• [6.193 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] EndpointSlice should support creating EndpointSlice API operations [Conformance]
test/e2e/network/endpointslice.go:355
  STEP: Creating a kubernetes client @ 07/29/23 15:53:28.934
  Jul 29 15:53:28.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 15:53:28.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:53:28.975
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:53:28.983
  STEP: getting /apis @ 07/29/23 15:53:28.99
  STEP: getting /apis/discovery.k8s.io @ 07/29/23 15:53:29
  STEP: getting /apis/discovery.k8s.iov1 @ 07/29/23 15:53:29.002
  STEP: creating @ 07/29/23 15:53:29.004
  STEP: getting @ 07/29/23 15:53:29.036
  STEP: listing @ 07/29/23 15:53:29.041
  STEP: watching @ 07/29/23 15:53:29.048
  Jul 29 15:53:29.048: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 15:53:29.05
  STEP: cluster-wide watching @ 07/29/23 15:53:29.056
  Jul 29 15:53:29.057: INFO: starting watch
  STEP: patching @ 07/29/23 15:53:29.058
  STEP: updating @ 07/29/23 15:53:29.068
  Jul 29 15:53:29.085: INFO: waiting for watch events with expected annotations
  Jul 29 15:53:29.085: INFO: saw patched and updated annotations
  STEP: deleting @ 07/29/23 15:53:29.085
  STEP: deleting a collection @ 07/29/23 15:53:29.106
  Jul 29 15:53:29.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-4923" for this suite. @ 07/29/23 15:53:29.138
• [0.215 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicaSet should adopt matching pods on creation and release no longer matching pods [Conformance]
test/e2e/apps/replica_set.go:131
  STEP: Creating a kubernetes client @ 07/29/23 15:53:29.151
  Jul 29 15:53:29.151: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 15:53:29.154
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:53:29.183
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:53:29.188
  STEP: Given a Pod with a 'name' label pod-adoption-release is created @ 07/29/23 15:53:29.193
  STEP: When a replicaset with a matching selector is created @ 07/29/23 15:53:31.233
  STEP: Then the orphan pod is adopted @ 07/29/23 15:53:31.244
  STEP: When the matched label of one of its pods change @ 07/29/23 15:53:32.265
  Jul 29 15:53:32.275: INFO: Pod name pod-adoption-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/29/23 15:53:32.296
  Jul 29 15:53:33.322: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1028" for this suite. @ 07/29/23 15:53:33.33
• [4.189 seconds]
------------------------------
S
------------------------------
[sig-network] Services should find a service from listing all namespaces [Conformance]
test/e2e/network/service.go:3113
  STEP: Creating a kubernetes client @ 07/29/23 15:53:33.341
  Jul 29 15:53:33.341: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 15:53:33.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:53:33.373
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:53:33.379
  STEP: fetching services @ 07/29/23 15:53:33.386
  Jul 29 15:53:33.392: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-8119" for this suite. @ 07/29/23 15:53:33.401
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:215
  STEP: Creating a kubernetes client @ 07/29/23 15:53:33.421
  Jul 29 15:53:33.421: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:53:33.424
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:53:33.457
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:53:33.463
  STEP: Creating secret with name s-test-opt-del-bedba10c-dfe9-493a-ac91-b56b074c045d @ 07/29/23 15:53:33.477
  STEP: Creating secret with name s-test-opt-upd-56482c0e-d089-4268-9bd0-c8b80123beec @ 07/29/23 15:53:33.494
  STEP: Creating the pod @ 07/29/23 15:53:33.502
  STEP: Deleting secret s-test-opt-del-bedba10c-dfe9-493a-ac91-b56b074c045d @ 07/29/23 15:53:37.608
  STEP: Updating secret s-test-opt-upd-56482c0e-d089-4268-9bd0-c8b80123beec @ 07/29/23 15:53:37.622
  STEP: Creating secret with name s-test-opt-create-2e57bdde-6f22-4b2c-a692-2895760e8c94 @ 07/29/23 15:53:37.63
  STEP: waiting to observe update in volume @ 07/29/23 15:53:37.637
  Jul 29 15:54:52.461: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7496" for this suite. @ 07/29/23 15:54:52.475
• [79.069 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:46
  STEP: Creating a kubernetes client @ 07/29/23 15:54:52.491
  Jul 29 15:54:52.491: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 15:54:52.494
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:54:52.544
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:54:52.549
  STEP: Creating projection with secret that has name projected-secret-test-2600f7e6-296a-4203-97d4-d3d31f369a15 @ 07/29/23 15:54:52.555
  STEP: Creating a pod to test consume secrets @ 07/29/23 15:54:52.565
  STEP: Saw pod success @ 07/29/23 15:54:56.617
  Jul 29 15:54:56.623: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-secrets-d39cd34c-68fe-4631-a885-15590118dfe5 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 15:54:56.637
  Jul 29 15:54:56.660: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1338" for this suite. @ 07/29/23 15:54:56.668
• [4.189 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:177
  STEP: Creating a kubernetes client @ 07/29/23 15:54:56.682
  Jul 29 15:54:56.682: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 15:54:56.684
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:54:56.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:54:56.718
  STEP: Creating a pod to test emptydir 0666 on node default medium @ 07/29/23 15:54:56.722
  STEP: Saw pod success @ 07/29/23 15:55:00.764
  Jul 29 15:55:00.769: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-1af04dc2-3c6f-4289-8840-d2863e4f5a39 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 15:55:00.783
  Jul 29 15:55:00.810: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-3802" for this suite. @ 07/29/23 15:55:00.818
• [4.146 seconds]
------------------------------
SS
------------------------------
[sig-node] RuntimeClass should reject a Pod requesting a deleted RuntimeClass [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:156
  STEP: Creating a kubernetes client @ 07/29/23 15:55:00.829
  Jul 29 15:55:00.829: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 15:55:00.832
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:00.861
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:00.865
  STEP: Deleting RuntimeClass runtimeclass-8484-delete-me @ 07/29/23 15:55:00.876
  STEP: Waiting for the RuntimeClass to disappear @ 07/29/23 15:55:00.891
  Jul 29 15:55:00.911: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-8484" for this suite. @ 07/29/23 15:55:00.921
• [0.108 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: udp [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:124
  STEP: Creating a kubernetes client @ 07/29/23 15:55:00.94
  Jul 29 15:55:00.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 15:55:00.942
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:00.973
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:00.977
  STEP: Performing setup for networking test in namespace pod-network-test-5956 @ 07/29/23 15:55:00.982
  STEP: creating a selector @ 07/29/23 15:55:00.982
  STEP: Creating the service pods in kubernetes @ 07/29/23 15:55:00.983
  Jul 29 15:55:00.983: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 15:55:25.188
  Jul 29 15:55:27.246: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 15:55:27.246: INFO: Going to poll 10.233.64.164 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 15:55:27.253: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.64.164 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5956 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 15:55:27.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 15:55:27.256: INFO: ExecWithOptions: Clientset creation
  Jul 29 15:55:27.257: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5956/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.64.164+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 15:55:28.398: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 29 15:55:28.399: INFO: Going to poll 10.233.65.51 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 15:55:28.405: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.65.51 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5956 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 15:55:28.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 15:55:28.411: INFO: ExecWithOptions: Clientset creation
  Jul 29 15:55:28.412: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5956/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.65.51+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 15:55:29.519: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 29 15:55:29.519: INFO: Going to poll 10.233.66.190 on port 8081 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 15:55:29.528: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostName | nc -w 1 -u 10.233.66.190 8081 | grep -v '^\s*$'] Namespace:pod-network-test-5956 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 15:55:29.528: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 15:55:29.529: INFO: ExecWithOptions: Clientset creation
  Jul 29 15:55:29.530: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-5956/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostName+%7C+nc+-w+1+-u+10.233.66.190+8081+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 15:55:30.654: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 29 15:55:30.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-5956" for this suite. @ 07/29/23 15:55:30.669
• [29.743 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should apply changes to a resourcequota status [Conformance]
test/e2e/apimachinery/resource_quota.go:1013
  STEP: Creating a kubernetes client @ 07/29/23 15:55:30.685
  Jul 29 15:55:30.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 15:55:30.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:55:30.72
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:55:30.726
  STEP: Creating resourceQuota "e2e-rq-status-s9rqq" @ 07/29/23 15:55:30.742
  Jul 29 15:55:30.763: INFO: Resource quota "e2e-rq-status-s9rqq" reports spec: hard cpu limit of 500m
  Jul 29 15:55:30.763: INFO: Resource quota "e2e-rq-status-s9rqq" reports spec: hard memory limit of 500Mi
  STEP: Updating resourceQuota "e2e-rq-status-s9rqq" /status @ 07/29/23 15:55:30.763
  STEP: Confirm /status for "e2e-rq-status-s9rqq" resourceQuota via watch @ 07/29/23 15:55:30.779
  Jul 29 15:55:30.781: INFO: observed resourceQuota "e2e-rq-status-s9rqq" in namespace "resourcequota-5135" with hard status: v1.ResourceList(nil)
  Jul 29 15:55:30.782: INFO: Found resourceQuota "e2e-rq-status-s9rqq" in namespace "resourcequota-5135" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 29 15:55:30.782: INFO: ResourceQuota "e2e-rq-status-s9rqq" /status was updated
  STEP: Patching hard spec values for cpu & memory @ 07/29/23 15:55:30.789
  Jul 29 15:55:30.800: INFO: Resource quota "e2e-rq-status-s9rqq" reports spec: hard cpu limit of 1
  Jul 29 15:55:30.801: INFO: Resource quota "e2e-rq-status-s9rqq" reports spec: hard memory limit of 1Gi
  STEP: Patching "e2e-rq-status-s9rqq" /status @ 07/29/23 15:55:30.801
  STEP: Confirm /status for "e2e-rq-status-s9rqq" resourceQuota via watch @ 07/29/23 15:55:30.809
  Jul 29 15:55:30.811: INFO: observed resourceQuota "e2e-rq-status-s9rqq" in namespace "resourcequota-5135" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:500, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500m", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:524288000, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"500Mi", Format:"BinarySI"}}
  Jul 29 15:55:30.811: INFO: Found resourceQuota "e2e-rq-status-s9rqq" in namespace "resourcequota-5135" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul 29 15:55:30.811: INFO: ResourceQuota "e2e-rq-status-s9rqq" /status was patched
  STEP: Get "e2e-rq-status-s9rqq" /status @ 07/29/23 15:55:30.812
  Jul 29 15:55:30.823: INFO: Resourcequota "e2e-rq-status-s9rqq" reports status: hard cpu of 1
  Jul 29 15:55:30.823: INFO: Resourcequota "e2e-rq-status-s9rqq" reports status: hard memory of 1Gi
  STEP: Repatching "e2e-rq-status-s9rqq" /status before checking Spec is unchanged @ 07/29/23 15:55:30.828
  Jul 29 15:55:30.849: INFO: Resourcequota "e2e-rq-status-s9rqq" reports status: hard cpu of 2
  Jul 29 15:55:30.849: INFO: Resourcequota "e2e-rq-status-s9rqq" reports status: hard memory of 2Gi
  Jul 29 15:55:30.853: INFO: observed resourceQuota "e2e-rq-status-s9rqq" in namespace "resourcequota-5135" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:1, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:1073741824, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"1Gi", Format:"BinarySI"}}
  Jul 29 15:55:30.853: INFO: Found resourceQuota "e2e-rq-status-s9rqq" in namespace "resourcequota-5135" with hard status: v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:2, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2", Format:"DecimalSI"}, "memory":resource.Quantity{i:resource.int64Amount{value:2147483648, scale:0}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"2Gi", Format:"BinarySI"}}
  Jul 29 15:56:30.867: INFO: ResourceQuota "e2e-rq-status-s9rqq" Spec was unchanged and /status reset
  Jul 29 15:56:30.868: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-5135" for this suite. @ 07/29/23 15:56:30.879
• [60.208 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply a finalizer to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:398
  STEP: Creating a kubernetes client @ 07/29/23 15:56:30.895
  Jul 29 15:56:30.896: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 15:56:30.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:56:30.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:56:30.937
  STEP: Creating namespace "e2e-ns-88h4c" @ 07/29/23 15:56:30.942
  Jul 29 15:56:30.976: INFO: Namespace "e2e-ns-88h4c-7783" has []v1.FinalizerName{"kubernetes"}
  STEP: Adding e2e finalizer to namespace "e2e-ns-88h4c-7783" @ 07/29/23 15:56:30.976
  Jul 29 15:56:30.989: INFO: Namespace "e2e-ns-88h4c-7783" has []v1.FinalizerName{"kubernetes", "e2e.example.com/fakeFinalizer"}
  STEP: Removing e2e finalizer from namespace "e2e-ns-88h4c-7783" @ 07/29/23 15:56:30.989
  Jul 29 15:56:31.006: INFO: Namespace "e2e-ns-88h4c-7783" has []v1.FinalizerName{"kubernetes"}
  Jul 29 15:56:31.007: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-4281" for this suite. @ 07/29/23 15:56:31.015
  STEP: Destroying namespace "e2e-ns-88h4c-7783" for this suite. @ 07/29/23 15:56:31.028
• [0.142 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should delete a collection of pods [Conformance]
test/e2e/common/node/pods.go:846
  STEP: Creating a kubernetes client @ 07/29/23 15:56:31.044
  Jul 29 15:56:31.044: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 15:56:31.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:56:31.069
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:56:31.073
  STEP: Create set of pods @ 07/29/23 15:56:31.078
  Jul 29 15:56:31.094: INFO: created test-pod-1
  Jul 29 15:56:31.109: INFO: created test-pod-2
  Jul 29 15:56:31.159: INFO: created test-pod-3
  STEP: waiting for all 3 pods to be running @ 07/29/23 15:56:31.159
  STEP: waiting for all pods to be deleted @ 07/29/23 15:56:33.266
  Jul 29 15:56:33.273: INFO: Pod quantity 3 is different from expected quantity 0
  Jul 29 15:56:34.286: INFO: Pod quantity 3 is different from expected quantity 0
  Jul 29 15:56:35.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-162" for this suite. @ 07/29/23 15:56:35.297
• [4.266 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should list, patch and delete a collection of StatefulSets [Conformance]
test/e2e/apps/statefulset.go:912
  STEP: Creating a kubernetes client @ 07/29/23 15:56:35.324
  Jul 29 15:56:35.324: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 15:56:35.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:56:35.363
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:56:35.367
  STEP: Creating service test in namespace statefulset-3520 @ 07/29/23 15:56:35.372
  Jul 29 15:56:35.407: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 15:56:45.419: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: patching the StatefulSet @ 07/29/23 15:56:45.432
  W0729 15:56:45.455518      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 15:56:45.475: INFO: Found 1 stateful pods, waiting for 2
  Jul 29 15:56:55.487: INFO: Waiting for pod test-ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 15:56:55.488: INFO: Waiting for pod test-ss-1 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Listing all StatefulSets @ 07/29/23 15:56:55.498
  STEP: Delete all of the StatefulSets @ 07/29/23 15:56:55.505
  STEP: Verify that StatefulSets have been deleted @ 07/29/23 15:56:55.531
  Jul 29 15:56:55.538: INFO: Deleting all statefulset in ns statefulset-3520
  Jul 29 15:56:55.590: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3520" for this suite. @ 07/29/23 15:56:55.604
• [20.344 seconds]
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod with mountPath of existing file [Conformance]
test/e2e/storage/subpath.go:80
  STEP: Creating a kubernetes client @ 07/29/23 15:56:55.67
  Jul 29 15:56:55.670: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename subpath @ 07/29/23 15:56:55.674
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:56:55.712
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:56:55.716
  STEP: Setting up data @ 07/29/23 15:56:55.721
  STEP: Creating pod pod-subpath-test-configmap-dmqb @ 07/29/23 15:56:55.737
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 15:56:55.738
  STEP: Saw pod success @ 07/29/23 15:57:19.913
  Jul 29 15:57:19.920: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-subpath-test-configmap-dmqb container test-container-subpath-configmap-dmqb: <nil>
  STEP: delete the pod @ 07/29/23 15:57:19.953
  STEP: Deleting pod pod-subpath-test-configmap-dmqb @ 07/29/23 15:57:19.973
  Jul 29 15:57:19.974: INFO: Deleting pod "pod-subpath-test-configmap-dmqb" in namespace "subpath-8845"
  Jul 29 15:57:19.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-8845" for this suite. @ 07/29/23 15:57:19.989
• [24.328 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should block an eviction until the PDB is updated to allow it [Conformance]
test/e2e/apps/disruption.go:349
  STEP: Creating a kubernetes client @ 07/29/23 15:57:20.003
  Jul 29 15:57:20.003: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename disruption @ 07/29/23 15:57:20.006
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:57:20.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:57:20.036
  STEP: Creating a pdb that targets all three pods in a test replica set @ 07/29/23 15:57:20.039
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:57:20.046
  STEP: First trying to evict a pod which shouldn't be evictable @ 07/29/23 15:57:22.066
  STEP: Waiting for all pods to be running @ 07/29/23 15:57:22.066
  Jul 29 15:57:22.076: INFO: pods: 0 < 3
  STEP: locating a running pod @ 07/29/23 15:57:24.089
  STEP: Updating the pdb to allow a pod to be evicted @ 07/29/23 15:57:24.111
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:57:24.128
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/29/23 15:57:26.146
  STEP: Waiting for all pods to be running @ 07/29/23 15:57:26.146
  STEP: Waiting for the pdb to observed all healthy pods @ 07/29/23 15:57:26.163
  STEP: Patching the pdb to disallow a pod to be evicted @ 07/29/23 15:57:26.211
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:57:26.228
  STEP: Waiting for all pods to be running @ 07/29/23 15:57:28.287
  STEP: locating a running pod @ 07/29/23 15:57:28.293
  STEP: Deleting the pdb to allow a pod to be evicted @ 07/29/23 15:57:28.315
  STEP: Waiting for the pdb to be deleted @ 07/29/23 15:57:28.334
  STEP: Trying to evict the same pod we tried earlier which should now be evictable @ 07/29/23 15:57:28.339
  STEP: Waiting for all pods to be running @ 07/29/23 15:57:28.34
  Jul 29 15:57:28.371: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2781" for this suite. @ 07/29/23 15:57:28.381
• [8.395 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Probing container with readiness probe that fails should never be ready and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:107
  STEP: Creating a kubernetes client @ 07/29/23 15:57:28.401
  Jul 29 15:57:28.401: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 15:57:28.405
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:57:28.468
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:57:28.473
  Jul 29 15:58:28.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-1055" for this suite. @ 07/29/23 15:58:28.519
• [60.135 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController Listing PodDisruptionBudgets for all namespaces should list and delete a collection of PodDisruptionBudgets [Conformance]
test/e2e/apps/disruption.go:87
  STEP: Creating a kubernetes client @ 07/29/23 15:58:28.54
  Jul 29 15:58:28.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename disruption @ 07/29/23 15:58:28.543
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:28.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:28.584
  STEP: Creating a kubernetes client @ 07/29/23 15:58:28.589
  Jul 29 15:58:28.589: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename disruption-2 @ 07/29/23 15:58:28.592
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:28.628
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:28.634
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:58:28.646
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:58:30.671
  STEP: Waiting for the pdb to be processed @ 07/29/23 15:58:32.707
  STEP: listing a collection of PDBs across all namespaces @ 07/29/23 15:58:34.72
  STEP: listing a collection of PDBs in namespace disruption-2835 @ 07/29/23 15:58:34.727
  STEP: deleting a collection of PDBs @ 07/29/23 15:58:34.732
  STEP: Waiting for the PDB collection to be deleted @ 07/29/23 15:58:34.75
  Jul 29 15:58:34.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 15:58:34.763: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-2-6593" for this suite. @ 07/29/23 15:58:34.771
  STEP: Destroying namespace "disruption-2835" for this suite. @ 07/29/23 15:58:34.786
• [6.258 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIStorageCapacity  should support CSIStorageCapacities API operations [Conformance]
test/e2e/storage/csistoragecapacity.go:49
  STEP: Creating a kubernetes client @ 07/29/23 15:58:34.806
  Jul 29 15:58:34.807: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename csistoragecapacity @ 07/29/23 15:58:34.809
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:34.837
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:34.844
  STEP: getting /apis @ 07/29/23 15:58:34.85
  STEP: getting /apis/storage.k8s.io @ 07/29/23 15:58:34.86
  STEP: getting /apis/storage.k8s.io/v1 @ 07/29/23 15:58:34.863
  STEP: creating @ 07/29/23 15:58:34.865
  STEP: watching @ 07/29/23 15:58:34.893
  Jul 29 15:58:34.893: INFO: starting watch
  STEP: getting @ 07/29/23 15:58:34.907
  STEP: listing in namespace @ 07/29/23 15:58:34.912
  STEP: listing across namespaces @ 07/29/23 15:58:34.917
  STEP: patching @ 07/29/23 15:58:34.922
  STEP: updating @ 07/29/23 15:58:34.931
  Jul 29 15:58:34.939: INFO: waiting for watch events with expected annotations in namespace
  Jul 29 15:58:34.940: INFO: waiting for watch events with expected annotations across namespace
  STEP: deleting @ 07/29/23 15:58:34.94
  STEP: deleting a collection @ 07/29/23 15:58:34.966
  Jul 29 15:58:34.991: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csistoragecapacity-5934" for this suite. @ 07/29/23 15:58:35
• [0.206 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] validates lower priority pod preemption by critical pod [Conformance]
test/e2e/scheduling/preemption.go:224
  STEP: Creating a kubernetes client @ 07/29/23 15:58:35.014
  Jul 29 15:58:35.014: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 15:58:35.017
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:58:35.047
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:58:35.051
  Jul 29 15:58:35.078: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 15:59:35.139: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Create pods that use 4/5 of node resources. @ 07/29/23 15:59:35.144
  Jul 29 15:59:35.195: INFO: Created pod: pod0-0-sched-preemption-low-priority
  Jul 29 15:59:35.205: INFO: Created pod: pod0-1-sched-preemption-medium-priority
  Jul 29 15:59:35.306: INFO: Created pod: pod1-0-sched-preemption-medium-priority
  Jul 29 15:59:35.324: INFO: Created pod: pod1-1-sched-preemption-medium-priority
  Jul 29 15:59:35.373: INFO: Created pod: pod2-0-sched-preemption-medium-priority
  Jul 29 15:59:35.390: INFO: Created pod: pod2-1-sched-preemption-medium-priority
  STEP: Wait for pods to be scheduled. @ 07/29/23 15:59:35.39
  STEP: Run a critical pod that use same resources as that of a lower priority pod @ 07/29/23 15:59:39.469
  Jul 29 15:59:43.627: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-3478" for this suite. @ 07/29/23 15:59:43.758
• [68.765 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PreemptionExecutionPath runs ReplicaSets to verify preemption running path [Conformance]
test/e2e/scheduling/preemption.go:624
  STEP: Creating a kubernetes client @ 07/29/23 15:59:43.782
  Jul 29 15:59:43.783: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 15:59:43.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 15:59:43.847
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 15:59:43.855
  Jul 29 15:59:43.894: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 16:00:43.954: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/29/23 16:00:43.962
  Jul 29 16:00:43.962: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/29/23 16:00:43.963
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:00:43.996
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:00:44.001
  STEP: Finding an available node @ 07/29/23 16:00:44.006
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 16:00:44.006
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 16:00:46.044
  Jul 29 16:00:46.066: INFO: found a healthy node: ci9axai7aiv7-3
  Jul 29 16:00:52.232: INFO: pods created so far: [1 1 1]
  Jul 29 16:00:52.232: INFO: length of pods created so far: 3
  Jul 29 16:00:54.254: INFO: pods created so far: [2 2 1]
  Jul 29 16:01:01.257: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:01:01.316: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-4340" for this suite. @ 07/29/23 16:01:01.406
  STEP: Destroying namespace "sched-preemption-636" for this suite. @ 07/29/23 16:01:01.417
• [77.649 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be able to mount in a volume regardless of a different secret existing with same name in different namespace [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:99
  STEP: Creating a kubernetes client @ 07/29/23 16:01:01.436
  Jul 29 16:01:01.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:01:01.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:01.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:01.501
  STEP: Creating secret with name secret-test-f51f70aa-c3ec-4743-9daf-d4bbbdcb9465 @ 07/29/23 16:01:01.568
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:01:01.579
  STEP: Saw pod success @ 07/29/23 16:01:05.624
  Jul 29 16:01:05.637: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-secrets-4de86b03-80b8-4010-9cfc-731d57940d3c container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:01:05.666
  Jul 29 16:01:05.692: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2575" for this suite. @ 07/29/23 16:01:05.7
  STEP: Destroying namespace "secret-namespace-5207" for this suite. @ 07/29/23 16:01:05.712
• [4.288 seconds]
------------------------------
S
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass without PodOverhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:104
  STEP: Creating a kubernetes client @ 07/29/23 16:01:05.727
  Jul 29 16:01:05.727: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 16:01:05.731
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:05.76
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:05.765
  Jul 29 16:01:07.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-2209" for this suite. @ 07/29/23 16:01:07.845
• [2.133 seconds]
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] should include custom resource definition resources in discovery documents [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:198
  STEP: Creating a kubernetes client @ 07/29/23 16:01:07.86
  Jul 29 16:01:07.860: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 16:01:07.865
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:07.907
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:07.911
  STEP: fetching the /apis discovery document @ 07/29/23 16:01:07.915
  STEP: finding the apiextensions.k8s.io API group in the /apis discovery document @ 07/29/23 16:01:07.918
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis discovery document @ 07/29/23 16:01:07.918
  STEP: fetching the /apis/apiextensions.k8s.io discovery document @ 07/29/23 16:01:07.919
  STEP: finding the apiextensions.k8s.io/v1 API group/version in the /apis/apiextensions.k8s.io discovery document @ 07/29/23 16:01:07.921
  STEP: fetching the /apis/apiextensions.k8s.io/v1 discovery document @ 07/29/23 16:01:07.921
  STEP: finding customresourcedefinitions resources in the /apis/apiextensions.k8s.io/v1 discovery document @ 07/29/23 16:01:07.923
  Jul 29 16:01:07.925: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-868" for this suite. @ 07/29/23 16:01:07.933
• [0.086 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:89
  STEP: Creating a kubernetes client @ 07/29/23 16:01:07.947
  Jul 29 16:01:07.947: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:01:07.952
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:07.99
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:07.993
  STEP: Creating secret with name secret-test-map-a8bc92d9-ab23-4306-a496-0df717decbf4 @ 07/29/23 16:01:07.996
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:01:08.002
  STEP: Saw pod success @ 07/29/23 16:01:12.038
  Jul 29 16:01:12.044: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-secrets-ceeb2c8a-8c30-4bda-bcdb-b5f1e3bbf07e container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:01:12.056
  Jul 29 16:01:12.080: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-458" for this suite. @ 07/29/23 16:01:12.091
• [4.155 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] ConfigMap binary data should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:175
  STEP: Creating a kubernetes client @ 07/29/23 16:01:12.105
  Jul 29 16:01:12.106: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:01:12.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:12.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:12.143
  STEP: Creating configMap with name configmap-test-upd-301af0a3-5afd-441f-b100-54454541f97a @ 07/29/23 16:01:12.154
  STEP: Creating the pod @ 07/29/23 16:01:12.162
  STEP: Waiting for pod with text data @ 07/29/23 16:01:14.19
  STEP: Waiting for pod with binary data @ 07/29/23 16:01:14.202
  Jul 29 16:01:14.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5612" for this suite. @ 07/29/23 16:01:14.227
• [2.131 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replication controller. [Conformance]
test/e2e/apimachinery/resource_quota.go:395
  STEP: Creating a kubernetes client @ 07/29/23 16:01:14.24
  Jul 29 16:01:14.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:01:14.243
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:14.276
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:14.281
  STEP: Counting existing ResourceQuota @ 07/29/23 16:01:14.285
  STEP: Creating a ResourceQuota @ 07/29/23 16:01:19.291
  STEP: Ensuring resource quota status is calculated @ 07/29/23 16:01:19.3
  STEP: Creating a ReplicationController @ 07/29/23 16:01:21.309
  STEP: Ensuring resource quota status captures replication controller creation @ 07/29/23 16:01:21.33
  STEP: Deleting a ReplicationController @ 07/29/23 16:01:23.344
  STEP: Ensuring resource quota status released usage @ 07/29/23 16:01:23.355
  Jul 29 16:01:25.370: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8467" for this suite. @ 07/29/23 16:01:25.377
• [11.148 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:107
  STEP: Creating a kubernetes client @ 07/29/23 16:01:25.39
  Jul 29 16:01:25.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:01:25.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:25.426
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:25.433
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/29/23 16:01:25.438
  STEP: Saw pod success @ 07/29/23 16:01:29.495
  Jul 29 16:01:29.500: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-94babf2e-cce1-4fdb-9d39-3e539bcd3c87 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:01:29.514
  Jul 29 16:01:29.537: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-8079" for this suite. @ 07/29/23 16:01:29.546
• [4.168 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl patch should add annotations for pods in rc  [Conformance]
test/e2e/kubectl/kubectl.go:1640
  STEP: Creating a kubernetes client @ 07/29/23 16:01:29.562
  Jul 29 16:01:29.562: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:01:29.565
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:29.599
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:29.607
  STEP: creating Agnhost RC @ 07/29/23 16:01:29.612
  Jul 29 16:01:29.613: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6386 create -f -'
  Jul 29 16:01:30.341: INFO: stderr: ""
  Jul 29 16:01:30.341: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 16:01:30.341
  Jul 29 16:01:31.353: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:01:31.353: INFO: Found 0 / 1
  Jul 29 16:01:32.350: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:01:32.351: INFO: Found 1 / 1
  Jul 29 16:01:32.351: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  STEP: patching all pods @ 07/29/23 16:01:32.351
  Jul 29 16:01:32.357: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:01:32.357: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 16:01:32.358: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6386 patch pod agnhost-primary-lt5hm -p {"metadata":{"annotations":{"x":"y"}}}'
  Jul 29 16:01:32.517: INFO: stderr: ""
  Jul 29 16:01:32.517: INFO: stdout: "pod/agnhost-primary-lt5hm patched\n"
  STEP: checking annotations @ 07/29/23 16:01:32.517
  Jul 29 16:01:32.524: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:01:32.524: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 16:01:32.524: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6386" for this suite. @ 07/29/23 16:01:32.532
• [2.980 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing mutating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:645
  STEP: Creating a kubernetes client @ 07/29/23 16:01:32.544
  Jul 29 16:01:32.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:01:32.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:32.583
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:32.587
  STEP: Setting up server cert @ 07/29/23 16:01:32.631
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:01:33.662
  STEP: Deploying the webhook pod @ 07/29/23 16:01:33.68
  STEP: Wait for the deployment to be ready @ 07/29/23 16:01:33.7
  Jul 29 16:01:33.710: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 16:01:35.729
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:01:35.746
  Jul 29 16:01:36.747: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/29/23 16:01:36.852
  STEP: Creating a configMap that should be mutated @ 07/29/23 16:01:36.877
  STEP: Deleting the collection of validation webhooks @ 07/29/23 16:01:36.938
  STEP: Creating a configMap that should not be mutated @ 07/29/23 16:01:37.018
  Jul 29 16:01:37.037: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6891" for this suite. @ 07/29/23 16:01:37.115
  STEP: Destroying namespace "webhook-markers-313" for this suite. @ 07/29/23 16:01:37.126
• [4.592 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide pod UID as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:268
  STEP: Creating a kubernetes client @ 07/29/23 16:01:37.142
  Jul 29 16:01:37.142: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:01:37.145
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:37.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:37.171
  STEP: Creating a pod to test downward api env vars @ 07/29/23 16:01:37.176
  STEP: Saw pod success @ 07/29/23 16:01:41.215
  Jul 29 16:01:41.220: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downward-api-d9e5869a-ed22-40f0-a150-5e0abfae8b83 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:01:41.235
  Jul 29 16:01:41.261: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4617" for this suite. @ 07/29/23 16:01:41.269
• [4.138 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support pod.Spec.SecurityContext.RunAsUser And pod.Spec.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:129
  STEP: Creating a kubernetes client @ 07/29/23 16:01:41.295
  Jul 29 16:01:41.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename security-context @ 07/29/23 16:01:41.302
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:41.331
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:41.337
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/29/23 16:01:41.342
  STEP: Saw pod success @ 07/29/23 16:01:45.379
  Jul 29 16:01:45.385: INFO: Trying to get logs from node ci9axai7aiv7-3 pod security-context-b745da38-2f51-4202-9b81-82ca881896fc container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:01:45.398
  Jul 29 16:01:45.422: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4237" for this suite. @ 07/29/23 16:01:45.431
• [4.146 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should reject invalid sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:123
  STEP: Creating a kubernetes client @ 07/29/23 16:01:45.443
  Jul 29 16:01:45.443: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sysctl @ 07/29/23 16:01:45.446
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:45.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:45.479
  STEP: Creating a pod with one valid and two invalid sysctls @ 07/29/23 16:01:45.484
  Jul 29 16:01:45.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-7479" for this suite. @ 07/29/23 16:01:45.499
• [0.067 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should run through the lifecycle of a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:647
  STEP: Creating a kubernetes client @ 07/29/23 16:01:45.512
  Jul 29 16:01:45.512: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 16:01:45.513
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:45.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:45.551
  STEP: creating a ServiceAccount @ 07/29/23 16:01:45.556
  STEP: watching for the ServiceAccount to be added @ 07/29/23 16:01:45.57
  STEP: patching the ServiceAccount @ 07/29/23 16:01:45.575
  STEP: finding ServiceAccount in list of all ServiceAccounts (by LabelSelector) @ 07/29/23 16:01:45.586
  STEP: deleting the ServiceAccount @ 07/29/23 16:01:45.592
  Jul 29 16:01:45.618: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-4395" for this suite. @ 07/29/23 16:01:45.626
• [0.128 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to NodePort [Conformance]
test/e2e/network/service.go:1455
  STEP: Creating a kubernetes client @ 07/29/23 16:01:45.65
  Jul 29 16:01:45.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 16:01:45.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:01:45.682
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:01:45.689
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-5794 @ 07/29/23 16:01:45.696
  STEP: changing the ExternalName service to type=NodePort @ 07/29/23 16:01:45.706
  STEP: creating replication controller externalname-service in namespace services-5794 @ 07/29/23 16:01:45.742
  I0729 16:01:45.755522      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-5794, replica count: 2
  I0729 16:01:48.807856      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 1 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 16:01:51.808405      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:01:51.808: INFO: Creating new exec pod
  Jul 29 16:01:54.862: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5794 exec execpod7q926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 29 16:01:55.190: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 29 16:01:55.190: INFO: stdout: "externalname-service-7s4sm"
  Jul 29 16:01:55.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5794 exec execpod7q926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.18.193 80'
  Jul 29 16:01:55.450: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.18.193 80\nConnection to 10.233.18.193 80 port [tcp/http] succeeded!\n"
  Jul 29 16:01:55.450: INFO: stdout: "externalname-service-7s4sm"
  Jul 29 16:01:55.452: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5794 exec execpod7q926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.144 31744'
  Jul 29 16:01:55.674: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.144 31744\nConnection to 192.168.121.144 31744 port [tcp/*] succeeded!\n"
  Jul 29 16:01:55.674: INFO: stdout: ""
  Jul 29 16:01:56.675: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5794 exec execpod7q926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.144 31744'
  Jul 29 16:01:56.934: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.144 31744\nConnection to 192.168.121.144 31744 port [tcp/*] succeeded!\n"
  Jul 29 16:01:56.934: INFO: stdout: "externalname-service-4spjj"
  Jul 29 16:01:56.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5794 exec execpod7q926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.39 31744'
  Jul 29 16:01:57.189: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.39 31744\nConnection to 192.168.121.39 31744 port [tcp/*] succeeded!\n"
  Jul 29 16:01:57.189: INFO: stdout: ""
  Jul 29 16:01:58.191: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5794 exec execpod7q926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.39 31744'
  Jul 29 16:01:58.447: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.39 31744\nConnection to 192.168.121.39 31744 port [tcp/*] succeeded!\n"
  Jul 29 16:01:58.447: INFO: stdout: ""
  Jul 29 16:01:59.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5794 exec execpod7q926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.39 31744'
  Jul 29 16:01:59.430: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.39 31744\nConnection to 192.168.121.39 31744 port [tcp/*] succeeded!\n"
  Jul 29 16:01:59.430: INFO: stdout: ""
  Jul 29 16:02:00.190: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5794 exec execpod7q926 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.39 31744'
  Jul 29 16:02:00.442: INFO: stderr: "+ + ncecho -v hostName -t\n -w 2 192.168.121.39 31744\nConnection to 192.168.121.39 31744 port [tcp/*] succeeded!\n"
  Jul 29 16:02:00.442: INFO: stdout: "externalname-service-7s4sm"
  Jul 29 16:02:00.444: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:02:00.456: INFO: Cleaning up the ExternalName to NodePort test service
  STEP: Destroying namespace "services-5794" for this suite. @ 07/29/23 16:02:00.51
• [14.878 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all pods are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:243
  STEP: Creating a kubernetes client @ 07/29/23 16:02:00.531
  Jul 29 16:02:00.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 16:02:00.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:02:00.569
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:02:00.573
  STEP: Creating a test namespace @ 07/29/23 16:02:00.577
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:02:00.61
  STEP: Creating a pod in the namespace @ 07/29/23 16:02:00.615
  STEP: Waiting for the pod to have running status @ 07/29/23 16:02:00.626
  STEP: Deleting the namespace @ 07/29/23 16:02:02.641
  STEP: Waiting for the namespace to be removed. @ 07/29/23 16:02:02.653
  STEP: Recreating the namespace @ 07/29/23 16:02:13.663
  STEP: Verifying there are no pods in the namespace @ 07/29/23 16:02:13.691
  Jul 29 16:02:13.698: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6877" for this suite. @ 07/29/23 16:02:13.709
  STEP: Destroying namespace "nsdeletetest-7706" for this suite. @ 07/29/23 16:02:13.72
  Jul 29 16:02:13.726: INFO: Namespace nsdeletetest-7706 was already deleted
  STEP: Destroying namespace "nsdeletetest-988" for this suite. @ 07/29/23 16:02:13.726
• [13.205 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for the cluster  [Conformance]
test/e2e/network/dns.go:50
  STEP: Creating a kubernetes client @ 07/29/23 16:02:13.74
  Jul 29 16:02:13.740: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename dns @ 07/29/23 16:02:13.743
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:02:13.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:02:13.775
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/29/23 16:02:13.78
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@kubernetes.default.svc.cluster.local;check="$$(dig +tcp +noall +answer +search kubernetes.default.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@kubernetes.default.svc.cluster.local;sleep 1; done
   @ 07/29/23 16:02:13.78
  STEP: creating a pod to probe DNS @ 07/29/23 16:02:13.78
  STEP: submitting the pod to kubernetes @ 07/29/23 16:02:13.78
  STEP: retrieving the pod @ 07/29/23 16:02:15.814
  STEP: looking for the results for each expected name from probers @ 07/29/23 16:02:15.819
  Jul 29 16:02:15.854: INFO: DNS probes using dns-9789/dns-test-70a75676-a83a-4f5a-8517-d0ec09556810 succeeded

  Jul 29 16:02:15.854: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:02:15.866
  STEP: Destroying namespace "dns-9789" for this suite. @ 07/29/23 16:02:15.893
• [2.164 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should manage the lifecycle of a ResourceQuota [Conformance]
test/e2e/apimachinery/resource_quota.go:946
  STEP: Creating a kubernetes client @ 07/29/23 16:02:15.904
  Jul 29 16:02:15.904: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:02:15.91
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:02:15.938
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:02:15.942
  STEP: Creating a ResourceQuota @ 07/29/23 16:02:15.947
  STEP: Getting a ResourceQuota @ 07/29/23 16:02:15.964
  STEP: Listing all ResourceQuotas with LabelSelector @ 07/29/23 16:02:15.972
  STEP: Patching the ResourceQuota @ 07/29/23 16:02:15.977
  STEP: Deleting a Collection of ResourceQuotas @ 07/29/23 16:02:15.988
  STEP: Verifying the deleted ResourceQuota @ 07/29/23 16:02:16.005
  Jul 29 16:02:16.012: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7486" for this suite. @ 07/29/23 16:02:16.022
• [0.130 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert from CR v1 to CR v2 [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:141
  STEP: Creating a kubernetes client @ 07/29/23 16:02:16.042
  Jul 29 16:02:16.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-webhook @ 07/29/23 16:02:16.046
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:02:16.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:02:16.085
  STEP: Setting up server cert @ 07/29/23 16:02:16.091
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/29/23 16:02:17.157
  STEP: Deploying the custom resource conversion webhook pod @ 07/29/23 16:02:17.17
  STEP: Wait for the deployment to be ready @ 07/29/23 16:02:17.191
  Jul 29 16:02:17.202: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 16:02:19.234
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:02:19.261
  Jul 29 16:02:20.261: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 29 16:02:20.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Creating a v1 custom resource @ 07/29/23 16:02:23.066
  STEP: v2 custom resource should be converted @ 07/29/23 16:02:23.073
  Jul 29 16:02:23.079: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-1491" for this suite. @ 07/29/23 16:02:23.684
• [7.653 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] ReplicationController should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/rc.go:69
  STEP: Creating a kubernetes client @ 07/29/23 16:02:23.697
  Jul 29 16:02:23.697: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 16:02:23.699
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:02:23.77
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:02:23.777
  STEP: Creating replication controller my-hostname-basic-cb94fd5d-1621-455b-b58e-3178eccd4e67 @ 07/29/23 16:02:23.781
  Jul 29 16:02:23.796: INFO: Pod name my-hostname-basic-cb94fd5d-1621-455b-b58e-3178eccd4e67: Found 0 pods out of 1
  Jul 29 16:02:28.804: INFO: Pod name my-hostname-basic-cb94fd5d-1621-455b-b58e-3178eccd4e67: Found 1 pods out of 1
  Jul 29 16:02:28.804: INFO: Ensuring all pods for ReplicationController "my-hostname-basic-cb94fd5d-1621-455b-b58e-3178eccd4e67" are running
  Jul 29 16:02:28.808: INFO: Pod "my-hostname-basic-cb94fd5d-1621-455b-b58e-3178eccd4e67-qzh2h" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:02:23 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:02:25 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:02:25 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 16:02:23 +0000 UTC Reason: Message:}])
  Jul 29 16:02:28.809: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/29/23 16:02:28.809
  Jul 29 16:02:28.831: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-1057" for this suite. @ 07/29/23 16:02:28.841
• [5.154 seconds]
------------------------------
SSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message if TerminationMessagePath is set as non-root user and at a non-default path [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:195
  STEP: Creating a kubernetes client @ 07/29/23 16:02:28.853
  Jul 29 16:02:28.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 16:02:28.857
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:02:28.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:02:28.891
  STEP: create the container @ 07/29/23 16:02:28.895
  W0729 16:02:28.911781      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 16:02:28.912
  STEP: get the container status @ 07/29/23 16:02:31.951
  STEP: the container should be terminated @ 07/29/23 16:02:31.956
  STEP: the termination message should be set @ 07/29/23 16:02:31.956
  Jul 29 16:02:31.957: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/29/23 16:02:31.957
  Jul 29 16:02:31.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7653" for this suite. @ 07/29/23 16:02:32
• [3.157 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Burst scaling should run to completion even with unhealthy pods [Slow] [Conformance]
test/e2e/apps/statefulset.go:701
  STEP: Creating a kubernetes client @ 07/29/23 16:02:32.042
  Jul 29 16:02:32.042: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:02:32.044
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:02:32.129
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:02:32.138
  STEP: Creating service test in namespace statefulset-1639 @ 07/29/23 16:02:32.146
  STEP: Creating stateful set ss in namespace statefulset-1639 @ 07/29/23 16:02:32.155
  STEP: Waiting until all stateful set ss replicas will be running in namespace statefulset-1639 @ 07/29/23 16:02:32.177
  Jul 29 16:02:32.184: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 16:02:42.196: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Confirming that stateful set scale up will not halt with unhealthy stateful pod @ 07/29/23 16:02:42.197
  Jul 29 16:02:42.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-1639 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:02:42.474: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:02:42.474: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:02:42.474: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:02:42.484: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=true
  Jul 29 16:02:52.493: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:02:52.493: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:02:52.522: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Jul 29 16:02:52.522: INFO: ss-0  ci9axai7aiv7-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:42 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:32 +0000 UTC  }]
  Jul 29 16:02:52.522: INFO: 
  Jul 29 16:02:52.522: INFO: StatefulSet ss has not reached scale 3, at 1
  Jul 29 16:02:53.531: INFO: Verifying statefulset ss doesn't scale past 3 for another 8.993191395s
  Jul 29 16:02:54.545: INFO: Verifying statefulset ss doesn't scale past 3 for another 7.983422329s
  Jul 29 16:02:55.577: INFO: Verifying statefulset ss doesn't scale past 3 for another 6.96948952s
  Jul 29 16:02:56.587: INFO: Verifying statefulset ss doesn't scale past 3 for another 5.93803656s
  Jul 29 16:02:57.598: INFO: Verifying statefulset ss doesn't scale past 3 for another 4.927412418s
  Jul 29 16:02:58.609: INFO: Verifying statefulset ss doesn't scale past 3 for another 3.914952406s
  Jul 29 16:02:59.620: INFO: Verifying statefulset ss doesn't scale past 3 for another 2.906574389s
  Jul 29 16:03:00.631: INFO: Verifying statefulset ss doesn't scale past 3 for another 1.895129684s
  Jul 29 16:03:01.640: INFO: Verifying statefulset ss doesn't scale past 3 for another 884.395859ms
  STEP: Scaling up stateful set ss to 3 replicas and waiting until all of them will be running in namespace statefulset-1639 @ 07/29/23 16:03:02.641
  Jul 29 16:03:02.653: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-1639 exec ss-0 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:03:02.928: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:03:02.928: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:03:02.928: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-0: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:03:02.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-1639 exec ss-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:03:03.272: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 29 16:03:03.272: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:03:03.272: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:03:03.272: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-1639 exec ss-2 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:03:03.546: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\nmv: can't rename '/tmp/index.html': No such file or directory\n+ true\n"
  Jul 29 16:03:03.546: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:03:03.546: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss-2: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:03:03.598: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=false
  Jul 29 16:03:13.607: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:03:13.607: INFO: Waiting for pod ss-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:03:13.607: INFO: Waiting for pod ss-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Scale down will not halt with unhealthy stateful pod @ 07/29/23 16:03:13.607
  Jul 29 16:03:13.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-1639 exec ss-0 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:03:13.892: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:03:13.892: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:03:13.892: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-0: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:03:13.892: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-1639 exec ss-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:03:14.176: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:03:14.176: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:03:14.176: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:03:14.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-1639 exec ss-2 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:03:14.465: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:03:14.465: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:03:14.466: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss-2: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:03:14.466: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:03:14.472: INFO: Waiting for stateful set status.readyReplicas to become 0, currently 3
  Jul 29 16:03:24.492: INFO: Waiting for pod ss-0 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:03:24.492: INFO: Waiting for pod ss-1 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:03:24.492: INFO: Waiting for pod ss-2 to enter Running - Ready=false, currently Running - Ready=false
  Jul 29 16:03:24.522: INFO: POD   NODE            PHASE    GRACE  CONDITIONS
  Jul 29 16:03:24.522: INFO: ss-0  ci9axai7aiv7-3  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:32 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:03:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:03:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:32 +0000 UTC  }]
  Jul 29 16:03:24.524: INFO: ss-1  ci9axai7aiv7-1  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:03:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:03:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:52 +0000 UTC  }]
  Jul 29 16:03:24.524: INFO: ss-2  ci9axai7aiv7-2  Running         [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:52 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:03:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:03:14 +0000 UTC ContainersNotReady containers with unready status: [webserver]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:52 +0000 UTC  }]
  Jul 29 16:03:24.524: INFO: 
  Jul 29 16:03:24.524: INFO: StatefulSet ss has not reached scale 0, at 3
  Jul 29 16:03:25.535: INFO: POD   NODE            PHASE      GRACE  CONDITIONS
  Jul 29 16:03:25.535: INFO: ss-0  ci9axai7aiv7-3  Succeeded  30s    [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:32 +0000 UTC PodCompleted } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:03:14 +0000 UTC PodCompleted } {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:03:14 +0000 UTC PodCompleted } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 16:02:32 +0000 UTC  }]
  Jul 29 16:03:25.536: INFO: 
  Jul 29 16:03:25.536: INFO: StatefulSet ss has not reached scale 0, at 1
  Jul 29 16:03:26.543: INFO: Verifying statefulset ss doesn't scale past 0 for another 7.975866091s
  Jul 29 16:03:27.570: INFO: Verifying statefulset ss doesn't scale past 0 for another 6.968908299s
  Jul 29 16:03:28.582: INFO: Verifying statefulset ss doesn't scale past 0 for another 5.941624921s
  Jul 29 16:03:29.588: INFO: Verifying statefulset ss doesn't scale past 0 for another 4.93066459s
  Jul 29 16:03:30.598: INFO: Verifying statefulset ss doesn't scale past 0 for another 3.923932527s
  Jul 29 16:03:31.606: INFO: Verifying statefulset ss doesn't scale past 0 for another 2.914578244s
  Jul 29 16:03:32.613: INFO: Verifying statefulset ss doesn't scale past 0 for another 1.905276451s
  Jul 29 16:03:33.621: INFO: Verifying statefulset ss doesn't scale past 0 for another 899.075872ms
  STEP: Scaling down stateful set ss to 0 replicas and waiting until none of pods will run in namespacestatefulset-1639 @ 07/29/23 16:03:34.622
  Jul 29 16:03:34.638: INFO: Scaling statefulset ss to 0
  Jul 29 16:03:34.661: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:03:34.668: INFO: Deleting all statefulset in ns statefulset-1639
  Jul 29 16:03:34.674: INFO: Scaling statefulset ss to 0
  Jul 29 16:03:34.691: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:03:34.696: INFO: Deleting statefulset ss
  Jul 29 16:03:34.718: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1639" for this suite. @ 07/29/23 16:03:34.726
• [62.699 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide default limits.cpu/memory from node allocatable [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:218
  STEP: Creating a kubernetes client @ 07/29/23 16:03:34.749
  Jul 29 16:03:34.749: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:03:34.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:03:34.782
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:03:34.787
  STEP: Creating a pod to test downward api env vars @ 07/29/23 16:03:34.792
  STEP: Saw pod success @ 07/29/23 16:03:38.831
  Jul 29 16:03:38.835: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downward-api-82f31a3e-0be6-43ea-aa6b-725b9a6b5a1b container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:03:38.876
  Jul 29 16:03:38.927: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7762" for this suite. @ 07/29/23 16:03:38.936
• [4.199 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] CSIInlineVolumes should support CSIVolumeSource in Pod API [Conformance]
test/e2e/storage/csi_inline.go:131
  STEP: Creating a kubernetes client @ 07/29/23 16:03:38.954
  Jul 29 16:03:38.954: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/29/23 16:03:38.957
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:03:38.985
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:03:38.989
  STEP: creating @ 07/29/23 16:03:38.996
  STEP: getting @ 07/29/23 16:03:39.025
  STEP: listing in namespace @ 07/29/23 16:03:39.033
  STEP: patching @ 07/29/23 16:03:39.039
  STEP: deleting @ 07/29/23 16:03:39.049
  Jul 29 16:03:39.065: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-5197" for this suite. @ 07/29/23 16:03:39.084
• [0.176 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers and fail the pod if init containers fail on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:458
  STEP: Creating a kubernetes client @ 07/29/23 16:03:39.135
  Jul 29 16:03:39.135: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename init-container @ 07/29/23 16:03:39.137
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:03:39.22
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:03:39.225
  STEP: creating the pod @ 07/29/23 16:03:39.229
  Jul 29 16:03:39.230: INFO: PodSpec: initContainers in spec.initContainers
  Jul 29 16:03:43.140: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-658" for this suite. @ 07/29/23 16:03:43.182
• [4.064 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should create and stop a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:341
  STEP: Creating a kubernetes client @ 07/29/23 16:03:43.204
  Jul 29 16:03:43.204: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:03:43.206
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:03:43.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:03:43.243
  STEP: creating a replication controller @ 07/29/23 16:03:43.249
  Jul 29 16:03:43.250: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 create -f -'
  Jul 29 16:03:43.872: INFO: stderr: ""
  Jul 29 16:03:43.872: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 16:03:43.873
  Jul 29 16:03:43.874: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:03:44.083: INFO: stderr: ""
  Jul 29 16:03:44.083: INFO: stdout: "update-demo-nautilus-bpcnw update-demo-nautilus-tfgqc "
  Jul 29 16:03:44.084: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods update-demo-nautilus-bpcnw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:03:44.254: INFO: stderr: ""
  Jul 29 16:03:44.254: INFO: stdout: ""
  Jul 29 16:03:44.254: INFO: update-demo-nautilus-bpcnw is created but not running
  Jul 29 16:03:49.258: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:03:49.438: INFO: stderr: ""
  Jul 29 16:03:49.438: INFO: stdout: "update-demo-nautilus-bpcnw update-demo-nautilus-tfgqc "
  Jul 29 16:03:49.439: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods update-demo-nautilus-bpcnw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:03:49.602: INFO: stderr: ""
  Jul 29 16:03:49.602: INFO: stdout: ""
  Jul 29 16:03:49.602: INFO: update-demo-nautilus-bpcnw is created but not running
  Jul 29 16:03:54.603: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:03:54.772: INFO: stderr: ""
  Jul 29 16:03:54.772: INFO: stdout: "update-demo-nautilus-bpcnw update-demo-nautilus-tfgqc "
  Jul 29 16:03:54.772: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods update-demo-nautilus-bpcnw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:03:54.937: INFO: stderr: ""
  Jul 29 16:03:54.937: INFO: stdout: ""
  Jul 29 16:03:54.937: INFO: update-demo-nautilus-bpcnw is created but not running
  Jul 29 16:03:59.939: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 16:04:00.109: INFO: stderr: ""
  Jul 29 16:04:00.109: INFO: stdout: "update-demo-nautilus-bpcnw update-demo-nautilus-tfgqc "
  Jul 29 16:04:00.110: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods update-demo-nautilus-bpcnw -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:04:00.265: INFO: stderr: ""
  Jul 29 16:04:00.265: INFO: stdout: "true"
  Jul 29 16:04:00.266: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods update-demo-nautilus-bpcnw -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 16:04:00.433: INFO: stderr: ""
  Jul 29 16:04:00.433: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 16:04:00.433: INFO: validating pod update-demo-nautilus-bpcnw
  Jul 29 16:04:00.449: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 16:04:00.449: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 16:04:00.450: INFO: update-demo-nautilus-bpcnw is verified up and running
  Jul 29 16:04:00.450: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods update-demo-nautilus-tfgqc -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 16:04:00.590: INFO: stderr: ""
  Jul 29 16:04:00.590: INFO: stdout: "true"
  Jul 29 16:04:00.590: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods update-demo-nautilus-tfgqc -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 16:04:00.751: INFO: stderr: ""
  Jul 29 16:04:00.751: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 16:04:00.751: INFO: validating pod update-demo-nautilus-tfgqc
  Jul 29 16:04:00.768: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 16:04:00.768: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 16:04:00.768: INFO: update-demo-nautilus-tfgqc is verified up and running
  STEP: using delete to clean up resources @ 07/29/23 16:04:00.768
  Jul 29 16:04:00.768: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 delete --grace-period=0 --force -f -'
  Jul 29 16:04:00.908: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:04:00.908: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 29 16:04:00.909: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get rc,svc -l name=update-demo --no-headers'
  Jul 29 16:04:01.125: INFO: stderr: "No resources found in kubectl-512 namespace.\n"
  Jul 29 16:04:01.125: INFO: stdout: ""
  Jul 29 16:04:01.126: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-512 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 16:04:01.327: INFO: stderr: ""
  Jul 29 16:04:01.327: INFO: stdout: ""
  Jul 29 16:04:01.327: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-512" for this suite. @ 07/29/23 16:04:01.336
• [18.141 seconds]
------------------------------
SSS
------------------------------
[sig-node] Containers should use the image defaults if command and args are blank [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:41
  STEP: Creating a kubernetes client @ 07/29/23 16:04:01.346
  Jul 29 16:04:01.346: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename containers @ 07/29/23 16:04:01.349
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:01.378
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:01.383
  Jul 29 16:04:03.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-3712" for this suite. @ 07/29/23 16:04:03.459
• [2.125 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl run pod should create a pod from an image when restart is Never  [Conformance]
test/e2e/kubectl/kubectl.go:1701
  STEP: Creating a kubernetes client @ 07/29/23 16:04:03.478
  Jul 29 16:04:03.478: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:04:03.481
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:03.529
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:03.534
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 16:04:03.538
  Jul 29 16:04:03.539: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1278 run e2e-test-httpd-pod --restart=Never --pod-running-timeout=2m0s --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4'
  Jul 29 16:04:03.707: INFO: stderr: ""
  Jul 29 16:04:03.707: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/29/23 16:04:03.707
  Jul 29 16:04:03.715: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1278 delete pods e2e-test-httpd-pod'
  Jul 29 16:04:06.408: INFO: stderr: ""
  Jul 29 16:04:06.408: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 16:04:06.408: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1278" for this suite. @ 07/29/23 16:04:06.422
• [2.958 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl api-versions should check if v1 is in available api versions  [Conformance]
test/e2e/kubectl/kubectl.go:830
  STEP: Creating a kubernetes client @ 07/29/23 16:04:06.437
  Jul 29 16:04:06.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:04:06.442
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:06.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:06.489
  STEP: validating api versions @ 07/29/23 16:04:06.499
  Jul 29 16:04:06.500: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-3324 api-versions'
  Jul 29 16:04:06.664: INFO: stderr: ""
  Jul 29 16:04:06.664: INFO: stdout: "admissionregistration.k8s.io/v1\napiextensions.k8s.io/v1\napiregistration.k8s.io/v1\napps/v1\nauthentication.k8s.io/v1\nauthorization.k8s.io/v1\nautoscaling/v1\nautoscaling/v2\nbatch/v1\ncertificates.k8s.io/v1\ncilium.io/v2\ncilium.io/v2alpha1\ncoordination.k8s.io/v1\ndiscovery.k8s.io/v1\nevents.k8s.io/v1\nflowcontrol.apiserver.k8s.io/v1beta2\nflowcontrol.apiserver.k8s.io/v1beta3\nnetworking.k8s.io/v1\nnode.k8s.io/v1\npolicy/v1\nrbac.authorization.k8s.io/v1\nscheduling.k8s.io/v1\nstorage.k8s.io/v1\nv1\n"
  Jul 29 16:04:06.664: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-3324" for this suite. @ 07/29/23 16:04:06.673
• [0.248 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] custom resource defaulting for requests and from storage works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:269
  STEP: Creating a kubernetes client @ 07/29/23 16:04:06.685
  Jul 29 16:04:06.685: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 16:04:06.687
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:06.724
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:06.729
  Jul 29 16:04:06.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:04:10.188: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-4733" for this suite. @ 07/29/23 16:04:10.2
• [3.543 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:109
  STEP: Creating a kubernetes client @ 07/29/23 16:04:10.232
  Jul 29 16:04:10.232: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:04:10.234
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:10.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:10.288
  STEP: Creating configMap with name configmap-test-volume-map-3db38168-6ba7-4076-b59f-bbacf81c896f @ 07/29/23 16:04:10.295
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:04:10.306
  STEP: Saw pod success @ 07/29/23 16:04:14.345
  Jul 29 16:04:14.352: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-204a28cb-8a54-47e4-beb6-e29d37582f18 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:04:14.367
  Jul 29 16:04:14.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5974" for this suite. @ 07/29/23 16:04:14.409
• [4.189 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:152
  STEP: Creating a kubernetes client @ 07/29/23 16:04:14.423
  Jul 29 16:04:14.423: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 16:04:14.425
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:14.452
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:14.457
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 16:04:14.473
  STEP: create the pod with lifecycle hook @ 07/29/23 16:04:18.528
  STEP: delete the pod with lifecycle hook @ 07/29/23 16:04:20.568
  STEP: check prestop hook @ 07/29/23 16:04:22.6
  Jul 29 16:04:22.649: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-2558" for this suite. @ 07/29/23 16:04:22.66
• [8.250 seconds]
------------------------------
SSSS
------------------------------
[sig-scheduling] LimitRange should list, patch and delete a LimitRange by collection [Conformance]
test/e2e/scheduling/limit_range.go:239
  STEP: Creating a kubernetes client @ 07/29/23 16:04:22.676
  Jul 29 16:04:22.677: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename limitrange @ 07/29/23 16:04:22.68
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:22.713
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:22.719
  STEP: Creating LimitRange "e2e-limitrange-kgj49" in namespace "limitrange-5400" @ 07/29/23 16:04:22.726
  STEP: Creating another limitRange in another namespace @ 07/29/23 16:04:22.739
  Jul 29 16:04:22.771: INFO: Namespace "e2e-limitrange-kgj49-532" created
  Jul 29 16:04:22.771: INFO: Creating LimitRange "e2e-limitrange-kgj49" in namespace "e2e-limitrange-kgj49-532"
  STEP: Listing all LimitRanges with label "e2e-test=e2e-limitrange-kgj49" @ 07/29/23 16:04:22.782
  Jul 29 16:04:22.790: INFO: Found 2 limitRanges
  STEP: Patching LimitRange "e2e-limitrange-kgj49" in "limitrange-5400" namespace @ 07/29/23 16:04:22.791
  Jul 29 16:04:22.803: INFO: LimitRange "e2e-limitrange-kgj49" has been patched
  STEP: Delete LimitRange "e2e-limitrange-kgj49" by Collection with labelSelector: "e2e-limitrange-kgj49=patched" @ 07/29/23 16:04:22.803
  STEP: Confirm that the limitRange "e2e-limitrange-kgj49" has been deleted @ 07/29/23 16:04:22.821
  Jul 29 16:04:22.821: INFO: Requesting list of LimitRange to confirm quantity
  Jul 29 16:04:22.830: INFO: Found 0 LimitRange with label "e2e-limitrange-kgj49=patched"
  Jul 29 16:04:22.830: INFO: LimitRange "e2e-limitrange-kgj49" has been deleted.
  STEP: Confirm that a single LimitRange still exists with label "e2e-test=e2e-limitrange-kgj49" @ 07/29/23 16:04:22.83
  Jul 29 16:04:22.837: INFO: Found 1 limitRange
  Jul 29 16:04:22.837: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-5400" for this suite. @ 07/29/23 16:04:22.846
  STEP: Destroying namespace "e2e-limitrange-kgj49-532" for this suite. @ 07/29/23 16:04:22.857
• [0.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:546
  STEP: Creating a kubernetes client @ 07/29/23 16:04:22.87
  Jul 29 16:04:22.870: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:04:22.872
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:04:22.92
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:04:22.927
  STEP: Creating pod test-grpc-1dedbca7-73e3-411b-82b6-629102a2c4a6 in namespace container-probe-316 @ 07/29/23 16:04:22.934
  Jul 29 16:04:24.971: INFO: Started pod test-grpc-1dedbca7-73e3-411b-82b6-629102a2c4a6 in namespace container-probe-316
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:04:24.972
  Jul 29 16:04:24.977: INFO: Initial restart count of pod test-grpc-1dedbca7-73e3-411b-82b6-629102a2c4a6 is 0
  Jul 29 16:05:29.314: INFO: Restart count of pod container-probe-316/test-grpc-1dedbca7-73e3-411b-82b6-629102a2c4a6 is now 1 (1m4.336292017s elapsed)
  Jul 29 16:05:29.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:05:29.326
  STEP: Destroying namespace "container-probe-316" for this suite. @ 07/29/23 16:05:29.351
• [66.494 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not be blocked by dependency circle [Conformance]
test/e2e/apimachinery/garbage_collector.go:817
  STEP: Creating a kubernetes client @ 07/29/23 16:05:29.373
  Jul 29 16:05:29.373: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename gc @ 07/29/23 16:05:29.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:05:29.423
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:05:29.428
  Jul 29 16:05:29.486: INFO: pod1.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod3", UID:"0a75b0f5-9447-4dc0-8091-4af6a9f380a3", Controller:(*bool)(0xc005815236), BlockOwnerDeletion:(*bool)(0xc005815237)}}
  Jul 29 16:05:29.506: INFO: pod2.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod1", UID:"9add9aa5-c3ef-46e0-8089-0795ee6cc827", Controller:(*bool)(0xc005815496), BlockOwnerDeletion:(*bool)(0xc005815497)}}
  Jul 29 16:05:29.514: INFO: pod3.ObjectMeta.OwnerReferences=[]v1.OwnerReference{v1.OwnerReference{APIVersion:"v1", Kind:"Pod", Name:"pod2", UID:"a59ee17a-9d3e-41ee-a198-29414f0a1da7", Controller:(*bool)(0xc0058156c6), BlockOwnerDeletion:(*bool)(0xc0058156c7)}}
  Jul 29 16:05:34.543: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-7943" for this suite. @ 07/29/23 16:05:34.553
• [5.192 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Multiple Pods [Serial] evicts pods with minTolerationSeconds [Disruptive] [Conformance]
test/e2e/node/taints.go:450
  STEP: Creating a kubernetes client @ 07/29/23 16:05:34.568
  Jul 29 16:05:34.568: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename taint-multiple-pods @ 07/29/23 16:05:34.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:05:34.609
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:05:34.616
  Jul 29 16:05:34.622: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 16:06:34.672: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:06:34.680: INFO: Starting informer...
  STEP: Starting pods... @ 07/29/23 16:06:34.681
  Jul 29 16:06:34.918: INFO: Pod1 is running on ci9axai7aiv7-3. Tainting Node
  Jul 29 16:06:37.163: INFO: Pod2 is running on ci9axai7aiv7-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/29/23 16:06:37.164
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 16:06:37.188
  STEP: Waiting for Pod1 and Pod2 to be deleted @ 07/29/23 16:06:37.194
  Jul 29 16:06:43.152: INFO: Noticed Pod "taint-eviction-b1" gets evicted.
  Jul 29 16:07:03.262: INFO: Noticed Pod "taint-eviction-b2" gets evicted.
  Jul 29 16:07:03.264: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 16:07:03.3
  STEP: Destroying namespace "taint-multiple-pods-7053" for this suite. @ 07/29/23 16:07:03.306
• [88.748 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-api-machinery] server version should find the server version [Conformance]
test/e2e/apimachinery/server_version.go:40
  STEP: Creating a kubernetes client @ 07/29/23 16:07:03.318
  Jul 29 16:07:03.318: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename server-version @ 07/29/23 16:07:03.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:03.356
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:03.36
  STEP: Request ServerVersion @ 07/29/23 16:07:03.364
  STEP: Confirm major version @ 07/29/23 16:07:03.366
  Jul 29 16:07:03.366: INFO: Major version: 1
  STEP: Confirm minor version @ 07/29/23 16:07:03.366
  Jul 29 16:07:03.366: INFO: cleanMinorVersion: 27
  Jul 29 16:07:03.366: INFO: Minor version: 27
  Jul 29 16:07:03.366: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "server-version-6951" for this suite. @ 07/29/23 16:07:03.375
• [0.068 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container with readiness probe should not be ready before initial delay and never restart [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:71
  STEP: Creating a kubernetes client @ 07/29/23 16:07:03.387
  Jul 29 16:07:03.387: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:07:03.389
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:03.412
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:03.414
  Jul 29 16:07:25.551: INFO: Container started at 2023-07-29 16:07:04 +0000 UTC, pod became ready at 2023-07-29 16:07:23 +0000 UTC
  Jul 29 16:07:25.551: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-probe-2140" for this suite. @ 07/29/23 16:07:25.564
• [22.198 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:222
  STEP: Creating a kubernetes client @ 07/29/23 16:07:25.587
  Jul 29 16:07:25.588: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:07:25.59
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:25.635
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:25.639
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:07:25.648
  STEP: Saw pod success @ 07/29/23 16:07:29.696
  Jul 29 16:07:29.706: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-d8980560-288a-414e-9b59-b49025612fce container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:07:29.741
  Jul 29 16:07:29.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4306" for this suite. @ 07/29/23 16:07:29.784
• [4.208 seconds]
------------------------------
[sig-apps] Deployment RollingUpdateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:105
  STEP: Creating a kubernetes client @ 07/29/23 16:07:29.797
  Jul 29 16:07:29.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:07:29.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:29.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:29.834
  Jul 29 16:07:29.839: INFO: Creating replica set "test-rolling-update-controller" (going to be adopted)
  Jul 29 16:07:29.855: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 16:07:34.866: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 16:07:34.866
  Jul 29 16:07:34.866: INFO: Creating deployment "test-rolling-update-deployment"
  Jul 29 16:07:34.877: INFO: Ensuring deployment "test-rolling-update-deployment" gets the next revision from the one the adopted replica set "test-rolling-update-controller" has
  Jul 29 16:07:34.890: INFO: new replicaset for deployment "test-rolling-update-deployment" is yet to be created
  Jul 29 16:07:36.905: INFO: Ensuring status for deployment "test-rolling-update-deployment" is the expected
  Jul 29 16:07:36.909: INFO: Ensuring deployment "test-rolling-update-deployment" has one old replica set (the one it adopted)
  Jul 29 16:07:36.926: INFO: Deployment "test-rolling-update-deployment":
  &Deployment{ObjectMeta:{test-rolling-update-deployment  deployment-9501  a0bad813-cdf7-45d4-b13a-93beffc781a8 12622 1 2023-07-29 16:07:34 +0000 UTC <nil> <nil> map[name:sample-pod] map[deployment.kubernetes.io/revision:3546343826724305833] [] [] [{e2e.test Update apps/v1 2023-07-29 16:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049e68e8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:07:34 +0000 UTC,LastTransitionTime:2023-07-29 16:07:34 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rolling-update-deployment-656d657cd8" has successfully progressed.,LastUpdateTime:2023-07-29 16:07:36 +0000 UTC,LastTransitionTime:2023-07-29 16:07:34 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 16:07:36.932: INFO: New ReplicaSet "test-rolling-update-deployment-656d657cd8" of Deployment "test-rolling-update-deployment":
  &ReplicaSet{ObjectMeta:{test-rolling-update-deployment-656d657cd8  deployment-9501  ea9d0b3c-3a23-4358-aa82-7c1410067723 12611 1 2023-07-29 16:07:34 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305833] [{apps/v1 Deployment test-rolling-update-deployment a0bad813-cdf7-45d4-b13a-93beffc781a8 0xc0049e6de7 0xc0049e6de8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0bad813-cdf7-45d4-b13a-93beffc781a8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:07:36 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod-template-hash: 656d657cd8,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0049e6e98 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:07:36.932: INFO: All old ReplicaSets of Deployment "test-rolling-update-deployment":
  Jul 29 16:07:36.933: INFO: &ReplicaSet{ObjectMeta:{test-rolling-update-controller  deployment-9501  7cbfa5ec-29c6-40a2-8d71-138c80d7a243 12620 2 2023-07-29 16:07:29 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:3546343826724305832] [{apps/v1 Deployment test-rolling-update-deployment a0bad813-cdf7-45d4-b13a-93beffc781a8 0xc0049e6cb7 0xc0049e6cb8}] [] [{e2e.test Update apps/v1 2023-07-29 16:07:29 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:07:36 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"a0bad813-cdf7-45d4-b13a-93beffc781a8\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:07:36 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0049e6d78 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:07:36.939: INFO: Pod "test-rolling-update-deployment-656d657cd8-h72d5" is available:
  &Pod{ObjectMeta:{test-rolling-update-deployment-656d657cd8-h72d5 test-rolling-update-deployment-656d657cd8- deployment-9501  24ca699c-36f9-426b-b644-c3b5d83a4e6f 12610 0 2023-07-29 16:07:34 +0000 UTC <nil> <nil> map[name:sample-pod pod-template-hash:656d657cd8] map[] [{apps/v1 ReplicaSet test-rolling-update-deployment-656d657cd8 ea9d0b3c-3a23-4358-aa82-7c1410067723 0xc005435c27 0xc005435c28}] [] [{kube-controller-manager Update v1 2023-07-29 16:07:34 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ea9d0b3c-3a23-4358-aa82-7c1410067723\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:07:36 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.210\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-cmfsg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-cmfsg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:07:34 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:07:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:07:36 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:07:34 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.210,StartTime:2023-07-29 16:07:34 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:07:35 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://a658045e1aa9a6ab00b78bf33a08cd5d142c407c57599798ad24e1cc69969292,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.210,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 16:07:36.939: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9501" for this suite. @ 07/29/23 16:07:36.949
• [7.163 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should verify changes to a daemon set status [Conformance]
test/e2e/apps/daemon_set.go:875
  STEP: Creating a kubernetes client @ 07/29/23 16:07:36.964
  Jul 29 16:07:36.964: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 16:07:36.966
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:36.998
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:37.003
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 16:07:37.06
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 16:07:37.067
  Jul 29 16:07:37.083: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:07:37.083: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:07:38.131: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:07:38.131: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:07:39.100: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 16:07:39.101: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Getting /status @ 07/29/23 16:07:39.108
  Jul 29 16:07:39.120: INFO: Daemon Set daemon-set has Conditions: []
  STEP: updating the DaemonSet Status @ 07/29/23 16:07:39.12
  Jul 29 16:07:39.140: INFO: updatedStatus.Conditions: []v1.DaemonSetCondition{v1.DaemonSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the daemon set status to be updated @ 07/29/23 16:07:39.14
  Jul 29 16:07:39.145: INFO: Observed &DaemonSet event: ADDED
  Jul 29 16:07:39.146: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.147: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.148: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.149: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.149: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.150: INFO: Found daemon set daemon-set in namespace daemonsets-3751 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 16:07:39.151: INFO: Daemon set daemon-set has an updated status
  STEP: patching the DaemonSet Status @ 07/29/23 16:07:39.151
  STEP: watching for the daemon set status to be patched @ 07/29/23 16:07:39.165
  Jul 29 16:07:39.168: INFO: Observed &DaemonSet event: ADDED
  Jul 29 16:07:39.169: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.169: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.170: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.170: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.171: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.171: INFO: Observed daemon set daemon-set in namespace daemonsets-3751 with annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 16:07:39.171: INFO: Observed &DaemonSet event: MODIFIED
  Jul 29 16:07:39.171: INFO: Found daemon set daemon-set in namespace daemonsets-3751 with labels: map[daemonset-name:daemon-set] annotations: map[deprecated.daemonset.template.generation:1] & Conditions: [{StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }]
  Jul 29 16:07:39.171: INFO: Daemon set daemon-set has a patched status
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 16:07:39.179
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-3751, will wait for the garbage collector to delete the pods @ 07/29/23 16:07:39.179
  Jul 29 16:07:39.251: INFO: Deleting DaemonSet.extensions daemon-set took: 15.388556ms
  Jul 29 16:07:39.352: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.076228ms
  Jul 29 16:07:40.661: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:07:40.661: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 16:07:40.667: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"12702"},"items":null}

  Jul 29 16:07:40.674: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"12702"},"items":null}

  Jul 29 16:07:40.706: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-3751" for this suite. @ 07/29/23 16:07:40.717
• [3.769 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:250
  STEP: Creating a kubernetes client @ 07/29/23 16:07:40.735
  Jul 29 16:07:40.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:07:40.738
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:40.769
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:40.774
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:07:40.779
  STEP: Saw pod success @ 07/29/23 16:07:44.822
  Jul 29 16:07:44.828: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-b014775f-368f-465a-804c-e2303fce09c5 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:07:44.843
  Jul 29 16:07:44.876: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1606" for this suite. @ 07/29/23 16:07:44.885
• [4.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:97
  STEP: Creating a kubernetes client @ 07/29/23 16:07:44.902
  Jul 29 16:07:44.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:07:44.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:44.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:44.948
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/29/23 16:07:44.955
  STEP: Saw pod success @ 07/29/23 16:07:49.003
  Jul 29 16:07:49.008: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-6dc1316e-deca-4ebc-bebe-affec6d5f9c1 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:07:49.022
  Jul 29 16:07:49.050: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-390" for this suite. @ 07/29/23 16:07:49.058
• [4.179 seconds]
------------------------------
S
------------------------------
[sig-network] DNS should provide DNS for pods for Subdomain [Conformance]
test/e2e/network/dns.go:286
  STEP: Creating a kubernetes client @ 07/29/23 16:07:49.084
  Jul 29 16:07:49.084: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename dns @ 07/29/23 16:07:49.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:07:49.127
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:07:49.133
  STEP: Creating a test headless service @ 07/29/23 16:07:49.138
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6295.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service-2.dns-6295.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6295.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service-2.dns-6295.svc.cluster.local;sleep 1; done
   @ 07/29/23 16:07:49.149
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local;check="$$(dig +notcp +noall +answer +search dns-test-service-2.dns-6295.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service-2.dns-6295.svc.cluster.local;check="$$(dig +tcp +noall +answer +search dns-test-service-2.dns-6295.svc.cluster.local A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service-2.dns-6295.svc.cluster.local;sleep 1; done
   @ 07/29/23 16:07:49.15
  STEP: creating a pod to probe DNS @ 07/29/23 16:07:49.15
  STEP: submitting the pod to kubernetes @ 07/29/23 16:07:49.15
  STEP: retrieving the pod @ 07/29/23 16:07:53.209
  STEP: looking for the results for each expected name from probers @ 07/29/23 16:07:53.215
  Jul 29 16:07:53.228: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:53.235: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:53.252: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:53.257: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:53.262: INFO: Unable to read jessie_udp@dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:53.267: INFO: Unable to read jessie_tcp@dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:53.267: INFO: Lookups using dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_udp@dns-test-service-2.dns-6295.svc.cluster.local jessie_tcp@dns-test-service-2.dns-6295.svc.cluster.local]

  Jul 29 16:07:58.277: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:58.283: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:58.301: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:58.307: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:07:58.317: INFO: Lookups using dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local]

  Jul 29 16:08:03.276: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:03.282: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:03.298: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:03.302: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:03.314: INFO: Lookups using dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local]

  Jul 29 16:08:08.281: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:08.293: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:08.321: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:08.328: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:08.345: INFO: Lookups using dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local]

  Jul 29 16:08:13.280: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:13.288: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:13.312: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:13.320: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:13.332: INFO: Lookups using dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local]

  Jul 29 16:08:18.276: INFO: Unable to read wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:18.286: INFO: Unable to read wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:18.308: INFO: Unable to read jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:18.316: INFO: Unable to read jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local from pod dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73: the server could not find the requested resource (get pods dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73)
  Jul 29 16:08:18.331: INFO: Lookups using dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73 failed for: [wheezy_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local wheezy_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_udp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local jessie_tcp@dns-querier-2.dns-test-service-2.dns-6295.svc.cluster.local]

  Jul 29 16:08:23.325: INFO: DNS probes using dns-6295/dns-test-ea90fe45-acd4-407a-adc3-f2559a147e73 succeeded

  Jul 29 16:08:23.325: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:08:23.334
  STEP: deleting the test headless service @ 07/29/23 16:08:23.364
  STEP: Destroying namespace "dns-6295" for this suite. @ 07/29/23 16:08:23.403
• [34.333 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context should support container.SecurityContext.RunAsUser And container.SecurityContext.RunAsGroup [LinuxOnly] [Conformance]
test/e2e/node/security_context.go:164
  STEP: Creating a kubernetes client @ 07/29/23 16:08:23.433
  Jul 29 16:08:23.433: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename security-context @ 07/29/23 16:08:23.436
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:23.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:23.519
  STEP: Creating a pod to test pod.Spec.SecurityContext.RunAsUser @ 07/29/23 16:08:23.525
  STEP: Saw pod success @ 07/29/23 16:08:27.615
  Jul 29 16:08:27.621: INFO: Trying to get logs from node ci9axai7aiv7-3 pod security-context-238a011e-c2b1-4712-a205-39e002a32da9 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:08:27.638
  Jul 29 16:08:27.674: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-4919" for this suite. @ 07/29/23 16:08:27.683
• [4.262 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should serve multiport endpoints from pods  [Conformance]
test/e2e/network/service.go:846
  STEP: Creating a kubernetes client @ 07/29/23 16:08:27.696
  Jul 29 16:08:27.696: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 16:08:27.698
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:27.731
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:27.736
  STEP: creating service multi-endpoint-test in namespace services-3529 @ 07/29/23 16:08:27.75
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3529 to expose endpoints map[] @ 07/29/23 16:08:27.771
  Jul 29 16:08:27.786: INFO: Failed go get Endpoints object: endpoints "multi-endpoint-test" not found
  Jul 29 16:08:28.803: INFO: successfully validated that service multi-endpoint-test in namespace services-3529 exposes endpoints map[]
  STEP: Creating pod pod1 in namespace services-3529 @ 07/29/23 16:08:28.803
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3529 to expose endpoints map[pod1:[100]] @ 07/29/23 16:08:30.843
  Jul 29 16:08:30.863: INFO: successfully validated that service multi-endpoint-test in namespace services-3529 exposes endpoints map[pod1:[100]]
  STEP: Creating pod pod2 in namespace services-3529 @ 07/29/23 16:08:30.864
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3529 to expose endpoints map[pod1:[100] pod2:[101]] @ 07/29/23 16:08:32.914
  Jul 29 16:08:32.953: INFO: successfully validated that service multi-endpoint-test in namespace services-3529 exposes endpoints map[pod1:[100] pod2:[101]]
  STEP: Checking if the Service forwards traffic to pods @ 07/29/23 16:08:32.954
  Jul 29 16:08:32.954: INFO: Creating new exec pod
  Jul 29 16:08:35.982: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3529 exec execpodhhjz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 80'
  Jul 29 16:08:36.294: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 80\nConnection to multi-endpoint-test 80 port [tcp/http] succeeded!\n"
  Jul 29 16:08:36.294: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:08:36.295: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3529 exec execpodhhjz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.18.75 80'
  Jul 29 16:08:36.567: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.18.75 80\nConnection to 10.233.18.75 80 port [tcp/http] succeeded!\n"
  Jul 29 16:08:36.567: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:08:36.567: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3529 exec execpodhhjz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 multi-endpoint-test 81'
  Jul 29 16:08:36.827: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 multi-endpoint-test 81\nConnection to multi-endpoint-test 81 port [tcp/*] succeeded!\n"
  Jul 29 16:08:36.827: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:08:36.828: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3529 exec execpodhhjz7 -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.18.75 81'
  Jul 29 16:08:37.094: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.18.75 81\nConnection to 10.233.18.75 81 port [tcp/*] succeeded!\n"
  Jul 29 16:08:37.094: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  STEP: Deleting pod pod1 in namespace services-3529 @ 07/29/23 16:08:37.094
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3529 to expose endpoints map[pod2:[101]] @ 07/29/23 16:08:37.119
  Jul 29 16:08:38.239: INFO: successfully validated that service multi-endpoint-test in namespace services-3529 exposes endpoints map[pod2:[101]]
  STEP: Deleting pod pod2 in namespace services-3529 @ 07/29/23 16:08:38.239
  STEP: waiting up to 3m0s for service multi-endpoint-test in namespace services-3529 to expose endpoints map[] @ 07/29/23 16:08:38.302
  Jul 29 16:08:39.343: INFO: successfully validated that service multi-endpoint-test in namespace services-3529 exposes endpoints map[]
  Jul 29 16:08:39.343: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3529" for this suite. @ 07/29/23 16:08:39.392
• [11.717 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:168
  STEP: Creating a kubernetes client @ 07/29/23 16:08:39.425
  Jul 29 16:08:39.425: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 16:08:39.428
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:39.466
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:39.472
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 16:08:39.488
  STEP: create the pod with lifecycle hook @ 07/29/23 16:08:41.529
  STEP: check poststart hook @ 07/29/23 16:08:43.592
  STEP: delete the pod with lifecycle hook @ 07/29/23 16:08:43.611
  Jul 29 16:08:45.642: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-156" for this suite. @ 07/29/23 16:08:45.655
• [6.242 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command (container entrypoint) [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:75
  STEP: Creating a kubernetes client @ 07/29/23 16:08:45.671
  Jul 29 16:08:45.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename containers @ 07/29/23 16:08:45.675
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:45.709
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:45.716
  STEP: Creating a pod to test override command @ 07/29/23 16:08:45.719
  STEP: Saw pod success @ 07/29/23 16:08:49.761
  Jul 29 16:08:49.767: INFO: Trying to get logs from node ci9axai7aiv7-3 pod client-containers-f40565bc-8792-4529-a91c-2d719f15a041 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:08:49.787
  Jul 29 16:08:49.820: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-4687" for this suite. @ 07/29/23 16:08:49.827
• [4.168 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-instrumentation] Events API should delete a collection of events [Conformance]
test/e2e/instrumentation/events.go:207
  STEP: Creating a kubernetes client @ 07/29/23 16:08:49.845
  Jul 29 16:08:49.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename events @ 07/29/23 16:08:49.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:49.883
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:49.887
  STEP: Create set of events @ 07/29/23 16:08:49.891
  STEP: get a list of Events with a label in the current namespace @ 07/29/23 16:08:49.925
  STEP: delete a list of events @ 07/29/23 16:08:49.932
  Jul 29 16:08:49.932: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/29/23 16:08:49.97
  Jul 29 16:08:49.977: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-5556" for this suite. @ 07/29/23 16:08:49.987
• [0.156 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should orphan pods created by rc if delete options say so [Conformance]
test/e2e/apimachinery/garbage_collector.go:379
  STEP: Creating a kubernetes client @ 07/29/23 16:08:50.002
  Jul 29 16:08:50.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename gc @ 07/29/23 16:08:50.005
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:08:50.034
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:08:50.038
  STEP: create the rc @ 07/29/23 16:08:50.049
  W0729 16:08:50.057802      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/29/23 16:08:56.281
  STEP: wait for the rc to be deleted @ 07/29/23 16:08:56.419
  STEP: wait for 30 seconds to see if the garbage collector mistakenly deletes the pods @ 07/29/23 16:09:01.724
  STEP: Gathering metrics @ 07/29/23 16:09:31.756
  Jul 29 16:09:31.941: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 16:09:31.941: INFO: Deleting pod "simpletest.rc-29rg8" in namespace "gc-140"
  Jul 29 16:09:31.968: INFO: Deleting pod "simpletest.rc-2kdgl" in namespace "gc-140"
  Jul 29 16:09:32.018: INFO: Deleting pod "simpletest.rc-449q4" in namespace "gc-140"
  Jul 29 16:09:32.114: INFO: Deleting pod "simpletest.rc-46c6g" in namespace "gc-140"
  Jul 29 16:09:32.197: INFO: Deleting pod "simpletest.rc-5hn2h" in namespace "gc-140"
  Jul 29 16:09:32.234: INFO: Deleting pod "simpletest.rc-5w2x7" in namespace "gc-140"
  Jul 29 16:09:32.269: INFO: Deleting pod "simpletest.rc-64kj5" in namespace "gc-140"
  Jul 29 16:09:32.358: INFO: Deleting pod "simpletest.rc-64rw2" in namespace "gc-140"
  Jul 29 16:09:32.415: INFO: Deleting pod "simpletest.rc-7dc7w" in namespace "gc-140"
  Jul 29 16:09:32.539: INFO: Deleting pod "simpletest.rc-7drwp" in namespace "gc-140"
  Jul 29 16:09:32.639: INFO: Deleting pod "simpletest.rc-7fknj" in namespace "gc-140"
  Jul 29 16:09:32.719: INFO: Deleting pod "simpletest.rc-7k5t2" in namespace "gc-140"
  Jul 29 16:09:32.821: INFO: Deleting pod "simpletest.rc-7p859" in namespace "gc-140"
  Jul 29 16:09:32.878: INFO: Deleting pod "simpletest.rc-8ccgk" in namespace "gc-140"
  Jul 29 16:09:32.949: INFO: Deleting pod "simpletest.rc-97fjz" in namespace "gc-140"
  Jul 29 16:09:33.031: INFO: Deleting pod "simpletest.rc-97gcx" in namespace "gc-140"
  Jul 29 16:09:33.123: INFO: Deleting pod "simpletest.rc-9j95t" in namespace "gc-140"
  Jul 29 16:09:33.209: INFO: Deleting pod "simpletest.rc-9p7cr" in namespace "gc-140"
  Jul 29 16:09:33.319: INFO: Deleting pod "simpletest.rc-bfmjv" in namespace "gc-140"
  Jul 29 16:09:33.369: INFO: Deleting pod "simpletest.rc-bjb9c" in namespace "gc-140"
  Jul 29 16:09:33.403: INFO: Deleting pod "simpletest.rc-bksbq" in namespace "gc-140"
  Jul 29 16:09:33.474: INFO: Deleting pod "simpletest.rc-bls9r" in namespace "gc-140"
  Jul 29 16:09:33.531: INFO: Deleting pod "simpletest.rc-brww8" in namespace "gc-140"
  Jul 29 16:09:33.607: INFO: Deleting pod "simpletest.rc-bwtcp" in namespace "gc-140"
  Jul 29 16:09:33.641: INFO: Deleting pod "simpletest.rc-c2hjf" in namespace "gc-140"
  Jul 29 16:09:33.715: INFO: Deleting pod "simpletest.rc-cnzgl" in namespace "gc-140"
  Jul 29 16:09:33.777: INFO: Deleting pod "simpletest.rc-d7hd7" in namespace "gc-140"
  Jul 29 16:09:33.854: INFO: Deleting pod "simpletest.rc-dg5t5" in namespace "gc-140"
  Jul 29 16:09:33.967: INFO: Deleting pod "simpletest.rc-dkmxw" in namespace "gc-140"
  Jul 29 16:09:34.053: INFO: Deleting pod "simpletest.rc-dqjgh" in namespace "gc-140"
  Jul 29 16:09:34.100: INFO: Deleting pod "simpletest.rc-dt5ss" in namespace "gc-140"
  Jul 29 16:09:34.242: INFO: Deleting pod "simpletest.rc-fb22r" in namespace "gc-140"
  Jul 29 16:09:34.400: INFO: Deleting pod "simpletest.rc-fnlhk" in namespace "gc-140"
  Jul 29 16:09:34.645: INFO: Deleting pod "simpletest.rc-fnscd" in namespace "gc-140"
  Jul 29 16:09:34.726: INFO: Deleting pod "simpletest.rc-fq56w" in namespace "gc-140"
  Jul 29 16:09:34.799: INFO: Deleting pod "simpletest.rc-fvp4t" in namespace "gc-140"
  Jul 29 16:09:34.889: INFO: Deleting pod "simpletest.rc-gdcxj" in namespace "gc-140"
  Jul 29 16:09:34.982: INFO: Deleting pod "simpletest.rc-ggs2k" in namespace "gc-140"
  Jul 29 16:09:35.019: INFO: Deleting pod "simpletest.rc-ghq9k" in namespace "gc-140"
  Jul 29 16:09:35.120: INFO: Deleting pod "simpletest.rc-gk9jw" in namespace "gc-140"
  Jul 29 16:09:35.229: INFO: Deleting pod "simpletest.rc-gqk5g" in namespace "gc-140"
  Jul 29 16:09:35.299: INFO: Deleting pod "simpletest.rc-hllmj" in namespace "gc-140"
  Jul 29 16:09:35.443: INFO: Deleting pod "simpletest.rc-hn9sj" in namespace "gc-140"
  Jul 29 16:09:35.535: INFO: Deleting pod "simpletest.rc-hnrg2" in namespace "gc-140"
  Jul 29 16:09:35.670: INFO: Deleting pod "simpletest.rc-j59l6" in namespace "gc-140"
  Jul 29 16:09:35.858: INFO: Deleting pod "simpletest.rc-jbtdb" in namespace "gc-140"
  Jul 29 16:09:35.953: INFO: Deleting pod "simpletest.rc-jbwds" in namespace "gc-140"
  Jul 29 16:09:36.069: INFO: Deleting pod "simpletest.rc-jqzz4" in namespace "gc-140"
  Jul 29 16:09:36.148: INFO: Deleting pod "simpletest.rc-jrzdr" in namespace "gc-140"
  Jul 29 16:09:36.346: INFO: Deleting pod "simpletest.rc-k6dzq" in namespace "gc-140"
  Jul 29 16:09:36.393: INFO: Deleting pod "simpletest.rc-k98s9" in namespace "gc-140"
  Jul 29 16:09:36.431: INFO: Deleting pod "simpletest.rc-l6ffd" in namespace "gc-140"
  Jul 29 16:09:36.502: INFO: Deleting pod "simpletest.rc-l8kz2" in namespace "gc-140"
  Jul 29 16:09:36.543: INFO: Deleting pod "simpletest.rc-lft6m" in namespace "gc-140"
  Jul 29 16:09:36.638: INFO: Deleting pod "simpletest.rc-lgkcm" in namespace "gc-140"
  Jul 29 16:09:36.671: INFO: Deleting pod "simpletest.rc-lqrgz" in namespace "gc-140"
  Jul 29 16:09:36.765: INFO: Deleting pod "simpletest.rc-lxz78" in namespace "gc-140"
  Jul 29 16:09:36.833: INFO: Deleting pod "simpletest.rc-m2b2c" in namespace "gc-140"
  Jul 29 16:09:36.936: INFO: Deleting pod "simpletest.rc-mdqxq" in namespace "gc-140"
  Jul 29 16:09:37.043: INFO: Deleting pod "simpletest.rc-mgj5m" in namespace "gc-140"
  Jul 29 16:09:37.105: INFO: Deleting pod "simpletest.rc-mpp9p" in namespace "gc-140"
  Jul 29 16:09:37.163: INFO: Deleting pod "simpletest.rc-n28dd" in namespace "gc-140"
  Jul 29 16:09:37.223: INFO: Deleting pod "simpletest.rc-ncmvh" in namespace "gc-140"
  Jul 29 16:09:37.284: INFO: Deleting pod "simpletest.rc-nldx6" in namespace "gc-140"
  Jul 29 16:09:37.383: INFO: Deleting pod "simpletest.rc-nn6bq" in namespace "gc-140"
  Jul 29 16:09:37.458: INFO: Deleting pod "simpletest.rc-nwhs5" in namespace "gc-140"
  Jul 29 16:09:37.522: INFO: Deleting pod "simpletest.rc-p4tdf" in namespace "gc-140"
  Jul 29 16:09:37.591: INFO: Deleting pod "simpletest.rc-p5bxj" in namespace "gc-140"
  Jul 29 16:09:37.741: INFO: Deleting pod "simpletest.rc-p64xm" in namespace "gc-140"
  Jul 29 16:09:37.871: INFO: Deleting pod "simpletest.rc-pqphp" in namespace "gc-140"
  Jul 29 16:09:37.940: INFO: Deleting pod "simpletest.rc-pvh26" in namespace "gc-140"
  Jul 29 16:09:38.035: INFO: Deleting pod "simpletest.rc-q7xqk" in namespace "gc-140"
  Jul 29 16:09:38.155: INFO: Deleting pod "simpletest.rc-q8gcj" in namespace "gc-140"
  Jul 29 16:09:38.292: INFO: Deleting pod "simpletest.rc-qflg2" in namespace "gc-140"
  Jul 29 16:09:38.424: INFO: Deleting pod "simpletest.rc-qgvn9" in namespace "gc-140"
  Jul 29 16:09:38.498: INFO: Deleting pod "simpletest.rc-qh5zt" in namespace "gc-140"
  Jul 29 16:09:38.624: INFO: Deleting pod "simpletest.rc-rgn6c" in namespace "gc-140"
  Jul 29 16:09:38.691: INFO: Deleting pod "simpletest.rc-rhqb7" in namespace "gc-140"
  Jul 29 16:09:38.787: INFO: Deleting pod "simpletest.rc-rq2xd" in namespace "gc-140"
  Jul 29 16:09:38.870: INFO: Deleting pod "simpletest.rc-s9glh" in namespace "gc-140"
  Jul 29 16:09:38.966: INFO: Deleting pod "simpletest.rc-sg9m5" in namespace "gc-140"
  Jul 29 16:09:39.040: INFO: Deleting pod "simpletest.rc-snttj" in namespace "gc-140"
  Jul 29 16:09:39.106: INFO: Deleting pod "simpletest.rc-t2pwt" in namespace "gc-140"
  Jul 29 16:09:39.332: INFO: Deleting pod "simpletest.rc-t8d4q" in namespace "gc-140"
  Jul 29 16:09:39.373: INFO: Deleting pod "simpletest.rc-t9l55" in namespace "gc-140"
  Jul 29 16:09:39.442: INFO: Deleting pod "simpletest.rc-tmn7f" in namespace "gc-140"
  Jul 29 16:09:39.498: INFO: Deleting pod "simpletest.rc-twnrb" in namespace "gc-140"
  Jul 29 16:09:39.561: INFO: Deleting pod "simpletest.rc-v6gpx" in namespace "gc-140"
  Jul 29 16:09:39.663: INFO: Deleting pod "simpletest.rc-vm465" in namespace "gc-140"
  Jul 29 16:09:39.834: INFO: Deleting pod "simpletest.rc-vnds9" in namespace "gc-140"
  Jul 29 16:09:39.896: INFO: Deleting pod "simpletest.rc-vp9t9" in namespace "gc-140"
  Jul 29 16:09:39.959: INFO: Deleting pod "simpletest.rc-w2xgb" in namespace "gc-140"
  Jul 29 16:09:40.056: INFO: Deleting pod "simpletest.rc-w8dwd" in namespace "gc-140"
  Jul 29 16:09:40.120: INFO: Deleting pod "simpletest.rc-wfpzx" in namespace "gc-140"
  Jul 29 16:09:40.176: INFO: Deleting pod "simpletest.rc-xp6kp" in namespace "gc-140"
  Jul 29 16:09:40.232: INFO: Deleting pod "simpletest.rc-zdrx2" in namespace "gc-140"
  Jul 29 16:09:40.290: INFO: Deleting pod "simpletest.rc-zq254" in namespace "gc-140"
  Jul 29 16:09:40.393: INFO: Deleting pod "simpletest.rc-zqgpb" in namespace "gc-140"
  Jul 29 16:09:40.437: INFO: Deleting pod "simpletest.rc-zvkl4" in namespace "gc-140"
  Jul 29 16:09:40.493: INFO: Deleting pod "simpletest.rc-zvm55" in namespace "gc-140"
  Jul 29 16:09:40.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-140" for this suite. @ 07/29/23 16:09:40.563
• [50.595 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] removes definition from spec when one version gets changed to not be served [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:442
  STEP: Creating a kubernetes client @ 07/29/23 16:09:40.599
  Jul 29 16:09:40.599: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 16:09:40.608
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:09:40.695
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:09:40.699
  STEP: set up a multi version CRD @ 07/29/23 16:09:40.706
  Jul 29 16:09:40.708: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: mark a version not serverd @ 07/29/23 16:09:46.317
  STEP: check the unserved version gets removed @ 07/29/23 16:09:46.37
  STEP: check the other version is not changed @ 07/29/23 16:09:48.3
  Jul 29 16:09:52.807: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-1617" for this suite. @ 07/29/23 16:09:52.831
• [12.245 seconds]
------------------------------
SS
------------------------------
[sig-network] Services should be able to change the type from ClusterIP to ExternalName [Conformance]
test/e2e/network/service.go:1493
  STEP: Creating a kubernetes client @ 07/29/23 16:09:52.845
  Jul 29 16:09:52.845: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 16:09:52.848
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:09:52.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:09:52.886
  STEP: creating a service clusterip-service with the type=ClusterIP in namespace services-2625 @ 07/29/23 16:09:52.891
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/29/23 16:09:52.908
  STEP: creating service externalsvc in namespace services-2625 @ 07/29/23 16:09:52.908
  STEP: creating replication controller externalsvc in namespace services-2625 @ 07/29/23 16:09:52.942
  I0729 16:09:52.955889      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-2625, replica count: 2
  I0729 16:09:56.006980      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the ClusterIP service to type=ExternalName @ 07/29/23 16:09:56.015
  Jul 29 16:09:56.050: INFO: Creating new exec pod
  Jul 29 16:09:58.087: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-2625 exec execpodkmhfc -- /bin/sh -x -c nslookup clusterip-service.services-2625.svc.cluster.local'
  Jul 29 16:09:58.486: INFO: stderr: "+ nslookup clusterip-service.services-2625.svc.cluster.local\n"
  Jul 29 16:09:58.486: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nclusterip-service.services-2625.svc.cluster.local\tcanonical name = externalsvc.services-2625.svc.cluster.local.\nName:\texternalsvc.services-2625.svc.cluster.local\nAddress: 10.233.9.152\n\n"
  Jul 29 16:09:58.487: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-2625, will wait for the garbage collector to delete the pods @ 07/29/23 16:09:58.501
  Jul 29 16:09:58.577: INFO: Deleting ReplicationController externalsvc took: 19.134263ms
  Jul 29 16:09:58.678: INFO: Terminating ReplicationController externalsvc pods took: 100.67902ms
  Jul 29 16:10:00.817: INFO: Cleaning up the ClusterIP to ExternalName test service
  STEP: Destroying namespace "services-2625" for this suite. @ 07/29/23 16:10:00.844
• [8.023 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Ephemeral Containers [NodeConformance] will start an ephemeral container in an existing pod [Conformance]
test/e2e/common/node/ephemeral_containers.go:46
  STEP: Creating a kubernetes client @ 07/29/23 16:10:00.872
  Jul 29 16:10:00.872: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename ephemeral-containers-test @ 07/29/23 16:10:00.875
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:10:00.924
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:10:00.929
  STEP: creating a target pod @ 07/29/23 16:10:00.935
  STEP: adding an ephemeral container @ 07/29/23 16:10:02.995
  STEP: checking pod container endpoints @ 07/29/23 16:10:05.039
  Jul 29 16:10:05.040: INFO: ExecWithOptions {Command:[/bin/echo marco] Namespace:ephemeral-containers-test-7850 PodName:ephemeral-containers-target-pod ContainerName:debugger Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:10:05.040: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:10:05.042: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:10:05.042: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/ephemeral-containers-test-7850/pods/ephemeral-containers-target-pod/exec?command=%2Fbin%2Fecho&command=marco&container=debugger&container=debugger&stderr=true&stdout=true)
  Jul 29 16:10:05.132: INFO: Exec stderr: ""
  Jul 29 16:10:05.148: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ephemeral-containers-test-7850" for this suite. @ 07/29/23 16:10:05.155
• [4.298 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] Watchers should observe add, update, and delete watch notifications on configmaps [Conformance]
test/e2e/apimachinery/watch.go:60
  STEP: Creating a kubernetes client @ 07/29/23 16:10:05.171
  Jul 29 16:10:05.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename watch @ 07/29/23 16:10:05.173
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:10:05.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:10:05.206
  STEP: creating a watch on configmaps with label A @ 07/29/23 16:10:05.211
  STEP: creating a watch on configmaps with label B @ 07/29/23 16:10:05.213
  STEP: creating a watch on configmaps with label A or B @ 07/29/23 16:10:05.218
  STEP: creating a configmap with label A and ensuring the correct watchers observe the notification @ 07/29/23 16:10:05.221
  Jul 29 16:10:05.229: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3253  fb4fffaa-4575-4faa-b708-5120e31db86f 15578 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:10:05.230: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3253  fb4fffaa-4575-4faa-b708-5120e31db86f 15578 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A and ensuring the correct watchers observe the notification @ 07/29/23 16:10:05.23
  Jul 29 16:10:05.244: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3253  fb4fffaa-4575-4faa-b708-5120e31db86f 15579 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:10:05.244: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3253  fb4fffaa-4575-4faa-b708-5120e31db86f 15579 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying configmap A again and ensuring the correct watchers observe the notification @ 07/29/23 16:10:05.245
  Jul 29 16:10:05.264: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3253  fb4fffaa-4575-4faa-b708-5120e31db86f 15580 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:10:05.266: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3253  fb4fffaa-4575-4faa-b708-5120e31db86f 15580 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap A and ensuring the correct watchers observe the notification @ 07/29/23 16:10:05.266
  Jul 29 16:10:05.280: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3253  fb4fffaa-4575-4faa-b708-5120e31db86f 15581 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:10:05.282: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-a  watch-3253  fb4fffaa-4575-4faa-b708-5120e31db86f 15581 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-A] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: creating a configmap with label B and ensuring the correct watchers observe the notification @ 07/29/23 16:10:05.282
  Jul 29 16:10:05.294: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3253  2810214f-6225-43e7-8647-d7b292f3bf80 15582 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:10:05.295: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3253  2810214f-6225-43e7-8647-d7b292f3bf80 15582 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: deleting configmap B and ensuring the correct watchers observe the notification @ 07/29/23 16:10:15.3
  Jul 29 16:10:15.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3253  2810214f-6225-43e7-8647-d7b292f3bf80 15640 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:10:15.317: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-configmap-b  watch-3253  2810214f-6225-43e7-8647-d7b292f3bf80 15640 0 2023-07-29 16:10:05 +0000 UTC <nil> <nil> map[watch-this-configmap:multiple-watchers-B] map[] [] [] [{e2e.test Update v1 2023-07-29 16:10:05 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:10:25.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3253" for this suite. @ 07/29/23 16:10:25.333
• [20.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/configmap.go:93
  STEP: Creating a kubernetes client @ 07/29/23 16:10:25.349
  Jul 29 16:10:25.349: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:10:25.352
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:10:25.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:10:25.384
  STEP: Creating configMap configmap-3268/configmap-test-c8fedfc3-6c4e-4507-9063-6b2443184f21 @ 07/29/23 16:10:25.388
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:10:25.396
  STEP: Saw pod success @ 07/29/23 16:10:29.447
  Jul 29 16:10:29.452: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-d6aad14e-d124-43c7-98b4-0ba42cc7ce36 container env-test: <nil>
  STEP: delete the pod @ 07/29/23 16:10:29.464
  Jul 29 16:10:29.494: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3268" for this suite. @ 07/29/23 16:10:29.5
• [4.165 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform rolling updates and roll backs of template modifications [Conformance]
test/e2e/apps/statefulset.go:316
  STEP: Creating a kubernetes client @ 07/29/23 16:10:29.516
  Jul 29 16:10:29.516: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:10:29.518
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:10:29.546
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:10:29.552
  STEP: Creating service test in namespace statefulset-3904 @ 07/29/23 16:10:29.557
  STEP: Creating a new StatefulSet @ 07/29/23 16:10:29.568
  Jul 29 16:10:29.585: INFO: Found 0 stateful pods, waiting for 3
  Jul 29 16:10:39.598: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:10:39.599: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:10:39.600: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:10:39.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-3904 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:10:39.950: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:10:39.950: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:10:39.950: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  STEP: Updating StatefulSet template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/29/23 16:10:49.988
  Jul 29 16:10:50.016: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/29/23 16:10:50.017
  STEP: Updating Pods in reverse ordinal order @ 07/29/23 16:11:00.047
  Jul 29 16:11:00.053: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-3904 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:11:00.313: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:11:00.313: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:11:00.313: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:11:20.374: INFO: Waiting for StatefulSet statefulset-3904/ss2 to complete update
  Jul 29 16:11:20.374: INFO: Waiting for Pod statefulset-3904/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Rolling back to a previous revision @ 07/29/23 16:11:30.392
  Jul 29 16:11:30.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-3904 exec ss2-1 -- /bin/sh -x -c mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true'
  Jul 29 16:11:30.763: INFO: stderr: "+ mv -v /usr/local/apache2/htdocs/index.html /tmp/\n"
  Jul 29 16:11:30.763: INFO: stdout: "'/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'\n"
  Jul 29 16:11:30.763: INFO: stdout of mv -v /usr/local/apache2/htdocs/index.html /tmp/ || true on ss2-1: '/usr/local/apache2/htdocs/index.html' -> '/tmp/index.html'

  Jul 29 16:11:40.838: INFO: Updating stateful set ss2
  STEP: Rolling back update in reverse ordinal order @ 07/29/23 16:11:50.869
  Jul 29 16:11:50.877: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=statefulset-3904 exec ss2-1 -- /bin/sh -x -c mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true'
  Jul 29 16:11:51.131: INFO: stderr: "+ mv -v /tmp/index.html /usr/local/apache2/htdocs/\n"
  Jul 29 16:11:51.131: INFO: stdout: "'/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'\n"
  Jul 29 16:11:51.131: INFO: stdout of mv -v /tmp/index.html /usr/local/apache2/htdocs/ || true on ss2-1: '/tmp/index.html' -> '/usr/local/apache2/htdocs/index.html'

  Jul 29 16:12:01.196: INFO: Deleting all statefulset in ns statefulset-3904
  Jul 29 16:12:01.202: INFO: Scaling statefulset ss2 to 0
  Jul 29 16:12:11.249: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:12:11.259: INFO: Deleting statefulset ss2
  Jul 29 16:12:11.295: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-3904" for this suite. @ 07/29/23 16:12:11.306
• [101.812 seconds]
------------------------------
S
------------------------------
[sig-auth] ServiceAccounts should allow opting out of API token automount  [Conformance]
test/e2e/auth/service_accounts.go:161
  STEP: Creating a kubernetes client @ 07/29/23 16:12:11.329
  Jul 29 16:12:11.329: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 16:12:11.332
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:11.366
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:11.371
  Jul 29 16:12:11.407: INFO: created pod pod-service-account-defaultsa
  Jul 29 16:12:11.407: INFO: pod pod-service-account-defaultsa service account token volume mount: true
  Jul 29 16:12:11.415: INFO: created pod pod-service-account-mountsa
  Jul 29 16:12:11.416: INFO: pod pod-service-account-mountsa service account token volume mount: true
  Jul 29 16:12:11.428: INFO: created pod pod-service-account-nomountsa
  Jul 29 16:12:11.428: INFO: pod pod-service-account-nomountsa service account token volume mount: false
  Jul 29 16:12:11.462: INFO: created pod pod-service-account-defaultsa-mountspec
  Jul 29 16:12:11.462: INFO: pod pod-service-account-defaultsa-mountspec service account token volume mount: true
  Jul 29 16:12:11.473: INFO: created pod pod-service-account-mountsa-mountspec
  Jul 29 16:12:11.473: INFO: pod pod-service-account-mountsa-mountspec service account token volume mount: true
  Jul 29 16:12:11.482: INFO: created pod pod-service-account-nomountsa-mountspec
  Jul 29 16:12:11.482: INFO: pod pod-service-account-nomountsa-mountspec service account token volume mount: true
  Jul 29 16:12:11.493: INFO: created pod pod-service-account-defaultsa-nomountspec
  Jul 29 16:12:11.493: INFO: pod pod-service-account-defaultsa-nomountspec service account token volume mount: false
  Jul 29 16:12:11.503: INFO: created pod pod-service-account-mountsa-nomountspec
  Jul 29 16:12:11.503: INFO: pod pod-service-account-mountsa-nomountspec service account token volume mount: false
  Jul 29 16:12:11.517: INFO: created pod pod-service-account-nomountsa-nomountspec
  Jul 29 16:12:11.517: INFO: pod pod-service-account-nomountsa-nomountspec service account token volume mount: false
  Jul 29 16:12:11.517: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3866" for this suite. @ 07/29/23 16:12:11.552
• [0.291 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:131
  STEP: Creating a kubernetes client @ 07/29/23 16:12:11.628
  Jul 29 16:12:11.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:12:11.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:11.958
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:12.031
  STEP: Creating the pod @ 07/29/23 16:12:12.038
  Jul 29 16:12:14.658: INFO: Successfully updated pod "labelsupdate790df216-2d7d-4c79-a07f-d94cf7bde28e"
  Jul 29 16:12:16.701: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1303" for this suite. @ 07/29/23 16:12:16.714
• [5.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should have session affinity work for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2165
  STEP: Creating a kubernetes client @ 07/29/23 16:12:16.753
  Jul 29 16:12:16.753: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 16:12:16.759
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:16.817
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:16.822
  STEP: creating service in namespace services-5778 @ 07/29/23 16:12:16.828
  STEP: creating service affinity-clusterip in namespace services-5778 @ 07/29/23 16:12:16.828
  STEP: creating replication controller affinity-clusterip in namespace services-5778 @ 07/29/23 16:12:16.863
  I0729 16:12:16.872707      13 runners.go:194] Created replication controller with name: affinity-clusterip, namespace: services-5778, replica count: 3
  I0729 16:12:19.924414      13 runners.go:194] affinity-clusterip Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:12:19.936: INFO: Creating new exec pod
  Jul 29 16:12:22.972: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5778 exec execpod-affinityp245z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip 80'
  Jul 29 16:12:23.296: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip 80\nConnection to affinity-clusterip 80 port [tcp/http] succeeded!\n"
  Jul 29 16:12:23.296: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:12:23.297: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5778 exec execpod-affinityp245z -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.21.226 80'
  Jul 29 16:12:23.586: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.21.226 80\nConnection to 10.233.21.226 80 port [tcp/http] succeeded!\n"
  Jul 29 16:12:23.586: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:12:23.586: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5778 exec execpod-affinityp245z -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.21.226:80/ ; done'
  Jul 29 16:12:24.014: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.21.226:80/\n"
  Jul 29 16:12:24.015: INFO: stdout: "\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn\naffinity-clusterip-kjrmn"
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Received response from host: affinity-clusterip-kjrmn
  Jul 29 16:12:24.015: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:12:24.024: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip in namespace services-5778, will wait for the garbage collector to delete the pods @ 07/29/23 16:12:24.053
  Jul 29 16:12:24.127: INFO: Deleting ReplicationController affinity-clusterip took: 11.515374ms
  Jul 29 16:12:24.228: INFO: Terminating ReplicationController affinity-clusterip pods took: 100.602821ms
  STEP: Destroying namespace "services-5778" for this suite. @ 07/29/23 16:12:26.558
• [9.815 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Garbage collector should not delete dependents that have both valid owner and owner that's waiting for dependents to be deleted [Conformance]
test/e2e/apimachinery/garbage_collector.go:713
  STEP: Creating a kubernetes client @ 07/29/23 16:12:26.569
  Jul 29 16:12:26.569: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename gc @ 07/29/23 16:12:26.571
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:26.601
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:26.606
  STEP: create the rc1 @ 07/29/23 16:12:26.619
  STEP: create the rc2 @ 07/29/23 16:12:26.629
  STEP: set half of pods created by rc simpletest-rc-to-be-deleted to have rc simpletest-rc-to-stay as owner as well @ 07/29/23 16:12:32.791
  STEP: delete the rc simpletest-rc-to-be-deleted @ 07/29/23 16:12:37.643
  STEP: wait for the rc to be deleted @ 07/29/23 16:12:37.702
  Jul 29 16:12:42.790: INFO: 95 pods remaining
  Jul 29 16:12:42.790: INFO: 73 pods has nil DeletionTimestamp
  Jul 29 16:12:42.794: INFO: 
  Jul 29 16:12:47.935: INFO: 83 pods remaining
  Jul 29 16:12:47.935: INFO: 50 pods has nil DeletionTimestamp
  Jul 29 16:12:47.935: INFO: 
  STEP: Gathering metrics @ 07/29/23 16:12:52.732
  Jul 29 16:12:52.931: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 16:12:52.931: INFO: Deleting pod "simpletest-rc-to-be-deleted-2442m" in namespace "gc-5097"
  Jul 29 16:12:52.956: INFO: Deleting pod "simpletest-rc-to-be-deleted-29ws7" in namespace "gc-5097"
  Jul 29 16:12:53.000: INFO: Deleting pod "simpletest-rc-to-be-deleted-2s5g7" in namespace "gc-5097"
  Jul 29 16:12:53.042: INFO: Deleting pod "simpletest-rc-to-be-deleted-2v6b9" in namespace "gc-5097"
  Jul 29 16:12:53.092: INFO: Deleting pod "simpletest-rc-to-be-deleted-42ww9" in namespace "gc-5097"
  Jul 29 16:12:53.117: INFO: Deleting pod "simpletest-rc-to-be-deleted-4f6fl" in namespace "gc-5097"
  Jul 29 16:12:53.213: INFO: Deleting pod "simpletest-rc-to-be-deleted-5c9ck" in namespace "gc-5097"
  Jul 29 16:12:53.317: INFO: Deleting pod "simpletest-rc-to-be-deleted-5d26h" in namespace "gc-5097"
  Jul 29 16:12:53.391: INFO: Deleting pod "simpletest-rc-to-be-deleted-5mxp6" in namespace "gc-5097"
  Jul 29 16:12:53.478: INFO: Deleting pod "simpletest-rc-to-be-deleted-6lpnf" in namespace "gc-5097"
  Jul 29 16:12:53.528: INFO: Deleting pod "simpletest-rc-to-be-deleted-6mtjh" in namespace "gc-5097"
  Jul 29 16:12:53.600: INFO: Deleting pod "simpletest-rc-to-be-deleted-6qx7z" in namespace "gc-5097"
  Jul 29 16:12:53.727: INFO: Deleting pod "simpletest-rc-to-be-deleted-7kql4" in namespace "gc-5097"
  Jul 29 16:12:53.803: INFO: Deleting pod "simpletest-rc-to-be-deleted-7m8xd" in namespace "gc-5097"
  Jul 29 16:12:53.862: INFO: Deleting pod "simpletest-rc-to-be-deleted-7pcgn" in namespace "gc-5097"
  Jul 29 16:12:53.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-7sf25" in namespace "gc-5097"
  Jul 29 16:12:54.087: INFO: Deleting pod "simpletest-rc-to-be-deleted-8224n" in namespace "gc-5097"
  Jul 29 16:12:54.267: INFO: Deleting pod "simpletest-rc-to-be-deleted-8kq5s" in namespace "gc-5097"
  Jul 29 16:12:54.336: INFO: Deleting pod "simpletest-rc-to-be-deleted-97k6l" in namespace "gc-5097"
  Jul 29 16:12:54.385: INFO: Deleting pod "simpletest-rc-to-be-deleted-9bgrh" in namespace "gc-5097"
  Jul 29 16:12:54.448: INFO: Deleting pod "simpletest-rc-to-be-deleted-9nwlf" in namespace "gc-5097"
  Jul 29 16:12:54.535: INFO: Deleting pod "simpletest-rc-to-be-deleted-9rrc8" in namespace "gc-5097"
  Jul 29 16:12:54.603: INFO: Deleting pod "simpletest-rc-to-be-deleted-9zhm8" in namespace "gc-5097"
  Jul 29 16:12:54.648: INFO: Deleting pod "simpletest-rc-to-be-deleted-b5xtd" in namespace "gc-5097"
  Jul 29 16:12:54.721: INFO: Deleting pod "simpletest-rc-to-be-deleted-bb4l4" in namespace "gc-5097"
  Jul 29 16:12:54.796: INFO: Deleting pod "simpletest-rc-to-be-deleted-bflhj" in namespace "gc-5097"
  Jul 29 16:12:54.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-bjnrs" in namespace "gc-5097"
  Jul 29 16:12:54.965: INFO: Deleting pod "simpletest-rc-to-be-deleted-bzdff" in namespace "gc-5097"
  Jul 29 16:12:55.041: INFO: Deleting pod "simpletest-rc-to-be-deleted-c7hzp" in namespace "gc-5097"
  Jul 29 16:12:55.080: INFO: Deleting pod "simpletest-rc-to-be-deleted-chzn6" in namespace "gc-5097"
  Jul 29 16:12:55.276: INFO: Deleting pod "simpletest-rc-to-be-deleted-cmgqg" in namespace "gc-5097"
  Jul 29 16:12:55.735: INFO: Deleting pod "simpletest-rc-to-be-deleted-cp2lj" in namespace "gc-5097"
  Jul 29 16:12:55.825: INFO: Deleting pod "simpletest-rc-to-be-deleted-cwj6l" in namespace "gc-5097"
  Jul 29 16:12:55.888: INFO: Deleting pod "simpletest-rc-to-be-deleted-dp8vx" in namespace "gc-5097"
  Jul 29 16:12:55.997: INFO: Deleting pod "simpletest-rc-to-be-deleted-drx9j" in namespace "gc-5097"
  Jul 29 16:12:56.040: INFO: Deleting pod "simpletest-rc-to-be-deleted-dthzn" in namespace "gc-5097"
  Jul 29 16:12:56.073: INFO: Deleting pod "simpletest-rc-to-be-deleted-f9rhd" in namespace "gc-5097"
  Jul 29 16:12:56.113: INFO: Deleting pod "simpletest-rc-to-be-deleted-fqrth" in namespace "gc-5097"
  Jul 29 16:12:56.168: INFO: Deleting pod "simpletest-rc-to-be-deleted-fsz7b" in namespace "gc-5097"
  Jul 29 16:12:56.272: INFO: Deleting pod "simpletest-rc-to-be-deleted-fthb9" in namespace "gc-5097"
  Jul 29 16:12:56.356: INFO: Deleting pod "simpletest-rc-to-be-deleted-g5xkb" in namespace "gc-5097"
  Jul 29 16:12:56.468: INFO: Deleting pod "simpletest-rc-to-be-deleted-gbjtx" in namespace "gc-5097"
  Jul 29 16:12:56.522: INFO: Deleting pod "simpletest-rc-to-be-deleted-gg5rs" in namespace "gc-5097"
  Jul 29 16:12:56.592: INFO: Deleting pod "simpletest-rc-to-be-deleted-gtcnr" in namespace "gc-5097"
  Jul 29 16:12:56.659: INFO: Deleting pod "simpletest-rc-to-be-deleted-gvzfj" in namespace "gc-5097"
  Jul 29 16:12:56.737: INFO: Deleting pod "simpletest-rc-to-be-deleted-hjzmq" in namespace "gc-5097"
  Jul 29 16:12:56.783: INFO: Deleting pod "simpletest-rc-to-be-deleted-jmvc7" in namespace "gc-5097"
  Jul 29 16:12:56.831: INFO: Deleting pod "simpletest-rc-to-be-deleted-jvrdz" in namespace "gc-5097"
  Jul 29 16:12:56.903: INFO: Deleting pod "simpletest-rc-to-be-deleted-k2hgn" in namespace "gc-5097"
  Jul 29 16:12:57.007: INFO: Deleting pod "simpletest-rc-to-be-deleted-kfwgh" in namespace "gc-5097"
  Jul 29 16:12:57.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5097" for this suite. @ 07/29/23 16:12:57.091
• [30.573 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] Deployment should run the lifecycle of a Deployment [Conformance]
test/e2e/apps/deployment.go:185
  STEP: Creating a kubernetes client @ 07/29/23 16:12:57.143
  Jul 29 16:12:57.143: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:12:57.15
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:12:57.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:12:57.246
  STEP: creating a Deployment @ 07/29/23 16:12:57.267
  STEP: waiting for Deployment to be created @ 07/29/23 16:12:57.285
  STEP: waiting for all Replicas to be Ready @ 07/29/23 16:12:57.294
  Jul 29 16:12:57.298: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:12:57.298: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:12:57.379: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:12:57.379: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:12:57.536: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:12:57.537: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:12:57.687: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:12:57.687: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0 and labels map[test-deployment-static:true]
  Jul 29 16:12:58.589: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul 29 16:12:58.590: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1 and labels map[test-deployment-static:true]
  Jul 29 16:13:03.832: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2 and labels map[test-deployment-static:true]
  STEP: patching the Deployment @ 07/29/23 16:13:03.832
  W0729 16:13:03.852421      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 16:13:03.855: INFO: observed event type ADDED
  STEP: waiting for Replicas to scale @ 07/29/23 16:13:03.856
  Jul 29 16:13:03.858: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0
  Jul 29 16:13:03.859: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0
  Jul 29 16:13:03.859: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0
  Jul 29 16:13:03.859: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0
  Jul 29 16:13:03.859: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0
  Jul 29 16:13:03.860: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0
  Jul 29 16:13:03.860: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0
  Jul 29 16:13:03.860: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 0
  Jul 29 16:13:03.861: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:03.861: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:03.861: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:03.862: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:03.862: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:03.862: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:03.886: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:03.886: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:03.964: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:03.964: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:04.030: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:04.031: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:04.046: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:04.046: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:05.635: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:05.636: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:05.678: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  STEP: listing Deployments @ 07/29/23 16:13:05.679
  Jul 29 16:13:05.687: INFO: Found test-deployment with labels: map[test-deployment:patched test-deployment-static:true]
  STEP: updating the Deployment @ 07/29/23 16:13:05.687
  Jul 29 16:13:05.708: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  STEP: fetching the DeploymentStatus @ 07/29/23 16:13:05.708
  Jul 29 16:13:05.721: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:05.745: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:05.862: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:05.937: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:05.986: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:07.688: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:07.724: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:07.766: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:07.793: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2 and labels map[test-deployment:updated test-deployment-static:true]
  Jul 29 16:13:10.891: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 3 and labels map[test-deployment:updated test-deployment-static:true]
  STEP: patching the DeploymentStatus @ 07/29/23 16:13:10.941
  STEP: fetching the DeploymentStatus @ 07/29/23 16:13:10.953
  Jul 29 16:13:10.965: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:10.965: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:10.966: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:10.967: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:10.967: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 1
  Jul 29 16:13:10.968: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:10.969: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:10.969: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:10.970: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 2
  Jul 29 16:13:10.971: INFO: observed Deployment test-deployment in namespace deployment-5024 with ReadyReplicas 3
  STEP: deleting the Deployment @ 07/29/23 16:13:10.971
  Jul 29 16:13:10.990: INFO: observed event type MODIFIED
  Jul 29 16:13:10.990: INFO: observed event type MODIFIED
  Jul 29 16:13:10.990: INFO: observed event type MODIFIED
  Jul 29 16:13:10.990: INFO: observed event type MODIFIED
  Jul 29 16:13:10.990: INFO: observed event type MODIFIED
  Jul 29 16:13:10.990: INFO: observed event type MODIFIED
  Jul 29 16:13:10.991: INFO: observed event type MODIFIED
  Jul 29 16:13:10.991: INFO: observed event type MODIFIED
  Jul 29 16:13:10.991: INFO: observed event type MODIFIED
  Jul 29 16:13:10.991: INFO: observed event type MODIFIED
  Jul 29 16:13:10.991: INFO: observed event type MODIFIED
  Jul 29 16:13:10.997: INFO: Log out all the ReplicaSets if there is no deployment created
  Jul 29 16:13:11.004: INFO: ReplicaSet "test-deployment-58db457f5f":
  &ReplicaSet{ObjectMeta:{test-deployment-58db457f5f  deployment-5024  89244c17-10e0-4417-aff5-0e1d660446c4 18523 3 2023-07-29 16:12:57 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment 5db2d395-7958-4a15-8c0b-ef2c389fc2c7 0xc0035bf277 0xc0035bf278}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:13:05 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db2d395-7958-4a15-8c0b-ef2c389fc2c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:13:05 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 58db457f5f,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:58db457f5f test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035bf410 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:3,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul 29 16:13:11.012: INFO: ReplicaSet "test-deployment-5b5dcbcd95":
  &ReplicaSet{ObjectMeta:{test-deployment-5b5dcbcd95  deployment-5024  5b22ef6a-c3d9-4ce7-acdb-84a9f1c8dfeb 18958 4 2023-07-29 16:13:03 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-deployment 5db2d395-7958-4a15-8c0b-ef2c389fc2c7 0xc0035bf557 0xc0035bf558}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:13:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db2d395-7958-4a15-8c0b-ef2c389fc2c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:13:10 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 5b5dcbcd95,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/pause:3.9 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035bf820 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:4,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}

  Jul 29 16:13:11.021: INFO: pod: "test-deployment-5b5dcbcd95-86mp7":
  &Pod{ObjectMeta:{test-deployment-5b5dcbcd95-86mp7 test-deployment-5b5dcbcd95- deployment-5024  81aef596-ebbf-418e-a486-8c4927bafcbd 18954 0 2023-07-29 16:13:03 +0000 UTC 2023-07-29 16:13:11 +0000 UTC 0xc0035fa0a8 map[pod-template-hash:5b5dcbcd95 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-5b5dcbcd95 5b22ef6a-c3d9-4ce7-acdb-84a9f1c8dfeb 0xc0035fa0d7 0xc0035fa0d8}] [] [{kube-controller-manager Update v1 2023-07-29 16:13:03 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5b22ef6a-c3d9-4ce7-acdb-84a9f1c8dfeb\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:13:05 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.94\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-fl6tm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/pause:3.9,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-fl6tm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:03 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:03 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.94,StartTime:2023-07-29 16:13:03 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:13:04 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/pause:3.9,ImageID:registry.k8s.io/pause@sha256:7031c1b283388d2c2e09b57badb803c05ebed362dc88d84b480cc47f72a21097,ContainerID:cri-o://72b9d3d767703de3721c52f17e8642265064d4d21c307d12e6f0325ba06de153,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.94,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 29 16:13:11.022: INFO: ReplicaSet "test-deployment-6fc78d85c6":
  &ReplicaSet{ObjectMeta:{test-deployment-6fc78d85c6  deployment-5024  f0f9ce92-1350-420f-bd6e-d8bab8a55246 18950 2 2023-07-29 16:13:05 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:3] [{apps/v1 Deployment test-deployment 5db2d395-7958-4a15-8c0b-ef2c389fc2c7 0xc0035bf927 0xc0035bf928}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:13:07 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"5db2d395-7958-4a15-8c0b-ef2c389fc2c7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:13:10 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{pod-template-hash: 6fc78d85c6,test-deployment-static: true,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [] [] []} {[] [] [{test-deployment registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0035bfa50 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:2,FullyLabeledReplicas:2,ObservedGeneration:2,ReadyReplicas:2,AvailableReplicas:2,Conditions:[]ReplicaSetCondition{},},}

  Jul 29 16:13:11.029: INFO: pod: "test-deployment-6fc78d85c6-llspq":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-llspq test-deployment-6fc78d85c6- deployment-5024  384d4b33-e039-4224-b5a8-41e68f83030d 18679 0 2023-07-29 16:13:05 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 f0f9ce92-1350-420f-bd6e-d8bab8a55246 0xc0035e4e37 0xc0035e4e38}] [] [{kube-controller-manager Update v1 2023-07-29 16:13:05 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0f9ce92-1350-420f-bd6e-d8bab8a55246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:13:07 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.163\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-t5kf4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-t5kf4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:05 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:05 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.163,StartTime:2023-07-29 16:13:05 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:13:06 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://191ab7f8c45a3d9964a5521af1afa102aef5a48e269f4324725d0751e2880211,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.163,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 29 16:13:11.029: INFO: pod: "test-deployment-6fc78d85c6-sg6h2":
  &Pod{ObjectMeta:{test-deployment-6fc78d85c6-sg6h2 test-deployment-6fc78d85c6- deployment-5024  a158c12a-9d4d-4d4c-9ca5-e6828743b21f 18949 0 2023-07-29 16:13:07 +0000 UTC <nil> <nil> map[pod-template-hash:6fc78d85c6 test-deployment-static:true] map[] [{apps/v1 ReplicaSet test-deployment-6fc78d85c6 f0f9ce92-1350-420f-bd6e-d8bab8a55246 0xc0035e5497 0xc0035e5498}] [] [{kube-controller-manager Update v1 2023-07-29 16:13:07 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:pod-template-hash":{},"f:test-deployment-static":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"f0f9ce92-1350-420f-bd6e-d8bab8a55246\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"test-deployment\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:13:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.79\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jjrss,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:test-deployment,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jjrss,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*1,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:07 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:13:07 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.39,PodIP:10.233.64.79,StartTime:2023-07-29 16:13:07 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:test-deployment,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:13:10 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://45c17b64f4459e9d4a5a780c7da776f1f743fb62ef3d38466705d9707913368b,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.79,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}

  Jul 29 16:13:11.030: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-5024" for this suite. @ 07/29/23 16:13:11.038
• [13.909 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition listing custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:85
  STEP: Creating a kubernetes client @ 07/29/23 16:13:11.054
  Jul 29 16:13:11.054: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 16:13:11.057
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:11.105
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:11.113
  Jul 29 16:13:11.124: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:13:17.703: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9701" for this suite. @ 07/29/23 16:13:17.71
• [6.664 seconds]
------------------------------
SSSSS
------------------------------
[sig-apps] Job should apply changes to a job status [Conformance]
test/e2e/apps/job.go:642
  STEP: Creating a kubernetes client @ 07/29/23 16:13:17.72
  Jul 29 16:13:17.720: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename job @ 07/29/23 16:13:17.722
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:17.749
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:17.753
  STEP: Creating a job @ 07/29/23 16:13:17.758
  STEP: Ensure pods equal to parallelism count is attached to the job @ 07/29/23 16:13:17.769
  STEP: patching /status @ 07/29/23 16:13:19.778
  STEP: updating /status @ 07/29/23 16:13:19.792
  STEP: get /status @ 07/29/23 16:13:19.806
  Jul 29 16:13:19.813: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7672" for this suite. @ 07/29/23 16:13:19.821
• [2.113 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a validating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:402
  STEP: Creating a kubernetes client @ 07/29/23 16:13:19.835
  Jul 29 16:13:19.835: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:13:19.837
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:19.871
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:19.876
  STEP: Setting up server cert @ 07/29/23 16:13:19.921
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:13:21.339
  STEP: Deploying the webhook pod @ 07/29/23 16:13:21.353
  STEP: Wait for the deployment to be ready @ 07/29/23 16:13:21.373
  Jul 29 16:13:21.382: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 16:13:23.41
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:13:23.434
  Jul 29 16:13:24.434: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a validating webhook configuration @ 07/29/23 16:13:24.445
  Jul 29 16:13:24.474: INFO: Waiting for webhook configuration to be ready...
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 16:13:24.603
  STEP: Updating a validating webhook configuration's rules to not include the create operation @ 07/29/23 16:13:24.621
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 16:13:24.645
  STEP: Patching a validating webhook configuration's rules to include the create operation @ 07/29/23 16:13:24.662
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 16:13:24.675
  Jul 29 16:13:24.693: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7723" for this suite. @ 07/29/23 16:13:24.777
  STEP: Destroying namespace "webhook-markers-9085" for this suite. @ 07/29/23 16:13:24.794
• [4.972 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:89
  STEP: Creating a kubernetes client @ 07/29/23 16:13:24.811
  Jul 29 16:13:24.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:13:24.813
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:24.855
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:24.861
  STEP: Creating configMap with name projected-configmap-test-volume-map-83880405-b746-43bf-ab6e-262699191ff8 @ 07/29/23 16:13:24.867
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:13:24.879
  STEP: Saw pod success @ 07/29/23 16:13:28.927
  Jul 29 16:13:28.932: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-configmaps-e38cd2b3-c985-4332-ae86-77e0656b1c38 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:13:28.944
  Jul 29 16:13:28.972: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9099" for this suite. @ 07/29/23 16:13:28.98
• [4.180 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for service with type clusterIP [LinuxOnly] [Conformance]
test/e2e/network/service.go:2187
  STEP: Creating a kubernetes client @ 07/29/23 16:13:29.002
  Jul 29 16:13:29.002: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 16:13:29.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:29.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:29.05
  STEP: creating service in namespace services-3717 @ 07/29/23 16:13:29.058
  STEP: creating service affinity-clusterip-transition in namespace services-3717 @ 07/29/23 16:13:29.058
  STEP: creating replication controller affinity-clusterip-transition in namespace services-3717 @ 07/29/23 16:13:29.091
  I0729 16:13:29.105787      13 runners.go:194] Created replication controller with name: affinity-clusterip-transition, namespace: services-3717, replica count: 3
  I0729 16:13:32.158515      13 runners.go:194] affinity-clusterip-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:13:32.174: INFO: Creating new exec pod
  Jul 29 16:13:35.202: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3717 exec execpod-affinitydj5nd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-clusterip-transition 80'
  Jul 29 16:13:35.485: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-clusterip-transition 80\nConnection to affinity-clusterip-transition 80 port [tcp/http] succeeded!\n"
  Jul 29 16:13:35.486: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:13:35.486: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3717 exec execpod-affinitydj5nd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.8.99 80'
  Jul 29 16:13:35.745: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.8.99 80\nConnection to 10.233.8.99 80 port [tcp/http] succeeded!\n"
  Jul 29 16:13:35.745: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:13:35.764: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3717 exec execpod-affinitydj5nd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.8.99:80/ ; done'
  Jul 29 16:13:36.239: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n"
  Jul 29 16:13:36.239: INFO: stdout: "\naffinity-clusterip-transition-fwfnm\naffinity-clusterip-transition-fwfnm\naffinity-clusterip-transition-phwl6\naffinity-clusterip-transition-fwfnm\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-phwl6\naffinity-clusterip-transition-phwl6\naffinity-clusterip-transition-fwfnm\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-fwfnm\naffinity-clusterip-transition-fwfnm\naffinity-clusterip-transition-fwfnm\naffinity-clusterip-transition-phwl6\naffinity-clusterip-transition-phwl6\naffinity-clusterip-transition-fwfnm"
  Jul 29 16:13:36.239: INFO: Received response from host: affinity-clusterip-transition-fwfnm
  Jul 29 16:13:36.239: INFO: Received response from host: affinity-clusterip-transition-fwfnm
  Jul 29 16:13:36.239: INFO: Received response from host: affinity-clusterip-transition-phwl6
  Jul 29 16:13:36.239: INFO: Received response from host: affinity-clusterip-transition-fwfnm
  Jul 29 16:13:36.239: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.239: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.239: INFO: Received response from host: affinity-clusterip-transition-phwl6
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-phwl6
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-fwfnm
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-fwfnm
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-fwfnm
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-fwfnm
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-phwl6
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-phwl6
  Jul 29 16:13:36.240: INFO: Received response from host: affinity-clusterip-transition-fwfnm
  Jul 29 16:13:36.260: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3717 exec execpod-affinitydj5nd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://10.233.8.99:80/ ; done'
  Jul 29 16:13:36.729: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n+ echo\n+ curl -q -s --connect-timeout 2 http://10.233.8.99:80/\n"
  Jul 29 16:13:36.729: INFO: stdout: "\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn\naffinity-clusterip-transition-lqmkn"
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Received response from host: affinity-clusterip-transition-lqmkn
  Jul 29 16:13:36.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:13:36.736: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-clusterip-transition in namespace services-3717, will wait for the garbage collector to delete the pods @ 07/29/23 16:13:36.762
  Jul 29 16:13:36.833: INFO: Deleting ReplicationController affinity-clusterip-transition took: 14.147788ms
  Jul 29 16:13:36.933: INFO: Terminating ReplicationController affinity-clusterip-transition pods took: 100.665314ms
  STEP: Destroying namespace "services-3717" for this suite. @ 07/29/23 16:13:39.242
• [10.251 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should list and delete a collection of DaemonSets [Conformance]
test/e2e/apps/daemon_set.go:836
  STEP: Creating a kubernetes client @ 07/29/23 16:13:39.255
  Jul 29 16:13:39.256: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 16:13:39.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:39.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:39.304
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 16:13:39.36
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 16:13:39.375
  Jul 29 16:13:39.394: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:13:39.394: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:13:40.427: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:13:40.427: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:13:41.414: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 16:13:41.414: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: listing all DaemonSets @ 07/29/23 16:13:41.42
  STEP: DeleteCollection of the DaemonSets @ 07/29/23 16:13:41.427
  STEP: Verify that ReplicaSets have been deleted @ 07/29/23 16:13:41.441
  Jul 29 16:13:41.468: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"19449"},"items":null}

  Jul 29 16:13:41.480: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"19450"},"items":[{"metadata":{"name":"daemon-set-29gzj","generateName":"daemon-set-","namespace":"daemonsets-59","uid":"10aa5f33-7d55-4b33-8067-89fb05dfd744","resourceVersion":"19450","creationTimestamp":"2023-07-29T16:13:39Z","deletionTimestamp":"2023-07-29T16:14:11Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ed33e23e-1b59-4ab5-9fd7-fb3e3a4e0bdd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:13:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed33e23e-1b59-4ab5-9fd7-fb3e3a4e0bdd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:13:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.71\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-nfwsm","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-nfwsm","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ci9axai7aiv7-3","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ci9axai7aiv7-3"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:39Z"}],"hostIP":"192.168.121.144","podIP":"10.233.66.71","podIPs":[{"ip":"10.233.66.71"}],"startTime":"2023-07-29T16:13:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:13:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://e72d1dc95104362586a57e28575e2335fb1fe3826472ce32b4e17aa900b6a112","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-psrhj","generateName":"daemon-set-","namespace":"daemonsets-59","uid":"2716b414-a470-4567-98ee-f5b397b925c2","resourceVersion":"19447","creationTimestamp":"2023-07-29T16:13:39Z","deletionTimestamp":"2023-07-29T16:14:11Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ed33e23e-1b59-4ab5-9fd7-fb3e3a4e0bdd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:13:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed33e23e-1b59-4ab5-9fd7-fb3e3a4e0bdd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:13:41Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.17\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-xnk5z","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-xnk5z","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ci9axai7aiv7-1","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ci9axai7aiv7-1"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:39Z"}],"hostIP":"192.168.121.39","podIP":"10.233.64.17","podIPs":[{"ip":"10.233.64.17"}],"startTime":"2023-07-29T16:13:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:13:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://c803b853e0e061a5011d05dbdfdb6eeda45da7b965fb9ab1166249735c9b5586","started":true}],"qosClass":"BestEffort"}},{"metadata":{"name":"daemon-set-sx7gv","generateName":"daemon-set-","namespace":"daemonsets-59","uid":"1b1ba555-e156-4c10-a05f-2cc3d865754f","resourceVersion":"19448","creationTimestamp":"2023-07-29T16:13:39Z","deletionTimestamp":"2023-07-29T16:14:11Z","deletionGracePeriodSeconds":30,"labels":{"controller-revision-hash":"6974d7cff5","daemonset-name":"daemon-set","pod-template-generation":"1"},"ownerReferences":[{"apiVersion":"apps/v1","kind":"DaemonSet","name":"daemon-set","uid":"ed33e23e-1b59-4ab5-9fd7-fb3e3a4e0bdd","controller":true,"blockOwnerDeletion":true}],"managedFields":[{"manager":"kube-controller-manager","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:13:39Z","fieldsType":"FieldsV1","fieldsV1":{"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:controller-revision-hash":{},"f:daemonset-name":{},"f:pod-template-generation":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ed33e23e-1b59-4ab5-9fd7-fb3e3a4e0bdd\"}":{}}},"f:spec":{"f:affinity":{".":{},"f:nodeAffinity":{".":{},"f:requiredDuringSchedulingIgnoredDuringExecution":{}}},"f:containers":{"k:{\"name\":\"app\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:ports":{".":{},"k:{\"containerPort\":9376,\"protocol\":\"TCP\"}":{".":{},"f:containerPort":{},"f:protocol":{}}},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{},"f:tolerations":{}}}},{"manager":"kubelet","operation":"Update","apiVersion":"v1","time":"2023-07-29T16:13:40Z","fieldsType":"FieldsV1","fieldsV1":{"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.171\"}":{".":{},"f:ip":{}}},"f:startTime":{}}},"subresource":"status"}]},"spec":{"volumes":[{"name":"kube-api-access-z5xbj","projected":{"sources":[{"serviceAccountToken":{"expirationSeconds":3607,"path":"token"}},{"configMap":{"name":"kube-root-ca.crt","items":[{"key":"ca.crt","path":"ca.crt"}]}},{"downwardAPI":{"items":[{"path":"namespace","fieldRef":{"apiVersion":"v1","fieldPath":"metadata.namespace"}}]}}],"defaultMode":420}}],"containers":[{"name":"app","image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","ports":[{"containerPort":9376,"protocol":"TCP"}],"resources":{},"volumeMounts":[{"name":"kube-api-access-z5xbj","readOnly":true,"mountPath":"/var/run/secrets/kubernetes.io/serviceaccount"}],"terminationMessagePath":"/dev/termination-log","terminationMessagePolicy":"File","imagePullPolicy":"IfNotPresent","securityContext":{}}],"restartPolicy":"Always","terminationGracePeriodSeconds":30,"dnsPolicy":"ClusterFirst","serviceAccountName":"default","serviceAccount":"default","nodeName":"ci9axai7aiv7-2","securityContext":{},"affinity":{"nodeAffinity":{"requiredDuringSchedulingIgnoredDuringExecution":{"nodeSelectorTerms":[{"matchFields":[{"key":"metadata.name","operator":"In","values":["ci9axai7aiv7-2"]}]}]}}},"schedulerName":"default-scheduler","tolerations":[{"key":"node.kubernetes.io/not-ready","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/unreachable","operator":"Exists","effect":"NoExecute"},{"key":"node.kubernetes.io/disk-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/memory-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/pid-pressure","operator":"Exists","effect":"NoSchedule"},{"key":"node.kubernetes.io/unschedulable","operator":"Exists","effect":"NoSchedule"}],"priority":0,"enableServiceLinks":true,"preemptionPolicy":"PreemptLowerPriority"},"status":{"phase":"Running","conditions":[{"type":"Initialized","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:39Z"},{"type":"Ready","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:40Z"},{"type":"ContainersReady","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:40Z"},{"type":"PodScheduled","status":"True","lastProbeTime":null,"lastTransitionTime":"2023-07-29T16:13:39Z"}],"hostIP":"192.168.121.49","podIP":"10.233.65.171","podIPs":[{"ip":"10.233.65.171"}],"startTime":"2023-07-29T16:13:39Z","containerStatuses":[{"name":"app","state":{"running":{"startedAt":"2023-07-29T16:13:40Z"}},"lastState":{},"ready":true,"restartCount":0,"image":"registry.k8s.io/e2e-test-images/httpd:2.4.38-4","imageID":"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22","containerID":"cri-o://16dc5c99fc242bec0ed966dbe0ab3b22ce64f61e904060b236f4c0ce162e0d35","started":true}],"qosClass":"BestEffort"}}]}

  Jul 29 16:13:41.510: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-59" for this suite. @ 07/29/23 16:13:41.518
• [2.279 seconds]
------------------------------
[sig-node] Downward API should provide pod name, namespace and IP address as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:45
  STEP: Creating a kubernetes client @ 07/29/23 16:13:41.536
  Jul 29 16:13:41.536: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:13:41.539
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:41.566
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:41.57
  STEP: Creating a pod to test downward api env vars @ 07/29/23 16:13:41.575
  STEP: Saw pod success @ 07/29/23 16:13:45.613
  Jul 29 16:13:45.618: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downward-api-aa5ad3a3-7416-4dd5-99e0-8ec757bcd34c container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:13:45.639
  Jul 29 16:13:45.668: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-8886" for this suite. @ 07/29/23 16:13:45.677
• [4.153 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for pods for Hostname [Conformance]
test/e2e/network/dns.go:244
  STEP: Creating a kubernetes client @ 07/29/23 16:13:45.691
  Jul 29 16:13:45.691: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename dns @ 07/29/23 16:13:45.693
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:45.719
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:45.723
  STEP: Creating a test headless service @ 07/29/23 16:13:45.727
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2930.svc.cluster.local)" && echo OK > /results/wheezy_hosts@dns-querier-2.dns-test-service-2.dns-2930.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/wheezy_hosts@dns-querier-2;sleep 1; done
   @ 07/29/23 16:13:45.736
  STEP: Running these commands on jessie: for i in `seq 1 600`; do test -n "$$(getent hosts dns-querier-2.dns-test-service-2.dns-2930.svc.cluster.local)" && echo OK > /results/jessie_hosts@dns-querier-2.dns-test-service-2.dns-2930.svc.cluster.local;test -n "$$(getent hosts dns-querier-2)" && echo OK > /results/jessie_hosts@dns-querier-2;sleep 1; done
   @ 07/29/23 16:13:45.736
  STEP: creating a pod to probe DNS @ 07/29/23 16:13:45.737
  STEP: submitting the pod to kubernetes @ 07/29/23 16:13:45.737
  STEP: retrieving the pod @ 07/29/23 16:13:49.789
  STEP: looking for the results for each expected name from probers @ 07/29/23 16:13:49.796
  Jul 29 16:13:49.833: INFO: DNS probes using dns-2930/dns-test-7ad7f68c-c065-46ad-a1c5-7cbaccb8bf22 succeeded

  Jul 29 16:13:49.834: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:13:49.846
  STEP: deleting the test headless service @ 07/29/23 16:13:49.895
  STEP: Destroying namespace "dns-2930" for this suite. @ 07/29/23 16:13:49.945
• [4.286 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should guarantee kube-root-ca.crt exist in any namespace [Conformance]
test/e2e/auth/service_accounts.go:740
  STEP: Creating a kubernetes client @ 07/29/23 16:13:49.983
  Jul 29 16:13:49.983: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 16:13:49.985
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:50.024
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:50.03
  Jul 29 16:13:50.040: INFO: Got root ca configmap in namespace "svcaccounts-7884"
  Jul 29 16:13:50.053: INFO: Deleted root ca configmap in namespace "svcaccounts-7884"
  STEP: waiting for a new root ca configmap created @ 07/29/23 16:13:50.554
  Jul 29 16:13:50.560: INFO: Recreated root ca configmap in namespace "svcaccounts-7884"
  Jul 29 16:13:50.570: INFO: Updated root ca configmap in namespace "svcaccounts-7884"
  STEP: waiting for the root ca configmap reconciled @ 07/29/23 16:13:51.071
  Jul 29 16:13:51.078: INFO: Reconciled root ca configmap in namespace "svcaccounts-7884"
  Jul 29 16:13:51.078: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-7884" for this suite. @ 07/29/23 16:13:51.086
• [1.117 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl logs logs should be able to retrieve and filter logs  [Conformance]
test/e2e/kubectl/logs.go:114
  STEP: Creating a kubernetes client @ 07/29/23 16:13:51.104
  Jul 29 16:13:51.104: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl-logs @ 07/29/23 16:13:51.109
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:51.139
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:51.143
  STEP: creating an pod @ 07/29/23 16:13:51.147
  Jul 29 16:13:51.148: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-logs-6097 run logs-generator --image=registry.k8s.io/e2e-test-images/agnhost:2.43 --restart=Never --pod-running-timeout=2m0s -- logs-generator --log-lines-total 100 --run-duration 20s'
  Jul 29 16:13:51.321: INFO: stderr: ""
  Jul 29 16:13:51.321: INFO: stdout: "pod/logs-generator created\n"
  STEP: Waiting for log generator to start. @ 07/29/23 16:13:51.321
  Jul 29 16:13:51.321: INFO: Waiting up to 5m0s for 1 pods to be running and ready, or succeeded: [logs-generator]
  Jul 29 16:13:53.343: INFO: Wanted all 1 pods to be running and ready, or succeeded. Result: true. Pods: [logs-generator]
  STEP: checking for a matching strings @ 07/29/23 16:13:53.343
  Jul 29 16:13:53.343: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-logs-6097 logs logs-generator logs-generator'
  Jul 29 16:13:53.532: INFO: stderr: ""
  Jul 29 16:13:53.532: INFO: stdout: "I0729 16:13:52.383113       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/rvd 516\nI0729 16:13:52.583256       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6xf 275\nI0729 16:13:52.783689       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/x7dw 515\nI0729 16:13:52.983040       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/gzn2 506\nI0729 16:13:53.183374       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/d5bt 462\nI0729 16:13:53.382969       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/6b4 464\n"
  STEP: limiting log lines @ 07/29/23 16:13:53.532
  Jul 29 16:13:53.533: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-logs-6097 logs logs-generator logs-generator --tail=1'
  Jul 29 16:13:53.719: INFO: stderr: ""
  Jul 29 16:13:53.719: INFO: stdout: "I0729 16:13:53.582908       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/mvfq 483\n"
  Jul 29 16:13:53.719: INFO: got output "I0729 16:13:53.582908       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/mvfq 483\n"
  STEP: limiting log bytes @ 07/29/23 16:13:53.719
  Jul 29 16:13:53.719: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-logs-6097 logs logs-generator logs-generator --limit-bytes=1'
  Jul 29 16:13:53.900: INFO: stderr: ""
  Jul 29 16:13:53.900: INFO: stdout: "I"
  Jul 29 16:13:53.900: INFO: got output "I"
  STEP: exposing timestamps @ 07/29/23 16:13:53.9
  Jul 29 16:13:53.901: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-logs-6097 logs logs-generator logs-generator --tail=1 --timestamps'
  Jul 29 16:13:54.063: INFO: stderr: ""
  Jul 29 16:13:54.063: INFO: stdout: "2023-07-29T16:13:53.983531114Z I0729 16:13:53.983447       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/v765 564\n"
  Jul 29 16:13:54.063: INFO: got output "2023-07-29T16:13:53.983531114Z I0729 16:13:53.983447       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/v765 564\n"
  STEP: restricting to a time range @ 07/29/23 16:13:54.063
  Jul 29 16:13:56.564: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-logs-6097 logs logs-generator logs-generator --since=1s'
  Jul 29 16:13:56.721: INFO: stderr: ""
  Jul 29 16:13:56.721: INFO: stdout: "I0729 16:13:55.783693       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/jjxv 212\nI0729 16:13:55.983002       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/ssqp 587\nI0729 16:13:56.183366       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/pvz 488\nI0729 16:13:56.383801       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/l47r 517\nI0729 16:13:56.583084       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/xkph 341\n"
  Jul 29 16:13:56.721: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-logs-6097 logs logs-generator logs-generator --since=24h'
  Jul 29 16:13:56.856: INFO: stderr: ""
  Jul 29 16:13:56.856: INFO: stdout: "I0729 16:13:52.383113       1 logs_generator.go:76] 0 POST /api/v1/namespaces/default/pods/rvd 516\nI0729 16:13:52.583256       1 logs_generator.go:76] 1 PUT /api/v1/namespaces/ns/pods/6xf 275\nI0729 16:13:52.783689       1 logs_generator.go:76] 2 GET /api/v1/namespaces/kube-system/pods/x7dw 515\nI0729 16:13:52.983040       1 logs_generator.go:76] 3 POST /api/v1/namespaces/kube-system/pods/gzn2 506\nI0729 16:13:53.183374       1 logs_generator.go:76] 4 PUT /api/v1/namespaces/default/pods/d5bt 462\nI0729 16:13:53.382969       1 logs_generator.go:76] 5 GET /api/v1/namespaces/kube-system/pods/6b4 464\nI0729 16:13:53.582908       1 logs_generator.go:76] 6 PUT /api/v1/namespaces/default/pods/mvfq 483\nI0729 16:13:53.782888       1 logs_generator.go:76] 7 GET /api/v1/namespaces/kube-system/pods/lpz 553\nI0729 16:13:53.983447       1 logs_generator.go:76] 8 POST /api/v1/namespaces/default/pods/v765 564\nI0729 16:13:54.182996       1 logs_generator.go:76] 9 POST /api/v1/namespaces/kube-system/pods/pbr 418\nI0729 16:13:54.383359       1 logs_generator.go:76] 10 POST /api/v1/namespaces/ns/pods/pghk 441\nI0729 16:13:54.583761       1 logs_generator.go:76] 11 POST /api/v1/namespaces/kube-system/pods/2m8f 271\nI0729 16:13:54.783146       1 logs_generator.go:76] 12 POST /api/v1/namespaces/default/pods/b9j 211\nI0729 16:13:54.983612       1 logs_generator.go:76] 13 POST /api/v1/namespaces/default/pods/q9g 520\nI0729 16:13:55.183729       1 logs_generator.go:76] 14 POST /api/v1/namespaces/default/pods/jrb 434\nI0729 16:13:55.382949       1 logs_generator.go:76] 15 PUT /api/v1/namespaces/kube-system/pods/ph4z 425\nI0729 16:13:55.583320       1 logs_generator.go:76] 16 GET /api/v1/namespaces/kube-system/pods/498 311\nI0729 16:13:55.783693       1 logs_generator.go:76] 17 PUT /api/v1/namespaces/kube-system/pods/jjxv 212\nI0729 16:13:55.983002       1 logs_generator.go:76] 18 GET /api/v1/namespaces/ns/pods/ssqp 587\nI0729 16:13:56.183366       1 logs_generator.go:76] 19 GET /api/v1/namespaces/kube-system/pods/pvz 488\nI0729 16:13:56.383801       1 logs_generator.go:76] 20 POST /api/v1/namespaces/default/pods/l47r 517\nI0729 16:13:56.583084       1 logs_generator.go:76] 21 POST /api/v1/namespaces/ns/pods/xkph 341\nI0729 16:13:56.783638       1 logs_generator.go:76] 22 GET /api/v1/namespaces/kube-system/pods/vhh 225\n"
  Jul 29 16:13:56.857: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-logs-6097 delete pod logs-generator'
  Jul 29 16:13:57.975: INFO: stderr: ""
  Jul 29 16:13:57.975: INFO: stdout: "pod \"logs-generator\" deleted\n"
  Jul 29 16:13:57.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-logs-6097" for this suite. @ 07/29/23 16:13:57.986
• [6.892 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with backticks [Slow] [Conformance]
test/e2e/common/node/expansion.go:155
  STEP: Creating a kubernetes client @ 07/29/23 16:13:57.997
  Jul 29 16:13:57.997: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:13:57.999
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:13:58.033
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:13:58.039
  Jul 29 16:14:00.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:14:00.085: INFO: Deleting pod "var-expansion-8dbf0caa-4bd0-4259-9b8b-a68565545138" in namespace "var-expansion-4072"
  Jul 29 16:14:00.100: INFO: Wait up to 5m0s for pod "var-expansion-8dbf0caa-4bd0-4259-9b8b-a68565545138" to be fully deleted
  STEP: Destroying namespace "var-expansion-4072" for this suite. @ 07/29/23 16:14:02.131
• [4.146 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/configmap_volume.go:504
  STEP: Creating a kubernetes client @ 07/29/23 16:14:02.146
  Jul 29 16:14:02.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:14:02.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:02.257
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:02.262
  Jul 29 16:14:02.318: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-8084" for this suite. @ 07/29/23 16:14:02.325
• [0.187 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny attaching pod [Conformance]
test/e2e/apimachinery/webhook.go:209
  STEP: Creating a kubernetes client @ 07/29/23 16:14:02.344
  Jul 29 16:14:02.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:14:02.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:02.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:02.375
  STEP: Setting up server cert @ 07/29/23 16:14:02.422
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:14:03.487
  STEP: Deploying the webhook pod @ 07/29/23 16:14:03.505
  STEP: Wait for the deployment to be ready @ 07/29/23 16:14:03.532
  Jul 29 16:14:03.542: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 16:14:05.564
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:14:05.587
  Jul 29 16:14:06.588: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/29/23 16:14:06.598
  STEP: create a pod @ 07/29/23 16:14:06.627
  STEP: 'kubectl attach' the pod, should be denied by the webhook @ 07/29/23 16:14:08.66
  Jul 29 16:14:08.661: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=webhook-476 attach --namespace=webhook-476 to-be-attached-pod -i -c=container1'
  Jul 29 16:14:08.849: INFO: rc: 1
  Jul 29 16:14:08.849: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-476" for this suite. @ 07/29/23 16:14:08.963
  STEP: Destroying namespace "webhook-markers-9098" for this suite. @ 07/29/23 16:14:08.981
• [6.658 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test when starting a container that exits should run with the expected status [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:52
  STEP: Creating a kubernetes client @ 07/29/23 16:14:09.004
  Jul 29 16:14:09.004: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 16:14:09.009
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:09.052
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:09.056
  STEP: Container 'terminate-cmd-rpa': should get the expected 'RestartCount' @ 07/29/23 16:14:09.072
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Phase' @ 07/29/23 16:14:25.264
  STEP: Container 'terminate-cmd-rpa': should get the expected 'Ready' condition @ 07/29/23 16:14:25.272
  STEP: Container 'terminate-cmd-rpa': should get the expected 'State' @ 07/29/23 16:14:25.283
  STEP: Container 'terminate-cmd-rpa': should be possible to delete [NodeConformance] @ 07/29/23 16:14:25.283
  STEP: Container 'terminate-cmd-rpof': should get the expected 'RestartCount' @ 07/29/23 16:14:25.314
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Phase' @ 07/29/23 16:14:28.348
  STEP: Container 'terminate-cmd-rpof': should get the expected 'Ready' condition @ 07/29/23 16:14:29.361
  STEP: Container 'terminate-cmd-rpof': should get the expected 'State' @ 07/29/23 16:14:29.38
  STEP: Container 'terminate-cmd-rpof': should be possible to delete [NodeConformance] @ 07/29/23 16:14:29.38
  STEP: Container 'terminate-cmd-rpn': should get the expected 'RestartCount' @ 07/29/23 16:14:29.422
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Phase' @ 07/29/23 16:14:30.443
  STEP: Container 'terminate-cmd-rpn': should get the expected 'Ready' condition @ 07/29/23 16:14:32.461
  STEP: Container 'terminate-cmd-rpn': should get the expected 'State' @ 07/29/23 16:14:32.473
  STEP: Container 'terminate-cmd-rpn': should be possible to delete [NodeConformance] @ 07/29/23 16:14:32.473
  Jul 29 16:14:32.504: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-5258" for this suite. @ 07/29/23 16:14:32.528
• [23.533 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:134
  STEP: Creating a kubernetes client @ 07/29/23 16:14:32.541
  Jul 29 16:14:32.541: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:14:32.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:14:32.572
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:14:32.576
  STEP: Creating pod busybox-88c719dd-2f41-4d1a-b24f-9cddbec38bb4 in namespace container-probe-1224 @ 07/29/23 16:14:32.581
  Jul 29 16:14:34.624: INFO: Started pod busybox-88c719dd-2f41-4d1a-b24f-9cddbec38bb4 in namespace container-probe-1224
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:14:34.624
  Jul 29 16:14:34.631: INFO: Initial restart count of pod busybox-88c719dd-2f41-4d1a-b24f-9cddbec38bb4 is 0
  Jul 29 16:15:24.862: INFO: Restart count of pod container-probe-1224/busybox-88c719dd-2f41-4d1a-b24f-9cddbec38bb4 is now 1 (50.231624838s elapsed)
  Jul 29 16:15:24.863: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:15:24.875
  STEP: Destroying namespace "container-probe-1224" for this suite. @ 07/29/23 16:15:24.915
• [52.386 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should delete pods created by rc when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:321
  STEP: Creating a kubernetes client @ 07/29/23 16:15:24.928
  Jul 29 16:15:24.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename gc @ 07/29/23 16:15:24.934
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:15:24.971
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:15:24.975
  STEP: create the rc @ 07/29/23 16:15:24.979
  W0729 16:15:24.987128      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/29/23 16:15:30
  STEP: wait for all pods to be garbage collected @ 07/29/23 16:15:30.019
  STEP: Gathering metrics @ 07/29/23 16:15:35.033
  Jul 29 16:15:35.200: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 16:15:35.203: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-2561" for this suite. @ 07/29/23 16:15:35.211
• [10.292 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD with validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:69
  STEP: Creating a kubernetes client @ 07/29/23 16:15:35.224
  Jul 29 16:15:35.224: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 16:15:35.227
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:15:35.26
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:15:35.264
  Jul 29 16:15:35.269: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: kubectl validation (kubectl create and apply) allows request with known and required properties @ 07/29/23 16:15:37.085
  Jul 29 16:15:37.086: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 create -f -'
  Jul 29 16:15:38.768: INFO: stderr: ""
  Jul 29 16:15:38.768: INFO: stdout: "e2e-test-crd-publish-openapi-42-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 29 16:15:38.769: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 delete e2e-test-crd-publish-openapi-42-crds test-foo'
  Jul 29 16:15:39.026: INFO: stderr: ""
  Jul 29 16:15:39.027: INFO: stdout: "e2e-test-crd-publish-openapi-42-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  Jul 29 16:15:39.027: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 apply -f -'
  Jul 29 16:15:39.542: INFO: stderr: ""
  Jul 29 16:15:39.542: INFO: stdout: "e2e-test-crd-publish-openapi-42-crd.crd-publish-openapi-test-foo.example.com/test-foo created\n"
  Jul 29 16:15:39.543: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 delete e2e-test-crd-publish-openapi-42-crds test-foo'
  Jul 29 16:15:39.694: INFO: stderr: ""
  Jul 29 16:15:39.694: INFO: stdout: "e2e-test-crd-publish-openapi-42-crd.crd-publish-openapi-test-foo.example.com \"test-foo\" deleted\n"
  STEP: kubectl validation (kubectl create and apply) rejects request with value outside defined enum values @ 07/29/23 16:15:39.694
  Jul 29 16:15:39.695: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 create -f -'
  Jul 29 16:15:40.115: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request with unknown properties when disallowed by the schema @ 07/29/23 16:15:40.115
  Jul 29 16:15:40.115: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 create -f -'
  Jul 29 16:15:41.341: INFO: rc: 1
  Jul 29 16:15:41.341: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 apply -f -'
  Jul 29 16:15:41.751: INFO: rc: 1
  STEP: kubectl validation (kubectl create and apply) rejects request without required properties @ 07/29/23 16:15:41.751
  Jul 29 16:15:41.751: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 create -f -'
  Jul 29 16:15:42.193: INFO: rc: 1
  Jul 29 16:15:42.193: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 --namespace=crd-publish-openapi-40 apply -f -'
  Jul 29 16:15:42.596: INFO: rc: 1
  STEP: kubectl explain works to explain CR properties @ 07/29/23 16:15:42.597
  Jul 29 16:15:42.597: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 explain e2e-test-crd-publish-openapi-42-crds'
  Jul 29 16:15:43.039: INFO: stderr: ""
  Jul 29 16:15:43.039: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-42-crd\nVERSION:    v1\n\nDESCRIPTION:\n    Foo CRD for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Foo\n\n  status\t<Object>\n    Status of Foo\n\n\n"
  STEP: kubectl explain works to explain CR properties recursively @ 07/29/23 16:15:43.04
  Jul 29 16:15:43.040: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 explain e2e-test-crd-publish-openapi-42-crds.metadata'
  Jul 29 16:15:43.462: INFO: stderr: ""
  Jul 29 16:15:43.462: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-42-crd\nVERSION:    v1\n\nFIELD: metadata <ObjectMeta>\n\nDESCRIPTION:\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n    ObjectMeta is metadata that all persisted resources must have, which\n    includes all objects users must create.\n    \nFIELDS:\n  annotations\t<map[string]string>\n    Annotations is an unstructured key value map stored with a resource that may\n    be set by external tools to store and retrieve arbitrary metadata. They are\n    not queryable and should be preserved when modifying objects. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/annotations\n\n  creationTimestamp\t<string>\n    CreationTimestamp is a timestamp representing the server time when this\n    object was created. It is not guaranteed to be set in happens-before order\n    across separate operations. Clients may not set this value. It is\n    represented in RFC3339 form and is in UTC.\n    \n    Populated by the system. Read-only. Null for lists. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  deletionGracePeriodSeconds\t<integer>\n    Number of seconds allowed for this object to gracefully terminate before it\n    will be removed from the system. Only set when deletionTimestamp is also\n    set. May only be shortened. Read-only.\n\n  deletionTimestamp\t<string>\n    DeletionTimestamp is RFC 3339 date and time at which this resource will be\n    deleted. This field is set by the server when a graceful deletion is\n    requested by the user, and is not directly settable by a client. The\n    resource is expected to be deleted (no longer visible from resource lists,\n    and not reachable by name) after the time in this field, once the finalizers\n    list is empty. As long as the finalizers list contains items, deletion is\n    blocked. Once the deletionTimestamp is set, this value may not be unset or\n    be set further into the future, although it may be shortened or the resource\n    may be deleted prior to this time. For example, a user may request that a\n    pod is deleted in 30 seconds. The Kubelet will react by sending a graceful\n    termination signal to the containers in the pod. After that 30 seconds, the\n    Kubelet will send a hard termination signal (SIGKILL) to the container and\n    after cleanup, remove the pod from the API. In the presence of network\n    partitions, this object may still exist after this timestamp, until an\n    administrator or automated process can determine the resource is fully\n    terminated. If not set, graceful deletion of the object has not been\n    requested.\n    \n    Populated by the system when a graceful deletion is requested. Read-only.\n    More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  finalizers\t<[]string>\n    Must be empty before the object is deleted from the registry. Each entry is\n    an identifier for the responsible component that will remove the entry from\n    the list. If the deletionTimestamp of the object is non-nil, entries in this\n    list can only be removed. Finalizers may be processed and removed in any\n    order.  Order is NOT enforced because it introduces significant risk of\n    stuck finalizers. finalizers is a shared field, any actor with permission\n    can reorder it. If the finalizer list is processed in order, then this can\n    lead to a situation in which the component responsible for the first\n    finalizer in the list is waiting for a signal (field value, external system,\n    or other) produced by a component responsible for a finalizer later in the\n    list, resulting in a deadlock. Without enforced ordering finalizers are free\n    to order amongst themselves and are not vulnerable to ordering changes in\n    the list.\n\n  generateName\t<string>\n    GenerateName is an optional prefix, used by the server, to generate a unique\n    name ONLY IF the Name field has not been provided. If this field is used,\n    the name returned to the client will be different than the name passed. This\n    value will also be combined with a unique suffix. The provided value has the\n    same validation rules as the Name field, and may be truncated by the length\n    of the suffix required to make the value unique on the server.\n    \n    If this field is specified and the generated name exists, the server will\n    return a 409.\n    \n    Applied only if Name is not specified. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#idempotency\n\n  generation\t<integer>\n    A sequence number representing a specific generation of the desired state.\n    Populated by the system. Read-only.\n\n  labels\t<map[string]string>\n    Map of string keys and values that can be used to organize and categorize\n    (scope and select) objects. May match selectors of replication controllers\n    and services. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/labels\n\n  managedFields\t<[]ManagedFieldsEntry>\n    ManagedFields maps workflow-id and version to the set of fields that are\n    managed by that workflow. This is mostly for internal housekeeping, and\n    users typically shouldn't need to set or understand this field. A workflow\n    can be the user's name, a controller's name, or the name of a specific apply\n    path like \"ci-cd\". The set of fields is always in the version that the\n    workflow used when modifying the object.\n\n  name\t<string>\n    Name must be unique within a namespace. Is required when creating resources,\n    although some resources may allow a client to request the generation of an\n    appropriate name automatically. Name is primarily intended for creation\n    idempotence and configuration definition. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#names\n\n  namespace\t<string>\n    Namespace defines the space within which each name must be unique. An empty\n    namespace is equivalent to the \"default\" namespace, but \"default\" is the\n    canonical representation. Not all objects are required to be scoped to a\n    namespace - the value of this field for those objects will be empty.\n    \n    Must be a DNS_LABEL. Cannot be updated. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces\n\n  ownerReferences\t<[]OwnerReference>\n    List of objects depended by this object. If ALL objects in the list have\n    been deleted, this object will be garbage collected. If this object is\n    managed by a controller, then an entry in this list will point to this\n    controller, with the controller field set to true. There cannot be more than\n    one managing controller.\n\n  resourceVersion\t<string>\n    An opaque value that represents the internal version of this object that can\n    be used by clients to determine when objects have changed. May be used for\n    optimistic concurrency, change detection, and the watch operation on a\n    resource or set of resources. Clients must treat these values as opaque and\n    passed unmodified back to the server. They may only be valid for a\n    particular resource or set of resources.\n    \n    Populated by the system. Read-only. Value must be treated as opaque by\n    clients and . More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#concurrency-control-and-consistency\n\n  selfLink\t<string>\n    Deprecated: selfLink is a legacy read-only field that is no longer populated\n    by the system.\n\n  uid\t<string>\n    UID is the unique in time and space value for this object. It is typically\n    generated by the server on successful creation of a resource and is not\n    allowed to change on PUT operations.\n    \n    Populated by the system. Read-only. More info:\n    https://kubernetes.io/docs/concepts/overview/working-with-objects/names#uids\n\n\n"
  Jul 29 16:15:43.463: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 explain e2e-test-crd-publish-openapi-42-crds.spec'
  Jul 29 16:15:43.881: INFO: stderr: ""
  Jul 29 16:15:43.881: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-42-crd\nVERSION:    v1\n\nFIELD: spec <Object>\n\nDESCRIPTION:\n    Specification of Foo\n    \nFIELDS:\n  bars\t<[]Object>\n    List of Bars and their specs.\n\n\n"
  Jul 29 16:15:43.881: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 explain e2e-test-crd-publish-openapi-42-crds.spec.bars'
  Jul 29 16:15:44.282: INFO: stderr: ""
  Jul 29 16:15:44.282: INFO: stdout: "GROUP:      crd-publish-openapi-test-foo.example.com\nKIND:       e2e-test-crd-publish-openapi-42-crd\nVERSION:    v1\n\nFIELD: bars <[]Object>\n\nDESCRIPTION:\n    List of Bars and their specs.\n    \nFIELDS:\n  age\t<string>\n    Age of Bar.\n\n  bazs\t<[]string>\n    List of Bazs.\n\n  feeling\t<string>\n    Whether Bar is feeling great.\n\n  name\t<string> -required-\n    Name of Bar.\n\n\n"
  STEP: kubectl explain works to return error when explain is called on property that doesn't exist @ 07/29/23 16:15:44.283
  Jul 29 16:15:44.283: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-40 explain e2e-test-crd-publish-openapi-42-crds.spec.bars2'
  Jul 29 16:15:44.667: INFO: rc: 1
  Jul 29 16:15:46.419: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-40" for this suite. @ 07/29/23 16:15:46.442
• [11.234 seconds]
------------------------------
SS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should have a working scale subresource [Conformance]
test/e2e/apps/statefulset.go:852
  STEP: Creating a kubernetes client @ 07/29/23 16:15:46.461
  Jul 29 16:15:46.461: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:15:46.464
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:15:46.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:15:46.5
  STEP: Creating service test in namespace statefulset-1273 @ 07/29/23 16:15:46.505
  STEP: Creating statefulset ss in namespace statefulset-1273 @ 07/29/23 16:15:46.524
  Jul 29 16:15:46.539: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 16:15:56.551: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: getting scale subresource @ 07/29/23 16:15:56.563
  STEP: updating a scale subresource @ 07/29/23 16:15:56.571
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/29/23 16:15:56.582
  STEP: Patch a scale subresource @ 07/29/23 16:15:56.592
  STEP: verifying the statefulset Spec.Replicas was modified @ 07/29/23 16:15:56.611
  Jul 29 16:15:56.618: INFO: Deleting all statefulset in ns statefulset-1273
  Jul 29 16:15:56.626: INFO: Scaling statefulset ss to 0
  Jul 29 16:16:06.712: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:16:06.716: INFO: Deleting statefulset ss
  Jul 29 16:16:06.738: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-1273" for this suite. @ 07/29/23 16:16:06.751
• [20.302 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] HostPort validates that there is no conflict between pods with same hostPort but different hostIP and protocol [LinuxOnly] [Conformance]
test/e2e/network/hostport.go:63
  STEP: Creating a kubernetes client @ 07/29/23 16:16:06.77
  Jul 29 16:16:06.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename hostport @ 07/29/23 16:16:06.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:06.802
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:06.807
  STEP: Trying to create a pod(pod1) with hostport 54323 and hostIP 127.0.0.1 and expect scheduled @ 07/29/23 16:16:06.82
  STEP: Trying to create another pod(pod2) with hostport 54323 but hostIP 192.168.121.49 on the node which pod1 resides and expect scheduled @ 07/29/23 16:16:08.865
  STEP: Trying to create a third pod(pod3) with hostport 54323, hostIP 192.168.121.49 but use UDP protocol on the node which pod2 resides @ 07/29/23 16:16:10.896
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 127.0.0.1, port: 54323 @ 07/29/23 16:16:25.023
  Jul 29 16:16:25.023: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 --interface 192.168.121.49 http://127.0.0.1:54323/hostname] Namespace:hostport-5018 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:16:25.023: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:16:25.027: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:16:25.028: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5018/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+--interface+192.168.121.49+http%3A%2F%2F127.0.0.1%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.49, port: 54323 @ 07/29/23 16:16:25.222
  Jul 29 16:16:25.223: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g --connect-timeout 5 http://192.168.121.49:54323/hostname] Namespace:hostport-5018 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:16:25.223: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:16:25.224: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:16:25.224: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5018/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+--connect-timeout+5+http%3A%2F%2F192.168.121.49%3A54323%2Fhostname&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  STEP: checking connectivity from pod e2e-host-exec to serverIP: 192.168.121.49, port: 54323 UDP @ 07/29/23 16:16:25.347
  Jul 29 16:16:25.348: INFO: ExecWithOptions {Command:[/bin/sh -c echo hostname | nc -u -w 5 192.168.121.49 54323] Namespace:hostport-5018 PodName:e2e-host-exec ContainerName:e2e-host-exec Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:16:25.348: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:16:25.350: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:16:25.351: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/hostport-5018/pods/e2e-host-exec/exec?command=%2Fbin%2Fsh&command=-c&command=echo+hostname+%7C+nc+-u+-w+5+192.168.121.49+54323&container=e2e-host-exec&container=e2e-host-exec&stderr=true&stdout=true)
  Jul 29 16:16:30.451: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "hostport-5018" for this suite. @ 07/29/23 16:16:30.463
• [23.709 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Service endpoints latency should not be very high  [Conformance]
test/e2e/network/service_latency.go:59
  STEP: Creating a kubernetes client @ 07/29/23 16:16:30.487
  Jul 29 16:16:30.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename svc-latency @ 07/29/23 16:16:30.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:30.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:30.527
  Jul 29 16:16:30.534: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: creating replication controller svc-latency-rc in namespace svc-latency-7982 @ 07/29/23 16:16:30.537
  I0729 16:16:30.554621      13 runners.go:194] Created replication controller with name: svc-latency-rc, namespace: svc-latency-7982, replica count: 1
  I0729 16:16:31.606364      13 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 16:16:32.607000      13 runners.go:194] svc-latency-rc Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:16:32.729: INFO: Created: latency-svc-dm296
  Jul 29 16:16:32.742: INFO: Got endpoints: latency-svc-dm296 [34.544511ms]
  Jul 29 16:16:32.776: INFO: Created: latency-svc-58pkp
  Jul 29 16:16:32.790: INFO: Created: latency-svc-9ntzz
  Jul 29 16:16:32.810: INFO: Got endpoints: latency-svc-58pkp [67.008725ms]
  Jul 29 16:16:32.813: INFO: Created: latency-svc-k57k4
  Jul 29 16:16:32.841: INFO: Got endpoints: latency-svc-9ntzz [99.3315ms]
  Jul 29 16:16:32.843: INFO: Got endpoints: latency-svc-k57k4 [100.635991ms]
  Jul 29 16:16:32.850: INFO: Created: latency-svc-6vh9f
  Jul 29 16:16:32.865: INFO: Created: latency-svc-qlzqj
  Jul 29 16:16:32.866: INFO: Got endpoints: latency-svc-6vh9f [123.277836ms]
  Jul 29 16:16:32.875: INFO: Got endpoints: latency-svc-qlzqj [131.827069ms]
  Jul 29 16:16:32.886: INFO: Created: latency-svc-frk2p
  Jul 29 16:16:32.899: INFO: Created: latency-svc-cbhhz
  Jul 29 16:16:32.907: INFO: Got endpoints: latency-svc-frk2p [163.941121ms]
  Jul 29 16:16:32.913: INFO: Got endpoints: latency-svc-cbhhz [169.565464ms]
  Jul 29 16:16:32.919: INFO: Created: latency-svc-gdhs4
  Jul 29 16:16:32.931: INFO: Created: latency-svc-fvp76
  Jul 29 16:16:32.933: INFO: Got endpoints: latency-svc-gdhs4 [189.020262ms]
  Jul 29 16:16:32.948: INFO: Created: latency-svc-p2qhw
  Jul 29 16:16:32.954: INFO: Got endpoints: latency-svc-fvp76 [210.113997ms]
  Jul 29 16:16:32.960: INFO: Got endpoints: latency-svc-p2qhw [216.39266ms]
  Jul 29 16:16:33.081: INFO: Created: latency-svc-4sdsw
  Jul 29 16:16:33.086: INFO: Created: latency-svc-dz67m
  Jul 29 16:16:33.086: INFO: Created: latency-svc-6d79c
  Jul 29 16:16:33.120: INFO: Created: latency-svc-vvtlp
  Jul 29 16:16:33.120: INFO: Created: latency-svc-vghls
  Jul 29 16:16:33.121: INFO: Created: latency-svc-2rtp9
  Jul 29 16:16:33.121: INFO: Created: latency-svc-lc8lz
  Jul 29 16:16:33.121: INFO: Created: latency-svc-ptlsm
  Jul 29 16:16:33.121: INFO: Created: latency-svc-9qrqt
  Jul 29 16:16:33.122: INFO: Created: latency-svc-nxq84
  Jul 29 16:16:33.129: INFO: Created: latency-svc-r2rvq
  Jul 29 16:16:33.129: INFO: Created: latency-svc-kcsw7
  Jul 29 16:16:33.130: INFO: Created: latency-svc-5szh8
  Jul 29 16:16:33.130: INFO: Created: latency-svc-hs7dp
  Jul 29 16:16:33.130: INFO: Created: latency-svc-jmmvh
  Jul 29 16:16:33.178: INFO: Got endpoints: latency-svc-2rtp9 [433.774152ms]
  Jul 29 16:16:33.178: INFO: Got endpoints: latency-svc-4sdsw [224.10599ms]
  Jul 29 16:16:33.179: INFO: Got endpoints: latency-svc-6d79c [434.659315ms]
  Jul 29 16:16:33.191: INFO: Got endpoints: latency-svc-dz67m [349.875317ms]
  Jul 29 16:16:33.191: INFO: Got endpoints: latency-svc-lc8lz [316.217582ms]
  Jul 29 16:16:33.223: INFO: Got endpoints: latency-svc-9qrqt [262.453403ms]
  Jul 29 16:16:33.223: INFO: Got endpoints: latency-svc-r2rvq [413.164496ms]
  Jul 29 16:16:33.231: INFO: Got endpoints: latency-svc-hs7dp [323.917357ms]
  Jul 29 16:16:33.237: INFO: Created: latency-svc-7h2vc
  Jul 29 16:16:33.257: INFO: Got endpoints: latency-svc-vvtlp [513.005351ms]
  Jul 29 16:16:33.262: INFO: Got endpoints: latency-svc-vghls [418.516457ms]
  Jul 29 16:16:33.262: INFO: Got endpoints: latency-svc-ptlsm [518.430008ms]
  Jul 29 16:16:33.274: INFO: Got endpoints: latency-svc-kcsw7 [407.446592ms]
  Jul 29 16:16:33.275: INFO: Created: latency-svc-47c5r
  Jul 29 16:16:33.289: INFO: Got endpoints: latency-svc-jmmvh [356.22652ms]
  Jul 29 16:16:33.300: INFO: Got endpoints: latency-svc-5szh8 [556.759121ms]
  Jul 29 16:16:33.322: INFO: Created: latency-svc-hwxsj
  Jul 29 16:16:33.322: INFO: Got endpoints: latency-svc-nxq84 [409.360978ms]
  Jul 29 16:16:33.325: INFO: Got endpoints: latency-svc-47c5r [133.094227ms]
  Jul 29 16:16:33.325: INFO: Got endpoints: latency-svc-7h2vc [147.367312ms]
  Jul 29 16:16:33.326: INFO: Created: latency-svc-jr6f6
  Jul 29 16:16:33.339: INFO: Created: latency-svc-w9nz4
  Jul 29 16:16:33.349: INFO: Got endpoints: latency-svc-hwxsj [169.755587ms]
  Jul 29 16:16:33.357: INFO: Got endpoints: latency-svc-jr6f6 [94.113581ms]
  Jul 29 16:16:33.357: INFO: Got endpoints: latency-svc-w9nz4 [83.362549ms]
  Jul 29 16:16:33.364: INFO: Created: latency-svc-g2bt2
  Jul 29 16:16:33.370: INFO: Got endpoints: latency-svc-g2bt2 [178.541577ms]
  Jul 29 16:16:33.377: INFO: Created: latency-svc-ncx9m
  Jul 29 16:16:33.388: INFO: Created: latency-svc-mlfnr
  Jul 29 16:16:33.399: INFO: Got endpoints: latency-svc-ncx9m [176.102235ms]
  Jul 29 16:16:33.409: INFO: Got endpoints: latency-svc-mlfnr [186.287597ms]
  Jul 29 16:16:33.412: INFO: Created: latency-svc-xjpgj
  Jul 29 16:16:33.413: INFO: Got endpoints: latency-svc-xjpgj [181.560536ms]
  Jul 29 16:16:33.414: INFO: Created: latency-svc-nt7x5
  Jul 29 16:16:33.429: INFO: Got endpoints: latency-svc-nt7x5 [171.900739ms]
  Jul 29 16:16:33.532: INFO: Created: latency-svc-tcvhb
  Jul 29 16:16:33.533: INFO: Created: latency-svc-ssppc
  Jul 29 16:16:33.546: INFO: Created: latency-svc-ckrr2
  Jul 29 16:16:33.546: INFO: Created: latency-svc-6z8v6
  Jul 29 16:16:33.546: INFO: Created: latency-svc-rn599
  Jul 29 16:16:33.550: INFO: Created: latency-svc-lvdxm
  Jul 29 16:16:33.550: INFO: Created: latency-svc-pmjzf
  Jul 29 16:16:33.550: INFO: Created: latency-svc-4j4xj
  Jul 29 16:16:33.551: INFO: Created: latency-svc-hwvt7
  Jul 29 16:16:33.551: INFO: Created: latency-svc-dfdwh
  Jul 29 16:16:33.566: INFO: Created: latency-svc-dkgs9
  Jul 29 16:16:33.570: INFO: Got endpoints: latency-svc-tcvhb [390.771048ms]
  Jul 29 16:16:33.572: INFO: Created: latency-svc-m9bks
  Jul 29 16:16:33.582: INFO: Created: latency-svc-cn7rh
  Jul 29 16:16:33.583: INFO: Created: latency-svc-2pw95
  Jul 29 16:16:33.583: INFO: Created: latency-svc-t89px
  Jul 29 16:16:33.588: INFO: Got endpoints: latency-svc-6z8v6 [262.552603ms]
  Jul 29 16:16:33.588: INFO: Got endpoints: latency-svc-rn599 [325.673757ms]
  Jul 29 16:16:33.588: INFO: Got endpoints: latency-svc-ckrr2 [297.010948ms]
  Jul 29 16:16:33.588: INFO: Got endpoints: latency-svc-ssppc [188.032583ms]
  Jul 29 16:16:33.613: INFO: Got endpoints: latency-svc-m9bks [311.461517ms]
  Jul 29 16:16:33.634: INFO: Got endpoints: latency-svc-dfdwh [285.321686ms]
  Jul 29 16:16:33.634: INFO: Got endpoints: latency-svc-dkgs9 [309.536978ms]
  Jul 29 16:16:33.634: INFO: Got endpoints: latency-svc-4j4xj [204.522158ms]
  Jul 29 16:16:33.634: INFO: Got endpoints: latency-svc-cn7rh [220.890596ms]
  Jul 29 16:16:33.639: INFO: Created: latency-svc-mr5mz
  Jul 29 16:16:33.648: INFO: Got endpoints: latency-svc-t89px [324.593341ms]
  Jul 29 16:16:33.657: INFO: Created: latency-svc-p6pwf
  Jul 29 16:16:33.661: INFO: Got endpoints: latency-svc-hwvt7 [291.327225ms]
  Jul 29 16:16:33.667: INFO: Created: latency-svc-58h9d
  Jul 29 16:16:33.677: INFO: Created: latency-svc-g7q5b
  Jul 29 16:16:33.691: INFO: Created: latency-svc-mvbf9
  Jul 29 16:16:33.699: INFO: Got endpoints: latency-svc-2pw95 [288.515258ms]
  Jul 29 16:16:33.711: INFO: Created: latency-svc-n9vnp
  Jul 29 16:16:33.720: INFO: Created: latency-svc-gxsj9
  Jul 29 16:16:33.731: INFO: Created: latency-svc-nxbb6
  Jul 29 16:16:33.750: INFO: Got endpoints: latency-svc-pmjzf [392.516481ms]
  Jul 29 16:16:33.752: INFO: Created: latency-svc-z8g9n
  Jul 29 16:16:33.768: INFO: Created: latency-svc-jn42g
  Jul 29 16:16:33.786: INFO: Created: latency-svc-cnp4r
  Jul 29 16:16:33.808: INFO: Created: latency-svc-wggzb
  Jul 29 16:16:33.810: INFO: Got endpoints: latency-svc-lvdxm [452.353181ms]
  Jul 29 16:16:33.822: INFO: Created: latency-svc-l6qhh
  Jul 29 16:16:33.829: INFO: Created: latency-svc-glvf5
  Jul 29 16:16:33.839: INFO: Created: latency-svc-bd2z6
  Jul 29 16:16:33.845: INFO: Got endpoints: latency-svc-mr5mz [275.068851ms]
  Jul 29 16:16:33.859: INFO: Created: latency-svc-zw75q
  Jul 29 16:16:33.890: INFO: Got endpoints: latency-svc-p6pwf [301.759874ms]
  Jul 29 16:16:33.914: INFO: Created: latency-svc-vnlml
  Jul 29 16:16:33.947: INFO: Got endpoints: latency-svc-58h9d [357.968892ms]
  Jul 29 16:16:33.963: INFO: Created: latency-svc-wvw47
  Jul 29 16:16:33.995: INFO: Got endpoints: latency-svc-g7q5b [406.972221ms]
  Jul 29 16:16:34.016: INFO: Created: latency-svc-5twgg
  Jul 29 16:16:34.047: INFO: Got endpoints: latency-svc-mvbf9 [457.490654ms]
  Jul 29 16:16:34.066: INFO: Created: latency-svc-gx4ht
  Jul 29 16:16:34.094: INFO: Got endpoints: latency-svc-n9vnp [481.122574ms]
  Jul 29 16:16:34.121: INFO: Created: latency-svc-psvdr
  Jul 29 16:16:34.141: INFO: Got endpoints: latency-svc-gxsj9 [504.353915ms]
  Jul 29 16:16:34.180: INFO: Created: latency-svc-m2stc
  Jul 29 16:16:34.195: INFO: Got endpoints: latency-svc-nxbb6 [561.187976ms]
  Jul 29 16:16:34.216: INFO: Created: latency-svc-6452z
  Jul 29 16:16:34.250: INFO: Got endpoints: latency-svc-z8g9n [615.296669ms]
  Jul 29 16:16:34.264: INFO: Created: latency-svc-tx7z8
  Jul 29 16:16:34.297: INFO: Got endpoints: latency-svc-jn42g [661.167716ms]
  Jul 29 16:16:34.311: INFO: Created: latency-svc-4j9bv
  Jul 29 16:16:34.341: INFO: Got endpoints: latency-svc-cnp4r [693.123927ms]
  Jul 29 16:16:34.360: INFO: Created: latency-svc-scc4l
  Jul 29 16:16:34.392: INFO: Got endpoints: latency-svc-wggzb [730.65499ms]
  Jul 29 16:16:34.413: INFO: Created: latency-svc-jbttx
  Jul 29 16:16:34.448: INFO: Got endpoints: latency-svc-l6qhh [749.411669ms]
  Jul 29 16:16:34.467: INFO: Created: latency-svc-p9j9h
  Jul 29 16:16:34.493: INFO: Got endpoints: latency-svc-glvf5 [743.332546ms]
  Jul 29 16:16:34.516: INFO: Created: latency-svc-c7dk9
  Jul 29 16:16:34.547: INFO: Got endpoints: latency-svc-bd2z6 [736.386694ms]
  Jul 29 16:16:34.562: INFO: Created: latency-svc-7v9tx
  Jul 29 16:16:34.598: INFO: Got endpoints: latency-svc-zw75q [752.934587ms]
  Jul 29 16:16:34.616: INFO: Created: latency-svc-l9g8p
  Jul 29 16:16:34.642: INFO: Got endpoints: latency-svc-vnlml [751.573666ms]
  Jul 29 16:16:34.668: INFO: Created: latency-svc-frls8
  Jul 29 16:16:34.699: INFO: Got endpoints: latency-svc-wvw47 [752.055449ms]
  Jul 29 16:16:34.726: INFO: Created: latency-svc-hkr5x
  Jul 29 16:16:34.745: INFO: Got endpoints: latency-svc-5twgg [749.903014ms]
  Jul 29 16:16:34.769: INFO: Created: latency-svc-dfzcx
  Jul 29 16:16:34.797: INFO: Got endpoints: latency-svc-gx4ht [749.670534ms]
  Jul 29 16:16:34.823: INFO: Created: latency-svc-8dv7v
  Jul 29 16:16:34.854: INFO: Got endpoints: latency-svc-psvdr [760.038219ms]
  Jul 29 16:16:34.873: INFO: Created: latency-svc-59xdt
  Jul 29 16:16:34.892: INFO: Got endpoints: latency-svc-m2stc [751.049622ms]
  Jul 29 16:16:34.917: INFO: Created: latency-svc-l8b5k
  Jul 29 16:16:34.956: INFO: Got endpoints: latency-svc-6452z [759.768458ms]
  Jul 29 16:16:34.983: INFO: Created: latency-svc-rvdsr
  Jul 29 16:16:34.996: INFO: Got endpoints: latency-svc-tx7z8 [745.868317ms]
  Jul 29 16:16:35.011: INFO: Created: latency-svc-jlf4l
  Jul 29 16:16:35.044: INFO: Got endpoints: latency-svc-4j9bv [747.287043ms]
  Jul 29 16:16:35.065: INFO: Created: latency-svc-w24kk
  Jul 29 16:16:35.100: INFO: Got endpoints: latency-svc-scc4l [758.246941ms]
  Jul 29 16:16:35.119: INFO: Created: latency-svc-285hw
  Jul 29 16:16:35.153: INFO: Got endpoints: latency-svc-jbttx [760.565715ms]
  Jul 29 16:16:35.176: INFO: Created: latency-svc-4htcx
  Jul 29 16:16:35.190: INFO: Got endpoints: latency-svc-p9j9h [741.981909ms]
  Jul 29 16:16:35.215: INFO: Created: latency-svc-mk7g8
  Jul 29 16:16:35.240: INFO: Got endpoints: latency-svc-c7dk9 [747.043843ms]
  Jul 29 16:16:35.261: INFO: Created: latency-svc-sgq4j
  Jul 29 16:16:35.295: INFO: Got endpoints: latency-svc-7v9tx [747.849645ms]
  Jul 29 16:16:35.314: INFO: Created: latency-svc-5wrsd
  Jul 29 16:16:35.349: INFO: Got endpoints: latency-svc-l9g8p [751.047348ms]
  Jul 29 16:16:35.366: INFO: Created: latency-svc-pdspn
  Jul 29 16:16:35.394: INFO: Got endpoints: latency-svc-frls8 [751.871305ms]
  Jul 29 16:16:35.414: INFO: Created: latency-svc-k4dbn
  Jul 29 16:16:35.444: INFO: Got endpoints: latency-svc-hkr5x [744.84293ms]
  Jul 29 16:16:35.478: INFO: Created: latency-svc-4vt4m
  Jul 29 16:16:35.494: INFO: Got endpoints: latency-svc-dfzcx [748.173276ms]
  Jul 29 16:16:35.513: INFO: Created: latency-svc-vxrjr
  Jul 29 16:16:35.544: INFO: Got endpoints: latency-svc-8dv7v [746.970107ms]
  Jul 29 16:16:35.592: INFO: Got endpoints: latency-svc-59xdt [738.002565ms]
  Jul 29 16:16:35.608: INFO: Created: latency-svc-xwtkd
  Jul 29 16:16:35.619: INFO: Created: latency-svc-94mv6
  Jul 29 16:16:35.651: INFO: Got endpoints: latency-svc-l8b5k [758.111872ms]
  Jul 29 16:16:35.673: INFO: Created: latency-svc-bhn8s
  Jul 29 16:16:35.697: INFO: Got endpoints: latency-svc-rvdsr [740.980137ms]
  Jul 29 16:16:35.718: INFO: Created: latency-svc-kcs8t
  Jul 29 16:16:35.747: INFO: Got endpoints: latency-svc-jlf4l [751.422494ms]
  Jul 29 16:16:35.767: INFO: Created: latency-svc-7zr8s
  Jul 29 16:16:35.799: INFO: Got endpoints: latency-svc-w24kk [754.660279ms]
  Jul 29 16:16:35.825: INFO: Created: latency-svc-4hs2d
  Jul 29 16:16:35.849: INFO: Got endpoints: latency-svc-285hw [748.454888ms]
  Jul 29 16:16:35.871: INFO: Created: latency-svc-g7h6g
  Jul 29 16:16:35.895: INFO: Got endpoints: latency-svc-4htcx [742.082056ms]
  Jul 29 16:16:35.938: INFO: Created: latency-svc-p4x6j
  Jul 29 16:16:35.958: INFO: Got endpoints: latency-svc-mk7g8 [767.208056ms]
  Jul 29 16:16:35.987: INFO: Created: latency-svc-vlqb4
  Jul 29 16:16:36.001: INFO: Got endpoints: latency-svc-sgq4j [760.47567ms]
  Jul 29 16:16:36.043: INFO: Created: latency-svc-2s8mc
  Jul 29 16:16:36.055: INFO: Got endpoints: latency-svc-5wrsd [759.204689ms]
  Jul 29 16:16:36.079: INFO: Created: latency-svc-nvbr4
  Jul 29 16:16:36.091: INFO: Got endpoints: latency-svc-pdspn [741.655025ms]
  Jul 29 16:16:36.118: INFO: Created: latency-svc-vwj45
  Jul 29 16:16:36.148: INFO: Got endpoints: latency-svc-k4dbn [753.497382ms]
  Jul 29 16:16:36.217: INFO: Got endpoints: latency-svc-4vt4m [772.540883ms]
  Jul 29 16:16:36.239: INFO: Created: latency-svc-k5rds
  Jul 29 16:16:36.256: INFO: Got endpoints: latency-svc-vxrjr [761.823699ms]
  Jul 29 16:16:36.268: INFO: Created: latency-svc-5q7pf
  Jul 29 16:16:36.315: INFO: Got endpoints: latency-svc-xwtkd [771.530272ms]
  Jul 29 16:16:36.324: INFO: Created: latency-svc-pq2tt
  Jul 29 16:16:36.361: INFO: Got endpoints: latency-svc-94mv6 [767.990998ms]
  Jul 29 16:16:36.367: INFO: Created: latency-svc-8wr5z
  Jul 29 16:16:36.387: INFO: Created: latency-svc-pb9kg
  Jul 29 16:16:36.396: INFO: Got endpoints: latency-svc-bhn8s [744.949317ms]
  Jul 29 16:16:36.431: INFO: Created: latency-svc-rxb5k
  Jul 29 16:16:36.452: INFO: Got endpoints: latency-svc-kcs8t [755.011461ms]
  Jul 29 16:16:36.478: INFO: Created: latency-svc-wqfhz
  Jul 29 16:16:36.502: INFO: Got endpoints: latency-svc-7zr8s [755.027614ms]
  Jul 29 16:16:36.543: INFO: Created: latency-svc-8pk6m
  Jul 29 16:16:36.551: INFO: Got endpoints: latency-svc-4hs2d [751.912559ms]
  Jul 29 16:16:36.569: INFO: Created: latency-svc-zfg6p
  Jul 29 16:16:36.599: INFO: Got endpoints: latency-svc-g7h6g [750.375213ms]
  Jul 29 16:16:36.640: INFO: Created: latency-svc-76tmf
  Jul 29 16:16:36.651: INFO: Got endpoints: latency-svc-p4x6j [755.565305ms]
  Jul 29 16:16:36.669: INFO: Created: latency-svc-dnjhc
  Jul 29 16:16:36.698: INFO: Got endpoints: latency-svc-vlqb4 [739.598756ms]
  Jul 29 16:16:36.719: INFO: Created: latency-svc-npfm9
  Jul 29 16:16:36.747: INFO: Got endpoints: latency-svc-2s8mc [746.210533ms]
  Jul 29 16:16:36.771: INFO: Created: latency-svc-h7bjq
  Jul 29 16:16:36.803: INFO: Got endpoints: latency-svc-nvbr4 [747.374351ms]
  Jul 29 16:16:36.845: INFO: Created: latency-svc-2zbd8
  Jul 29 16:16:36.851: INFO: Got endpoints: latency-svc-vwj45 [759.309483ms]
  Jul 29 16:16:36.870: INFO: Created: latency-svc-xxzws
  Jul 29 16:16:36.891: INFO: Got endpoints: latency-svc-k5rds [743.064457ms]
  Jul 29 16:16:36.921: INFO: Created: latency-svc-k8f4r
  Jul 29 16:16:36.949: INFO: Got endpoints: latency-svc-5q7pf [731.456476ms]
  Jul 29 16:16:36.967: INFO: Created: latency-svc-fsklm
  Jul 29 16:16:36.992: INFO: Got endpoints: latency-svc-pq2tt [735.153519ms]
  Jul 29 16:16:37.010: INFO: Created: latency-svc-dldwp
  Jul 29 16:16:37.051: INFO: Got endpoints: latency-svc-8wr5z [735.774104ms]
  Jul 29 16:16:37.076: INFO: Created: latency-svc-qfsnc
  Jul 29 16:16:37.095: INFO: Got endpoints: latency-svc-pb9kg [733.435459ms]
  Jul 29 16:16:37.123: INFO: Created: latency-svc-sfhkb
  Jul 29 16:16:37.139: INFO: Got endpoints: latency-svc-rxb5k [743.53635ms]
  Jul 29 16:16:37.169: INFO: Created: latency-svc-m6z5s
  Jul 29 16:16:37.206: INFO: Got endpoints: latency-svc-wqfhz [754.236862ms]
  Jul 29 16:16:37.231: INFO: Created: latency-svc-nf5qh
  Jul 29 16:16:37.246: INFO: Got endpoints: latency-svc-8pk6m [743.495457ms]
  Jul 29 16:16:37.275: INFO: Created: latency-svc-p6shg
  Jul 29 16:16:37.295: INFO: Got endpoints: latency-svc-zfg6p [743.971482ms]
  Jul 29 16:16:37.311: INFO: Created: latency-svc-g9t98
  Jul 29 16:16:37.346: INFO: Got endpoints: latency-svc-76tmf [746.20775ms]
  Jul 29 16:16:37.363: INFO: Created: latency-svc-fng5n
  Jul 29 16:16:37.397: INFO: Got endpoints: latency-svc-dnjhc [745.800755ms]
  Jul 29 16:16:37.420: INFO: Created: latency-svc-lkqkt
  Jul 29 16:16:37.450: INFO: Got endpoints: latency-svc-npfm9 [751.970477ms]
  Jul 29 16:16:37.480: INFO: Created: latency-svc-smh24
  Jul 29 16:16:37.491: INFO: Got endpoints: latency-svc-h7bjq [743.715176ms]
  Jul 29 16:16:37.508: INFO: Created: latency-svc-zvdpc
  Jul 29 16:16:37.542: INFO: Got endpoints: latency-svc-2zbd8 [738.671407ms]
  Jul 29 16:16:37.573: INFO: Created: latency-svc-66nwp
  Jul 29 16:16:37.597: INFO: Got endpoints: latency-svc-xxzws [746.211475ms]
  Jul 29 16:16:37.610: INFO: Created: latency-svc-sxc67
  Jul 29 16:16:37.639: INFO: Got endpoints: latency-svc-k8f4r [747.088712ms]
  Jul 29 16:16:37.668: INFO: Created: latency-svc-bh2vs
  Jul 29 16:16:37.697: INFO: Got endpoints: latency-svc-fsklm [747.231744ms]
  Jul 29 16:16:37.715: INFO: Created: latency-svc-zjsd2
  Jul 29 16:16:37.744: INFO: Got endpoints: latency-svc-dldwp [752.195856ms]
  Jul 29 16:16:37.760: INFO: Created: latency-svc-lcdzd
  Jul 29 16:16:37.816: INFO: Got endpoints: latency-svc-qfsnc [764.275023ms]
  Jul 29 16:16:37.851: INFO: Got endpoints: latency-svc-sfhkb [756.003203ms]
  Jul 29 16:16:37.859: INFO: Created: latency-svc-4rgtj
  Jul 29 16:16:37.883: INFO: Created: latency-svc-zbfvs
  Jul 29 16:16:37.896: INFO: Got endpoints: latency-svc-m6z5s [755.940025ms]
  Jul 29 16:16:37.930: INFO: Created: latency-svc-s9rc8
  Jul 29 16:16:37.949: INFO: Got endpoints: latency-svc-nf5qh [742.682244ms]
  Jul 29 16:16:37.971: INFO: Created: latency-svc-cbzhz
  Jul 29 16:16:37.994: INFO: Got endpoints: latency-svc-p6shg [748.276192ms]
  Jul 29 16:16:38.020: INFO: Created: latency-svc-lhqts
  Jul 29 16:16:38.046: INFO: Got endpoints: latency-svc-g9t98 [750.286173ms]
  Jul 29 16:16:38.067: INFO: Created: latency-svc-pxvst
  Jul 29 16:16:38.096: INFO: Got endpoints: latency-svc-fng5n [750.199358ms]
  Jul 29 16:16:38.146: INFO: Got endpoints: latency-svc-lkqkt [748.546164ms]
  Jul 29 16:16:38.166: INFO: Created: latency-svc-zw69x
  Jul 29 16:16:38.177: INFO: Created: latency-svc-tltn8
  Jul 29 16:16:38.199: INFO: Got endpoints: latency-svc-smh24 [749.300547ms]
  Jul 29 16:16:38.217: INFO: Created: latency-svc-zcqvt
  Jul 29 16:16:38.242: INFO: Got endpoints: latency-svc-zvdpc [751.284877ms]
  Jul 29 16:16:38.265: INFO: Created: latency-svc-22rk2
  Jul 29 16:16:38.298: INFO: Got endpoints: latency-svc-66nwp [755.80322ms]
  Jul 29 16:16:38.322: INFO: Created: latency-svc-q4hfg
  Jul 29 16:16:38.350: INFO: Got endpoints: latency-svc-sxc67 [752.400069ms]
  Jul 29 16:16:38.364: INFO: Created: latency-svc-djfb2
  Jul 29 16:16:38.392: INFO: Got endpoints: latency-svc-bh2vs [752.664541ms]
  Jul 29 16:16:38.413: INFO: Created: latency-svc-gz7hw
  Jul 29 16:16:38.448: INFO: Got endpoints: latency-svc-zjsd2 [751.543641ms]
  Jul 29 16:16:38.466: INFO: Created: latency-svc-mjfnw
  Jul 29 16:16:38.497: INFO: Got endpoints: latency-svc-lcdzd [752.538292ms]
  Jul 29 16:16:38.516: INFO: Created: latency-svc-6vbjh
  Jul 29 16:16:38.545: INFO: Got endpoints: latency-svc-4rgtj [728.134392ms]
  Jul 29 16:16:38.563: INFO: Created: latency-svc-xjbpg
  Jul 29 16:16:38.600: INFO: Got endpoints: latency-svc-zbfvs [748.038299ms]
  Jul 29 16:16:38.616: INFO: Created: latency-svc-cgddz
  Jul 29 16:16:38.648: INFO: Got endpoints: latency-svc-s9rc8 [751.558621ms]
  Jul 29 16:16:38.667: INFO: Created: latency-svc-m47lr
  Jul 29 16:16:38.693: INFO: Got endpoints: latency-svc-cbzhz [743.158857ms]
  Jul 29 16:16:38.719: INFO: Created: latency-svc-9jst6
  Jul 29 16:16:38.756: INFO: Got endpoints: latency-svc-lhqts [760.786269ms]
  Jul 29 16:16:38.772: INFO: Created: latency-svc-84rvn
  Jul 29 16:16:38.800: INFO: Got endpoints: latency-svc-pxvst [753.912181ms]
  Jul 29 16:16:38.828: INFO: Created: latency-svc-q5kbz
  Jul 29 16:16:38.842: INFO: Got endpoints: latency-svc-zw69x [745.435899ms]
  Jul 29 16:16:38.870: INFO: Created: latency-svc-g4rs7
  Jul 29 16:16:38.895: INFO: Got endpoints: latency-svc-tltn8 [748.892021ms]
  Jul 29 16:16:38.922: INFO: Created: latency-svc-qqtkb
  Jul 29 16:16:38.954: INFO: Got endpoints: latency-svc-zcqvt [755.228569ms]
  Jul 29 16:16:38.972: INFO: Created: latency-svc-7dqwt
  Jul 29 16:16:38.996: INFO: Got endpoints: latency-svc-22rk2 [752.951517ms]
  Jul 29 16:16:39.021: INFO: Created: latency-svc-c7ckt
  Jul 29 16:16:39.044: INFO: Got endpoints: latency-svc-q4hfg [745.53389ms]
  Jul 29 16:16:39.080: INFO: Created: latency-svc-tnrvz
  Jul 29 16:16:39.090: INFO: Got endpoints: latency-svc-djfb2 [740.016007ms]
  Jul 29 16:16:39.118: INFO: Created: latency-svc-hjw5h
  Jul 29 16:16:39.147: INFO: Got endpoints: latency-svc-gz7hw [754.869801ms]
  Jul 29 16:16:39.191: INFO: Got endpoints: latency-svc-mjfnw [742.325775ms]
  Jul 29 16:16:39.296: INFO: Got endpoints: latency-svc-xjbpg [750.821279ms]
  Jul 29 16:16:39.307: INFO: Got endpoints: latency-svc-6vbjh [810.160795ms]
  Jul 29 16:16:39.324: INFO: Created: latency-svc-m5g56
  Jul 29 16:16:39.344: INFO: Created: latency-svc-p62dc
  Jul 29 16:16:39.351: INFO: Got endpoints: latency-svc-cgddz [751.373112ms]
  Jul 29 16:16:39.363: INFO: Created: latency-svc-cj4ts
  Jul 29 16:16:39.379: INFO: Created: latency-svc-v8cj6
  Jul 29 16:16:39.397: INFO: Created: latency-svc-h2t56
  Jul 29 16:16:39.398: INFO: Got endpoints: latency-svc-m47lr [750.10123ms]
  Jul 29 16:16:39.424: INFO: Created: latency-svc-n4kd7
  Jul 29 16:16:39.447: INFO: Got endpoints: latency-svc-9jst6 [754.146635ms]
  Jul 29 16:16:39.473: INFO: Created: latency-svc-7256z
  Jul 29 16:16:39.500: INFO: Got endpoints: latency-svc-84rvn [743.67895ms]
  Jul 29 16:16:39.523: INFO: Created: latency-svc-jxl5c
  Jul 29 16:16:39.545: INFO: Got endpoints: latency-svc-q5kbz [743.832321ms]
  Jul 29 16:16:39.601: INFO: Created: latency-svc-q85jr
  Jul 29 16:16:39.603: INFO: Got endpoints: latency-svc-g4rs7 [761.093809ms]
  Jul 29 16:16:39.623: INFO: Created: latency-svc-t5glj
  Jul 29 16:16:39.649: INFO: Got endpoints: latency-svc-qqtkb [753.570201ms]
  Jul 29 16:16:39.668: INFO: Created: latency-svc-wkvw5
  Jul 29 16:16:39.704: INFO: Got endpoints: latency-svc-7dqwt [749.35829ms]
  Jul 29 16:16:39.722: INFO: Created: latency-svc-x4zp2
  Jul 29 16:16:39.748: INFO: Got endpoints: latency-svc-c7ckt [751.759212ms]
  Jul 29 16:16:39.767: INFO: Created: latency-svc-m85ns
  Jul 29 16:16:39.801: INFO: Got endpoints: latency-svc-tnrvz [756.585564ms]
  Jul 29 16:16:39.820: INFO: Created: latency-svc-gg7b8
  Jul 29 16:16:39.847: INFO: Got endpoints: latency-svc-hjw5h [756.680955ms]
  Jul 29 16:16:39.866: INFO: Created: latency-svc-pwlwx
  Jul 29 16:16:39.897: INFO: Got endpoints: latency-svc-m5g56 [749.1053ms]
  Jul 29 16:16:39.925: INFO: Created: latency-svc-65psf
  Jul 29 16:16:39.952: INFO: Got endpoints: latency-svc-p62dc [760.74829ms]
  Jul 29 16:16:39.971: INFO: Created: latency-svc-s5gxg
  Jul 29 16:16:39.992: INFO: Got endpoints: latency-svc-cj4ts [694.824248ms]
  Jul 29 16:16:40.013: INFO: Created: latency-svc-grg74
  Jul 29 16:16:40.044: INFO: Got endpoints: latency-svc-v8cj6 [737.098374ms]
  Jul 29 16:16:40.061: INFO: Created: latency-svc-xczjf
  Jul 29 16:16:40.095: INFO: Got endpoints: latency-svc-h2t56 [744.017837ms]
  Jul 29 16:16:40.120: INFO: Created: latency-svc-p6dkm
  Jul 29 16:16:40.148: INFO: Got endpoints: latency-svc-n4kd7 [749.778224ms]
  Jul 29 16:16:40.170: INFO: Created: latency-svc-82ffl
  Jul 29 16:16:40.194: INFO: Got endpoints: latency-svc-7256z [746.321026ms]
  Jul 29 16:16:40.214: INFO: Created: latency-svc-77wz6
  Jul 29 16:16:40.241: INFO: Got endpoints: latency-svc-jxl5c [741.604097ms]
  Jul 29 16:16:40.277: INFO: Created: latency-svc-zlmh8
  Jul 29 16:16:40.300: INFO: Got endpoints: latency-svc-q85jr [755.615183ms]
  Jul 29 16:16:40.315: INFO: Created: latency-svc-l69hm
  Jul 29 16:16:40.342: INFO: Got endpoints: latency-svc-t5glj [739.120978ms]
  Jul 29 16:16:40.368: INFO: Created: latency-svc-lpmv5
  Jul 29 16:16:40.397: INFO: Got endpoints: latency-svc-wkvw5 [747.854239ms]
  Jul 29 16:16:40.419: INFO: Created: latency-svc-hn88w
  Jul 29 16:16:40.443: INFO: Got endpoints: latency-svc-x4zp2 [739.026789ms]
  Jul 29 16:16:40.466: INFO: Created: latency-svc-cr7p8
  Jul 29 16:16:40.508: INFO: Got endpoints: latency-svc-m85ns [759.802742ms]
  Jul 29 16:16:40.532: INFO: Created: latency-svc-fcqnt
  Jul 29 16:16:40.546: INFO: Got endpoints: latency-svc-gg7b8 [744.47974ms]
  Jul 29 16:16:40.572: INFO: Created: latency-svc-khmkd
  Jul 29 16:16:40.603: INFO: Got endpoints: latency-svc-pwlwx [755.772149ms]
  Jul 29 16:16:40.645: INFO: Got endpoints: latency-svc-65psf [747.586954ms]
  Jul 29 16:16:40.694: INFO: Got endpoints: latency-svc-s5gxg [742.019607ms]
  Jul 29 16:16:40.750: INFO: Got endpoints: latency-svc-grg74 [758.193481ms]
  Jul 29 16:16:40.804: INFO: Got endpoints: latency-svc-xczjf [759.930881ms]
  Jul 29 16:16:40.858: INFO: Got endpoints: latency-svc-p6dkm [762.901461ms]
  Jul 29 16:16:40.895: INFO: Got endpoints: latency-svc-82ffl [745.532903ms]
  Jul 29 16:16:40.945: INFO: Got endpoints: latency-svc-77wz6 [751.20106ms]
  Jul 29 16:16:40.996: INFO: Got endpoints: latency-svc-zlmh8 [754.774064ms]
  Jul 29 16:16:41.049: INFO: Got endpoints: latency-svc-l69hm [748.106865ms]
  Jul 29 16:16:41.095: INFO: Got endpoints: latency-svc-lpmv5 [752.241972ms]
  Jul 29 16:16:41.141: INFO: Got endpoints: latency-svc-hn88w [743.996361ms]
  Jul 29 16:16:41.200: INFO: Got endpoints: latency-svc-cr7p8 [757.078942ms]
  Jul 29 16:16:41.243: INFO: Got endpoints: latency-svc-fcqnt [735.73722ms]
  Jul 29 16:16:41.294: INFO: Got endpoints: latency-svc-khmkd [747.869065ms]
  Jul 29 16:16:41.295: INFO: Latencies: [67.008725ms 83.362549ms 94.113581ms 99.3315ms 100.635991ms 123.277836ms 131.827069ms 133.094227ms 147.367312ms 163.941121ms 169.565464ms 169.755587ms 171.900739ms 176.102235ms 178.541577ms 181.560536ms 186.287597ms 188.032583ms 189.020262ms 204.522158ms 210.113997ms 216.39266ms 220.890596ms 224.10599ms 262.453403ms 262.552603ms 275.068851ms 285.321686ms 288.515258ms 291.327225ms 297.010948ms 301.759874ms 309.536978ms 311.461517ms 316.217582ms 323.917357ms 324.593341ms 325.673757ms 349.875317ms 356.22652ms 357.968892ms 390.771048ms 392.516481ms 406.972221ms 407.446592ms 409.360978ms 413.164496ms 418.516457ms 433.774152ms 434.659315ms 452.353181ms 457.490654ms 481.122574ms 504.353915ms 513.005351ms 518.430008ms 556.759121ms 561.187976ms 615.296669ms 661.167716ms 693.123927ms 694.824248ms 728.134392ms 730.65499ms 731.456476ms 733.435459ms 735.153519ms 735.73722ms 735.774104ms 736.386694ms 737.098374ms 738.002565ms 738.671407ms 739.026789ms 739.120978ms 739.598756ms 740.016007ms 740.980137ms 741.604097ms 741.655025ms 741.981909ms 742.019607ms 742.082056ms 742.325775ms 742.682244ms 743.064457ms 743.158857ms 743.332546ms 743.495457ms 743.53635ms 743.67895ms 743.715176ms 743.832321ms 743.971482ms 743.996361ms 744.017837ms 744.47974ms 744.84293ms 744.949317ms 745.435899ms 745.532903ms 745.53389ms 745.800755ms 745.868317ms 746.20775ms 746.210533ms 746.211475ms 746.321026ms 746.970107ms 747.043843ms 747.088712ms 747.231744ms 747.287043ms 747.374351ms 747.586954ms 747.849645ms 747.854239ms 747.869065ms 748.038299ms 748.106865ms 748.173276ms 748.276192ms 748.454888ms 748.546164ms 748.892021ms 749.1053ms 749.300547ms 749.35829ms 749.411669ms 749.670534ms 749.778224ms 749.903014ms 750.10123ms 750.199358ms 750.286173ms 750.375213ms 750.821279ms 751.047348ms 751.049622ms 751.20106ms 751.284877ms 751.373112ms 751.422494ms 751.543641ms 751.558621ms 751.573666ms 751.759212ms 751.871305ms 751.912559ms 751.970477ms 752.055449ms 752.195856ms 752.241972ms 752.400069ms 752.538292ms 752.664541ms 752.934587ms 752.951517ms 753.497382ms 753.570201ms 753.912181ms 754.146635ms 754.236862ms 754.660279ms 754.774064ms 754.869801ms 755.011461ms 755.027614ms 755.228569ms 755.565305ms 755.615183ms 755.772149ms 755.80322ms 755.940025ms 756.003203ms 756.585564ms 756.680955ms 757.078942ms 758.111872ms 758.193481ms 758.246941ms 759.204689ms 759.309483ms 759.768458ms 759.802742ms 759.930881ms 760.038219ms 760.47567ms 760.565715ms 760.74829ms 760.786269ms 761.093809ms 761.823699ms 762.901461ms 764.275023ms 767.208056ms 767.990998ms 771.530272ms 772.540883ms 810.160795ms]
  Jul 29 16:16:41.295: INFO: 50 %ile: 745.532903ms
  Jul 29 16:16:41.296: INFO: 90 %ile: 758.246941ms
  Jul 29 16:16:41.296: INFO: 99 %ile: 772.540883ms
  Jul 29 16:16:41.296: INFO: Total sample count: 200
  Jul 29 16:16:41.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svc-latency-7982" for this suite. @ 07/29/23 16:16:41.305
• [10.836 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:69
  STEP: Creating a kubernetes client @ 07/29/23 16:16:41.325
  Jul 29 16:16:41.325: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:16:41.329
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:41.371
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:41.376
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:16:41.382
  STEP: Saw pod success @ 07/29/23 16:16:45.427
  Jul 29 16:16:45.436: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-078927d5-37b3-4c7b-ad50-a4e62b36427e container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:16:45.467
  Jul 29 16:16:45.492: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3357" for this suite. @ 07/29/23 16:16:45.499
• [4.185 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-node] Secrets should fail to create secret due to empty secret key [Conformance]
test/e2e/common/node/secrets.go:140
  STEP: Creating a kubernetes client @ 07/29/23 16:16:45.515
  Jul 29 16:16:45.515: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:16:45.517
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:45.545
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:45.55
  STEP: Creating projection with secret that has name secret-emptykey-test-514ec454-606a-48e2-a79a-46425cb3beec @ 07/29/23 16:16:45.555
  Jul 29 16:16:45.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-2908" for this suite. @ 07/29/23 16:16:45.563
• [0.060 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with readOnlyRootFilesystem should run the container with writable rootfs when readOnlyRootFilesystem=false [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:486
  STEP: Creating a kubernetes client @ 07/29/23 16:16:45.576
  Jul 29 16:16:45.576: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 16:16:45.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:45.598
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:45.603
  Jul 29 16:16:49.650: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6616" for this suite. @ 07/29/23 16:16:49.673
• [4.137 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if not matching  [Conformance]
test/e2e/scheduling/predicates.go:444
  STEP: Creating a kubernetes client @ 07/29/23 16:16:49.715
  Jul 29 16:16:49.715: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 16:16:49.717
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:49.747
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:49.753
  Jul 29 16:16:49.764: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 16:16:49.803: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:16:49.814: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-1 before test
  Jul 29 16:16:49.837: INFO: cilium-6928s from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.837: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:16:49.838: INFO: cilium-node-init-m4qsv from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.838: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:16:49.838: INFO: coredns-5d78c9869d-xxmxx from kube-system started at 2023-07-29 15:24:09 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.838: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:16:49.839: INFO: kube-addon-manager-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.839: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:16:49.839: INFO: kube-apiserver-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.839: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:16:49.839: INFO: kube-controller-manager-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.839: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:16:49.839: INFO: kube-proxy-dscp7 from kube-system started at 2023-07-29 15:11:26 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.840: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:16:49.840: INFO: kube-scheduler-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.840: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:16:49.840: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-7bqrz from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:16:49.840: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:16:49.841: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:16:49.841: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-2 before test
  Jul 29 16:16:49.860: INFO: cilium-node-init-c52xz from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: cilium-operator-64cdf5fc9d-9t2m7 from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container cilium-operator ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: cilium-pr7vs from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: coredns-5d78c9869d-7fz8w from kube-system started at 2023-07-29 15:24:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: kube-addon-manager-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: kube-apiserver-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: kube-controller-manager-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: kube-proxy-bgt8j from kube-system started at 2023-07-29 15:11:44 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: kube-scheduler-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-568gm from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:16:49.860: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:16:49.860: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-3 before test
  Jul 29 16:16:49.881: INFO: cilium-jsbkd from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.881: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:16:49.881: INFO: cilium-node-init-fs2dr from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.881: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:16:49.881: INFO: kube-proxy-bhtmh from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.881: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:16:49.881: INFO: busybox-readonly-false-02112873-e30e-4af1-a4cd-8c91a583c254 from security-context-test-6616 started at 2023-07-29 16:16:45 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.881: INFO: 	Container busybox-readonly-false-02112873-e30e-4af1-a4cd-8c91a583c254 ready: false, restart count 0
  Jul 29 16:16:49.881: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:29:07 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.881: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 16:16:49.881: INFO: sonobuoy-e2e-job-9fe8eb0b75ee4e08 from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:16:49.881: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 16:16:49.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:16:49.881: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-92k89 from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:16:49.881: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:16:49.881: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:16:49.881: INFO: svc-latency-rc-dfsdb from svc-latency-7982 started at 2023-07-29 16:16:30 +0000 UTC (1 container statuses recorded)
  Jul 29 16:16:49.881: INFO: 	Container svc-latency-rc ready: true, restart count 0
  STEP: Trying to schedule Pod with nonempty NodeSelector. @ 07/29/23 16:16:49.881
  STEP: Considering event: 
  Type = [Warning], Name = [restricted-pod.177662dca6c49350], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 node(s) didn't match Pod's node affinity/selector. preemption: 0/3 nodes are available: 3 Preemption is not helpful for scheduling..] @ 07/29/23 16:16:49.974
  Jul 29 16:16:50.967: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-5934" for this suite. @ 07/29/23 16:16:50.996
• [1.297 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] CronJob should support CronJob API operations [Conformance]
test/e2e/apps/cronjob.go:324
  STEP: Creating a kubernetes client @ 07/29/23 16:16:51.013
  Jul 29 16:16:51.013: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 16:16:51.016
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:51.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:51.11
  STEP: Creating a cronjob @ 07/29/23 16:16:51.119
  STEP: creating @ 07/29/23 16:16:51.119
  STEP: getting @ 07/29/23 16:16:51.151
  STEP: listing @ 07/29/23 16:16:51.162
  STEP: watching @ 07/29/23 16:16:51.175
  Jul 29 16:16:51.175: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 16:16:51.183
  STEP: cluster-wide watching @ 07/29/23 16:16:51.239
  Jul 29 16:16:51.239: INFO: starting watch
  STEP: patching @ 07/29/23 16:16:51.241
  STEP: updating @ 07/29/23 16:16:51.268
  Jul 29 16:16:51.307: INFO: waiting for watch events with expected annotations
  Jul 29 16:16:51.309: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/29/23 16:16:51.31
  STEP: updating /status @ 07/29/23 16:16:51.328
  STEP: get /status @ 07/29/23 16:16:51.361
  STEP: deleting @ 07/29/23 16:16:51.374
  STEP: deleting a collection @ 07/29/23 16:16:51.426
  Jul 29 16:16:51.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-4540" for this suite. @ 07/29/23 16:16:51.477
• [0.495 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: udp [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:95
  STEP: Creating a kubernetes client @ 07/29/23 16:16:51.513
  Jul 29 16:16:51.513: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 16:16:51.516
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:16:51.629
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:16:51.659
  STEP: Performing setup for networking test in namespace pod-network-test-7912 @ 07/29/23 16:16:51.675
  STEP: creating a selector @ 07/29/23 16:16:51.675
  STEP: Creating the service pods in kubernetes @ 07/29/23 16:16:51.675
  Jul 29 16:16:51.675: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 16:17:14.005
  Jul 29 16:17:16.043: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 16:17:16.043: INFO: Breadth first check of 10.233.64.91 on host 192.168.121.39...
  Jul 29 16:17:16.051: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.64:9080/dial?request=hostname&protocol=udp&host=10.233.64.91&port=8081&tries=1'] Namespace:pod-network-test-7912 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:17:16.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:17:16.054: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:17:16.055: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7912/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.64.91%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:17:16.208: INFO: Waiting for responses: map[]
  Jul 29 16:17:16.208: INFO: reached 10.233.64.91 after 0/1 tries
  Jul 29 16:17:16.208: INFO: Breadth first check of 10.233.65.21 on host 192.168.121.49...
  Jul 29 16:17:16.216: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.64:9080/dial?request=hostname&protocol=udp&host=10.233.65.21&port=8081&tries=1'] Namespace:pod-network-test-7912 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:17:16.216: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:17:16.217: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:17:16.217: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7912/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.65.21%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:17:16.343: INFO: Waiting for responses: map[]
  Jul 29 16:17:16.343: INFO: reached 10.233.65.21 after 0/1 tries
  Jul 29 16:17:16.343: INFO: Breadth first check of 10.233.66.69 on host 192.168.121.144...
  Jul 29 16:17:16.349: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.64:9080/dial?request=hostname&protocol=udp&host=10.233.66.69&port=8081&tries=1'] Namespace:pod-network-test-7912 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:17:16.350: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:17:16.352: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:17:16.352: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7912/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.64%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dudp%26host%3D10.233.66.69%26port%3D8081%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:17:16.461: INFO: Waiting for responses: map[]
  Jul 29 16:17:16.461: INFO: reached 10.233.66.69 after 0/1 tries
  Jul 29 16:17:16.461: INFO: Going to retry 0 out of 3 pods....
  Jul 29 16:17:16.462: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7912" for this suite. @ 07/29/23 16:17:16.472
• [24.971 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should validate Replicaset Status endpoints [Conformance]
test/e2e/apps/replica_set.go:176
  STEP: Creating a kubernetes client @ 07/29/23 16:17:16.493
  Jul 29 16:17:16.493: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 16:17:16.496
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:16.521
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:16.526
  STEP: Create a Replicaset @ 07/29/23 16:17:16.536
  STEP: Verify that the required pods have come up. @ 07/29/23 16:17:16.546
  Jul 29 16:17:16.554: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 16:17:21.564: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 16:17:21.564
  STEP: Getting /status @ 07/29/23 16:17:21.565
  Jul 29 16:17:21.572: INFO: Replicaset test-rs has Conditions: []
  STEP: updating the Replicaset Status @ 07/29/23 16:17:21.572
  Jul 29 16:17:21.603: INFO: updatedStatus.Conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the ReplicaSet status to be updated @ 07/29/23 16:17:21.603
  Jul 29 16:17:21.613: INFO: Observed &ReplicaSet event: ADDED
  Jul 29 16:17:21.614: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 16:17:21.614: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 16:17:21.623: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 16:17:21.623: INFO: Found replicaset test-rs in namespace replicaset-8402 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: [{StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}]
  Jul 29 16:17:21.624: INFO: Replicaset test-rs has an updated status
  STEP: patching the Replicaset Status @ 07/29/23 16:17:21.624
  Jul 29 16:17:21.624: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 29 16:17:21.647: INFO: Patched status conditions: []v1.ReplicaSetCondition{v1.ReplicaSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Replicaset status to be patched @ 07/29/23 16:17:21.647
  Jul 29 16:17:21.651: INFO: Observed &ReplicaSet event: ADDED
  Jul 29 16:17:21.652: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 16:17:21.653: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 16:17:21.654: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 16:17:21.654: INFO: Observed replicaset test-rs in namespace replicaset-8402 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 16:17:21.656: INFO: Observed &ReplicaSet event: MODIFIED
  Jul 29 16:17:21.656: INFO: Found replicaset test-rs in namespace replicaset-8402 with labels: map[name:sample-pod pod:httpd] annotations: map[] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC  }
  Jul 29 16:17:21.656: INFO: Replicaset test-rs has a patched status
  Jul 29 16:17:21.657: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-8402" for this suite. @ 07/29/23 16:17:21.666
• [5.187 seconds]
------------------------------
SSSSS
------------------------------
[sig-instrumentation] Events should delete a collection of events [Conformance]
test/e2e/instrumentation/core_events.go:175
  STEP: Creating a kubernetes client @ 07/29/23 16:17:21.681
  Jul 29 16:17:21.681: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename events @ 07/29/23 16:17:21.683
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:21.708
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:21.716
  STEP: Create set of events @ 07/29/23 16:17:21.719
  Jul 29 16:17:21.728: INFO: created test-event-1
  Jul 29 16:17:21.739: INFO: created test-event-2
  Jul 29 16:17:21.747: INFO: created test-event-3
  STEP: get a list of Events with a label in the current namespace @ 07/29/23 16:17:21.748
  STEP: delete collection of events @ 07/29/23 16:17:21.754
  Jul 29 16:17:21.754: INFO: requesting DeleteCollection of events
  STEP: check that the list of events matches the requested quantity @ 07/29/23 16:17:21.796
  Jul 29 16:17:21.796: INFO: requesting list of events to confirm quantity
  Jul 29 16:17:21.803: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-8242" for this suite. @ 07/29/23 16:17:21.809
• [0.136 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 should proxy through a service and a pod  [Conformance]
test/e2e/network/proxy.go:101
  STEP: Creating a kubernetes client @ 07/29/23 16:17:21.826
  Jul 29 16:17:21.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename proxy @ 07/29/23 16:17:21.829
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:21.859
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:21.864
  STEP: starting an echo server on multiple ports @ 07/29/23 16:17:21.889
  STEP: creating replication controller proxy-service-scp2t in namespace proxy-954 @ 07/29/23 16:17:21.889
  I0729 16:17:21.904267      13 runners.go:194] Created replication controller with name: proxy-service-scp2t, namespace: proxy-954, replica count: 1
  I0729 16:17:22.957133      13 runners.go:194] proxy-service-scp2t Pods: 1 out of 1 created, 0 running, 1 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  I0729 16:17:23.958458      13 runners.go:194] proxy-service-scp2t Pods: 1 out of 1 created, 0 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 1 runningButNotReady 
  I0729 16:17:24.959125      13 runners.go:194] proxy-service-scp2t Pods: 1 out of 1 created, 1 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:17:24.968: INFO: setup took 3.098675002s, starting test cases
  STEP: running 16 cases, 20 attempts per case, 320 total attempts @ 07/29/23 16:17:24.969
  Jul 29 16:17:24.995: INFO: (0) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 24.128989ms)
  Jul 29 16:17:24.996: INFO: (0) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 25.105155ms)
  Jul 29 16:17:24.996: INFO: (0) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 25.560618ms)
  Jul 29 16:17:24.996: INFO: (0) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 25.829477ms)
  Jul 29 16:17:24.996: INFO: (0) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 25.960004ms)
  Jul 29 16:17:24.996: INFO: (0) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 26.228839ms)
  Jul 29 16:17:24.996: INFO: (0) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 27.271419ms)
  Jul 29 16:17:24.996: INFO: (0) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 26.980681ms)
  Jul 29 16:17:24.997: INFO: (0) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 27.027761ms)
  Jul 29 16:17:25.000: INFO: (0) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 29.192883ms)
  Jul 29 16:17:25.000: INFO: (0) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 29.66455ms)
  Jul 29 16:17:25.011: INFO: (0) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 40.681462ms)
  Jul 29 16:17:25.011: INFO: (0) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 41.468981ms)
  Jul 29 16:17:25.011: INFO: (0) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 41.071461ms)
  Jul 29 16:17:25.013: INFO: (0) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 42.653971ms)
  Jul 29 16:17:25.014: INFO: (0) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 42.871127ms)
  Jul 29 16:17:25.033: INFO: (1) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 16.549597ms)
  Jul 29 16:17:25.033: INFO: (1) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 18.847534ms)
  Jul 29 16:17:25.035: INFO: (1) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 20.831314ms)
  Jul 29 16:17:25.036: INFO: (1) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 19.381774ms)
  Jul 29 16:17:25.036: INFO: (1) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 21.215881ms)
  Jul 29 16:17:25.037: INFO: (1) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 22.889134ms)
  Jul 29 16:17:25.037: INFO: (1) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 21.286732ms)
  Jul 29 16:17:25.039: INFO: (1) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 22.09519ms)
  Jul 29 16:17:25.039: INFO: (1) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 24.726436ms)
  Jul 29 16:17:25.042: INFO: (1) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 25.719543ms)
  Jul 29 16:17:25.045: INFO: (1) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 28.268209ms)
  Jul 29 16:17:25.045: INFO: (1) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 28.318697ms)
  Jul 29 16:17:25.045: INFO: (1) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 29.299912ms)
  Jul 29 16:17:25.045: INFO: (1) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 30.500761ms)
  Jul 29 16:17:25.045: INFO: (1) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 27.991968ms)
  Jul 29 16:17:25.045: INFO: (1) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 30.35346ms)
  Jul 29 16:17:25.067: INFO: (2) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 18.334858ms)
  Jul 29 16:17:25.067: INFO: (2) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 18.479854ms)
  Jul 29 16:17:25.072: INFO: (2) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 22.451385ms)
  Jul 29 16:17:25.079: INFO: (2) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 30.37063ms)
  Jul 29 16:17:25.080: INFO: (2) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 30.977044ms)
  Jul 29 16:17:25.082: INFO: (2) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 32.881468ms)
  Jul 29 16:17:25.084: INFO: (2) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 34.408591ms)
  Jul 29 16:17:25.083: INFO: (2) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 33.808634ms)
  Jul 29 16:17:25.083: INFO: (2) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 34.050229ms)
  Jul 29 16:17:25.083: INFO: (2) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 33.905804ms)
  Jul 29 16:17:25.085: INFO: (2) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 35.253114ms)
  Jul 29 16:17:25.090: INFO: (2) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 40.211168ms)
  Jul 29 16:17:25.091: INFO: (2) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 41.203183ms)
  Jul 29 16:17:25.091: INFO: (2) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 41.228333ms)
  Jul 29 16:17:25.097: INFO: (2) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 47.348558ms)
  Jul 29 16:17:25.098: INFO: (2) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 48.112901ms)
  Jul 29 16:17:25.114: INFO: (3) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 15.065343ms)
  Jul 29 16:17:25.118: INFO: (3) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 19.024209ms)
  Jul 29 16:17:25.119: INFO: (3) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 19.730283ms)
  Jul 29 16:17:25.119: INFO: (3) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 19.549234ms)
  Jul 29 16:17:25.120: INFO: (3) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 20.890083ms)
  Jul 29 16:17:25.121: INFO: (3) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 21.291533ms)
  Jul 29 16:17:25.122: INFO: (3) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 21.5484ms)
  Jul 29 16:17:25.122: INFO: (3) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 24.190506ms)
  Jul 29 16:17:25.129: INFO: (3) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 28.33944ms)
  Jul 29 16:17:25.130: INFO: (3) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 29.458082ms)
  Jul 29 16:17:25.130: INFO: (3) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 30.369444ms)
  Jul 29 16:17:25.130: INFO: (3) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 29.949009ms)
  Jul 29 16:17:25.131: INFO: (3) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 30.918432ms)
  Jul 29 16:17:25.132: INFO: (3) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 31.406143ms)
  Jul 29 16:17:25.133: INFO: (3) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 32.480128ms)
  Jul 29 16:17:25.134: INFO: (3) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 32.737991ms)
  Jul 29 16:17:25.147: INFO: (4) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 12.717338ms)
  Jul 29 16:17:25.147: INFO: (4) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 12.941715ms)
  Jul 29 16:17:25.152: INFO: (4) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 16.619808ms)
  Jul 29 16:17:25.152: INFO: (4) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 17.930535ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 20.720296ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 20.638044ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 20.323321ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 20.945669ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 20.123282ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 20.387314ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 20.06982ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 20.619229ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 20.048595ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 19.905933ms)
  Jul 29 16:17:25.155: INFO: (4) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 21.212528ms)
  Jul 29 16:17:25.159: INFO: (4) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 24.459369ms)
  Jul 29 16:17:25.171: INFO: (5) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 11.710938ms)
  Jul 29 16:17:25.171: INFO: (5) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 11.55209ms)
  Jul 29 16:17:25.172: INFO: (5) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 12.570581ms)
  Jul 29 16:17:25.174: INFO: (5) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 12.901249ms)
  Jul 29 16:17:25.174: INFO: (5) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 14.193116ms)
  Jul 29 16:17:25.174: INFO: (5) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 14.795322ms)
  Jul 29 16:17:25.176: INFO: (5) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 15.402231ms)
  Jul 29 16:17:25.176: INFO: (5) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 16.333801ms)
  Jul 29 16:17:25.177: INFO: (5) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 16.031066ms)
  Jul 29 16:17:25.177: INFO: (5) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 16.259488ms)
  Jul 29 16:17:25.177: INFO: (5) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 16.372035ms)
  Jul 29 16:17:25.177: INFO: (5) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 16.712988ms)
  Jul 29 16:17:25.178: INFO: (5) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 18.011185ms)
  Jul 29 16:17:25.179: INFO: (5) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 19.150327ms)
  Jul 29 16:17:25.180: INFO: (5) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 19.198876ms)
  Jul 29 16:17:25.181: INFO: (5) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 20.716959ms)
  Jul 29 16:17:25.195: INFO: (6) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 12.435665ms)
  Jul 29 16:17:25.195: INFO: (6) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 12.875972ms)
  Jul 29 16:17:25.195: INFO: (6) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 13.1249ms)
  Jul 29 16:17:25.195: INFO: (6) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 12.553207ms)
  Jul 29 16:17:25.195: INFO: (6) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 12.631513ms)
  Jul 29 16:17:25.195: INFO: (6) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 13.718695ms)
  Jul 29 16:17:25.197: INFO: (6) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 14.132988ms)
  Jul 29 16:17:25.199: INFO: (6) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 16.468983ms)
  Jul 29 16:17:25.200: INFO: (6) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 17.142902ms)
  Jul 29 16:17:25.201: INFO: (6) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 17.738069ms)
  Jul 29 16:17:25.201: INFO: (6) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 18.502221ms)
  Jul 29 16:17:25.201: INFO: (6) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 18.160384ms)
  Jul 29 16:17:25.203: INFO: (6) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 20.140575ms)
  Jul 29 16:17:25.203: INFO: (6) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 20.368881ms)
  Jul 29 16:17:25.203: INFO: (6) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 21.880913ms)
  Jul 29 16:17:25.207: INFO: (6) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 25.184325ms)
  Jul 29 16:17:25.222: INFO: (7) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 14.102651ms)
  Jul 29 16:17:25.224: INFO: (7) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 16.648202ms)
  Jul 29 16:17:25.224: INFO: (7) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 16.56319ms)
  Jul 29 16:17:25.228: INFO: (7) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 20.53812ms)
  Jul 29 16:17:25.228: INFO: (7) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 20.568794ms)
  Jul 29 16:17:25.229: INFO: (7) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 20.950103ms)
  Jul 29 16:17:25.229: INFO: (7) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 20.561941ms)
  Jul 29 16:17:25.230: INFO: (7) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 21.856234ms)
  Jul 29 16:17:25.230: INFO: (7) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 22.079888ms)
  Jul 29 16:17:25.230: INFO: (7) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 22.31159ms)
  Jul 29 16:17:25.231: INFO: (7) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 22.961919ms)
  Jul 29 16:17:25.231: INFO: (7) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 23.418405ms)
  Jul 29 16:17:25.231: INFO: (7) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 23.270317ms)
  Jul 29 16:17:25.232: INFO: (7) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 24.372654ms)
  Jul 29 16:17:25.233: INFO: (7) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 24.880853ms)
  Jul 29 16:17:25.234: INFO: (7) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 26.594655ms)
  Jul 29 16:17:25.245: INFO: (8) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 11.034941ms)
  Jul 29 16:17:25.247: INFO: (8) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 12.794033ms)
  Jul 29 16:17:25.250: INFO: (8) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 15.54706ms)
  Jul 29 16:17:25.252: INFO: (8) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 17.469714ms)
  Jul 29 16:17:25.253: INFO: (8) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 18.075574ms)
  Jul 29 16:17:25.253: INFO: (8) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 18.375968ms)
  Jul 29 16:17:25.254: INFO: (8) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 18.919627ms)
  Jul 29 16:17:25.255: INFO: (8) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 20.058897ms)
  Jul 29 16:17:25.257: INFO: (8) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 21.807561ms)
  Jul 29 16:17:25.259: INFO: (8) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 23.949597ms)
  Jul 29 16:17:25.259: INFO: (8) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 24.79102ms)
  Jul 29 16:17:25.260: INFO: (8) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 25.165792ms)
  Jul 29 16:17:25.262: INFO: (8) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 26.927833ms)
  Jul 29 16:17:25.264: INFO: (8) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 29.837924ms)
  Jul 29 16:17:25.266: INFO: (8) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 30.979607ms)
  Jul 29 16:17:25.266: INFO: (8) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 31.616334ms)
  Jul 29 16:17:25.281: INFO: (9) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 13.974915ms)
  Jul 29 16:17:25.282: INFO: (9) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 15.145839ms)
  Jul 29 16:17:25.283: INFO: (9) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 16.392478ms)
  Jul 29 16:17:25.283: INFO: (9) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 16.607324ms)
  Jul 29 16:17:25.284: INFO: (9) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 16.19657ms)
  Jul 29 16:17:25.284: INFO: (9) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 17.25067ms)
  Jul 29 16:17:25.285: INFO: (9) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 17.287101ms)
  Jul 29 16:17:25.286: INFO: (9) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 18.725068ms)
  Jul 29 16:17:25.286: INFO: (9) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 18.83043ms)
  Jul 29 16:17:25.287: INFO: (9) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 19.05074ms)
  Jul 29 16:17:25.287: INFO: (9) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 19.108481ms)
  Jul 29 16:17:25.287: INFO: (9) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 20.108082ms)
  Jul 29 16:17:25.289: INFO: (9) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 22.115503ms)
  Jul 29 16:17:25.289: INFO: (9) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 22.206186ms)
  Jul 29 16:17:25.289: INFO: (9) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 22.672818ms)
  Jul 29 16:17:25.291: INFO: (9) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 24.36556ms)
  Jul 29 16:17:25.300: INFO: (10) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 8.475853ms)
  Jul 29 16:17:25.301: INFO: (10) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 9.139032ms)
  Jul 29 16:17:25.303: INFO: (10) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 10.924844ms)
  Jul 29 16:17:25.305: INFO: (10) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 12.545208ms)
  Jul 29 16:17:25.305: INFO: (10) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 12.536441ms)
  Jul 29 16:17:25.305: INFO: (10) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 13.550688ms)
  Jul 29 16:17:25.306: INFO: (10) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 13.409855ms)
  Jul 29 16:17:25.309: INFO: (10) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 16.892211ms)
  Jul 29 16:17:25.310: INFO: (10) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 17.254695ms)
  Jul 29 16:17:25.310: INFO: (10) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 17.720617ms)
  Jul 29 16:17:25.310: INFO: (10) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 18.473245ms)
  Jul 29 16:17:25.311: INFO: (10) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 18.908853ms)
  Jul 29 16:17:25.312: INFO: (10) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 19.695448ms)
  Jul 29 16:17:25.312: INFO: (10) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 19.649943ms)
  Jul 29 16:17:25.313: INFO: (10) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 20.578666ms)
  Jul 29 16:17:25.315: INFO: (10) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 22.750345ms)
  Jul 29 16:17:25.328: INFO: (11) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 12.104073ms)
  Jul 29 16:17:25.328: INFO: (11) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 12.63837ms)
  Jul 29 16:17:25.333: INFO: (11) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 17.259701ms)
  Jul 29 16:17:25.335: INFO: (11) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 19.2251ms)
  Jul 29 16:17:25.336: INFO: (11) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 20.641407ms)
  Jul 29 16:17:25.337: INFO: (11) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 21.389025ms)
  Jul 29 16:17:25.337: INFO: (11) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 21.17603ms)
  Jul 29 16:17:25.337: INFO: (11) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 21.380706ms)
  Jul 29 16:17:25.337: INFO: (11) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 21.721653ms)
  Jul 29 16:17:25.339: INFO: (11) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 23.628161ms)
  Jul 29 16:17:25.340: INFO: (11) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 24.444047ms)
  Jul 29 16:17:25.340: INFO: (11) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 24.631802ms)
  Jul 29 16:17:25.340: INFO: (11) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 24.476807ms)
  Jul 29 16:17:25.340: INFO: (11) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 24.87887ms)
  Jul 29 16:17:25.341: INFO: (11) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 25.164775ms)
  Jul 29 16:17:25.341: INFO: (11) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 25.265207ms)
  Jul 29 16:17:25.353: INFO: (12) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 11.409413ms)
  Jul 29 16:17:25.353: INFO: (12) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 10.281394ms)
  Jul 29 16:17:25.353: INFO: (12) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 11.924568ms)
  Jul 29 16:17:25.353: INFO: (12) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 11.86369ms)
  Jul 29 16:17:25.354: INFO: (12) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 11.864898ms)
  Jul 29 16:17:25.357: INFO: (12) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 14.63655ms)
  Jul 29 16:17:25.358: INFO: (12) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 15.658346ms)
  Jul 29 16:17:25.358: INFO: (12) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 15.707037ms)
  Jul 29 16:17:25.358: INFO: (12) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 15.731148ms)
  Jul 29 16:17:25.360: INFO: (12) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 18.366744ms)
  Jul 29 16:17:25.360: INFO: (12) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 17.616638ms)
  Jul 29 16:17:25.360: INFO: (12) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 19.042082ms)
  Jul 29 16:17:25.361: INFO: (12) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 18.361408ms)
  Jul 29 16:17:25.361: INFO: (12) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 19.621733ms)
  Jul 29 16:17:25.362: INFO: (12) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 20.526018ms)
  Jul 29 16:17:25.362: INFO: (12) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 20.657483ms)
  Jul 29 16:17:25.379: INFO: (13) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 16.035414ms)
  Jul 29 16:17:25.382: INFO: (13) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 18.854839ms)
  Jul 29 16:17:25.382: INFO: (13) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 19.08107ms)
  Jul 29 16:17:25.384: INFO: (13) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 21.199106ms)
  Jul 29 16:17:25.384: INFO: (13) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 21.730862ms)
  Jul 29 16:17:25.385: INFO: (13) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 22.285721ms)
  Jul 29 16:17:25.385: INFO: (13) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 22.238792ms)
  Jul 29 16:17:25.387: INFO: (13) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 23.617372ms)
  Jul 29 16:17:25.388: INFO: (13) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 24.395625ms)
  Jul 29 16:17:25.388: INFO: (13) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 24.779768ms)
  Jul 29 16:17:25.388: INFO: (13) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 25.52755ms)
  Jul 29 16:17:25.389: INFO: (13) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 25.339761ms)
  Jul 29 16:17:25.389: INFO: (13) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 25.536311ms)
  Jul 29 16:17:25.389: INFO: (13) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 25.73373ms)
  Jul 29 16:17:25.389: INFO: (13) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 26.261088ms)
  Jul 29 16:17:25.392: INFO: (13) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 28.44696ms)
  Jul 29 16:17:25.402: INFO: (14) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 10.059738ms)
  Jul 29 16:17:25.403: INFO: (14) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 11.00315ms)
  Jul 29 16:17:25.407: INFO: (14) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 14.326467ms)
  Jul 29 16:17:25.407: INFO: (14) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 14.286528ms)
  Jul 29 16:17:25.407: INFO: (14) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 14.911471ms)
  Jul 29 16:17:25.409: INFO: (14) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 17.355269ms)
  Jul 29 16:17:25.410: INFO: (14) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 17.017868ms)
  Jul 29 16:17:25.410: INFO: (14) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 17.69036ms)
  Jul 29 16:17:25.412: INFO: (14) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 19.343794ms)
  Jul 29 16:17:25.412: INFO: (14) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 19.463212ms)
  Jul 29 16:17:25.412: INFO: (14) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 19.854478ms)
  Jul 29 16:17:25.412: INFO: (14) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 20.330789ms)
  Jul 29 16:17:25.412: INFO: (14) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 19.480169ms)
  Jul 29 16:17:25.412: INFO: (14) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 20.083654ms)
  Jul 29 16:17:25.412: INFO: (14) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 19.471927ms)
  Jul 29 16:17:25.413: INFO: (14) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 20.515432ms)
  Jul 29 16:17:25.431: INFO: (15) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 17.185797ms)
  Jul 29 16:17:25.431: INFO: (15) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 17.364515ms)
  Jul 29 16:17:25.431: INFO: (15) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 17.251762ms)
  Jul 29 16:17:25.431: INFO: (15) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 18.287351ms)
  Jul 29 16:17:25.431: INFO: (15) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 18.148114ms)
  Jul 29 16:17:25.432: INFO: (15) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 19.381176ms)
  Jul 29 16:17:25.433: INFO: (15) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 20.0509ms)
  Jul 29 16:17:25.434: INFO: (15) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 20.67009ms)
  Jul 29 16:17:25.434: INFO: (15) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 21.036657ms)
  Jul 29 16:17:25.434: INFO: (15) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 21.016916ms)
  Jul 29 16:17:25.435: INFO: (15) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 21.712839ms)
  Jul 29 16:17:25.438: INFO: (15) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 24.574757ms)
  Jul 29 16:17:25.439: INFO: (15) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 25.50507ms)
  Jul 29 16:17:25.440: INFO: (15) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 25.942496ms)
  Jul 29 16:17:25.440: INFO: (15) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 27.073608ms)
  Jul 29 16:17:25.441: INFO: (15) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 27.841589ms)
  Jul 29 16:17:25.451: INFO: (16) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 9.775758ms)
  Jul 29 16:17:25.452: INFO: (16) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 10.459892ms)
  Jul 29 16:17:25.453: INFO: (16) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 10.778777ms)
  Jul 29 16:17:25.455: INFO: (16) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 12.071139ms)
  Jul 29 16:17:25.456: INFO: (16) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 13.222103ms)
  Jul 29 16:17:25.456: INFO: (16) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 14.366055ms)
  Jul 29 16:17:25.457: INFO: (16) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 14.268162ms)
  Jul 29 16:17:25.458: INFO: (16) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 15.723909ms)
  Jul 29 16:17:25.460: INFO: (16) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 17.713584ms)
  Jul 29 16:17:25.464: INFO: (16) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 22.212596ms)
  Jul 29 16:17:25.464: INFO: (16) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 22.524573ms)
  Jul 29 16:17:25.465: INFO: (16) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 22.611591ms)
  Jul 29 16:17:25.465: INFO: (16) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 23.582385ms)
  Jul 29 16:17:25.466: INFO: (16) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 24.525526ms)
  Jul 29 16:17:25.467: INFO: (16) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 24.826908ms)
  Jul 29 16:17:25.469: INFO: (16) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 26.718668ms)
  Jul 29 16:17:25.477: INFO: (17) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 7.88531ms)
  Jul 29 16:17:25.481: INFO: (17) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 11.155433ms)
  Jul 29 16:17:25.482: INFO: (17) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 12.509556ms)
  Jul 29 16:17:25.485: INFO: (17) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 14.653029ms)
  Jul 29 16:17:25.489: INFO: (17) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 19.418664ms)
  Jul 29 16:17:25.489: INFO: (17) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 19.064746ms)
  Jul 29 16:17:25.490: INFO: (17) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 19.180446ms)
  Jul 29 16:17:25.491: INFO: (17) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 19.859258ms)
  Jul 29 16:17:25.491: INFO: (17) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 20.497702ms)
  Jul 29 16:17:25.491: INFO: (17) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 20.175176ms)
  Jul 29 16:17:25.491: INFO: (17) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 21.122876ms)
  Jul 29 16:17:25.492: INFO: (17) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 21.27851ms)
  Jul 29 16:17:25.492: INFO: (17) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 21.969232ms)
  Jul 29 16:17:25.493: INFO: (17) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 22.668026ms)
  Jul 29 16:17:25.493: INFO: (17) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 22.357505ms)
  Jul 29 16:17:25.496: INFO: (17) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 24.786489ms)
  Jul 29 16:17:25.507: INFO: (18) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 10.887997ms)
  Jul 29 16:17:25.509: INFO: (18) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 12.223694ms)
  Jul 29 16:17:25.509: INFO: (18) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 13.22913ms)
  Jul 29 16:17:25.512: INFO: (18) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 14.960641ms)
  Jul 29 16:17:25.512: INFO: (18) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 15.046132ms)
  Jul 29 16:17:25.512: INFO: (18) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 15.588369ms)
  Jul 29 16:17:25.512: INFO: (18) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 16.01316ms)
  Jul 29 16:17:25.514: INFO: (18) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 17.31035ms)
  Jul 29 16:17:25.515: INFO: (18) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 17.17608ms)
  Jul 29 16:17:25.515: INFO: (18) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 17.525865ms)
  Jul 29 16:17:25.515: INFO: (18) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 18.795864ms)
  Jul 29 16:17:25.515: INFO: (18) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 18.813698ms)
  Jul 29 16:17:25.516: INFO: (18) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 19.878914ms)
  Jul 29 16:17:25.517: INFO: (18) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 20.301729ms)
  Jul 29 16:17:25.517: INFO: (18) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 21.064037ms)
  Jul 29 16:17:25.519: INFO: (18) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 22.447914ms)
  Jul 29 16:17:25.529: INFO: (19) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:1080/proxy/rewriteme">t... (200; 8.956555ms)
  Jul 29 16:17:25.531: INFO: (19) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:162/proxy/: bar (200; 10.868714ms)
  Jul 29 16:17:25.532: INFO: (19) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:1080/proxy/rewriteme">test</... (200; 11.93615ms)
  Jul 29 16:17:25.533: INFO: (19) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:460/proxy/: tls baz (200; 13.108659ms)
  Jul 29 16:17:25.534: INFO: (19) /api/v1/namespaces/proxy-954/pods/http:proxy-service-scp2t-9np7t:160/proxy/: foo (200; 13.317084ms)
  Jul 29 16:17:25.534: INFO: (19) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:443/proxy/tlsrewriteme... (200; 13.682209ms)
  Jul 29 16:17:25.535: INFO: (19) /api/v1/namespaces/proxy-954/pods/https:proxy-service-scp2t-9np7t:462/proxy/: tls qux (200; 14.662174ms)
  Jul 29 16:17:25.537: INFO: (19) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:162/proxy/: bar (200; 16.217161ms)
  Jul 29 16:17:25.537: INFO: (19) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname1/proxy/: foo (200; 16.236673ms)
  Jul 29 16:17:25.537: INFO: (19) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/: <a href="/api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t/proxy/rewriteme">test</a> (200; 16.92774ms)
  Jul 29 16:17:25.537: INFO: (19) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname2/proxy/: tls qux (200; 17.09207ms)
  Jul 29 16:17:25.538: INFO: (19) /api/v1/namespaces/proxy-954/services/https:proxy-service-scp2t:tlsportname1/proxy/: tls baz (200; 17.118521ms)
  Jul 29 16:17:25.540: INFO: (19) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname1/proxy/: foo (200; 19.866956ms)
  Jul 29 16:17:25.543: INFO: (19) /api/v1/namespaces/proxy-954/pods/proxy-service-scp2t-9np7t:160/proxy/: foo (200; 22.553002ms)
  Jul 29 16:17:25.544: INFO: (19) /api/v1/namespaces/proxy-954/services/http:proxy-service-scp2t:portname2/proxy/: bar (200; 24.001399ms)
  Jul 29 16:17:25.545: INFO: (19) /api/v1/namespaces/proxy-954/services/proxy-service-scp2t:portname2/proxy/: bar (200; 24.681589ms)
  Jul 29 16:17:25.545: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController proxy-service-scp2t in namespace proxy-954, will wait for the garbage collector to delete the pods @ 07/29/23 16:17:25.551
  Jul 29 16:17:25.620: INFO: Deleting ReplicationController proxy-service-scp2t took: 13.521413ms
  Jul 29 16:17:25.721: INFO: Terminating ReplicationController proxy-service-scp2t pods took: 101.202229ms
  STEP: Destroying namespace "proxy-954" for this suite. @ 07/29/23 16:17:28.124
• [6.313 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and ensure its status is promptly calculated. [Conformance]
test/e2e/apimachinery/resource_quota.go:76
  STEP: Creating a kubernetes client @ 07/29/23 16:17:28.146
  Jul 29 16:17:28.146: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:17:28.148
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:28.177
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:28.181
  STEP: Counting existing ResourceQuota @ 07/29/23 16:17:28.186
  STEP: Creating a ResourceQuota @ 07/29/23 16:17:33.197
  STEP: Ensuring resource quota status is calculated @ 07/29/23 16:17:33.212
  Jul 29 16:17:35.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7994" for this suite. @ 07/29/23 16:17:35.228
• [7.094 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected combined should project all components that make up the projection API [Projection][NodeConformance] [Conformance]
test/e2e/common/storage/projected_combined.go:44
  STEP: Creating a kubernetes client @ 07/29/23 16:17:35.256
  Jul 29 16:17:35.257: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:17:35.259
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:35.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:35.333
  STEP: Creating configMap with name configmap-projected-all-test-volume-ad4f427a-c07e-4df5-9a59-335a005508dc @ 07/29/23 16:17:35.338
  STEP: Creating secret with name secret-projected-all-test-volume-211b8c3d-4217-4761-a9f7-a0ec16b01e68 @ 07/29/23 16:17:35.348
  STEP: Creating a pod to test Check all projections for projected volume plugin @ 07/29/23 16:17:35.359
  STEP: Saw pod success @ 07/29/23 16:17:39.401
  Jul 29 16:17:39.406: INFO: Trying to get logs from node ci9axai7aiv7-3 pod projected-volume-c68fd50a-b82f-4bb2-965e-8a0f9968b780 container projected-all-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:17:39.422
  Jul 29 16:17:39.448: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5298" for this suite. @ 07/29/23 16:17:39.457
• [4.216 seconds]
------------------------------
SS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:78
  STEP: Creating a kubernetes client @ 07/29/23 16:17:39.473
  Jul 29 16:17:39.473: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:17:39.476
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:39.504
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:39.509
  STEP: Creating projection with secret that has name projected-secret-test-map-99568c0b-ff86-43bf-adf0-55f592561abd @ 07/29/23 16:17:39.513
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:17:39.524
  STEP: Saw pod success @ 07/29/23 16:17:43.595
  Jul 29 16:17:43.602: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-secrets-117bec06-1679-4692-9398-a0ba5826249b container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:17:43.614
  Jul 29 16:17:43.640: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-5021" for this suite. @ 07/29/23 16:17:43.647
• [4.184 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with mappings and Item Mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:88
  STEP: Creating a kubernetes client @ 07/29/23 16:17:43.66
  Jul 29 16:17:43.660: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:17:43.661
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:43.684
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:43.688
  STEP: Creating projection with secret that has name projected-secret-test-map-70ced63e-4136-4c42-abe1-5a6f68efc1c9 @ 07/29/23 16:17:43.692
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:17:43.703
  STEP: Saw pod success @ 07/29/23 16:17:47.739
  Jul 29 16:17:47.745: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-secrets-6d7c7298-f7d4-4790-bc21-884d42680ba6 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:17:47.759
  Jul 29 16:17:47.785: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7593" for this suite. @ 07/29/23 16:17:47.795
• [4.150 seconds]
------------------------------
SSSS
------------------------------
[sig-node] RuntimeClass should schedule a Pod requesting a RuntimeClass and initialize its Overhead [NodeConformance] [Conformance]
test/e2e/common/node/runtimeclass.go:129
  STEP: Creating a kubernetes client @ 07/29/23 16:17:47.811
  Jul 29 16:17:47.811: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 16:17:47.814
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:47.841
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:47.847
  Jul 29 16:17:47.888: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-5311" for this suite. @ 07/29/23 16:17:47.912
• [0.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should contain environment variables for services [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:445
  STEP: Creating a kubernetes client @ 07/29/23 16:17:47.933
  Jul 29 16:17:47.933: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:17:47.935
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:47.963
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:47.969
  STEP: Saw pod success @ 07/29/23 16:17:54.111
  Jul 29 16:17:54.120: INFO: Trying to get logs from node ci9axai7aiv7-3 pod client-envvars-a4276ea9-7471-4b1a-81cc-94a4e8d8d1bc container env3cont: <nil>
  STEP: delete the pod @ 07/29/23 16:17:54.167
  Jul 29 16:17:54.216: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9880" for this suite. @ 07/29/23 16:17:54.226
• [6.306 seconds]
------------------------------
SS
------------------------------
[sig-node] Probing container should *not* be restarted with a exec "cat /tmp/health" liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:151
  STEP: Creating a kubernetes client @ 07/29/23 16:17:54.239
  Jul 29 16:17:54.239: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:17:54.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:17:54.269
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:17:54.274
  STEP: Creating pod busybox-f02a1fd3-3a53-4dd0-a554-42dae96d37b7 in namespace container-probe-9625 @ 07/29/23 16:17:54.279
  Jul 29 16:17:56.312: INFO: Started pod busybox-f02a1fd3-3a53-4dd0-a554-42dae96d37b7 in namespace container-probe-9625
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:17:56.312
  Jul 29 16:17:56.319: INFO: Initial restart count of pod busybox-f02a1fd3-3a53-4dd0-a554-42dae96d37b7 is 0
  Jul 29 16:21:57.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:21:57.564
  STEP: Destroying namespace "container-probe-9625" for this suite. @ 07/29/23 16:21:57.591
• [243.386 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Variable Expansion should allow composing env vars into new env vars [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:47
  STEP: Creating a kubernetes client @ 07/29/23 16:21:57.627
  Jul 29 16:21:57.627: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:21:57.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:21:57.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:21:57.678
  STEP: Creating a pod to test env composition @ 07/29/23 16:21:57.682
  STEP: Saw pod success @ 07/29/23 16:22:01.719
  Jul 29 16:22:01.726: INFO: Trying to get logs from node ci9axai7aiv7-3 pod var-expansion-a18f1718-646e-4668-a6e3-aac6d7e3fde0 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:22:01.769
  Jul 29 16:22:01.796: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-8352" for this suite. @ 07/29/23 16:22:01.807
• [4.196 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:67
  STEP: Creating a kubernetes client @ 07/29/23 16:22:01.824
  Jul 29 16:22:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:22:01.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:22:01.87
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:22:01.875
  STEP: Creating projection with secret that has name projected-secret-test-7bf0bef7-9be4-4349-9f1a-2ace67b2a532 @ 07/29/23 16:22:01.881
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:22:01.889
  STEP: Saw pod success @ 07/29/23 16:22:05.938
  Jul 29 16:22:05.947: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-secrets-d7d779cd-ff95-47d3-91fe-91e97ae58df8 container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:22:05.967
  Jul 29 16:22:05.998: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-4760" for this suite. @ 07/29/23 16:22:06.008
• [4.197 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should perform canary updates and phased rolling updates of template modifications [Conformance]
test/e2e/apps/statefulset.go:327
  STEP: Creating a kubernetes client @ 07/29/23 16:22:06.025
  Jul 29 16:22:06.025: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:22:06.027
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:22:06.067
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:22:06.074
  STEP: Creating service test in namespace statefulset-6429 @ 07/29/23 16:22:06.081
  STEP: Creating a new StatefulSet @ 07/29/23 16:22:06.096
  Jul 29 16:22:06.132: INFO: Found 0 stateful pods, waiting for 3
  Jul 29 16:22:16.146: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:22:16.146: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:22:16.146: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Updating stateful set template: update image from registry.k8s.io/e2e-test-images/httpd:2.4.38-4 to registry.k8s.io/e2e-test-images/httpd:2.4.39-4 @ 07/29/23 16:22:16.169
  Jul 29 16:22:16.204: INFO: Updating stateful set ss2
  STEP: Creating a new revision @ 07/29/23 16:22:16.204
  STEP: Not applying an update when the partition is greater than the number of replicas @ 07/29/23 16:22:26.243
  STEP: Performing a canary update @ 07/29/23 16:22:26.244
  Jul 29 16:22:26.275: INFO: Updating stateful set ss2
  Jul 29 16:22:26.296: INFO: Waiting for Pod statefulset-6429/ss2-2 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  STEP: Restoring Pods to the correct revision when they are deleted @ 07/29/23 16:22:36.315
  Jul 29 16:22:36.451: INFO: Found 1 stateful pods, waiting for 3
  Jul 29 16:22:46.467: INFO: Waiting for pod ss2-0 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:22:46.467: INFO: Waiting for pod ss2-1 to enter Running - Ready=true, currently Running - Ready=true
  Jul 29 16:22:46.467: INFO: Waiting for pod ss2-2 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Performing a phased rolling update @ 07/29/23 16:22:46.48
  Jul 29 16:22:46.512: INFO: Updating stateful set ss2
  Jul 29 16:22:46.552: INFO: Waiting for Pod statefulset-6429/ss2-1 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul 29 16:22:56.612: INFO: Updating stateful set ss2
  Jul 29 16:22:56.629: INFO: Waiting for StatefulSet statefulset-6429/ss2 to complete update
  Jul 29 16:22:56.629: INFO: Waiting for Pod statefulset-6429/ss2-0 to have revision ss2-5459d8585b update revision ss2-7b6c9599d5
  Jul 29 16:23:06.644: INFO: Deleting all statefulset in ns statefulset-6429
  Jul 29 16:23:06.650: INFO: Scaling statefulset ss2 to 0
  Jul 29 16:23:16.692: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:23:16.701: INFO: Deleting statefulset ss2
  Jul 29 16:23:16.729: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-6429" for this suite. @ 07/29/23 16:23:16.739
• [70.731 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to switch session affinity for NodePort service [LinuxOnly] [Conformance]
test/e2e/network/service.go:2224
  STEP: Creating a kubernetes client @ 07/29/23 16:23:16.761
  Jul 29 16:23:16.761: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 16:23:16.764
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:23:16.794
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:23:16.8
  STEP: creating service in namespace services-4864 @ 07/29/23 16:23:16.806
  STEP: creating service affinity-nodeport-transition in namespace services-4864 @ 07/29/23 16:23:16.806
  STEP: creating replication controller affinity-nodeport-transition in namespace services-4864 @ 07/29/23 16:23:16.836
  I0729 16:23:16.860239      13 runners.go:194] Created replication controller with name: affinity-nodeport-transition, namespace: services-4864, replica count: 3
  I0729 16:23:19.912939      13 runners.go:194] affinity-nodeport-transition Pods: 3 out of 3 created, 3 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 16:23:19.939: INFO: Creating new exec pod
  Jul 29 16:23:22.990: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4864 exec execpod-affinityg5kkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 affinity-nodeport-transition 80'
  Jul 29 16:23:23.390: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 affinity-nodeport-transition 80\nConnection to affinity-nodeport-transition 80 port [tcp/http] succeeded!\n"
  Jul 29 16:23:23.390: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:23:23.390: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4864 exec execpod-affinityg5kkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.16.187 80'
  Jul 29 16:23:23.702: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.16.187 80\nConnection to 10.233.16.187 80 port [tcp/http] succeeded!\n"
  Jul 29 16:23:23.702: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:23:23.703: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4864 exec execpod-affinityg5kkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.144 30853'
  Jul 29 16:23:23.991: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.144 30853\nConnection to 192.168.121.144 30853 port [tcp/*] succeeded!\n"
  Jul 29 16:23:23.991: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:23:23.991: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4864 exec execpod-affinityg5kkd -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.49 30853'
  Jul 29 16:23:24.218: INFO: stderr: "+ + echo hostName\nnc -v -t -w 2 192.168.121.49 30853\nConnection to 192.168.121.49 30853 port [tcp/*] succeeded!\n"
  Jul 29 16:23:24.218: INFO: stdout: "HTTP/1.1 400 Bad Request\r\nContent-Type: text/plain; charset=utf-8\r\nConnection: close\r\n\r\n400 Bad Request"
  Jul 29 16:23:24.237: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4864 exec execpod-affinityg5kkd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.39:30853/ ; done'
  Jul 29 16:23:24.729: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n"
  Jul 29 16:23:24.729: INFO: stdout: "\naffinity-nodeport-transition-gwmc9\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-2znsw\naffinity-nodeport-transition-gwmc9\naffinity-nodeport-transition-gwmc9\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-2znsw\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-2znsw\naffinity-nodeport-transition-2znsw\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-gwmc9\naffinity-nodeport-transition-2znsw\naffinity-nodeport-transition-gwmc9\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt"
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-gwmc9
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-2znsw
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-gwmc9
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-gwmc9
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-2znsw
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-2znsw
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-2znsw
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-gwmc9
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-2znsw
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-gwmc9
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:24.729: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:24.746: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-4864 exec execpod-affinityg5kkd -- /bin/sh -x -c for i in $(seq 0 15); do echo; curl -q -s --connect-timeout 2 http://192.168.121.39:30853/ ; done'
  Jul 29 16:23:25.129: INFO: stderr: "+ seq 0 15\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n+ echo\n+ curl -q -s --connect-timeout 2 http://192.168.121.39:30853/\n"
  Jul 29 16:23:25.130: INFO: stdout: "\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt\naffinity-nodeport-transition-6vtxt"
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Received response from host: affinity-nodeport-transition-6vtxt
  Jul 29 16:23:25.131: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:23:25.140: INFO: Cleaning up the exec pod
  STEP: deleting ReplicationController affinity-nodeport-transition in namespace services-4864, will wait for the garbage collector to delete the pods @ 07/29/23 16:23:25.189
  Jul 29 16:23:25.258: INFO: Deleting ReplicationController affinity-nodeport-transition took: 11.118429ms
  Jul 29 16:23:25.359: INFO: Terminating ReplicationController affinity-nodeport-transition pods took: 101.343708ms
  STEP: Destroying namespace "services-4864" for this suite. @ 07/29/23 16:23:27.635
• [10.885 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PreStop should call prestop when killing a pod  [Conformance]
test/e2e/node/pre_stop.go:169
  STEP: Creating a kubernetes client @ 07/29/23 16:23:27.65
  Jul 29 16:23:27.650: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename prestop @ 07/29/23 16:23:27.653
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:23:27.686
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:23:27.691
  STEP: Creating server pod server in namespace prestop-7063 @ 07/29/23 16:23:27.695
  STEP: Waiting for pods to come up. @ 07/29/23 16:23:27.708
  STEP: Creating tester pod tester in namespace prestop-7063 @ 07/29/23 16:23:29.739
  STEP: Deleting pre-stop pod @ 07/29/23 16:23:31.774
  Jul 29 16:23:36.798: INFO: Saw: {
  	"Hostname": "server",
  	"Sent": null,
  	"Received": {
  		"prestop": 1
  	},
  	"Errors": null,
  	"Log": [
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up.",
  		"default/nettest has 0 endpoints ([]), which is less than 8 as expected. Waiting for all endpoints to come up."
  	],
  	"StillContactingPeers": true
  }
  Jul 29 16:23:36.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Deleting the server pod @ 07/29/23 16:23:36.811
  STEP: Destroying namespace "prestop-7063" for this suite. @ 07/29/23 16:23:36.857
• [9.223 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply a valid CR for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:168
  STEP: Creating a kubernetes client @ 07/29/23 16:23:36.88
  Jul 29 16:23:36.880: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 16:23:36.888
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:23:36.935
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:23:36.939
  Jul 29 16:23:36.943: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  W0729 16:23:39.746279      13 warnings.go:70] unknown field "alpha"
  W0729 16:23:39.746340      13 warnings.go:70] unknown field "beta"
  W0729 16:23:39.746464      13 warnings.go:70] unknown field "delta"
  W0729 16:23:39.746504      13 warnings.go:70] unknown field "epsilon"
  W0729 16:23:39.746638      13 warnings.go:70] unknown field "gamma"
  Jul 29 16:23:40.314: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5566" for this suite. @ 07/29/23 16:23:40.345
• [3.480 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should unconditionally reject operations on fail closed webhook [Conformance]
test/e2e/apimachinery/webhook.go:237
  STEP: Creating a kubernetes client @ 07/29/23 16:23:40.364
  Jul 29 16:23:40.364: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:23:40.368
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:23:40.405
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:23:40.41
  STEP: Setting up server cert @ 07/29/23 16:23:40.451
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:23:41.323
  STEP: Deploying the webhook pod @ 07/29/23 16:23:41.337
  STEP: Wait for the deployment to be ready @ 07/29/23 16:23:41.358
  Jul 29 16:23:41.372: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 16:23:43.396
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:23:43.416
  Jul 29 16:23:44.417: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a webhook that server cannot talk to, with fail closed policy, via the AdmissionRegistration API @ 07/29/23 16:23:44.424
  STEP: create a namespace for the webhook @ 07/29/23 16:23:44.465
  STEP: create a configmap should be unconditionally rejected by the webhook @ 07/29/23 16:23:44.508
  Jul 29 16:23:44.690: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5827" for this suite. @ 07/29/23 16:23:44.813
  STEP: Destroying namespace "webhook-markers-2703" for this suite. @ 07/29/23 16:23:44.828
  STEP: Destroying namespace "fail-closed-namespace-319" for this suite. @ 07/29/23 16:23:44.84
• [4.493 seconds]
------------------------------
[sig-apps] Job should delete a job [Conformance]
test/e2e/apps/job.go:485
  STEP: Creating a kubernetes client @ 07/29/23 16:23:44.857
  Jul 29 16:23:44.857: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename job @ 07/29/23 16:23:44.86
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:23:44.887
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:23:44.892
  STEP: Creating a job @ 07/29/23 16:23:44.898
  STEP: Ensuring active pods == parallelism @ 07/29/23 16:23:44.912
  STEP: delete a job @ 07/29/23 16:23:46.924
  STEP: deleting Job.batch foo in namespace job-7756, will wait for the garbage collector to delete the pods @ 07/29/23 16:23:46.925
  Jul 29 16:23:47.004: INFO: Deleting Job.batch foo took: 21.746494ms
  Jul 29 16:23:47.104: INFO: Terminating Job.batch foo pods took: 100.407814ms
  STEP: Ensuring job was deleted @ 07/29/23 16:24:18.806
  Jul 29 16:24:18.819: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-7756" for this suite. @ 07/29/23 16:24:18.829
• [33.988 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] Should recreate evicted statefulset [Conformance]
test/e2e/apps/statefulset.go:743
  STEP: Creating a kubernetes client @ 07/29/23 16:24:18.851
  Jul 29 16:24:18.852: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:24:18.855
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:18.885
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:18.891
  STEP: Creating service test in namespace statefulset-9466 @ 07/29/23 16:24:18.896
  STEP: Looking for a node to schedule stateful set and pod @ 07/29/23 16:24:18.909
  STEP: Creating pod with conflicting port in namespace statefulset-9466 @ 07/29/23 16:24:18.935
  STEP: Waiting until pod test-pod will start running in namespace statefulset-9466 @ 07/29/23 16:24:18.952
  STEP: Creating statefulset with conflicting port in namespace statefulset-9466 @ 07/29/23 16:24:20.98
  STEP: Waiting until stateful pod ss-0 will be recreated and deleted at least once in namespace statefulset-9466 @ 07/29/23 16:24:20.99
  Jul 29 16:24:21.019: INFO: Observed stateful pod in namespace: statefulset-9466, name: ss-0, uid: 513e3b00-d29e-4bb3-a024-19452cbde073, status phase: Pending. Waiting for statefulset controller to delete.
  Jul 29 16:24:21.051: INFO: Observed stateful pod in namespace: statefulset-9466, name: ss-0, uid: 513e3b00-d29e-4bb3-a024-19452cbde073, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 29 16:24:21.090: INFO: Observed stateful pod in namespace: statefulset-9466, name: ss-0, uid: 513e3b00-d29e-4bb3-a024-19452cbde073, status phase: Failed. Waiting for statefulset controller to delete.
  Jul 29 16:24:21.096: INFO: Observed delete event for stateful pod ss-0 in namespace statefulset-9466
  STEP: Removing pod with conflicting port in namespace statefulset-9466 @ 07/29/23 16:24:21.097
  STEP: Waiting when stateful pod ss-0 will be recreated in namespace statefulset-9466 and will be in running state @ 07/29/23 16:24:21.132
  Jul 29 16:24:35.211: INFO: Deleting all statefulset in ns statefulset-9466
  Jul 29 16:24:35.215: INFO: Scaling statefulset ss to 0
  Jul 29 16:24:45.255: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:24:45.261: INFO: Deleting statefulset ss
  Jul 29 16:24:45.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-9466" for this suite. @ 07/29/23 16:24:45.307
• [26.469 seconds]
------------------------------
SSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:89
  STEP: Creating a kubernetes client @ 07/29/23 16:24:45.322
  Jul 29 16:24:45.322: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:24:45.326
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:45.361
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:45.366
  STEP: Creating configMap with name configmap-test-volume-map-252760b5-6f80-432d-ac2b-34647823274b @ 07/29/23 16:24:45.37
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:24:45.378
  STEP: Saw pod success @ 07/29/23 16:24:49.416
  Jul 29 16:24:49.424: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-eeca3e9b-df49-4927-849f-ecbfb4b2a166 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:24:49.463
  Jul 29 16:24:49.493: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-9275" for this suite. @ 07/29/23 16:24:49.504
• [4.195 seconds]
------------------------------
SS
------------------------------
[sig-node] KubeletManagedEtcHosts should test kubelet managed /etc/hosts file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet_etc_hosts.go:64
  STEP: Creating a kubernetes client @ 07/29/23 16:24:49.518
  Jul 29 16:24:49.519: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename e2e-kubelet-etc-hosts @ 07/29/23 16:24:49.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:49.56
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:49.566
  STEP: Setting up the test @ 07/29/23 16:24:49.572
  STEP: Creating hostNetwork=false pod @ 07/29/23 16:24:49.572
  STEP: Creating hostNetwork=true pod @ 07/29/23 16:24:53.634
  STEP: Running the test @ 07/29/23 16:24:55.683
  STEP: Verifying /etc/hosts of container is kubelet-managed for pod with hostNetwork=false @ 07/29/23 16:24:55.683
  Jul 29 16:24:55.683: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:55.684: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:55.686: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:55.686: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 16:24:55.818: INFO: Exec stderr: ""
  Jul 29 16:24:55.818: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:55.818: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:55.821: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:55.821: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 16:24:55.939: INFO: Exec stderr: ""
  Jul 29 16:24:55.939: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:55.939: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:55.941: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:55.941: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 16:24:56.079: INFO: Exec stderr: ""
  Jul 29 16:24:56.080: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:56.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:56.082: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:56.083: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 16:24:56.181: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts of container is not kubelet-managed since container specifies /etc/hosts mount @ 07/29/23 16:24:56.181
  Jul 29 16:24:56.181: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:56.182: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:56.185: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:56.185: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 29 16:24:56.296: INFO: Exec stderr: ""
  Jul 29 16:24:56.297: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-pod ContainerName:busybox-3 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:56.297: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:56.299: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:56.299: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-3&container=busybox-3&stderr=true&stdout=true)
  Jul 29 16:24:56.387: INFO: Exec stderr: ""
  STEP: Verifying /etc/hosts content of container is not kubelet-managed for pod with hostNetwork=true @ 07/29/23 16:24:56.388
  Jul 29 16:24:56.388: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:56.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:56.390: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:56.391: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 16:24:56.510: INFO: Exec stderr: ""
  Jul 29 16:24:56.510: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-host-network-pod ContainerName:busybox-1 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:56.510: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:56.511: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:56.512: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-1&container=busybox-1&stderr=true&stdout=true)
  Jul 29 16:24:56.610: INFO: Exec stderr: ""
  Jul 29 16:24:56.610: INFO: ExecWithOptions {Command:[cat /etc/hosts] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:56.610: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:56.612: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:56.612: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 16:24:56.719: INFO: Exec stderr: ""
  Jul 29 16:24:56.719: INFO: ExecWithOptions {Command:[cat /etc/hosts-original] Namespace:e2e-kubelet-etc-hosts-5891 PodName:test-host-network-pod ContainerName:busybox-2 Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:24:56.719: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:24:56.722: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:24:56.722: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/e2e-kubelet-etc-hosts-5891/pods/test-host-network-pod/exec?command=cat&command=%2Fetc%2Fhosts-original&container=busybox-2&container=busybox-2&stderr=true&stdout=true)
  Jul 29 16:24:56.821: INFO: Exec stderr: ""
  Jul 29 16:24:56.821: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "e2e-kubelet-etc-hosts-5891" for this suite. @ 07/29/23 16:24:56.833
• [7.328 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Proxy server should support --unix-socket=/path  [Conformance]
test/e2e/kubectl/kubectl.go:1800
  STEP: Creating a kubernetes client @ 07/29/23 16:24:56.85
  Jul 29 16:24:56.850: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:24:56.854
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:56.881
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:56.885
  STEP: Starting the proxy @ 07/29/23 16:24:56.89
  Jul 29 16:24:56.892: INFO: Asynchronously running '/usr/local/bin/kubectl kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-2922 proxy --unix-socket=/tmp/kubectl-proxy-unix787282092/test'
  STEP: retrieving proxy /api/ output @ 07/29/23 16:24:57.009
  Jul 29 16:24:57.011: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-2922" for this suite. @ 07/29/23 16:24:57.02
• [0.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:124
  STEP: Creating a kubernetes client @ 07/29/23 16:24:57.035
  Jul 29 16:24:57.035: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:24:57.036
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:24:57.074
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:24:57.079
  STEP: Creating configMap with name configmap-test-upd-43feec65-6cea-4265-8903-c6543b5ceda6 @ 07/29/23 16:24:57.095
  STEP: Creating the pod @ 07/29/23 16:24:57.105
  STEP: Updating configmap configmap-test-upd-43feec65-6cea-4265-8903-c6543b5ceda6 @ 07/29/23 16:24:59.175
  STEP: waiting to observe update in volume @ 07/29/23 16:24:59.184
  Jul 29 16:26:32.192: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7540" for this suite. @ 07/29/23 16:26:32.204
• [95.182 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should manage the lifecycle of a job [Conformance]
test/e2e/apps/job.go:713
  STEP: Creating a kubernetes client @ 07/29/23 16:26:32.219
  Jul 29 16:26:32.220: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename job @ 07/29/23 16:26:32.223
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:32.254
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:32.26
  STEP: Creating a suspended job @ 07/29/23 16:26:32.281
  STEP: Patching the Job @ 07/29/23 16:26:32.296
  STEP: Watching for Job to be patched @ 07/29/23 16:26:32.338
  Jul 29 16:26:32.344: INFO: Event ADDED observed for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 29 16:26:32.345: INFO: Event MODIFIED observed for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking:]
  Jul 29 16:26:32.345: INFO: Event MODIFIED found for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking:]
  STEP: Updating the job @ 07/29/23 16:26:32.345
  STEP: Watching for Job to be updated @ 07/29/23 16:26:32.364
  Jul 29 16:26:32.376: INFO: Event MODIFIED found for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 16:26:32.377: INFO: Found Job annotations: map[string]string{"batch.kubernetes.io/job-tracking":"", "updated":"true"}
  STEP: Listing all Jobs with LabelSelector @ 07/29/23 16:26:32.377
  Jul 29 16:26:32.386: INFO: Job: e2e-4qj4p as labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p]
  STEP: Waiting for job to complete @ 07/29/23 16:26:32.387
  STEP: Delete a job collection with a labelselector @ 07/29/23 16:26:42.397
  STEP: Watching for Job to be deleted @ 07/29/23 16:26:42.413
  Jul 29 16:26:42.417: INFO: Event MODIFIED observed for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 16:26:42.418: INFO: Event MODIFIED observed for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 16:26:42.419: INFO: Event MODIFIED observed for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 16:26:42.419: INFO: Event MODIFIED observed for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 16:26:42.419: INFO: Event MODIFIED observed for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  Jul 29 16:26:42.420: INFO: Event DELETED found for Job e2e-4qj4p in namespace job-2373 with labels: map[e2e-4qj4p:patched e2e-job-label:e2e-4qj4p] and annotations: map[batch.kubernetes.io/job-tracking: updated:true]
  STEP: Relist jobs to confirm deletion @ 07/29/23 16:26:42.42
  Jul 29 16:26:42.426: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-2373" for this suite. @ 07/29/23 16:26:42.435
• [10.237 seconds]
------------------------------
S
------------------------------
[sig-storage] Secrets should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:125
  STEP: Creating a kubernetes client @ 07/29/23 16:26:42.457
  Jul 29 16:26:42.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:26:42.462
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:42.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:42.499
  STEP: Creating secret with name secret-test-4a041424-7e24-4014-b3f1-819ea0b9bfa7 @ 07/29/23 16:26:42.504
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:26:42.514
  STEP: Saw pod success @ 07/29/23 16:26:46.561
  Jul 29 16:26:46.567: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-secrets-5c900863-5556-438f-99f3-4c9fa60fb318 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:26:46.598
  Jul 29 16:26:46.621: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-3616" for this suite. @ 07/29/23 16:26:46.629
• [4.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:74
  STEP: Creating a kubernetes client @ 07/29/23 16:26:46.643
  Jul 29 16:26:46.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 16:26:46.646
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:46.67
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:46.674
  STEP: Creating configMap with name configmap-test-volume-c5ba5e9d-8053-4c98-959b-44155d621df5 @ 07/29/23 16:26:46.678
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:26:46.685
  STEP: Saw pod success @ 07/29/23 16:26:50.731
  Jul 29 16:26:50.736: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-7bb1c3b9-370d-401b-97b0-91235c9c6e6e container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:26:50.747
  Jul 29 16:26:50.773: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-3993" for this suite. @ 07/29/23 16:26:50.782
• [4.149 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] CustomResourceConversionWebhook [Privileged:ClusterAdmin] should be able to convert a non homogeneous list of CRs [Conformance]
test/e2e/apimachinery/crd_conversion_webhook.go:176
  STEP: Creating a kubernetes client @ 07/29/23 16:26:50.796
  Jul 29 16:26:50.796: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-webhook @ 07/29/23 16:26:50.799
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:50.824
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:50.831
  STEP: Setting up server cert @ 07/29/23 16:26:50.836
  STEP: Create role binding to let cr conversion webhook read extension-apiserver-authentication @ 07/29/23 16:26:51.878
  STEP: Deploying the custom resource conversion webhook pod @ 07/29/23 16:26:51.893
  STEP: Wait for the deployment to be ready @ 07/29/23 16:26:51.917
  Jul 29 16:26:51.936: INFO: new replicaset for deployment "sample-crd-conversion-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 16:26:53.956
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:26:53.978
  Jul 29 16:26:54.979: INFO: Waiting for amount of service:e2e-test-crd-conversion-webhook endpoints to be 1
  Jul 29 16:26:54.989: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Creating a v1 custom resource @ 07/29/23 16:26:57.753
  STEP: Create a v2 custom resource @ 07/29/23 16:26:57.79
  STEP: List CRs in v1 @ 07/29/23 16:26:58.121
  STEP: List CRs in v2 @ 07/29/23 16:26:58.147
  Jul 29 16:26:58.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-webhook-7823" for this suite. @ 07/29/23 16:26:58.818
• [8.034 seconds]
------------------------------
SSSSSS
------------------------------
[sig-network] Services should be able to change the type from NodePort to ExternalName [Conformance]
test/e2e/network/service.go:1533
  STEP: Creating a kubernetes client @ 07/29/23 16:26:58.831
  Jul 29 16:26:58.831: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 16:26:58.836
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:26:58.893
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:26:58.897
  STEP: creating a service nodeport-service with the type=NodePort in namespace services-3730 @ 07/29/23 16:26:58.902
  STEP: Creating active service to test reachability when its FQDN is referred as externalName for another service @ 07/29/23 16:26:58.946
  STEP: creating service externalsvc in namespace services-3730 @ 07/29/23 16:26:58.946
  STEP: creating replication controller externalsvc in namespace services-3730 @ 07/29/23 16:26:58.997
  I0729 16:26:59.016568      13 runners.go:194] Created replication controller with name: externalsvc, namespace: services-3730, replica count: 2
  I0729 16:27:02.069421      13 runners.go:194] externalsvc Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  STEP: changing the NodePort service to type=ExternalName @ 07/29/23 16:27:02.088
  Jul 29 16:27:02.169: INFO: Creating new exec pod
  Jul 29 16:27:04.203: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-3730 exec execpodch4ds -- /bin/sh -x -c nslookup nodeport-service.services-3730.svc.cluster.local'
  Jul 29 16:27:04.557: INFO: stderr: "+ nslookup nodeport-service.services-3730.svc.cluster.local\n"
  Jul 29 16:27:04.557: INFO: stdout: "Server:\t\t10.233.0.10\nAddress:\t10.233.0.10#53\n\nnodeport-service.services-3730.svc.cluster.local\tcanonical name = externalsvc.services-3730.svc.cluster.local.\nName:\texternalsvc.services-3730.svc.cluster.local\nAddress: 10.233.22.190\n\n"
  Jul 29 16:27:04.557: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController externalsvc in namespace services-3730, will wait for the garbage collector to delete the pods @ 07/29/23 16:27:04.568
  Jul 29 16:27:04.640: INFO: Deleting ReplicationController externalsvc took: 13.722995ms
  Jul 29 16:27:04.740: INFO: Terminating ReplicationController externalsvc pods took: 100.582052ms
  Jul 29 16:27:06.678: INFO: Cleaning up the NodePort to ExternalName test service
  STEP: Destroying namespace "services-3730" for this suite. @ 07/29/23 16:27:06.71
• [7.902 seconds]
------------------------------
SSS
------------------------------
[sig-apps] CronJob should schedule multiple jobs concurrently [Conformance]
test/e2e/apps/cronjob.go:70
  STEP: Creating a kubernetes client @ 07/29/23 16:27:06.734
  Jul 29 16:27:06.734: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 16:27:06.736
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:27:06.766
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:27:06.774
  STEP: Creating a cronjob @ 07/29/23 16:27:06.779
  STEP: Ensuring more than one job is running at a time @ 07/29/23 16:27:06.789
  STEP: Ensuring at least two running jobs exists by listing jobs explicitly @ 07/29/23 16:29:00.797
  STEP: Removing cronjob @ 07/29/23 16:29:00.805
  Jul 29 16:29:00.814: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-8797" for this suite. @ 07/29/23 16:29:00.823
• [114.105 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:117
  STEP: Creating a kubernetes client @ 07/29/23 16:29:00.85
  Jul 29 16:29:00.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:29:00.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:00.977
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:00.98
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/29/23 16:29:00.986
  STEP: Saw pod success @ 07/29/23 16:29:05.031
  Jul 29 16:29:05.037: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-e222c731-28e6-4942-8200-554ea84fc94a container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:29:05.088
  Jul 29 16:29:05.121: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9004" for this suite. @ 07/29/23 16:29:05.137
• [4.300 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide host IP as an env var [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:91
  STEP: Creating a kubernetes client @ 07/29/23 16:29:05.158
  Jul 29 16:29:05.159: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:29:05.16
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:05.194
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:05.199
  STEP: Creating a pod to test downward api env vars @ 07/29/23 16:29:05.204
  STEP: Saw pod success @ 07/29/23 16:29:09.262
  Jul 29 16:29:09.270: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downward-api-38342bb5-5c78-4b5c-98f1-4bbc9ed7b5a3 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:29:09.286
  Jul 29 16:29:09.319: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6673" for this suite. @ 07/29/23 16:29:09.33
• [4.183 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:208
  STEP: Creating a kubernetes client @ 07/29/23 16:29:09.354
  Jul 29 16:29:09.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:29:09.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:09.385
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:09.39
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:29:09.394
  STEP: Saw pod success @ 07/29/23 16:29:13.441
  Jul 29 16:29:13.449: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-d955a7b1-0238-482e-a51d-0eb452d1c7e1 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:29:13.462
  Jul 29 16:29:13.489: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8586" for this suite. @ 07/29/23 16:29:13.499
• [4.164 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] SubjectReview should support SubjectReview API operations [Conformance]
test/e2e/auth/subjectreviews.go:50
  STEP: Creating a kubernetes client @ 07/29/23 16:29:13.539
  Jul 29 16:29:13.540: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename subjectreview @ 07/29/23 16:29:13.544
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:13.564
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:13.568
  STEP: Creating a Serviceaccount "e2e" in namespace "subjectreview-6597" @ 07/29/23 16:29:13.573
  Jul 29 16:29:13.581: INFO: saUsername: "system:serviceaccount:subjectreview-6597:e2e"
  Jul 29 16:29:13.581: INFO: saGroups: []string{"system:authenticated", "system:serviceaccounts", "system:serviceaccounts:subjectreview-6597"}
  Jul 29 16:29:13.581: INFO: saUID: "fddb460d-c60e-40e8-8844-7420fc9adbba"
  STEP: Creating clientset to impersonate "system:serviceaccount:subjectreview-6597:e2e" @ 07/29/23 16:29:13.581
  STEP: Creating SubjectAccessReview for "system:serviceaccount:subjectreview-6597:e2e" @ 07/29/23 16:29:13.581
  Jul 29 16:29:13.584: INFO: sarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  STEP: Verifying as "system:serviceaccount:subjectreview-6597:e2e" api 'list' configmaps in "subjectreview-6597" namespace @ 07/29/23 16:29:13.585
  Jul 29 16:29:13.588: INFO: SubjectAccessReview has been verified
  STEP: Creating a LocalSubjectAccessReview for "system:serviceaccount:subjectreview-6597:e2e" @ 07/29/23 16:29:13.589
  Jul 29 16:29:13.595: INFO: lsarResponse Status: v1.SubjectAccessReviewStatus{Allowed:false, Denied:false, Reason:"", EvaluationError:""}
  Jul 29 16:29:13.596: INFO: LocalSubjectAccessReview has been verified
  Jul 29 16:29:13.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subjectreview-6597" for this suite. @ 07/29/23 16:29:13.608
• [0.080 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:163
  STEP: Creating a kubernetes client @ 07/29/23 16:29:13.628
  Jul 29 16:29:13.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:29:13.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:13.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:13.663
  STEP: Creating the pod @ 07/29/23 16:29:13.67
  Jul 29 16:29:16.269: INFO: Successfully updated pod "annotationupdatefd68454a-4ebe-400c-aa51-8874bd4ef9c7"
  Jul 29 16:29:18.312: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3064" for this suite. @ 07/29/23 16:29:18.324
• [4.707 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected secret should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:56
  STEP: Creating a kubernetes client @ 07/29/23 16:29:18.344
  Jul 29 16:29:18.344: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:29:18.347
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:18.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:18.388
  STEP: Creating projection with secret that has name projected-secret-test-793d43cc-1ca1-4522-a5ee-99b20f148f95 @ 07/29/23 16:29:18.393
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:29:18.403
  STEP: Saw pod success @ 07/29/23 16:29:22.46
  Jul 29 16:29:22.469: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-secrets-48decece-831d-4f8d-b414-16619775027c container projected-secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:29:22.48
  Jul 29 16:29:22.501: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7955" for this suite. @ 07/29/23 16:29:22.507
• [4.173 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny pod and configmap creation [Conformance]
test/e2e/apimachinery/webhook.go:198
  STEP: Creating a kubernetes client @ 07/29/23 16:29:22.518
  Jul 29 16:29:22.518: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:29:22.521
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:22.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:22.547
  STEP: Setting up server cert @ 07/29/23 16:29:22.581
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:29:23.076
  STEP: Deploying the webhook pod @ 07/29/23 16:29:23.084
  STEP: Wait for the deployment to be ready @ 07/29/23 16:29:23.105
  Jul 29 16:29:23.121: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Jul 29 16:29:25.141: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 29, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 29, 23, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 29, 23, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 29, 23, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/29/23 16:29:27.148
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:29:27.175
  Jul 29 16:29:28.175: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the webhook via the AdmissionRegistration API @ 07/29/23 16:29:28.187
  STEP: create a pod that should be denied by the webhook @ 07/29/23 16:29:28.22
  STEP: create a pod that causes the webhook to hang @ 07/29/23 16:29:28.248
  STEP: create a configmap that should be denied by the webhook @ 07/29/23 16:29:38.265
  STEP: create a configmap that should be admitted by the webhook @ 07/29/23 16:29:38.285
  STEP: update (PUT) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/29/23 16:29:38.305
  STEP: update (PATCH) the admitted configmap to a non-compliant one should be rejected by the webhook @ 07/29/23 16:29:38.323
  STEP: create a namespace that bypass the webhook @ 07/29/23 16:29:38.339
  STEP: create a configmap that violates the webhook policy but is in a whitelisted namespace @ 07/29/23 16:29:38.367
  Jul 29 16:29:38.384: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6368" for this suite. @ 07/29/23 16:29:38.523
  STEP: Destroying namespace "webhook-markers-1283" for this suite. @ 07/29/23 16:29:38.55
  STEP: Destroying namespace "exempted-namespace-4484" for this suite. @ 07/29/23 16:29:38.573
• [16.077 seconds]
------------------------------
SS
------------------------------
[sig-api-machinery] Garbage collector should keep the rc around until all its pods are deleted if the deleteOptions says so [Conformance]
test/e2e/apimachinery/garbage_collector.go:638
  STEP: Creating a kubernetes client @ 07/29/23 16:29:38.595
  Jul 29 16:29:38.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename gc @ 07/29/23 16:29:38.599
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:29:38.634
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:29:38.639
  STEP: create the rc @ 07/29/23 16:29:38.663
  W0729 16:29:38.676634      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: delete the rc @ 07/29/23 16:29:44.897
  STEP: wait for the rc to be deleted @ 07/29/23 16:29:45.111
  Jul 29 16:29:46.492: INFO: 94 pods remaining
  Jul 29 16:29:46.492: INFO: 80 pods has nil DeletionTimestamp
  Jul 29 16:29:46.492: INFO: 
  Jul 29 16:29:47.388: INFO: 87 pods remaining
  Jul 29 16:29:47.388: INFO: 69 pods has nil DeletionTimestamp
  Jul 29 16:29:47.389: INFO: 
  Jul 29 16:29:48.740: INFO: 78 pods remaining
  Jul 29 16:29:48.741: INFO: 56 pods has nil DeletionTimestamp
  Jul 29 16:29:48.741: INFO: 
  Jul 29 16:29:49.562: INFO: 70 pods remaining
  Jul 29 16:29:49.562: INFO: 40 pods has nil DeletionTimestamp
  Jul 29 16:29:49.562: INFO: 
  Jul 29 16:29:50.291: INFO: 67 pods remaining
  Jul 29 16:29:50.304: INFO: 30 pods has nil DeletionTimestamp
  Jul 29 16:29:50.304: INFO: 
  Jul 29 16:29:51.950: INFO: 60 pods remaining
  Jul 29 16:29:51.951: INFO: 12 pods has nil DeletionTimestamp
  Jul 29 16:29:51.951: INFO: 
  Jul 29 16:29:52.384: INFO: 59 pods remaining
  Jul 29 16:29:52.384: INFO: 2 pods has nil DeletionTimestamp
  Jul 29 16:29:52.384: INFO: 
  Jul 29 16:29:53.201: INFO: 55 pods remaining
  Jul 29 16:29:53.201: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:29:53.201: INFO: 
  Jul 29 16:29:54.266: INFO: 47 pods remaining
  Jul 29 16:29:54.266: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:29:54.267: INFO: 
  Jul 29 16:29:55.405: INFO: 39 pods remaining
  Jul 29 16:29:55.406: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:29:55.406: INFO: 
  Jul 29 16:29:56.172: INFO: 37 pods remaining
  Jul 29 16:29:56.173: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:29:56.173: INFO: 
  Jul 29 16:29:57.231: INFO: 27 pods remaining
  Jul 29 16:29:57.231: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:29:57.231: INFO: 
  Jul 29 16:29:58.133: INFO: 22 pods remaining
  Jul 29 16:29:58.133: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:29:58.133: INFO: 
  Jul 29 16:29:59.478: INFO: 15 pods remaining
  Jul 29 16:29:59.478: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:29:59.481: INFO: 
  Jul 29 16:30:00.152: INFO: 8 pods remaining
  Jul 29 16:30:00.153: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:30:00.153: INFO: 
  Jul 29 16:30:01.294: INFO: 1 pods remaining
  Jul 29 16:30:01.294: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:30:01.294: INFO: 
  Jul 29 16:30:02.153: INFO: 0 pods remaining
  Jul 29 16:30:02.153: INFO: 0 pods has nil DeletionTimestamp
  Jul 29 16:30:02.153: INFO: 
  STEP: Gathering metrics @ 07/29/23 16:30:03.183
  Jul 29 16:30:03.687: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 16:30:03.691: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5489" for this suite. @ 07/29/23 16:30:03.7
• [25.117 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume as non-root with defaultMode and fsGroup set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:68
  STEP: Creating a kubernetes client @ 07/29/23 16:30:03.713
  Jul 29 16:30:03.713: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:30:03.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:03.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:03.906
  STEP: Creating secret with name secret-test-a3e10d99-bde1-4289-b39e-c3bcc547061a @ 07/29/23 16:30:03.913
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:30:03.954
  STEP: Saw pod success @ 07/29/23 16:30:08.126
  Jul 29 16:30:08.156: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-secrets-0c150bd0-213b-4557-9cb6-91d1b7d474fa container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:30:08.208
  Jul 29 16:30:08.278: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-506" for this suite. @ 07/29/23 16:30:08.306
• [4.638 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] updates the published spec when one version gets renamed [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:391
  STEP: Creating a kubernetes client @ 07/29/23 16:30:08.352
  Jul 29 16:30:08.352: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 16:30:08.356
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:08.435
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:08.46
  STEP: set up a multi version CRD @ 07/29/23 16:30:08.478
  Jul 29 16:30:08.479: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: rename a version @ 07/29/23 16:30:15.603
  STEP: check the new version name is served @ 07/29/23 16:30:15.649
  STEP: check the old version name is removed @ 07/29/23 16:30:17.673
  STEP: check the other version is not changed @ 07/29/23 16:30:18.653
  Jul 29 16:30:23.016: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3952" for this suite. @ 07/29/23 16:30:23.034
• [14.695 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Daemon set [Serial] should rollback without unnecessary restarts [Conformance]
test/e2e/apps/daemon_set.go:443
  STEP: Creating a kubernetes client @ 07/29/23 16:30:23.05
  Jul 29 16:30:23.050: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 16:30:23.053
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:23.13
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:23.135
  Jul 29 16:30:23.209: INFO: Create a RollingUpdate DaemonSet
  Jul 29 16:30:23.226: INFO: Check that daemon pods launch on every node of the cluster
  Jul 29 16:30:23.245: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:30:23.246: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:30:24.283: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:30:24.284: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:30:25.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 16:30:25.266: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:30:26.266: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 16:30:26.267: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  Jul 29 16:30:26.267: INFO: Update the DaemonSet to trigger a rollout
  Jul 29 16:30:26.284: INFO: Updating DaemonSet daemon-set
  Jul 29 16:30:27.337: INFO: Roll back the DaemonSet before rollout is complete
  Jul 29 16:30:27.359: INFO: Updating DaemonSet daemon-set
  Jul 29 16:30:27.359: INFO: Make sure DaemonSet rollback is complete
  Jul 29 16:30:27.369: INFO: Wrong image for pod: daemon-set-kvxdk. Expected: registry.k8s.io/e2e-test-images/httpd:2.4.38-4, got: foo:non-existent.
  Jul 29 16:30:27.369: INFO: Pod daemon-set-kvxdk is not available
  Jul 29 16:30:33.406: INFO: Pod daemon-set-k2459 is not available
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 16:30:33.463
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8408, will wait for the garbage collector to delete the pods @ 07/29/23 16:30:33.463
  Jul 29 16:30:33.610: INFO: Deleting DaemonSet.extensions daemon-set took: 86.3032ms
  Jul 29 16:30:33.811: INFO: Terminating DaemonSet.extensions daemon-set pods took: 200.867668ms
  Jul 29 16:30:36.018: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:30:36.018: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 16:30:36.025: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"27897"},"items":null}

  Jul 29 16:30:36.031: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"27897"},"items":null}

  Jul 29 16:30:36.061: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8408" for this suite. @ 07/29/23 16:30:36.069
• [13.031 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected configMap optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:174
  STEP: Creating a kubernetes client @ 07/29/23 16:30:36.083
  Jul 29 16:30:36.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:30:36.088
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:30:36.138
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:30:36.145
  STEP: Creating configMap with name cm-test-opt-del-51e56fc8-8812-49f1-9ac6-7e3ad00c716e @ 07/29/23 16:30:36.161
  STEP: Creating configMap with name cm-test-opt-upd-e68feed2-1724-4b29-b7ff-20ea3c779864 @ 07/29/23 16:30:36.17
  STEP: Creating the pod @ 07/29/23 16:30:36.178
  STEP: Deleting configmap cm-test-opt-del-51e56fc8-8812-49f1-9ac6-7e3ad00c716e @ 07/29/23 16:30:38.254
  STEP: Updating configmap cm-test-opt-upd-e68feed2-1724-4b29-b7ff-20ea3c779864 @ 07/29/23 16:30:38.264
  STEP: Creating configMap with name cm-test-opt-create-88bc88aa-872f-40ce-8dca-b8df59d28c46 @ 07/29/23 16:30:38.272
  STEP: waiting to observe update in volume @ 07/29/23 16:30:38.278
  Jul 29 16:32:05.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3864" for this suite. @ 07/29/23 16:32:05.174
• [89.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should retry creating failed daemon pods [Conformance]
test/e2e/apps/daemon_set.go:305
  STEP: Creating a kubernetes client @ 07/29/23 16:32:05.193
  Jul 29 16:32:05.193: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 16:32:05.196
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:32:05.239
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:32:05.242
  STEP: Creating a simple DaemonSet "daemon-set" @ 07/29/23 16:32:05.283
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 16:32:05.293
  Jul 29 16:32:05.307: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:32:05.307: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:32:06.331: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:32:06.331: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:32:07.330: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 16:32:07.330: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Set a daemon pod's phase to 'Failed', check that the daemon pod is revived. @ 07/29/23 16:32:07.337
  Jul 29 16:32:07.385: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 16:32:07.385: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Wait for the failed daemon pod to be completely deleted. @ 07/29/23 16:32:07.385
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 16:32:08.406
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-8493, will wait for the garbage collector to delete the pods @ 07/29/23 16:32:08.406
  Jul 29 16:32:08.479: INFO: Deleting DaemonSet.extensions daemon-set took: 16.175144ms
  Jul 29 16:32:08.580: INFO: Terminating DaemonSet.extensions daemon-set pods took: 100.850637ms
  Jul 29 16:32:11.390: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:32:11.390: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 16:32:11.400: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"28275"},"items":null}

  Jul 29 16:32:11.407: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"28275"},"items":null}

  Jul 29 16:32:11.435: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-8493" for this suite. @ 07/29/23 16:32:11.444
• [6.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a GRPC liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:523
  STEP: Creating a kubernetes client @ 07/29/23 16:32:11.479
  Jul 29 16:32:11.480: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:32:11.485
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:32:11.511
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:32:11.519
  STEP: Creating pod test-grpc-8ba050d6-5bbc-4701-ab98-2d1d3d20722e in namespace container-probe-8071 @ 07/29/23 16:32:11.526
  Jul 29 16:32:13.560: INFO: Started pod test-grpc-8ba050d6-5bbc-4701-ab98-2d1d3d20722e in namespace container-probe-8071
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:32:13.56
  Jul 29 16:32:13.565: INFO: Initial restart count of pod test-grpc-8ba050d6-5bbc-4701-ab98-2d1d3d20722e is 0
  Jul 29 16:36:14.786: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:36:14.801
  STEP: Destroying namespace "container-probe-8071" for this suite. @ 07/29/23 16:36:14.842
• [243.379 seconds]
------------------------------
SSSSS
------------------------------
[sig-network] IngressClass API  should support creating IngressClass API operations [Conformance]
test/e2e/network/ingressclass.go:266
  STEP: Creating a kubernetes client @ 07/29/23 16:36:14.858
  Jul 29 16:36:14.858: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename ingressclass @ 07/29/23 16:36:14.861
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:36:14.903
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:36:14.909
  STEP: getting /apis @ 07/29/23 16:36:14.913
  STEP: getting /apis/networking.k8s.io @ 07/29/23 16:36:14.92
  STEP: getting /apis/networking.k8s.iov1 @ 07/29/23 16:36:14.922
  STEP: creating @ 07/29/23 16:36:14.923
  STEP: getting @ 07/29/23 16:36:14.954
  STEP: listing @ 07/29/23 16:36:14.961
  STEP: watching @ 07/29/23 16:36:14.966
  Jul 29 16:36:14.967: INFO: starting watch
  STEP: patching @ 07/29/23 16:36:14.968
  STEP: updating @ 07/29/23 16:36:14.981
  Jul 29 16:36:14.994: INFO: waiting for watch events with expected annotations
  Jul 29 16:36:14.994: INFO: saw patched and updated annotations
  STEP: deleting @ 07/29/23 16:36:14.994
  STEP: deleting a collection @ 07/29/23 16:36:15.015
  Jul 29 16:36:15.042: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingressclass-8980" for this suite. @ 07/29/23 16:36:15.05
• [0.201 seconds]
------------------------------
SSS
------------------------------
[sig-node] Pods should get a host IP [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:205
  STEP: Creating a kubernetes client @ 07/29/23 16:36:15.06
  Jul 29 16:36:15.061: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:36:15.062
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:36:15.088
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:36:15.093
  STEP: creating pod @ 07/29/23 16:36:15.1
  Jul 29 16:36:17.169: INFO: Pod pod-hostip-87463a69-1af4-460a-b752-048b6b9fe6bf has hostIP: 192.168.121.144
  Jul 29 16:36:17.169: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-8893" for this suite. @ 07/29/23 16:36:17.177
• [2.127 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPreemption [Serial] PriorityClass endpoints verify PriorityClass endpoints can be operated with different HTTP methods [Conformance]
test/e2e/scheduling/preemption.go:812
  STEP: Creating a kubernetes client @ 07/29/23 16:36:17.194
  Jul 29 16:36:17.194: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-preemption @ 07/29/23 16:36:17.197
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:36:17.221
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:36:17.225
  Jul 29 16:36:17.253: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 16:37:17.305: INFO: Waiting for terminating namespaces to be deleted...
  STEP: Creating a kubernetes client @ 07/29/23 16:37:17.314
  Jul 29 16:37:17.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-preemption-path @ 07/29/23 16:37:17.321
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:17.353
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:17.357
  Jul 29 16:37:17.388: INFO: PriorityClass.scheduling.k8s.io "p1" is invalid: value: Forbidden: may not be changed in an update.
  Jul 29 16:37:17.395: INFO: PriorityClass.scheduling.k8s.io "p2" is invalid: value: Forbidden: may not be changed in an update.
  Jul 29 16:37:17.433: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 16:37:17.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-preemption-path-382" for this suite. @ 07/29/23 16:37:17.546
  STEP: Destroying namespace "sched-preemption-8084" for this suite. @ 07/29/23 16:37:17.56
• [60.377 seconds]
------------------------------
[sig-instrumentation] Events API should ensure that an event can be fetched, patched, deleted, and listed [Conformance]
test/e2e/instrumentation/events.go:98
  STEP: Creating a kubernetes client @ 07/29/23 16:37:17.571
  Jul 29 16:37:17.572: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename events @ 07/29/23 16:37:17.574
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:17.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:17.603
  STEP: creating a test event @ 07/29/23 16:37:17.607
  STEP: listing events in all namespaces @ 07/29/23 16:37:17.619
  STEP: listing events in test namespace @ 07/29/23 16:37:17.625
  STEP: listing events with field selection filtering on source @ 07/29/23 16:37:17.63
  STEP: listing events with field selection filtering on reportingController @ 07/29/23 16:37:17.635
  STEP: getting the test event @ 07/29/23 16:37:17.639
  STEP: patching the test event @ 07/29/23 16:37:17.643
  STEP: getting the test event @ 07/29/23 16:37:17.658
  STEP: updating the test event @ 07/29/23 16:37:17.664
  STEP: getting the test event @ 07/29/23 16:37:17.674
  STEP: deleting the test event @ 07/29/23 16:37:17.678
  STEP: listing events in all namespaces @ 07/29/23 16:37:17.69
  STEP: listing events in test namespace @ 07/29/23 16:37:17.699
  Jul 29 16:37:17.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "events-992" for this suite. @ 07/29/23 16:37:17.715
• [0.157 seconds]
------------------------------
[sig-storage] Projected secret should be consumable in multiple volumes in a pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_secret.go:119
  STEP: Creating a kubernetes client @ 07/29/23 16:37:17.729
  Jul 29 16:37:17.729: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:37:17.732
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:17.761
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:17.767
  STEP: Creating secret with name projected-secret-test-59adf25b-21c1-415c-bab5-88b24eba786f @ 07/29/23 16:37:17.772
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:37:17.782
  STEP: Saw pod success @ 07/29/23 16:37:21.82
  Jul 29 16:37:21.827: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-secrets-7cefa332-2e37-4d6b-96f5-e53c3a11606f container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:37:21.87
  Jul 29 16:37:21.902: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-3961" for this suite. @ 07/29/23 16:37:21.912
• [4.196 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Security Context when creating containers with AllowPrivilegeEscalation should not allow privilege escalation when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:609
  STEP: Creating a kubernetes client @ 07/29/23 16:37:21.928
  Jul 29 16:37:21.928: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 16:37:21.93
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:21.954
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:21.958
  Jul 29 16:37:28.029: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-6571" for this suite. @ 07/29/23 16:37:28.037
• [6.120 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from file when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:248
  STEP: Creating a kubernetes client @ 07/29/23 16:37:28.059
  Jul 29 16:37:28.060: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 16:37:28.061
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:28.091
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:28.096
  STEP: create the container @ 07/29/23 16:37:28.102
  W0729 16:37:28.119178      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 16:37:28.119
  STEP: get the container status @ 07/29/23 16:37:31.174
  STEP: the container should be terminated @ 07/29/23 16:37:31.18
  STEP: the termination message should be set @ 07/29/23 16:37:31.18
  Jul 29 16:37:31.180: INFO: Expected: &{OK} to match Container's Termination Message: OK --
  STEP: delete the container @ 07/29/23 16:37:31.18
  Jul 29 16:37:31.200: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-7842" for this suite. @ 07/29/23 16:37:31.214
• [3.167 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Pods Extended Pods Set QOS Class should be set on Pods with matching resource requests and limits for memory and cpu [Conformance]
test/e2e/node/pods.go:163
  STEP: Creating a kubernetes client @ 07/29/23 16:37:31.231
  Jul 29 16:37:31.231: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 16:37:31.233
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:31.265
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:31.27
  STEP: creating the pod @ 07/29/23 16:37:31.273
  STEP: submitting the pod to kubernetes @ 07/29/23 16:37:31.274
  STEP: verifying QOS class is set on the pod @ 07/29/23 16:37:31.287
  Jul 29 16:37:31.296: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7514" for this suite. @ 07/29/23 16:37:31.309
• [0.097 seconds]
------------------------------
[sig-api-machinery] FieldValidation should create/apply a CR with unknown fields for CRD with no validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:289
  STEP: Creating a kubernetes client @ 07/29/23 16:37:31.328
  Jul 29 16:37:31.328: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 16:37:31.33
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:31.379
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:31.384
  Jul 29 16:37:31.388: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:37:34.656: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-8626" for this suite. @ 07/29/23 16:37:34.69
• [3.375 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should support rollover [Conformance]
test/e2e/apps/deployment.go:132
  STEP: Creating a kubernetes client @ 07/29/23 16:37:34.714
  Jul 29 16:37:34.714: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:37:34.716
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:34.742
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:34.747
  Jul 29 16:37:34.767: INFO: Pod name rollover-pod: Found 0 pods out of 1
  Jul 29 16:37:39.777: INFO: Pod name rollover-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 16:37:39.777
  Jul 29 16:37:39.777: INFO: Waiting for pods owned by replica set "test-rollover-controller" to become ready
  Jul 29 16:37:41.788: INFO: Creating deployment "test-rollover-deployment"
  Jul 29 16:37:41.812: INFO: Make sure deployment "test-rollover-deployment" performs scaling operations
  Jul 29 16:37:43.832: INFO: Check revision of new replica set for deployment "test-rollover-deployment"
  Jul 29 16:37:43.845: INFO: Ensure that both replica sets have 1 created replica
  Jul 29 16:37:43.857: INFO: Rollover old replica sets for deployment "test-rollover-deployment" with new image update
  Jul 29 16:37:43.873: INFO: Updating deployment test-rollover-deployment
  Jul 29 16:37:43.874: INFO: Wait deployment "test-rollover-deployment" to be observed by the deployment controller
  Jul 29 16:37:45.902: INFO: Wait for revision update of deployment "test-rollover-deployment" to 2
  Jul 29 16:37:45.916: INFO: Make sure deployment "test-rollover-deployment" is complete
  Jul 29 16:37:45.931: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 16:37:45.931: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:37:47.948: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 16:37:47.948: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:37:49.951: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 16:37:49.952: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:37:51.951: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 16:37:51.952: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:37:53.949: INFO: all replica sets need to contain the pod-template-hash label
  Jul 29 16:37:53.949: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:2, Replicas:2, UpdatedReplicas:1, ReadyReplicas:2, AvailableReplicas:1, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 37, 45, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 37, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"test-rollover-deployment-57777854c9\" is progressing."}}, CollisionCount:(*int32)(nil)}
  Jul 29 16:37:55.946: INFO: 
  Jul 29 16:37:55.946: INFO: Ensure that both old replica sets have no replicas
  Jul 29 16:37:55.962: INFO: Deployment "test-rollover-deployment":
  &Deployment{ObjectMeta:{test-rollover-deployment  deployment-518  def3650b-b7cd-4c9a-aa99-fb8946393ff9 29374 2 2023-07-29 16:37:41 +0000 UTC <nil> <nil> map[name:rollover-pod] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 16:37:43 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:minReadySeconds":{},"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033a6768 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:0,MaxSurge:1,},},MinReadySeconds:10,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 16:37:41 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-rollover-deployment-57777854c9" has successfully progressed.,LastUpdateTime:2023-07-29 16:37:55 +0000 UTC,LastTransitionTime:2023-07-29 16:37:41 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 16:37:55.969: INFO: New ReplicaSet "test-rollover-deployment-57777854c9" of Deployment "test-rollover-deployment":
  &ReplicaSet{ObjectMeta:{test-rollover-deployment-57777854c9  deployment-518  192ec9de-427f-4520-8d9f-abdf1e2a0a9a 29363 2 2023-07-29 16:37:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-rollover-deployment def3650b-b7cd-4c9a-aa99-fb8946393ff9 0xc0033a6c47 0xc0033a6c48}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:37:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def3650b-b7cd-4c9a-aa99-fb8946393ff9\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:55 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 57777854c9,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033a6cf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:2,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:37:55.970: INFO: All old ReplicaSets of Deployment "test-rollover-deployment":
  Jul 29 16:37:55.971: INFO: &ReplicaSet{ObjectMeta:{test-rollover-controller  deployment-518  7cbdb681-9e05-4166-a701-f52fac55d34a 29373 2 2023-07-29 16:37:34 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2] [{apps/v1 Deployment test-rollover-deployment def3650b-b7cd-4c9a-aa99-fb8946393ff9 0xc0033a6b07 0xc0033a6b08}] [] [{e2e.test Update apps/v1 2023-07-29 16:37:34 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:55 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def3650b-b7cd-4c9a-aa99-fb8946393ff9\"}":{}}},"f:spec":{"f:replicas":{}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:55 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0033a6bd8 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:37:55.971: INFO: &ReplicaSet{ObjectMeta:{test-rollover-deployment-58779b56b4  deployment-518  5bf796b3-f2f0-4789-ae2f-c520d489aeb3 29324 2 2023-07-29 16:37:41 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-rollover-deployment def3650b-b7cd-4c9a-aa99-fb8946393ff9 0xc0033a6d77 0xc0033a6d78}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:37:43 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"def3650b-b7cd-4c9a-aa99-fb8946393ff9\"}":{}}},"f:spec":{"f:minReadySeconds":{},"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"redis-slave\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:37:43 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: rollover-pod,pod-template-hash: 58779b56b4,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:58779b56b4] map[] [] [] []} {[] [] [{redis-slave gcr.io/google_samples/gb-redisslave:nonexistent [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0033a6e28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:10,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:37:55.980: INFO: Pod "test-rollover-deployment-57777854c9-fn5qs" is available:
  &Pod{ObjectMeta:{test-rollover-deployment-57777854c9-fn5qs test-rollover-deployment-57777854c9- deployment-518  d8c5d187-c34f-49e4-a990-c92dbd925648 29338 0 2023-07-29 16:37:43 +0000 UTC <nil> <nil> map[name:rollover-pod pod-template-hash:57777854c9] map[] [{apps/v1 ReplicaSet test-rollover-deployment-57777854c9 192ec9de-427f-4520-8d9f-abdf1e2a0a9a 0xc005484737 0xc005484738}] [] [{kube-controller-manager Update v1 2023-07-29 16:37:43 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"192ec9de-427f-4520-8d9f-abdf1e2a0a9a\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:37:45 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.44\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9gpq4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9gpq4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:44 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:45 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:37:43 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.44,StartTime:2023-07-29 16:37:44 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:agnhost,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:37:44 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,ImageID:registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e,ContainerID:cri-o://6bd911a27060b218acedb435d830a392f3687ad098e1383ccc4e42d0bec5d0de,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.44,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 16:37:55.981: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-518" for this suite. @ 07/29/23 16:37:55.991
• [21.292 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:187
  STEP: Creating a kubernetes client @ 07/29/23 16:37:56.008
  Jul 29 16:37:56.008: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:37:56.012
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:37:56.043
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:37:56.047
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/29/23 16:37:56.053
  STEP: Saw pod success @ 07/29/23 16:38:00.094
  Jul 29 16:38:00.103: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-337ac5ea-38bf-4713-b3e3-a8e78e4b8a23 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:38:00.122
  Jul 29 16:38:00.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4147" for this suite. @ 07/29/23 16:38:00.164
• [4.171 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a volume subpath [Conformance]
test/e2e/common/node/expansion.go:115
  STEP: Creating a kubernetes client @ 07/29/23 16:38:00.188
  Jul 29 16:38:00.188: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:38:00.19
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:00.216
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:00.224
  STEP: Creating a pod to test substitution in volume subpath @ 07/29/23 16:38:00.23
  STEP: Saw pod success @ 07/29/23 16:38:04.283
  Jul 29 16:38:04.290: INFO: Trying to get logs from node ci9axai7aiv7-3 pod var-expansion-299e665e-5f34-4862-9fd8-2c855dde7ca3 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:38:04.303
  Jul 29 16:38:04.330: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-1120" for this suite. @ 07/29/23 16:38:04.338
• [4.164 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should release no longer matching pods [Conformance]
test/e2e/apps/rc.go:103
  STEP: Creating a kubernetes client @ 07/29/23 16:38:04.354
  Jul 29 16:38:04.354: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 16:38:04.357
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:04.383
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:04.392
  STEP: Given a ReplicationController is created @ 07/29/23 16:38:04.397
  STEP: When the matched label of one of its pods change @ 07/29/23 16:38:04.407
  Jul 29 16:38:04.416: INFO: Pod name pod-release: Found 0 pods out of 1
  Jul 29 16:38:09.425: INFO: Pod name pod-release: Found 1 pods out of 1
  STEP: Then the pod is released @ 07/29/23 16:38:09.449
  Jul 29 16:38:10.463: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2734" for this suite. @ 07/29/23 16:38:10.471
• [6.131 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a configMap. [Conformance]
test/e2e/apimachinery/resource_quota.go:328
  STEP: Creating a kubernetes client @ 07/29/23 16:38:10.489
  Jul 29 16:38:10.489: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:38:10.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:10.515
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:10.519
  STEP: Counting existing ResourceQuota @ 07/29/23 16:38:27.544
  STEP: Creating a ResourceQuota @ 07/29/23 16:38:32.567
  STEP: Ensuring resource quota status is calculated @ 07/29/23 16:38:32.578
  STEP: Creating a ConfigMap @ 07/29/23 16:38:34.587
  STEP: Ensuring resource quota status captures configMap creation @ 07/29/23 16:38:34.615
  STEP: Deleting a ConfigMap @ 07/29/23 16:38:36.625
  STEP: Ensuring resource quota status released usage @ 07/29/23 16:38:36.636
  Jul 29 16:38:38.647: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-6818" for this suite. @ 07/29/23 16:38:38.656
• [28.182 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should be able to update and delete ResourceQuota. [Conformance]
test/e2e/apimachinery/resource_quota.go:887
  STEP: Creating a kubernetes client @ 07/29/23 16:38:38.679
  Jul 29 16:38:38.679: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 16:38:38.681
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:38.705
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:38.71
  STEP: Creating a ResourceQuota @ 07/29/23 16:38:38.713
  STEP: Getting a ResourceQuota @ 07/29/23 16:38:38.722
  STEP: Updating a ResourceQuota @ 07/29/23 16:38:38.73
  STEP: Verifying a ResourceQuota was modified @ 07/29/23 16:38:38.746
  STEP: Deleting a ResourceQuota @ 07/29/23 16:38:38.75
  STEP: Verifying the deleted ResourceQuota @ 07/29/23 16:38:38.764
  Jul 29 16:38:38.771: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2458" for this suite. @ 07/29/23 16:38:38.78
• [0.115 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command in a pod should print the output to logs [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:52
  STEP: Creating a kubernetes client @ 07/29/23 16:38:38.804
  Jul 29 16:38:38.804: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 16:38:38.806
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:38.829
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:38.834
  Jul 29 16:38:40.887: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8680" for this suite. @ 07/29/23 16:38:40.894
• [2.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource with different stored version [Conformance]
test/e2e/apimachinery/webhook.go:314
  STEP: Creating a kubernetes client @ 07/29/23 16:38:40.914
  Jul 29 16:38:40.914: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:38:40.916
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:40.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:40.948
  STEP: Setting up server cert @ 07/29/23 16:38:40.994
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:38:41.716
  STEP: Deploying the webhook pod @ 07/29/23 16:38:41.732
  STEP: Wait for the deployment to be ready @ 07/29/23 16:38:41.758
  Jul 29 16:38:41.774: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Jul 29 16:38:43.809: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 38, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 38, 41, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 38, 41, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 38, 41, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/29/23 16:38:45.816
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:38:45.832
  Jul 29 16:38:46.833: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 16:38:46.840: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-8604-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 16:38:47.37
  STEP: Creating a custom resource while v1 is storage version @ 07/29/23 16:38:47.406
  STEP: Patching Custom Resource Definition to set v2 as storage @ 07/29/23 16:38:49.864
  STEP: Patching the custom resource while v2 is storage version @ 07/29/23 16:38:49.893
  Jul 29 16:38:49.912: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7779" for this suite. @ 07/29/23 16:38:50.574
  STEP: Destroying namespace "webhook-markers-604" for this suite. @ 07/29/23 16:38:50.592
• [9.695 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-auth] Certificates API [Privileged:ClusterAdmin] should support CSR API operations [Conformance]
test/e2e/auth/certificates.go:200
  STEP: Creating a kubernetes client @ 07/29/23 16:38:50.613
  Jul 29 16:38:50.613: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename certificates @ 07/29/23 16:38:50.615
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:50.667
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:50.671
  STEP: getting /apis @ 07/29/23 16:38:51.865
  STEP: getting /apis/certificates.k8s.io @ 07/29/23 16:38:51.874
  STEP: getting /apis/certificates.k8s.io/v1 @ 07/29/23 16:38:51.876
  STEP: creating @ 07/29/23 16:38:51.878
  STEP: getting @ 07/29/23 16:38:51.911
  STEP: listing @ 07/29/23 16:38:51.917
  STEP: watching @ 07/29/23 16:38:51.923
  Jul 29 16:38:51.923: INFO: starting watch
  STEP: patching @ 07/29/23 16:38:51.929
  STEP: updating @ 07/29/23 16:38:51.946
  Jul 29 16:38:51.956: INFO: waiting for watch events with expected annotations
  Jul 29 16:38:51.956: INFO: saw patched and updated annotations
  STEP: getting /approval @ 07/29/23 16:38:51.956
  STEP: patching /approval @ 07/29/23 16:38:51.963
  STEP: updating /approval @ 07/29/23 16:38:51.972
  STEP: getting /status @ 07/29/23 16:38:51.986
  STEP: patching /status @ 07/29/23 16:38:51.994
  STEP: updating /status @ 07/29/23 16:38:52.005
  STEP: deleting @ 07/29/23 16:38:52.015
  STEP: deleting a collection @ 07/29/23 16:38:52.046
  Jul 29 16:38:52.073: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "certificates-6812" for this suite. @ 07/29/23 16:38:52.107
• [1.510 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service Proxy [Conformance]
test/e2e/network/proxy.go:380
  STEP: Creating a kubernetes client @ 07/29/23 16:38:52.134
  Jul 29 16:38:52.134: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename proxy @ 07/29/23 16:38:52.138
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:52.167
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:52.174
  Jul 29 16:38:52.177: INFO: Creating pod...
  Jul 29 16:38:54.212: INFO: Creating service...
  Jul 29 16:38:54.236: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/pods/agnhost/proxy?method=DELETE
  Jul 29 16:38:54.256: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 16:38:54.256: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/pods/agnhost/proxy?method=OPTIONS
  Jul 29 16:38:54.271: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 16:38:54.271: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/pods/agnhost/proxy?method=PATCH
  Jul 29 16:38:54.283: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 16:38:54.284: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/pods/agnhost/proxy?method=POST
  Jul 29 16:38:54.303: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 16:38:54.303: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/pods/agnhost/proxy?method=PUT
  Jul 29 16:38:54.311: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 16:38:54.311: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/services/e2e-proxy-test-service/proxy?method=DELETE
  Jul 29 16:38:54.317: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 16:38:54.318: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/services/e2e-proxy-test-service/proxy?method=OPTIONS
  Jul 29 16:38:54.335: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 16:38:54.335: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/services/e2e-proxy-test-service/proxy?method=PATCH
  Jul 29 16:38:54.345: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 16:38:54.345: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/services/e2e-proxy-test-service/proxy?method=POST
  Jul 29 16:38:54.370: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 16:38:54.372: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/services/e2e-proxy-test-service/proxy?method=PUT
  Jul 29 16:38:54.383: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 16:38:54.384: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/pods/agnhost/proxy?method=GET
  Jul 29 16:38:54.390: INFO: http.Client request:GET StatusCode:301
  Jul 29 16:38:54.390: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/services/e2e-proxy-test-service/proxy?method=GET
  Jul 29 16:38:54.397: INFO: http.Client request:GET StatusCode:301
  Jul 29 16:38:54.397: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/pods/agnhost/proxy?method=HEAD
  Jul 29 16:38:54.405: INFO: http.Client request:HEAD StatusCode:301
  Jul 29 16:38:54.406: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-7523/services/e2e-proxy-test-service/proxy?method=HEAD
  Jul 29 16:38:54.411: INFO: http.Client request:HEAD StatusCode:301
  Jul 29 16:38:54.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-7523" for this suite. @ 07/29/23 16:38:54.423
• [2.299 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message from log output if TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:216
  STEP: Creating a kubernetes client @ 07/29/23 16:38:54.435
  Jul 29 16:38:54.436: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 16:38:54.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:54.461
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:54.466
  STEP: create the container @ 07/29/23 16:38:54.476
  W0729 16:38:54.491116      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Failed @ 07/29/23 16:38:54.491
  STEP: get the container status @ 07/29/23 16:38:57.522
  STEP: the container should be terminated @ 07/29/23 16:38:57.531
  STEP: the termination message should be set @ 07/29/23 16:38:57.532
  Jul 29 16:38:57.532: INFO: Expected: &{DONE} to match Container's Termination Message: DONE --
  STEP: delete the container @ 07/29/23 16:38:57.533
  Jul 29 16:38:57.552: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-2573" for this suite. @ 07/29/23 16:38:57.566
• [3.141 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:74
  STEP: Creating a kubernetes client @ 07/29/23 16:38:57.577
  Jul 29 16:38:57.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:38:57.58
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:38:57.61
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:38:57.615
  STEP: Creating configMap with name projected-configmap-test-volume-b279cacb-fbcf-4d14-8732-398c84b54066 @ 07/29/23 16:38:57.62
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:38:57.627
  STEP: Saw pod success @ 07/29/23 16:39:01.659
  Jul 29 16:39:01.664: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-configmaps-c744c3bc-bc2b-42c2-bcd0-ed489da832e8 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:39:01.675
  Jul 29 16:39:01.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1267" for this suite. @ 07/29/23 16:39:01.709
• [4.144 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should replace a pod template [Conformance]
test/e2e/common/node/podtemplates.go:176
  STEP: Creating a kubernetes client @ 07/29/23 16:39:01.736
  Jul 29 16:39:01.736: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 16:39:01.737
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:39:01.767
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:39:01.771
  STEP: Create a pod template @ 07/29/23 16:39:01.775
  STEP: Replace a pod template @ 07/29/23 16:39:01.783
  Jul 29 16:39:01.799: INFO: Found updated podtemplate annotation: "true"

  Jul 29 16:39:01.799: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-3336" for this suite. @ 07/29/23 16:39:01.809
• [0.086 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers should receive events on concurrent watches in same order [Conformance]
test/e2e/apimachinery/watch.go:334
  STEP: Creating a kubernetes client @ 07/29/23 16:39:01.824
  Jul 29 16:39:01.824: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename watch @ 07/29/23 16:39:01.826
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:39:01.88
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:39:01.884
  STEP: getting a starting resourceVersion @ 07/29/23 16:39:01.893
  STEP: starting a background goroutine to produce watch events @ 07/29/23 16:39:01.908
  STEP: creating watches starting from each resource version of the events produced and verifying they all receive resource versions in the same order @ 07/29/23 16:39:01.909
  Jul 29 16:39:04.636: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-3763" for this suite. @ 07/29/23 16:39:04.687
• [2.918 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-architecture] Conformance Tests should have at least two untainted nodes [Conformance]
test/e2e/architecture/conformance.go:39
  STEP: Creating a kubernetes client @ 07/29/23 16:39:04.749
  Jul 29 16:39:04.750: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename conformance-tests @ 07/29/23 16:39:04.752
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:39:04.779
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:39:04.784
  STEP: Getting node addresses @ 07/29/23 16:39:04.788
  Jul 29 16:39:04.788: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  Jul 29 16:39:04.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "conformance-tests-3949" for this suite. @ 07/29/23 16:39:04.811
• [0.074 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ControllerRevision [Serial] should manage the lifecycle of a ControllerRevision [Conformance]
test/e2e/apps/controller_revision.go:124
  STEP: Creating a kubernetes client @ 07/29/23 16:39:04.832
  Jul 29 16:39:04.832: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename controllerrevisions @ 07/29/23 16:39:04.834
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:39:04.865
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:39:04.868
  STEP: Creating DaemonSet "e2e-wd9fp-daemon-set" @ 07/29/23 16:39:04.916
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 16:39:04.925
  Jul 29 16:39:04.944: INFO: Number of nodes with available pods controlled by daemonset e2e-wd9fp-daemon-set: 0
  Jul 29 16:39:04.945: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:39:05.970: INFO: Number of nodes with available pods controlled by daemonset e2e-wd9fp-daemon-set: 0
  Jul 29 16:39:05.970: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:39:06.975: INFO: Number of nodes with available pods controlled by daemonset e2e-wd9fp-daemon-set: 3
  Jul 29 16:39:06.976: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset e2e-wd9fp-daemon-set
  STEP: Confirm DaemonSet "e2e-wd9fp-daemon-set" successfully created with "daemonset-name=e2e-wd9fp-daemon-set" label @ 07/29/23 16:39:06.983
  STEP: Listing all ControllerRevisions with label "daemonset-name=e2e-wd9fp-daemon-set" @ 07/29/23 16:39:07
  Jul 29 16:39:07.006: INFO: Located ControllerRevision: "e2e-wd9fp-daemon-set-867f5f8d57"
  STEP: Patching ControllerRevision "e2e-wd9fp-daemon-set-867f5f8d57" @ 07/29/23 16:39:07.011
  Jul 29 16:39:07.022: INFO: e2e-wd9fp-daemon-set-867f5f8d57 has been patched
  STEP: Create a new ControllerRevision @ 07/29/23 16:39:07.022
  Jul 29 16:39:07.063: INFO: Created ControllerRevision: e2e-wd9fp-daemon-set-67f994f665
  STEP: Confirm that there are two ControllerRevisions @ 07/29/23 16:39:07.063
  Jul 29 16:39:07.063: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 16:39:07.071: INFO: Found 2 ControllerRevisions
  STEP: Deleting ControllerRevision "e2e-wd9fp-daemon-set-867f5f8d57" @ 07/29/23 16:39:07.072
  STEP: Confirm that there is only one ControllerRevision @ 07/29/23 16:39:07.086
  Jul 29 16:39:07.087: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 16:39:07.093: INFO: Found 1 ControllerRevisions
  STEP: Updating ControllerRevision "e2e-wd9fp-daemon-set-67f994f665" @ 07/29/23 16:39:07.104
  Jul 29 16:39:07.126: INFO: e2e-wd9fp-daemon-set-67f994f665 has been updated
  STEP: Generate another ControllerRevision by patching the Daemonset @ 07/29/23 16:39:07.126
  W0729 16:39:07.144601      13 warnings.go:70] unknown field "updateStrategy"
  STEP: Confirm that there are two ControllerRevisions @ 07/29/23 16:39:07.144
  Jul 29 16:39:07.144: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 16:39:08.152: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 16:39:08.162: INFO: Found 2 ControllerRevisions
  STEP: Removing a ControllerRevision via 'DeleteCollection' with labelSelector: "e2e-wd9fp-daemon-set-67f994f665=updated" @ 07/29/23 16:39:08.162
  STEP: Confirm that there is only one ControllerRevision @ 07/29/23 16:39:08.187
  Jul 29 16:39:08.187: INFO: Requesting list of ControllerRevisions to confirm quantity
  Jul 29 16:39:08.192: INFO: Found 1 ControllerRevisions
  Jul 29 16:39:08.196: INFO: ControllerRevision "e2e-wd9fp-daemon-set-848fc9dd64" has revision 3
  STEP: Deleting DaemonSet "e2e-wd9fp-daemon-set" @ 07/29/23 16:39:08.2
  STEP: deleting DaemonSet.extensions e2e-wd9fp-daemon-set in namespace controllerrevisions-2227, will wait for the garbage collector to delete the pods @ 07/29/23 16:39:08.2
  Jul 29 16:39:08.271: INFO: Deleting DaemonSet.extensions e2e-wd9fp-daemon-set took: 12.650962ms
  Jul 29 16:39:08.372: INFO: Terminating DaemonSet.extensions e2e-wd9fp-daemon-set pods took: 101.115658ms
  Jul 29 16:39:10.081: INFO: Number of nodes with available pods controlled by daemonset e2e-wd9fp-daemon-set: 0
  Jul 29 16:39:10.081: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset e2e-wd9fp-daemon-set
  Jul 29 16:39:10.090: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"30187"},"items":null}

  Jul 29 16:39:10.121: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"30187"},"items":null}

  Jul 29 16:39:10.165: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "controllerrevisions-2227" for this suite. @ 07/29/23 16:39:10.176
• [5.356 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment deployment should delete old replica sets [Conformance]
test/e2e/apps/deployment.go:122
  STEP: Creating a kubernetes client @ 07/29/23 16:39:10.191
  Jul 29 16:39:10.191: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:39:10.194
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:39:10.227
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:39:10.232
  Jul 29 16:39:10.255: INFO: Pod name cleanup-pod: Found 0 pods out of 1
  Jul 29 16:39:15.272: INFO: Pod name cleanup-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 16:39:15.272
  Jul 29 16:39:15.272: INFO: Creating deployment test-cleanup-deployment
  STEP: Waiting for deployment test-cleanup-deployment history to be cleaned up @ 07/29/23 16:39:15.29
  Jul 29 16:39:15.322: INFO: Deployment "test-cleanup-deployment":
  &Deployment{ObjectMeta:{test-cleanup-deployment  deployment-9654  84b63c44-63db-4c59-b4aa-9c4d7650d39d 30254 1 2023-07-29 16:39:15 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] [{e2e.test Update apps/v1 2023-07-29 16:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc004538168 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*0,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:0,Replicas:0,UpdatedReplicas:0,AvailableReplicas:0,UnavailableReplicas:0,Conditions:[]DeploymentCondition{},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 29 16:39:15.331: INFO: New ReplicaSet of Deployment "test-cleanup-deployment" is nil.
  Jul 29 16:39:15.331: INFO: All old ReplicaSets of Deployment "test-cleanup-deployment":
  Jul 29 16:39:15.331: INFO: &ReplicaSet{ObjectMeta:{test-cleanup-controller  deployment-9654  b77f8c46-df76-44af-a495-a4c5708c2282 30256 1 2023-07-29 16:39:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 Deployment test-cleanup-deployment 84b63c44-63db-4c59-b4aa-9c4d7650d39d 0xc0038c07c7 0xc0038c07c8}] [] [{e2e.test Update apps/v1 2023-07-29 16:39:10 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:39:11 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 16:39:15 +0000 UTC FieldsV1 {"f:metadata":{"f:ownerReferences":{".":{},"k:{\"uid\":\"84b63c44-63db-4c59-b4aa-9c4d7650d39d\"}":{}}}} }]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: cleanup-pod,pod: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent nil false false false}] [] Always 0xc0038c0888 <nil> ClusterFirst map[]   <nil>  false false false <nil> PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:39:15.354: INFO: Pod "test-cleanup-controller-d8dxh" is available:
  &Pod{ObjectMeta:{test-cleanup-controller-d8dxh test-cleanup-controller- deployment-9654  1aae936e-7ec6-4f89-8a22-854d80e80a70 30232 0 2023-07-29 16:39:10 +0000 UTC <nil> <nil> map[name:cleanup-pod pod:httpd] map[] [{apps/v1 ReplicaSet test-cleanup-controller b77f8c46-df76-44af-a495-a4c5708c2282 0xc0038c0b87 0xc0038c0b88}] [] [{kube-controller-manager Update v1 2023-07-29 16:39:10 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"b77f8c46-df76-44af-a495-a4c5708c2282\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:39:11 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.68\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-5nzkf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-5nzkf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:nil,Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:39:10 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:39:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:39:11 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:39:10 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.68,StartTime:2023-07-29 16:39:10 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 16:39:11 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://51f4fa3e4d32ce872ce115cd0524023a72670cb91b804e4cb3a6294ba722ac91,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.68,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 16:39:15.354: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9654" for this suite. @ 07/29/23 16:39:15.388
• [5.225 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to start watching from a specific resource version [Conformance]
test/e2e/apimachinery/watch.go:142
  STEP: Creating a kubernetes client @ 07/29/23 16:39:15.42
  Jul 29 16:39:15.420: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename watch @ 07/29/23 16:39:15.423
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:39:15.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:39:15.489
  STEP: creating a new configmap @ 07/29/23 16:39:15.5
  STEP: modifying the configmap once @ 07/29/23 16:39:15.524
  STEP: modifying the configmap a second time @ 07/29/23 16:39:15.564
  STEP: deleting the configmap @ 07/29/23 16:39:15.583
  STEP: creating a watch on configmaps from the resource version returned by the first update @ 07/29/23 16:39:15.594
  STEP: Expecting to observe notifications for all changes to the configmap after the first update @ 07/29/23 16:39:15.596
  Jul 29 16:39:15.597: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4058  b0f24daa-1061-4af9-b135-799223fa2ef4 30276 0 2023-07-29 16:39:15 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 16:39:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:39:15.597: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-resource-version  watch-4058  b0f24daa-1061-4af9-b135-799223fa2ef4 30277 0 2023-07-29 16:39:15 +0000 UTC <nil> <nil> map[watch-this-configmap:from-resource-version] map[] [] [] [{e2e.test Update v1 2023-07-29 16:39:15 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:39:15.597: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-4058" for this suite. @ 07/29/23 16:39:15.607
• [0.198 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-node] Containers should be able to override the image's default command and arguments [NodeConformance] [Conformance]
test/e2e/common/node/containers.go:89
  STEP: Creating a kubernetes client @ 07/29/23 16:39:15.622
  Jul 29 16:39:15.622: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename containers @ 07/29/23 16:39:15.624
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:39:15.647
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:39:15.651
  STEP: Creating a pod to test override all @ 07/29/23 16:39:15.658
  STEP: Saw pod success @ 07/29/23 16:39:19.709
  Jul 29 16:39:19.714: INFO: Trying to get logs from node ci9axai7aiv7-3 pod client-containers-5da660b3-8c58-424e-9bbf-ab346aa87a74 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:39:19.726
  Jul 29 16:39:19.755: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "containers-314" for this suite. @ 07/29/23 16:39:19.764
• [4.154 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] NoExecuteTaintManager Single Pod [Serial] removing taint cancels eviction [Disruptive] [Conformance]
test/e2e/node/taints.go:290
  STEP: Creating a kubernetes client @ 07/29/23 16:39:19.784
  Jul 29 16:39:19.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename taint-single-pod @ 07/29/23 16:39:19.787
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:39:19.81
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:39:19.816
  Jul 29 16:39:19.820: INFO: Waiting up to 1m0s for all nodes to be ready
  Jul 29 16:40:19.866: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:40:19.874: INFO: Starting informer...
  STEP: Starting pod... @ 07/29/23 16:40:19.875
  Jul 29 16:40:20.109: INFO: Pod is running on ci9axai7aiv7-3. Tainting Node
  STEP: Trying to apply a taint on the Node @ 07/29/23 16:40:20.109
  STEP: verifying the node has the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 16:40:20.146
  STEP: Waiting short time to make sure Pod is queued for deletion @ 07/29/23 16:40:20.159
  Jul 29 16:40:20.159: INFO: Pod wasn't evicted. Proceeding
  Jul 29 16:40:20.159: INFO: Removing taint from Node
  STEP: verifying the node doesn't have the taint kubernetes.io/e2e-evict-taint-key=evictTaintVal:NoExecute @ 07/29/23 16:40:20.196
  STEP: Waiting some time to make sure that toleration time passed. @ 07/29/23 16:40:20.203
  Jul 29 16:41:35.204: INFO: Pod wasn't evicted. Test successful
  Jul 29 16:41:35.208: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "taint-single-pod-727" for this suite. @ 07/29/23 16:41:35.22
• [135.445 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] Namespaces [Serial] should ensure that all services are removed when a namespace is deleted [Conformance]
test/e2e/apimachinery/namespace.go:252
  STEP: Creating a kubernetes client @ 07/29/23 16:41:35.232
  Jul 29 16:41:35.233: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 16:41:35.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:41:35.259
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:41:35.266
  STEP: Creating a test namespace @ 07/29/23 16:41:35.27
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:41:35.293
  STEP: Creating a service in the namespace @ 07/29/23 16:41:35.298
  STEP: Deleting the namespace @ 07/29/23 16:41:35.321
  STEP: Waiting for the namespace to be removed. @ 07/29/23 16:41:35.343
  STEP: Recreating the namespace @ 07/29/23 16:41:41.355
  STEP: Verifying there is no service in the namespace @ 07/29/23 16:41:41.388
  Jul 29 16:41:41.395: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-6568" for this suite. @ 07/29/23 16:41:41.403
  STEP: Destroying namespace "nsdeletetest-4525" for this suite. @ 07/29/23 16:41:41.419
  Jul 29 16:41:41.426: INFO: Namespace nsdeletetest-4525 was already deleted
  STEP: Destroying namespace "nsdeletetest-4645" for this suite. @ 07/29/23 16:41:41.426
• [6.207 seconds]
------------------------------
SSS
------------------------------
[sig-node] RuntimeClass  should support RuntimeClasses API operations [Conformance]
test/e2e/common/node/runtimeclass.go:189
  STEP: Creating a kubernetes client @ 07/29/23 16:41:41.441
  Jul 29 16:41:41.441: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename runtimeclass @ 07/29/23 16:41:41.443
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:41:41.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:41:41.473
  STEP: getting /apis @ 07/29/23 16:41:41.479
  STEP: getting /apis/node.k8s.io @ 07/29/23 16:41:41.488
  STEP: getting /apis/node.k8s.io/v1 @ 07/29/23 16:41:41.491
  STEP: creating @ 07/29/23 16:41:41.493
  STEP: watching @ 07/29/23 16:41:41.525
  Jul 29 16:41:41.525: INFO: starting watch
  STEP: getting @ 07/29/23 16:41:41.537
  STEP: listing @ 07/29/23 16:41:41.542
  STEP: patching @ 07/29/23 16:41:41.546
  STEP: updating @ 07/29/23 16:41:41.554
  Jul 29 16:41:41.561: INFO: waiting for watch events with expected annotations
  STEP: deleting @ 07/29/23 16:41:41.562
  STEP: deleting a collection @ 07/29/23 16:41:41.581
  Jul 29 16:41:41.603: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "runtimeclass-1536" for this suite. @ 07/29/23 16:41:41.61
• [0.178 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Guestbook application should create and stop a working application  [Conformance]
test/e2e/kubectl/kubectl.go:396
  STEP: Creating a kubernetes client @ 07/29/23 16:41:41.628
  Jul 29 16:41:41.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:41:41.631
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:41:41.658
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:41:41.664
  STEP: creating all guestbook components @ 07/29/23 16:41:41.682
  Jul 29 16:41:41.682: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-replica
    labels:
      app: agnhost
      role: replica
      tier: backend
  spec:
    ports:
    - port: 6379
    selector:
      app: agnhost
      role: replica
      tier: backend

  Jul 29 16:41:41.688: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 create -f -'
  Jul 29 16:41:43.432: INFO: stderr: ""
  Jul 29 16:41:43.432: INFO: stdout: "service/agnhost-replica created\n"
  Jul 29 16:41:43.433: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: agnhost-primary
    labels:
      app: agnhost
      role: primary
      tier: backend
  spec:
    ports:
    - port: 6379
      targetPort: 6379
    selector:
      app: agnhost
      role: primary
      tier: backend

  Jul 29 16:41:43.433: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 create -f -'
  Jul 29 16:41:44.011: INFO: stderr: ""
  Jul 29 16:41:44.011: INFO: stdout: "service/agnhost-primary created\n"
  Jul 29 16:41:44.011: INFO: apiVersion: v1
  kind: Service
  metadata:
    name: frontend
    labels:
      app: guestbook
      tier: frontend
  spec:
    # if your cluster supports it, uncomment the following to automatically create
    # an external load-balanced IP for the frontend service.
    # type: LoadBalancer
    ports:
    - port: 80
    selector:
      app: guestbook
      tier: frontend

  Jul 29 16:41:44.012: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 create -f -'
  Jul 29 16:41:44.547: INFO: stderr: ""
  Jul 29 16:41:44.548: INFO: stdout: "service/frontend created\n"
  Jul 29 16:41:44.548: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: frontend
  spec:
    replicas: 3
    selector:
      matchLabels:
        app: guestbook
        tier: frontend
    template:
      metadata:
        labels:
          app: guestbook
          tier: frontend
      spec:
        containers:
        - name: guestbook-frontend
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--backend-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 80

  Jul 29 16:41:44.549: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 create -f -'
  Jul 29 16:41:45.010: INFO: stderr: ""
  Jul 29 16:41:45.010: INFO: stdout: "deployment.apps/frontend created\n"
  Jul 29 16:41:45.010: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-primary
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: agnhost
        role: primary
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: primary
          tier: backend
      spec:
        containers:
        - name: primary
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 29 16:41:45.011: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 create -f -'
  Jul 29 16:41:45.679: INFO: stderr: ""
  Jul 29 16:41:45.679: INFO: stdout: "deployment.apps/agnhost-primary created\n"
  Jul 29 16:41:45.679: INFO: apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: agnhost-replica
  spec:
    replicas: 2
    selector:
      matchLabels:
        app: agnhost
        role: replica
        tier: backend
    template:
      metadata:
        labels:
          app: agnhost
          role: replica
          tier: backend
      spec:
        containers:
        - name: replica
          image: registry.k8s.io/e2e-test-images/agnhost:2.43
          args: [ "guestbook", "--replicaof", "agnhost-primary", "--http-port", "6379" ]
          resources:
            requests:
              cpu: 100m
              memory: 100Mi
          ports:
          - containerPort: 6379

  Jul 29 16:41:45.679: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 create -f -'
  Jul 29 16:41:46.837: INFO: stderr: ""
  Jul 29 16:41:46.837: INFO: stdout: "deployment.apps/agnhost-replica created\n"
  STEP: validating guestbook app @ 07/29/23 16:41:46.837
  Jul 29 16:41:46.837: INFO: Waiting for all frontend pods to be Running.
  Jul 29 16:41:51.888: INFO: Waiting for frontend to serve content.
  Jul 29 16:41:51.926: INFO: Trying to add a new entry to the guestbook.
  Jul 29 16:41:51.956: INFO: Verifying that added entry can be retrieved.
  STEP: using delete to clean up resources @ 07/29/23 16:41:51.98
  Jul 29 16:41:51.981: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 delete --grace-period=0 --force -f -'
  Jul 29 16:41:52.133: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:41:52.133: INFO: stdout: "service \"agnhost-replica\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:41:52.133
  Jul 29 16:41:52.134: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 delete --grace-period=0 --force -f -'
  Jul 29 16:41:52.316: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:41:52.316: INFO: stdout: "service \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:41:52.316
  Jul 29 16:41:52.316: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 delete --grace-period=0 --force -f -'
  Jul 29 16:41:52.459: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:41:52.459: INFO: stdout: "service \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:41:52.459
  Jul 29 16:41:52.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 delete --grace-period=0 --force -f -'
  Jul 29 16:41:52.608: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:41:52.608: INFO: stdout: "deployment.apps \"frontend\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:41:52.608
  Jul 29 16:41:52.608: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 delete --grace-period=0 --force -f -'
  Jul 29 16:41:52.840: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:41:52.840: INFO: stdout: "deployment.apps \"agnhost-primary\" force deleted\n"
  STEP: using delete to clean up resources @ 07/29/23 16:41:52.84
  Jul 29 16:41:52.840: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6139 delete --grace-period=0 --force -f -'
  Jul 29 16:41:53.057: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 16:41:53.057: INFO: stdout: "deployment.apps \"agnhost-replica\" force deleted\n"
  Jul 29 16:41:53.057: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6139" for this suite. @ 07/29/23 16:41:53.098
• [11.508 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with configmap pod [Conformance]
test/e2e/storage/subpath.go:70
  STEP: Creating a kubernetes client @ 07/29/23 16:41:53.138
  Jul 29 16:41:53.138: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename subpath @ 07/29/23 16:41:53.153
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:41:53.201
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:41:53.205
  STEP: Setting up data @ 07/29/23 16:41:53.209
  STEP: Creating pod pod-subpath-test-configmap-2gpn @ 07/29/23 16:41:53.235
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 16:41:53.235
  STEP: Saw pod success @ 07/29/23 16:42:17.371
  Jul 29 16:42:17.377: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-subpath-test-configmap-2gpn container test-container-subpath-configmap-2gpn: <nil>
  STEP: delete the pod @ 07/29/23 16:42:17.407
  STEP: Deleting pod pod-subpath-test-configmap-2gpn @ 07/29/23 16:42:17.437
  Jul 29 16:42:17.437: INFO: Deleting pod "pod-subpath-test-configmap-2gpn" in namespace "subpath-6316"
  Jul 29 16:42:17.442: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-6316" for this suite. @ 07/29/23 16:42:17.452
• [24.325 seconds]
------------------------------
SS
------------------------------
[sig-network] EndpointSlice should create and delete Endpoints and EndpointSlices for a Service with a selector specified [Conformance]
test/e2e/network/endpointslice.go:104
  STEP: Creating a kubernetes client @ 07/29/23 16:42:17.464
  Jul 29 16:42:17.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename endpointslice @ 07/29/23 16:42:17.467
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:17.491
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:17.495
  Jul 29 16:42:19.633: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslice-8391" for this suite. @ 07/29/23 16:42:19.647
• [2.197 seconds]
------------------------------
SSSSS
------------------------------
[sig-cli] Kubectl client Kubectl describe should check if kubectl describe prints relevant information for rc and pods  [Conformance]
test/e2e/kubectl/kubectl.go:1341
  STEP: Creating a kubernetes client @ 07/29/23 16:42:19.665
  Jul 29 16:42:19.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:42:19.668
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:19.699
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:19.704
  Jul 29 16:42:19.709: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6895 create -f -'
  Jul 29 16:42:20.348: INFO: stderr: ""
  Jul 29 16:42:20.348: INFO: stdout: "replicationcontroller/agnhost-primary created\n"
  Jul 29 16:42:20.348: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6895 create -f -'
  Jul 29 16:42:21.063: INFO: stderr: ""
  Jul 29 16:42:21.063: INFO: stdout: "service/agnhost-primary created\n"
  STEP: Waiting for Agnhost primary to start. @ 07/29/23 16:42:21.063
  Jul 29 16:42:22.071: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:42:22.071: INFO: Found 1 / 1
  Jul 29 16:42:22.071: INFO: WaitFor completed with timeout 5m0s.  Pods found = 1 out of 1
  Jul 29 16:42:22.079: INFO: Selector matched 1 pods for map[app:agnhost]
  Jul 29 16:42:22.079: INFO: ForEach: Found 1 pods from the filter.  Now looping through them.
  Jul 29 16:42:22.080: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6895 describe pod agnhost-primary-kdm26'
  Jul 29 16:42:22.288: INFO: stderr: ""
  Jul 29 16:42:22.288: INFO: stdout: "Name:             agnhost-primary-kdm26\nNamespace:        kubectl-6895\nPriority:         0\nService Account:  default\nNode:             ci9axai7aiv7-3/192.168.121.144\nStart Time:       Sat, 29 Jul 2023 16:42:20 +0000\nLabels:           app=agnhost\n                  role=primary\nAnnotations:      <none>\nStatus:           Running\nIP:               10.233.66.125\nIPs:\n  IP:           10.233.66.125\nControlled By:  ReplicationController/agnhost-primary\nContainers:\n  agnhost-primary:\n    Container ID:   cri-o://b1948275860048c8696cb970ae2f9696e5036a3a852ef803b5ff6deedca68137\n    Image:          registry.k8s.io/e2e-test-images/agnhost:2.43\n    Image ID:       registry.k8s.io/e2e-test-images/agnhost@sha256:16bbf38c463a4223d8cfe4da12bc61010b082a79b4bb003e2d3ba3ece5dd5f9e\n    Port:           6379/TCP\n    Host Port:      0/TCP\n    State:          Running\n      Started:      Sat, 29 Jul 2023 16:42:21 +0000\n    Ready:          True\n    Restart Count:  0\n    Environment:    <none>\n    Mounts:\n      /var/run/secrets/kubernetes.io/serviceaccount from kube-api-access-jd9wq (ro)\nConditions:\n  Type              Status\n  Initialized       True \n  Ready             True \n  ContainersReady   True \n  PodScheduled      True \nVolumes:\n  kube-api-access-jd9wq:\n    Type:                    Projected (a volume that contains injected data from multiple sources)\n    TokenExpirationSeconds:  3607\n    ConfigMapName:           kube-root-ca.crt\n    ConfigMapOptional:       <nil>\n    DownwardAPI:             true\nQoS Class:                   BestEffort\nNode-Selectors:              <none>\nTolerations:                 node.kubernetes.io/not-ready:NoExecute op=Exists for 300s\n                             node.kubernetes.io/unreachable:NoExecute op=Exists for 300s\nEvents:\n  Type    Reason     Age   From               Message\n  ----    ------     ----  ----               -------\n  Normal  Scheduled  2s    default-scheduler  Successfully assigned kubectl-6895/agnhost-primary-kdm26 to ci9axai7aiv7-3\n  Normal  Pulled     1s    kubelet            Container image \"registry.k8s.io/e2e-test-images/agnhost:2.43\" already present on machine\n  Normal  Created    1s    kubelet            Created container agnhost-primary\n  Normal  Started    1s    kubelet            Started container agnhost-primary\n"
  Jul 29 16:42:22.289: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6895 describe rc agnhost-primary'
  Jul 29 16:42:22.443: INFO: stderr: ""
  Jul 29 16:42:22.443: INFO: stdout: "Name:         agnhost-primary\nNamespace:    kubectl-6895\nSelector:     app=agnhost,role=primary\nLabels:       app=agnhost\n              role=primary\nAnnotations:  <none>\nReplicas:     1 current / 1 desired\nPods Status:  1 Running / 0 Waiting / 0 Succeeded / 0 Failed\nPod Template:\n  Labels:  app=agnhost\n           role=primary\n  Containers:\n   agnhost-primary:\n    Image:        registry.k8s.io/e2e-test-images/agnhost:2.43\n    Port:         6379/TCP\n    Host Port:    0/TCP\n    Environment:  <none>\n    Mounts:       <none>\n  Volumes:        <none>\nEvents:\n  Type    Reason            Age   From                    Message\n  ----    ------            ----  ----                    -------\n  Normal  SuccessfulCreate  2s    replication-controller  Created pod: agnhost-primary-kdm26\n"
  Jul 29 16:42:22.444: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6895 describe service agnhost-primary'
  Jul 29 16:42:22.602: INFO: stderr: ""
  Jul 29 16:42:22.602: INFO: stdout: "Name:              agnhost-primary\nNamespace:         kubectl-6895\nLabels:            app=agnhost\n                   role=primary\nAnnotations:       <none>\nSelector:          app=agnhost,role=primary\nType:              ClusterIP\nIP Family Policy:  SingleStack\nIP Families:       IPv4\nIP:                10.233.49.130\nIPs:               10.233.49.130\nPort:              <unset>  6379/TCP\nTargetPort:        agnhost-server/TCP\nEndpoints:         10.233.66.125:6379\nSession Affinity:  None\nEvents:            <none>\n"
  Jul 29 16:42:22.612: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6895 describe node ci9axai7aiv7-1'
  Jul 29 16:42:22.818: INFO: stderr: ""
  Jul 29 16:42:22.819: INFO: stdout: "Name:               ci9axai7aiv7-1\nRoles:              control-plane\nLabels:             beta.kubernetes.io/arch=amd64\n                    beta.kubernetes.io/os=linux\n                    kubernetes.io/arch=amd64\n                    kubernetes.io/hostname=ci9axai7aiv7-1\n                    kubernetes.io/os=linux\n                    node-role.kubernetes.io/control-plane=\n                    node.kubernetes.io/exclude-from-external-load-balancers=\nAnnotations:        kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/crio/crio.sock\n                    node.alpha.kubernetes.io/ttl: 0\n                    volumes.kubernetes.io/controller-managed-attach-detach: true\nCreationTimestamp:  Sat, 29 Jul 2023 15:11:09 +0000\nTaints:             <none>\nUnschedulable:      false\nLease:\n  HolderIdentity:  ci9axai7aiv7-1\n  AcquireTime:     <unset>\n  RenewTime:       Sat, 29 Jul 2023 16:42:17 +0000\nConditions:\n  Type                 Status  LastHeartbeatTime                 LastTransitionTime                Reason                       Message\n  ----                 ------  -----------------                 ------------------                ------                       -------\n  NetworkUnavailable   False   Sat, 29 Jul 2023 15:24:06 +0000   Sat, 29 Jul 2023 15:24:06 +0000   CiliumIsUp                   Cilium is running on this node\n  MemoryPressure       False   Sat, 29 Jul 2023 16:37:26 +0000   Sat, 29 Jul 2023 15:11:01 +0000   KubeletHasSufficientMemory   kubelet has sufficient memory available\n  DiskPressure         False   Sat, 29 Jul 2023 16:37:26 +0000   Sat, 29 Jul 2023 15:11:01 +0000   KubeletHasNoDiskPressure     kubelet has no disk pressure\n  PIDPressure          False   Sat, 29 Jul 2023 16:37:26 +0000   Sat, 29 Jul 2023 15:11:01 +0000   KubeletHasSufficientPID      kubelet has sufficient PID available\n  Ready                True    Sat, 29 Jul 2023 16:37:26 +0000   Sat, 29 Jul 2023 15:25:15 +0000   KubeletReady                 kubelet is posting ready status. AppArmor enabled\nAddresses:\n  InternalIP:  192.168.121.39\n  Hostname:    ci9axai7aiv7-1\nCapacity:\n  cpu:                2\n  ephemeral-storage:  115008636Ki\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             8127912Ki\n  pods:               110\nAllocatable:\n  cpu:                1600m\n  ephemeral-storage:  111880401014\n  hugepages-1Gi:      0\n  hugepages-2Mi:      0\n  memory:             3278248Ki\n  pods:               110\nSystem Info:\n  Machine ID:                 6bc638182c6c4b5bb4e8bc9f9867edfb\n  System UUID:                6bc63818-2c6c-4b5b-b4e8-bc9f9867edfb\n  Boot ID:                    e1fda2c0-94fb-4e93-b9fa-587d1f1085a5\n  Kernel Version:             5.19.0-50-generic\n  OS Image:                   Ubuntu 22.04.2 LTS\n  Operating System:           linux\n  Architecture:               amd64\n  Container Runtime Version:  cri-o://1.27.1\n  Kubelet Version:            v1.27.4\n  Kube-Proxy Version:         v1.27.4\nPodCIDR:                      10.233.64.0/24\nPodCIDRs:                     10.233.64.0/24\nNon-terminated Pods:          (9 in total)\n  Namespace                   Name                                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age\n  ---------                   ----                                                       ------------  ----------  ---------------  -------------  ---\n  kube-system                 cilium-6928s                                               100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         79m\n  kube-system                 cilium-node-init-m4qsv                                     100m (6%)     0 (0%)      100Mi (3%)       0 (0%)         79m\n  kube-system                 coredns-5d78c9869d-xxmxx                                   100m (6%)     0 (0%)      70Mi (2%)        170Mi (5%)     78m\n  kube-system                 kube-addon-manager-ci9axai7aiv7-1                          5m (0%)       0 (0%)      50Mi (1%)        0 (0%)         79m\n  kube-system                 kube-apiserver-ci9axai7aiv7-1                              250m (15%)    0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 kube-controller-manager-ci9axai7aiv7-1                     200m (12%)    0 (0%)      0 (0%)           0 (0%)         91m\n  kube-system                 kube-proxy-dscp7                                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         90m\n  kube-system                 kube-scheduler-ci9axai7aiv7-1                              100m (6%)     0 (0%)      0 (0%)           0 (0%)         91m\n  sonobuoy                    sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-7bqrz    0 (0%)        0 (0%)      0 (0%)           0 (0%)         73m\nAllocated resources:\n  (Total limits may be over 100 percent, i.e., overcommitted.)\n  Resource           Requests    Limits\n  --------           --------    ------\n  cpu                855m (53%)  0 (0%)\n  memory             320Mi (9%)  170Mi (5%)\n  ephemeral-storage  0 (0%)      0 (0%)\n  hugepages-1Gi      0 (0%)      0 (0%)\n  hugepages-2Mi      0 (0%)      0 (0%)\nEvents:              <none>\n"
  Jul 29 16:42:22.819: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6895 describe namespace kubectl-6895'
  Jul 29 16:42:22.999: INFO: stderr: ""
  Jul 29 16:42:22.999: INFO: stdout: "Name:         kubectl-6895\nLabels:       e2e-framework=kubectl\n              e2e-run=17f5218d-1ccd-4f9b-86d8-e116b9c61702\n              kubernetes.io/metadata.name=kubectl-6895\n              pod-security.kubernetes.io/enforce=baseline\nAnnotations:  <none>\nStatus:       Active\n\nNo resource quota.\n\nNo LimitRange resource.\n"
  Jul 29 16:42:22.999: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6895" for this suite. @ 07/29/23 16:42:23.008
• [3.354 seconds]
------------------------------
SS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that NodeSelector is respected if matching  [Conformance]
test/e2e/scheduling/predicates.go:467
  STEP: Creating a kubernetes client @ 07/29/23 16:42:23.02
  Jul 29 16:42:23.020: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 16:42:23.025
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:23.05
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:23.054
  Jul 29 16:42:23.059: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 16:42:23.078: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:42:23.084: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-1 before test
  Jul 29 16:42:23.109: INFO: cilium-6928s from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: cilium-node-init-m4qsv from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: coredns-5d78c9869d-xxmxx from kube-system started at 2023-07-29 15:24:09 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: kube-addon-manager-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: kube-apiserver-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: kube-controller-manager-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: kube-proxy-dscp7 from kube-system started at 2023-07-29 15:11:26 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: kube-scheduler-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-7bqrz from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:42:23.109: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:42:23.109: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-2 before test
  Jul 29 16:42:23.129: INFO: cilium-node-init-c52xz from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: cilium-operator-64cdf5fc9d-9t2m7 from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container cilium-operator ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: cilium-pr7vs from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: coredns-5d78c9869d-7fz8w from kube-system started at 2023-07-29 15:24:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: kube-addon-manager-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: kube-apiserver-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: kube-controller-manager-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: kube-proxy-bgt8j from kube-system started at 2023-07-29 15:11:44 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: kube-scheduler-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-568gm from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:42:23.129: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:42:23.129: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-3 before test
  Jul 29 16:42:23.153: INFO: cilium-jsbkd from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.153: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:42:23.154: INFO: cilium-node-init-fs2dr from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.154: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:42:23.154: INFO: kube-proxy-bhtmh from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.154: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:42:23.155: INFO: agnhost-primary-kdm26 from kubectl-6895 started at 2023-07-29 16:42:20 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.155: INFO: 	Container agnhost-primary ready: true, restart count 0
  Jul 29 16:42:23.155: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:29:07 +0000 UTC (1 container statuses recorded)
  Jul 29 16:42:23.156: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 16:42:23.156: INFO: sonobuoy-e2e-job-9fe8eb0b75ee4e08 from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:42:23.156: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 16:42:23.156: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:42:23.157: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-92k89 from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:42:23.157: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:42:23.157: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 16:42:23.158
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 16:42:25.233
  STEP: Trying to apply a random label on the found node. @ 07/29/23 16:42:25.248
  STEP: verifying the node has the label kubernetes.io/e2e-d50d0926-a356-475e-8b56-62fdc87d938e 42 @ 07/29/23 16:42:25.263
  STEP: Trying to relaunch the pod, now with labels. @ 07/29/23 16:42:25.286
  STEP: removing the label kubernetes.io/e2e-d50d0926-a356-475e-8b56-62fdc87d938e off the node ci9axai7aiv7-3 @ 07/29/23 16:42:27.327
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-d50d0926-a356-475e-8b56-62fdc87d938e @ 07/29/23 16:42:27.346
  Jul 29 16:42:27.352: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-9227" for this suite. @ 07/29/23 16:42:27.361
• [4.353 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should succeed in writing subpaths in container [Slow] [Conformance]
test/e2e/common/node/expansion.go:300
  STEP: Creating a kubernetes client @ 07/29/23 16:42:27.383
  Jul 29 16:42:27.383: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:42:27.386
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:42:27.414
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:42:27.418
  STEP: creating the pod @ 07/29/23 16:42:27.424
  STEP: waiting for pod running @ 07/29/23 16:42:27.442
  STEP: creating a file in subpath @ 07/29/23 16:42:29.475
  Jul 29 16:42:29.482: INFO: ExecWithOptions {Command:[/bin/sh -c touch /volume_mount/mypath/foo/test.log] Namespace:var-expansion-3037 PodName:var-expansion-fc57ea07-a450-4615-b8ee-137fbb67ce1a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:42:29.482: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:42:29.484: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:42:29.484: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3037/pods/var-expansion-fc57ea07-a450-4615-b8ee-137fbb67ce1a/exec?command=%2Fbin%2Fsh&command=-c&command=touch+%2Fvolume_mount%2Fmypath%2Ffoo%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: test for file in mounted path @ 07/29/23 16:42:29.587
  Jul 29 16:42:29.598: INFO: ExecWithOptions {Command:[/bin/sh -c test -f /subpath_mount/test.log] Namespace:var-expansion-3037 PodName:var-expansion-fc57ea07-a450-4615-b8ee-137fbb67ce1a ContainerName:dapi-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:42:29.598: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:42:29.600: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:42:29.600: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/var-expansion-3037/pods/var-expansion-fc57ea07-a450-4615-b8ee-137fbb67ce1a/exec?command=%2Fbin%2Fsh&command=-c&command=test+-f+%2Fsubpath_mount%2Ftest.log&container=dapi-container&container=dapi-container&stderr=true&stdout=true)
  STEP: updating the annotation value @ 07/29/23 16:42:29.693
  Jul 29 16:42:30.221: INFO: Successfully updated pod "var-expansion-fc57ea07-a450-4615-b8ee-137fbb67ce1a"
  STEP: waiting for annotated pod running @ 07/29/23 16:42:30.223
  STEP: deleting the pod gracefully @ 07/29/23 16:42:30.231
  Jul 29 16:42:30.232: INFO: Deleting pod "var-expansion-fc57ea07-a450-4615-b8ee-137fbb67ce1a" in namespace "var-expansion-3037"
  Jul 29 16:42:30.245: INFO: Wait up to 5m0s for pod "var-expansion-fc57ea07-a450-4615-b8ee-137fbb67ce1a" to be fully deleted
  Jul 29 16:43:02.398: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-3037" for this suite. @ 07/29/23 16:43:02.409
• [35.044 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should not start app containers if init containers fail on a RestartAlways pod [Conformance]
test/e2e/common/node/init_container.go:334
  STEP: Creating a kubernetes client @ 07/29/23 16:43:02.429
  Jul 29 16:43:02.429: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename init-container @ 07/29/23 16:43:02.431
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:02.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:02.47
  STEP: creating the pod @ 07/29/23 16:43:02.475
  Jul 29 16:43:02.475: INFO: PodSpec: initContainers in spec.initContainers
  Jul 29 16:43:45.365: INFO: init container has failed twice: &v1.Pod{TypeMeta:v1.TypeMeta{Kind:"", APIVersion:""}, ObjectMeta:v1.ObjectMeta{Name:"pod-init-646636cf-95a4-4f90-ac0c-b28d8ec53426", GenerateName:"", Namespace:"init-container-9177", SelfLink:"", UID:"7579d732-806c-45d9-9dec-da7e7dda5bdc", ResourceVersion:"31469", Generation:0, CreationTimestamp:time.Date(2023, time.July, 29, 16, 43, 2, 0, time.Local), DeletionTimestamp:<nil>, DeletionGracePeriodSeconds:(*int64)(nil), Labels:map[string]string{"name":"foo", "time":"475400734"}, Annotations:map[string]string(nil), OwnerReferences:[]v1.OwnerReference(nil), Finalizers:[]string(nil), ManagedFields:[]v1.ManagedFieldsEntry{v1.ManagedFieldsEntry{Manager:"e2e.test", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 16, 43, 2, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc004540570), Subresource:""}, v1.ManagedFieldsEntry{Manager:"kubelet", Operation:"Update", APIVersion:"v1", Time:time.Date(2023, time.July, 29, 16, 43, 45, 0, time.Local), FieldsType:"FieldsV1", FieldsV1:(*v1.FieldsV1)(0xc0045405a0), Subresource:"status"}}}, Spec:v1.PodSpec{Volumes:[]v1.Volume{v1.Volume{Name:"kube-api-access-ghlrx", VolumeSource:v1.VolumeSource{HostPath:(*v1.HostPathVolumeSource)(nil), EmptyDir:(*v1.EmptyDirVolumeSource)(nil), GCEPersistentDisk:(*v1.GCEPersistentDiskVolumeSource)(nil), AWSElasticBlockStore:(*v1.AWSElasticBlockStoreVolumeSource)(nil), GitRepo:(*v1.GitRepoVolumeSource)(nil), Secret:(*v1.SecretVolumeSource)(nil), NFS:(*v1.NFSVolumeSource)(nil), ISCSI:(*v1.ISCSIVolumeSource)(nil), Glusterfs:(*v1.GlusterfsVolumeSource)(nil), PersistentVolumeClaim:(*v1.PersistentVolumeClaimVolumeSource)(nil), RBD:(*v1.RBDVolumeSource)(nil), FlexVolume:(*v1.FlexVolumeSource)(nil), Cinder:(*v1.CinderVolumeSource)(nil), CephFS:(*v1.CephFSVolumeSource)(nil), Flocker:(*v1.FlockerVolumeSource)(nil), DownwardAPI:(*v1.DownwardAPIVolumeSource)(nil), FC:(*v1.FCVolumeSource)(nil), AzureFile:(*v1.AzureFileVolumeSource)(nil), ConfigMap:(*v1.ConfigMapVolumeSource)(nil), VsphereVolume:(*v1.VsphereVirtualDiskVolumeSource)(nil), Quobyte:(*v1.QuobyteVolumeSource)(nil), AzureDisk:(*v1.AzureDiskVolumeSource)(nil), PhotonPersistentDisk:(*v1.PhotonPersistentDiskVolumeSource)(nil), Projected:(*v1.ProjectedVolumeSource)(0xc0053771e0), PortworxVolume:(*v1.PortworxVolumeSource)(nil), ScaleIO:(*v1.ScaleIOVolumeSource)(nil), StorageOS:(*v1.StorageOSVolumeSource)(nil), CSI:(*v1.CSIVolumeSource)(nil), Ephemeral:(*v1.EphemeralVolumeSource)(nil)}}}, InitContainers:[]v1.Container{v1.Container{Name:"init1", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/false"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ghlrx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}, v1.Container{Name:"init2", Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", Command:[]string{"/bin/true"}, Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList(nil), Requests:v1.ResourceList(nil), Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ghlrx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, Containers:[]v1.Container{v1.Container{Name:"run1", Image:"registry.k8s.io/pause:3.9", Command:[]string(nil), Args:[]string(nil), WorkingDir:"", Ports:[]v1.ContainerPort(nil), EnvFrom:[]v1.EnvFromSource(nil), Env:[]v1.EnvVar(nil), Resources:v1.ResourceRequirements{Limits:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Requests:v1.ResourceList{"cpu":resource.Quantity{i:resource.int64Amount{value:100, scale:-3}, d:resource.infDecAmount{Dec:(*inf.Dec)(nil)}, s:"100m", Format:"DecimalSI"}}, Claims:[]v1.ResourceClaim(nil)}, ResizePolicy:[]v1.ContainerResizePolicy(nil), VolumeMounts:[]v1.VolumeMount{v1.VolumeMount{Name:"kube-api-access-ghlrx", ReadOnly:true, MountPath:"/var/run/secrets/kubernetes.io/serviceaccount", SubPath:"", MountPropagation:(*v1.MountPropagationMode)(nil), SubPathExpr:""}}, VolumeDevices:[]v1.VolumeDevice(nil), LivenessProbe:(*v1.Probe)(nil), ReadinessProbe:(*v1.Probe)(nil), StartupProbe:(*v1.Probe)(nil), Lifecycle:(*v1.Lifecycle)(nil), TerminationMessagePath:"/dev/termination-log", TerminationMessagePolicy:"File", ImagePullPolicy:"IfNotPresent", SecurityContext:(*v1.SecurityContext)(nil), Stdin:false, StdinOnce:false, TTY:false}}, EphemeralContainers:[]v1.EphemeralContainer(nil), RestartPolicy:"Always", TerminationGracePeriodSeconds:(*int64)(0xc0049e72f8), ActiveDeadlineSeconds:(*int64)(nil), DNSPolicy:"ClusterFirst", NodeSelector:map[string]string(nil), ServiceAccountName:"default", DeprecatedServiceAccount:"default", AutomountServiceAccountToken:(*bool)(nil), NodeName:"ci9axai7aiv7-3", HostNetwork:false, HostPID:false, HostIPC:false, ShareProcessNamespace:(*bool)(nil), SecurityContext:(*v1.PodSecurityContext)(0xc000477f80), ImagePullSecrets:[]v1.LocalObjectReference(nil), Hostname:"", Subdomain:"", Affinity:(*v1.Affinity)(nil), SchedulerName:"default-scheduler", Tolerations:[]v1.Toleration{v1.Toleration{Key:"node.kubernetes.io/not-ready", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0049e7380)}, v1.Toleration{Key:"node.kubernetes.io/unreachable", Operator:"Exists", Value:"", Effect:"NoExecute", TolerationSeconds:(*int64)(0xc0049e73a0)}}, HostAliases:[]v1.HostAlias(nil), PriorityClassName:"", Priority:(*int32)(0xc0049e73a8), DNSConfig:(*v1.PodDNSConfig)(nil), ReadinessGates:[]v1.PodReadinessGate(nil), RuntimeClassName:(*string)(nil), EnableServiceLinks:(*bool)(0xc0049e73ac), PreemptionPolicy:(*v1.PreemptionPolicy)(0xc00135bf90), Overhead:v1.ResourceList(nil), TopologySpreadConstraints:[]v1.TopologySpreadConstraint(nil), SetHostnameAsFQDN:(*bool)(nil), OS:(*v1.PodOS)(nil), HostUsers:(*bool)(nil), SchedulingGates:[]v1.PodSchedulingGate(nil), ResourceClaims:[]v1.PodResourceClaim(nil)}, Status:v1.PodStatus{Phase:"Pending", Conditions:[]v1.PodCondition{v1.PodCondition{Type:"Initialized", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 43, 2, 0, time.Local), Reason:"ContainersNotInitialized", Message:"containers with incomplete status: [init1 init2]"}, v1.PodCondition{Type:"Ready", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 43, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"ContainersReady", Status:"False", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 43, 2, 0, time.Local), Reason:"ContainersNotReady", Message:"containers with unready status: [run1]"}, v1.PodCondition{Type:"PodScheduled", Status:"True", LastProbeTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(2023, time.July, 29, 16, 43, 2, 0, time.Local), Reason:"", Message:""}}, Message:"", Reason:"", NominatedNodeName:"", HostIP:"192.168.121.144", PodIP:"10.233.66.80", PodIPs:[]v1.PodIP{v1.PodIP{IP:"10.233.66.80"}}, StartTime:time.Date(2023, time.July, 29, 16, 43, 2, 0, time.Local), InitContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"init1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006b4070)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(0xc0006b40e0)}, Ready:false, RestartCount:3, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"registry.k8s.io/e2e-test-images/busybox@sha256:2e0f836850e09b8b7cc937681d6194537a09fbd5f6b9e08f4d646a85128e8937", ContainerID:"cri-o://24648786b860b1e28232e8aaa4ad5785e2a805281cc69ab50982a8794fff39e9", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}, v1.ContainerStatus{Name:"init2", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005377260), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/e2e-test-images/busybox:1.29-4", ImageID:"", ContainerID:"", Started:(*bool)(nil), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, ContainerStatuses:[]v1.ContainerStatus{v1.ContainerStatus{Name:"run1", State:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(0xc005377240), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, LastTerminationState:v1.ContainerState{Waiting:(*v1.ContainerStateWaiting)(nil), Running:(*v1.ContainerStateRunning)(nil), Terminated:(*v1.ContainerStateTerminated)(nil)}, Ready:false, RestartCount:0, Image:"registry.k8s.io/pause:3.9", ImageID:"", ContainerID:"", Started:(*bool)(0xc0049e742f), AllocatedResources:v1.ResourceList(nil), Resources:(*v1.ResourceRequirements)(nil)}}, QOSClass:"Burstable", EphemeralContainerStatuses:[]v1.ContainerStatus(nil), Resize:""}}
  Jul 29 16:43:45.367: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9177" for this suite. @ 07/29/23 16:43:45.374
• [42.965 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling an agnhost Pod with hostAliases should write entries to /etc/hosts [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:148
  STEP: Creating a kubernetes client @ 07/29/23 16:43:45.403
  Jul 29 16:43:45.403: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 16:43:45.406
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:45.428
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:45.433
  STEP: Waiting for pod completion @ 07/29/23 16:43:45.455
  Jul 29 16:43:49.544: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-9941" for this suite. @ 07/29/23 16:43:49.558
• [4.170 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with mappings as non-root [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:109
  STEP: Creating a kubernetes client @ 07/29/23 16:43:49.596
  Jul 29 16:43:49.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:43:49.598
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:49.63
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:49.635
  STEP: Creating configMap with name projected-configmap-test-volume-map-01038b92-afcb-4af5-9139-487c8f52afc5 @ 07/29/23 16:43:49.639
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:43:49.65
  STEP: Saw pod success @ 07/29/23 16:43:53.701
  Jul 29 16:43:53.707: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-configmaps-0307aa2b-9924-417b-9f06-3e4d29b174ad container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:43:53.718
  Jul 29 16:43:53.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9866" for this suite. @ 07/29/23 16:43:53.757
• [4.176 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should be able to restart watching from the last resource version observed by the previous watch [Conformance]
test/e2e/apimachinery/watch.go:191
  STEP: Creating a kubernetes client @ 07/29/23 16:43:53.772
  Jul 29 16:43:53.773: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename watch @ 07/29/23 16:43:53.774
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:53.799
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:53.803
  STEP: creating a watch on configmaps @ 07/29/23 16:43:53.809
  STEP: creating a new configmap @ 07/29/23 16:43:53.811
  STEP: modifying the configmap once @ 07/29/23 16:43:53.817
  STEP: closing the watch once it receives two notifications @ 07/29/23 16:43:53.834
  Jul 29 16:43:53.835: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2510  28ee7b5d-41f4-485a-bf48-5a238d9cbd91 31546 0 2023-07-29 16:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 16:43:53 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:43:53.835: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2510  28ee7b5d-41f4-485a-bf48-5a238d9cbd91 31547 0 2023-07-29 16:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 16:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time, while the watch is closed @ 07/29/23 16:43:53.836
  STEP: creating a new watch on configmaps from the last resource version observed by the first watch @ 07/29/23 16:43:53.856
  STEP: deleting the configmap @ 07/29/23 16:43:53.858
  STEP: Expecting to observe notifications for all changes to the configmap since the first watch closed @ 07/29/23 16:43:53.87
  Jul 29 16:43:53.870: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2510  28ee7b5d-41f4-485a-bf48-5a238d9cbd91 31548 0 2023-07-29 16:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 16:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:43:53.870: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-watch-closed  watch-2510  28ee7b5d-41f4-485a-bf48-5a238d9cbd91 31549 0 2023-07-29 16:43:53 +0000 UTC <nil> <nil> map[watch-this-configmap:watch-closed-and-restarted] map[] [] [] [{e2e.test Update v1 2023-07-29 16:43:53 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 16:43:53.870: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-2510" for this suite. @ 07/29/23 16:43:53.88
• [0.123 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] Garbage collector should delete RS created by deployment when not orphaning [Conformance]
test/e2e/apimachinery/garbage_collector.go:479
  STEP: Creating a kubernetes client @ 07/29/23 16:43:53.897
  Jul 29 16:43:53.898: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename gc @ 07/29/23 16:43:53.9
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:53.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:53.953
  STEP: create the deployment @ 07/29/23 16:43:53.959
  W0729 16:43:53.969487      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/29/23 16:43:53.969
  STEP: delete the deployment @ 07/29/23 16:43:54.511
  STEP: wait for all rs to be garbage collected @ 07/29/23 16:43:54.542
  STEP: expected 0 rs, got 1 rs @ 07/29/23 16:43:54.566
  STEP: expected 0 pods, got 2 pods @ 07/29/23 16:43:54.609
  STEP: Gathering metrics @ 07/29/23 16:43:55.134
  Jul 29 16:43:55.388: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 16:43:55.388: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-8231" for this suite. @ 07/29/23 16:43:55.395
• [1.511 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] CustomResourceDefinition Watch [Privileged:ClusterAdmin] CustomResourceDefinition Watch watch on custom resource definition objects [Conformance]
test/e2e/apimachinery/crd_watch.go:51
  STEP: Creating a kubernetes client @ 07/29/23 16:43:55.409
  Jul 29 16:43:55.409: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-watch @ 07/29/23 16:43:55.411
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:43:55.442
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:43:55.445
  Jul 29 16:43:55.451: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Creating first CR  @ 07/29/23 16:43:58.141
  Jul 29 16:43:58.150: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:43:58Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:43:58Z]] name:name1 resourceVersion:31621 uid:de044609-4878-483d-92b6-21c21676f0e0] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Creating second CR @ 07/29/23 16:44:08.151
  Jul 29 16:44:08.163: INFO: Got : ADDED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:44:08Z generation:1 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:44:08Z]] name:name2 resourceVersion:31672 uid:92293c27-1c99-4f35-9b64-b2969e740942] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying first CR @ 07/29/23 16:44:18.163
  Jul 29 16:44:18.178: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:43:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:44:18Z]] name:name1 resourceVersion:31695 uid:de044609-4878-483d-92b6-21c21676f0e0] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Modifying second CR @ 07/29/23 16:44:28.178
  Jul 29 16:44:28.191: INFO: Got : MODIFIED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:44:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:44:28Z]] name:name2 resourceVersion:31718 uid:92293c27-1c99-4f35-9b64-b2969e740942] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting first CR @ 07/29/23 16:44:38.193
  Jul 29 16:44:38.211: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:43:58Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:44:18Z]] name:name1 resourceVersion:31740 uid:de044609-4878-483d-92b6-21c21676f0e0] num:map[num1:9223372036854775807 num2:1000000]]}
  STEP: Deleting second CR @ 07/29/23 16:44:48.212
  Jul 29 16:44:48.228: INFO: Got : DELETED &{map[apiVersion:mygroup.example.com/v1beta1 content:map[key:value] dummy:test kind:WishIHadChosenNoxu metadata:map[creationTimestamp:2023-07-29T16:44:08Z generation:2 managedFields:[map[apiVersion:mygroup.example.com/v1beta1 fieldsType:FieldsV1 fieldsV1:map[f:content:map[.:map[] f:key:map[]] f:dummy:map[] f:num:map[.:map[] f:num1:map[] f:num2:map[]]] manager:e2e.test operation:Update time:2023-07-29T16:44:28Z]] name:name2 resourceVersion:31763 uid:92293c27-1c99-4f35-9b64-b2969e740942] num:map[num1:9223372036854775807 num2:1000000]]}
  Jul 29 16:44:58.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-watch-4139" for this suite. @ 07/29/23 16:44:58.763
• [63.366 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should verify that a failing subpath expansion can be modified during the lifecycle of a container [Slow] [Conformance]
test/e2e/common/node/expansion.go:228
  STEP: Creating a kubernetes client @ 07/29/23 16:44:58.779
  Jul 29 16:44:58.779: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:44:58.781
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:44:58.804
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:44:58.808
  STEP: creating the pod with failed condition @ 07/29/23 16:44:58.812
  STEP: updating the pod @ 07/29/23 16:46:58.853
  Jul 29 16:46:59.372: INFO: Successfully updated pod "var-expansion-960abff7-4733-4240-8db8-ff8bb70608c2"
  STEP: waiting for pod running @ 07/29/23 16:46:59.372
  STEP: deleting the pod gracefully @ 07/29/23 16:47:01.396
  Jul 29 16:47:01.397: INFO: Deleting pod "var-expansion-960abff7-4733-4240-8db8-ff8bb70608c2" in namespace "var-expansion-9230"
  Jul 29 16:47:01.411: INFO: Wait up to 5m0s for pod "var-expansion-960abff7-4733-4240-8db8-ff8bb70608c2" to be fully deleted
  Jul 29 16:47:33.602: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-9230" for this suite. @ 07/29/23 16:47:33.611
• [154.848 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet Replicaset should have a working scale subresource [Conformance]
test/e2e/apps/replica_set.go:143
  STEP: Creating a kubernetes client @ 07/29/23 16:47:33.631
  Jul 29 16:47:33.631: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 16:47:33.634
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:47:33.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:47:33.665
  STEP: Creating replica set "test-rs" that asks for more than the allowed pod quota @ 07/29/23 16:47:33.67
  Jul 29 16:47:33.683: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 16:47:38.693: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 16:47:38.693
  STEP: getting scale subresource @ 07/29/23 16:47:38.694
  STEP: updating a scale subresource @ 07/29/23 16:47:38.7
  STEP: verifying the replicaset Spec.Replicas was modified @ 07/29/23 16:47:38.71
  STEP: Patch a scale subresource @ 07/29/23 16:47:38.719
  Jul 29 16:47:38.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1905" for this suite. @ 07/29/23 16:47:38.755
• [5.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply changes to a namespace status [Conformance]
test/e2e/apimachinery/namespace.go:303
  STEP: Creating a kubernetes client @ 07/29/23 16:47:38.795
  Jul 29 16:47:38.795: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 16:47:38.797
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:47:38.827
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:47:38.836
  STEP: Read namespace status @ 07/29/23 16:47:38.84
  Jul 29 16:47:38.846: INFO: Status: v1.NamespaceStatus{Phase:"Active", Conditions:[]v1.NamespaceCondition(nil)}
  STEP: Patch namespace status @ 07/29/23 16:47:38.846
  Jul 29 16:47:38.855: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusPatch", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Patched by an e2e test"}
  STEP: Update namespace status @ 07/29/23 16:47:38.855
  Jul 29 16:47:38.873: INFO: Status.Condition: v1.NamespaceCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Updated by an e2e test"}
  Jul 29 16:47:38.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-7069" for this suite. @ 07/29/23 16:47:38.881
• [0.099 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:147
  STEP: Creating a kubernetes client @ 07/29/23 16:47:38.897
  Jul 29 16:47:38.897: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:47:38.899
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:47:38.93
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:47:38.934
  STEP: Creating a pod to test emptydir 0777 on tmpfs @ 07/29/23 16:47:38.939
  STEP: Saw pod success @ 07/29/23 16:47:42.974
  Jul 29 16:47:42.979: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-d1ad6c2b-87f5-46dc-962c-df8dc3e4180e container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:47:43.007
  Jul 29 16:47:43.031: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7268" for this suite. @ 07/29/23 16:47:43.038
• [4.154 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] FieldValidation should detect duplicates in a CR when preserving unknown fields [Conformance]
test/e2e/apimachinery/field_validation.go:622
  STEP: Creating a kubernetes client @ 07/29/23 16:47:43.052
  Jul 29 16:47:43.052: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 16:47:43.054
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:47:43.079
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:47:43.085
  Jul 29 16:47:43.090: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  W0729 16:47:45.805092      13 warnings.go:70] unknown field "alpha"
  W0729 16:47:45.805168      13 warnings.go:70] unknown field "beta"
  W0729 16:47:45.805179      13 warnings.go:70] unknown field "delta"
  W0729 16:47:45.805271      13 warnings.go:70] unknown field "epsilon"
  W0729 16:47:45.805287      13 warnings.go:70] unknown field "gamma"
  Jul 29 16:47:46.387: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7116" for this suite. @ 07/29/23 16:47:46.423
• [3.384 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-storage] Downward API volume should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:85
  STEP: Creating a kubernetes client @ 07/29/23 16:47:46.439
  Jul 29 16:47:46.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:47:46.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:47:46.484
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:47:46.489
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:47:46.493
  STEP: Saw pod success @ 07/29/23 16:47:50.538
  Jul 29 16:47:50.549: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-f6863cab-049f-444b-9c0e-57bcd3ca91ff container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:47:50.561
  Jul 29 16:47:50.591: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-3381" for this suite. @ 07/29/23 16:47:50.602
• [4.174 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should be able to deny custom resource creation, update and deletion [Conformance]
test/e2e/apimachinery/webhook.go:220
  STEP: Creating a kubernetes client @ 07/29/23 16:47:50.617
  Jul 29 16:47:50.617: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:47:50.619
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:47:50.64
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:47:50.644
  STEP: Setting up server cert @ 07/29/23 16:47:50.683
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:47:51.395
  STEP: Deploying the webhook pod @ 07/29/23 16:47:51.41
  STEP: Wait for the deployment to be ready @ 07/29/23 16:47:51.434
  Jul 29 16:47:51.452: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  Jul 29 16:47:53.478: INFO: deployment status: v1.DeploymentStatus{ObservedGeneration:1, Replicas:1, UpdatedReplicas:1, ReadyReplicas:0, AvailableReplicas:0, UnavailableReplicas:1, Conditions:[]v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"False", LastUpdateTime:time.Date(2023, time.July, 29, 16, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 47, 51, 0, time.Local), Reason:"MinimumReplicasUnavailable", Message:"Deployment does not have minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 16, 47, 51, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 16, 47, 51, 0, time.Local), Reason:"ReplicaSetUpdated", Message:"ReplicaSet \"sample-webhook-deployment-7497495989\" is progressing."}}, CollisionCount:(*int32)(nil)}
  STEP: Deploying the webhook service @ 07/29/23 16:47:55.488
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:47:55.51
  Jul 29 16:47:56.511: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 16:47:56.526: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Registering the custom resource webhook via the AdmissionRegistration API @ 07/29/23 16:47:57.052
  STEP: Creating a custom resource that should be denied by the webhook @ 07/29/23 16:47:57.082
  STEP: Creating a custom resource whose deletion would be denied by the webhook @ 07/29/23 16:47:59.201
  STEP: Updating the custom resource with disallowed data should be denied @ 07/29/23 16:47:59.213
  STEP: Deleting the custom resource should be denied @ 07/29/23 16:47:59.231
  STEP: Remove the offending key and value from the custom resource data @ 07/29/23 16:47:59.243
  STEP: Deleting the updated custom resource should be successful @ 07/29/23 16:47:59.259
  Jul 29 16:47:59.287: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-7962" for this suite. @ 07/29/23 16:47:59.927
  STEP: Destroying namespace "webhook-markers-342" for this suite. @ 07/29/23 16:47:59.943
• [9.338 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] StatefulSet Basic StatefulSet functionality [StatefulSetBasic] should validate Statefulset Status endpoints [Conformance]
test/e2e/apps/statefulset.go:981
  STEP: Creating a kubernetes client @ 07/29/23 16:47:59.959
  Jul 29 16:47:59.959: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename statefulset @ 07/29/23 16:47:59.962
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:47:59.994
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:48:00
  STEP: Creating service test in namespace statefulset-5230 @ 07/29/23 16:48:00.007
  STEP: Creating statefulset ss in namespace statefulset-5230 @ 07/29/23 16:48:00.021
  Jul 29 16:48:00.038: INFO: Found 0 stateful pods, waiting for 1
  Jul 29 16:48:10.050: INFO: Waiting for pod ss-0 to enter Running - Ready=true, currently Running - Ready=true
  STEP: Patch Statefulset to include a label @ 07/29/23 16:48:10.061
  STEP: Getting /status @ 07/29/23 16:48:10.077
  Jul 29 16:48:10.086: INFO: StatefulSet ss has Conditions: []v1.StatefulSetCondition(nil)
  STEP: updating the StatefulSet Status @ 07/29/23 16:48:10.086
  Jul 29 16:48:10.107: INFO: updatedStatus.Conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusUpdate", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the statefulset status to be updated @ 07/29/23 16:48:10.107
  Jul 29 16:48:10.120: INFO: Observed &StatefulSet event: ADDED
  Jul 29 16:48:10.120: INFO: Found Statefulset ss in namespace statefulset-5230 with labels: map[e2e:testing] annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 16:48:10.120: INFO: Statefulset ss has an updated status
  STEP: patching the Statefulset Status @ 07/29/23 16:48:10.12
  Jul 29 16:48:10.120: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  Jul 29 16:48:10.138: INFO: Patched status conditions: []v1.StatefulSetCondition{v1.StatefulSetCondition{Type:"StatusPatched", Status:"True", LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Statefulset status to be patched @ 07/29/23 16:48:10.138
  Jul 29 16:48:10.143: INFO: Observed &StatefulSet event: ADDED
  Jul 29 16:48:10.143: INFO: Observed Statefulset ss in namespace statefulset-5230 with annotations: map[] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 16:48:10.143: INFO: Observed &StatefulSet event: MODIFIED
  Jul 29 16:48:10.144: INFO: Deleting all statefulset in ns statefulset-5230
  Jul 29 16:48:10.154: INFO: Scaling statefulset ss to 0
  Jul 29 16:48:20.189: INFO: Waiting for statefulset status.replicas updated to 0
  Jul 29 16:48:20.193: INFO: Deleting statefulset ss
  Jul 29 16:48:20.218: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "statefulset-5230" for this suite. @ 07/29/23 16:48:20.229
• [20.288 seconds]
------------------------------
SS
------------------------------
[sig-storage] Secrets should be immutable if `immutable` field is set [Conformance]
test/e2e/common/storage/secrets_volume.go:386
  STEP: Creating a kubernetes client @ 07/29/23 16:48:20.25
  Jul 29 16:48:20.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:48:20.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:48:20.282
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:48:20.286
  Jul 29 16:48:20.350: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7314" for this suite. @ 07/29/23 16:48:20.359
• [0.124 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:47
  STEP: Creating a kubernetes client @ 07/29/23 16:48:20.375
  Jul 29 16:48:20.375: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:48:20.377
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:48:20.398
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:48:20.402
  STEP: Creating configMap with name projected-configmap-test-volume-6f4a0b69-f9ed-41ab-8cca-3575bb1f0ea5 @ 07/29/23 16:48:20.407
  STEP: Creating a pod to test consume configMaps @ 07/29/23 16:48:20.414
  STEP: Saw pod success @ 07/29/23 16:48:24.454
  Jul 29 16:48:24.460: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-configmaps-8dc921d2-3271-4b79-b54f-413791a9d89f container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 16:48:24.475
  Jul 29 16:48:24.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-644" for this suite. @ 07/29/23 16:48:24.515
• [4.155 seconds]
------------------------------
SSSS
------------------------------
[sig-node] Security Context When creating a container with runAsUser should run the container with uid 65534 [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:347
  STEP: Creating a kubernetes client @ 07/29/23 16:48:24.533
  Jul 29 16:48:24.533: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 16:48:24.536
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:48:24.568
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:48:24.574
  Jul 29 16:48:28.625: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-7609" for this suite. @ 07/29/23 16:48:28.635
• [4.113 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0777,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:217
  STEP: Creating a kubernetes client @ 07/29/23 16:48:28.646
  Jul 29 16:48:28.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:48:28.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:48:28.669
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:48:28.675
  STEP: Creating a pod to test emptydir 0777 on node default medium @ 07/29/23 16:48:28.679
  STEP: Saw pod success @ 07/29/23 16:48:32.712
  Jul 29 16:48:32.716: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-cf857c42-872e-4c7d-b25f-aefacb2e5d29 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:48:32.729
  Jul 29 16:48:32.757: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-905" for this suite. @ 07/29/23 16:48:32.764
• [4.133 seconds]
------------------------------
S
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should have an terminated reason [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:110
  STEP: Creating a kubernetes client @ 07/29/23 16:48:32.779
  Jul 29 16:48:32.780: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 16:48:32.782
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:48:32.803
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:48:32.815
  Jul 29 16:48:36.853: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-8583" for this suite. @ 07/29/23 16:48:36.863
• [4.099 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-scheduling] LimitRange should create a LimitRange with defaults and ensure pod has those defaults applied. [Conformance]
test/e2e/scheduling/limit_range.go:61
  STEP: Creating a kubernetes client @ 07/29/23 16:48:36.879
  Jul 29 16:48:36.879: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename limitrange @ 07/29/23 16:48:36.881
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:48:36.913
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:48:36.92
  STEP: Creating a LimitRange @ 07/29/23 16:48:36.925
  STEP: Setting up watch @ 07/29/23 16:48:36.925
  STEP: Submitting a LimitRange @ 07/29/23 16:48:37.033
  STEP: Verifying LimitRange creation was observed @ 07/29/23 16:48:37.045
  STEP: Fetching the LimitRange to ensure it has proper values @ 07/29/23 16:48:37.045
  Jul 29 16:48:37.049: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 29 16:48:37.050: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with no resource requirements @ 07/29/23 16:48:37.05
  STEP: Ensuring Pod has resource requirements applied from LimitRange @ 07/29/23 16:48:37.058
  Jul 29 16:48:37.071: INFO: Verifying requests: expected map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}] with actual map[cpu:{{100 -3} {<nil>} 100m DecimalSI} ephemeral-storage:{{214748364800 0} {<nil>}  BinarySI} memory:{{209715200 0} {<nil>}  BinarySI}]
  Jul 29 16:48:37.072: INFO: Verifying limits: expected map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{500 -3} {<nil>} 500m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Creating a Pod with partial resource requirements @ 07/29/23 16:48:37.072
  STEP: Ensuring Pod has merged resource requirements applied from LimitRange @ 07/29/23 16:48:37.082
  Jul 29 16:48:37.088: INFO: Verifying requests: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{161061273600 0} {<nil>} 150Gi BinarySI} memory:{{157286400 0} {<nil>} 150Mi BinarySI}]
  Jul 29 16:48:37.088: INFO: Verifying limits: expected map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}] with actual map[cpu:{{300 -3} {<nil>} 300m DecimalSI} ephemeral-storage:{{536870912000 0} {<nil>} 500Gi BinarySI} memory:{{524288000 0} {<nil>} 500Mi BinarySI}]
  STEP: Failing to create a Pod with less than min resources @ 07/29/23 16:48:37.088
  STEP: Failing to create a Pod with more than max resources @ 07/29/23 16:48:37.097
  STEP: Updating a LimitRange @ 07/29/23 16:48:37.105
  STEP: Verifying LimitRange updating is effective @ 07/29/23 16:48:37.122
  STEP: Creating a Pod with less than former min resources @ 07/29/23 16:48:39.128
  STEP: Failing to create a Pod with more than max resources @ 07/29/23 16:48:39.156
  STEP: Deleting a LimitRange @ 07/29/23 16:48:39.163
  STEP: Verifying the LimitRange was deleted @ 07/29/23 16:48:39.19
  Jul 29 16:48:44.204: INFO: limitRange is already deleted
  STEP: Creating a Pod with more than former max resources @ 07/29/23 16:48:44.204
  Jul 29 16:48:44.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "limitrange-1806" for this suite. @ 07/29/23 16:48:44.233
• [7.369 seconds]
------------------------------
SSSSSS
------------------------------
[sig-node] Probing container should *not* be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:214
  STEP: Creating a kubernetes client @ 07/29/23 16:48:44.251
  Jul 29 16:48:44.251: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 16:48:44.254
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:48:44.279
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:48:44.284
  STEP: Creating pod test-webserver-043834db-8f05-482a-8d17-31b21e0db1ab in namespace container-probe-2987 @ 07/29/23 16:48:44.29
  Jul 29 16:48:46.328: INFO: Started pod test-webserver-043834db-8f05-482a-8d17-31b21e0db1ab in namespace container-probe-2987
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 16:48:46.329
  Jul 29 16:48:46.337: INFO: Initial restart count of pod test-webserver-043834db-8f05-482a-8d17-31b21e0db1ab is 0
  Jul 29 16:52:47.507: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 16:52:47.519
  STEP: Destroying namespace "container-probe-2987" for this suite. @ 07/29/23 16:52:47.562
• [243.342 seconds]
------------------------------
SS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with secret pod [Conformance]
test/e2e/storage/subpath.go:60
  STEP: Creating a kubernetes client @ 07/29/23 16:52:47.594
  Jul 29 16:52:47.595: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename subpath @ 07/29/23 16:52:47.607
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:52:47.656
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:52:47.663
  STEP: Setting up data @ 07/29/23 16:52:47.667
  STEP: Creating pod pod-subpath-test-secret-r844 @ 07/29/23 16:52:47.685
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 16:52:47.685
  STEP: Saw pod success @ 07/29/23 16:53:11.825
  Jul 29 16:53:11.832: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-subpath-test-secret-r844 container test-container-subpath-secret-r844: <nil>
  STEP: delete the pod @ 07/29/23 16:53:11.87
  STEP: Deleting pod pod-subpath-test-secret-r844 @ 07/29/23 16:53:11.892
  Jul 29 16:53:11.892: INFO: Deleting pod "pod-subpath-test-secret-r844" in namespace "subpath-9178"
  Jul 29 16:53:11.898: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-9178" for this suite. @ 07/29/23 16:53:11.905
• [24.325 seconds]
------------------------------
SSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:236
  STEP: Creating a kubernetes client @ 07/29/23 16:53:11.923
  Jul 29 16:53:11.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:53:11.926
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:11.952
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:11.957
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:53:11.961
  STEP: Saw pod success @ 07/29/23 16:53:15.998
  Jul 29 16:53:16.007: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-1b1cd24f-f702-46e1-97ba-d6f7416b781d container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:53:16.02
  Jul 29 16:53:16.053: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-6070" for this suite. @ 07/29/23 16:53:16.062
• [4.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's command [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:76
  STEP: Creating a kubernetes client @ 07/29/23 16:53:16.076
  Jul 29 16:53:16.076: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 16:53:16.078
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:16.109
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:16.114
  STEP: Creating a pod to test substitution in container's command @ 07/29/23 16:53:16.12
  STEP: Saw pod success @ 07/29/23 16:53:20.156
  Jul 29 16:53:20.163: INFO: Trying to get logs from node ci9axai7aiv7-3 pod var-expansion-20c9861d-c12a-4f6d-827e-ab603681df7b container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 16:53:20.177
  Jul 29 16:53:20.206: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-4371" for this suite. @ 07/29/23 16:53:20.216
• [4.153 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should observe PodDisruptionBudget status updated [Conformance]
test/e2e/apps/disruption.go:141
  STEP: Creating a kubernetes client @ 07/29/23 16:53:20.246
  Jul 29 16:53:20.246: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename disruption @ 07/29/23 16:53:20.248
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:20.275
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:20.28
  STEP: Waiting for the pdb to be processed @ 07/29/23 16:53:20.3
  STEP: Waiting for all pods to be running @ 07/29/23 16:53:22.364
  Jul 29 16:53:22.379: INFO: running pods: 0 < 3
  Jul 29 16:53:24.400: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-712" for this suite. @ 07/29/23 16:53:24.411
• [4.177 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should create/apply an invalid CR with extra properties for CRD with validation schema [Conformance]
test/e2e/apimachinery/field_validation.go:350
  STEP: Creating a kubernetes client @ 07/29/23 16:53:24.43
  Jul 29 16:53:24.430: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 16:53:24.435
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:24.474
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:24.479
  Jul 29 16:53:24.485: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  W0729 16:53:24.488261      13 field_validation.go:423] props: &JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{spec: {  <nil>  object   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[cronSpec:{  <nil>  string   nil <nil> false <nil> false <nil> <nil> ^(\d+|\*)(/\d+)?(\s+(\d+|\*)(/\d+)?){4}$ <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} foo:{  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []} ports:{  <nil>  array   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] &JSONSchemaPropsOrArray{Schema:&JSONSchemaProps{ID:,Schema:,Ref:nil,Description:,Type:object,Format:,Title:,Default:nil,Maximum:nil,ExclusiveMaximum:false,Minimum:nil,ExclusiveMinimum:false,MaxLength:nil,MinLength:nil,Pattern:,MaxItems:nil,MinItems:nil,UniqueItems:false,MultipleOf:nil,Enum:[]JSON{},MaxProperties:nil,MinProperties:nil,Required:[containerPort protocol],Items:nil,AllOf:[]JSONSchemaProps{},OneOf:[]JSONSchemaProps{},AnyOf:[]JSONSchemaProps{},Not:nil,Properties:map[string]JSONSchemaProps{containerPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostIP: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},hostPort: {  <nil>  integer int32  nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},name: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},protocol: {  <nil>  string   nil <nil> false <nil> false <nil> <nil>  <nil> <nil> false <nil> [] <nil> <nil> [] nil [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},},JSONSchemas:[]JSONSchemaProps{},} [] [] [] nil map[] nil map[] map[] nil map[] nil nil false <nil> false false [containerPort protocol] 0xc005a93800 <nil> []}] nil map[] map[] nil map[] nil nil false <nil> false false [] <nil> <nil> []},},AdditionalProperties:nil,PatternProperties:map[string]JSONSchemaProps{},Dependencies:JSONSchemaDependencies{},AdditionalItems:nil,Definitions:JSONSchemaDefinitions{},ExternalDocs:nil,Example:nil,Nullable:false,XPreserveUnknownFields:nil,XEmbeddedResource:false,XIntOrString:false,XListMapKeys:[],XListType:nil,XMapType:nil,XValidations:[]ValidationRule{},}
  W0729 16:53:27.241819      13 warnings.go:70] unknown field "alpha"
  W0729 16:53:27.241861      13 warnings.go:70] unknown field "beta"
  W0729 16:53:27.241878      13 warnings.go:70] unknown field "delta"
  W0729 16:53:27.241896      13 warnings.go:70] unknown field "epsilon"
  W0729 16:53:27.241923      13 warnings.go:70] unknown field "gamma"
  Jul 29 16:53:27.802: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-7905" for this suite. @ 07/29/23 16:53:27.833
• [3.414 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should run and stop simple daemon [Conformance]
test/e2e/apps/daemon_set.go:177
  STEP: Creating a kubernetes client @ 07/29/23 16:53:27.848
  Jul 29 16:53:27.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 16:53:27.851
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:27.877
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:27.881
  STEP: Creating simple DaemonSet "daemon-set" @ 07/29/23 16:53:27.922
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 16:53:27.93
  Jul 29 16:53:27.953: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:53:27.953: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:53:28.971: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:53:28.972: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:53:29.990: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 16:53:29.990: INFO: Node ci9axai7aiv7-3 is running 0 daemon pod, expected 1
  Jul 29 16:53:30.967: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 16:53:30.967: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Stop a daemon pod, check that the daemon pod is revived. @ 07/29/23 16:53:30.983
  Jul 29 16:53:31.063: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 16:53:31.064: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:53:32.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 16:53:32.086: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:53:33.084: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 16:53:33.084: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 16:53:34.086: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 16:53:34.087: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 16:53:34.095
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6244, will wait for the garbage collector to delete the pods @ 07/29/23 16:53:34.095
  Jul 29 16:53:34.176: INFO: Deleting DaemonSet.extensions daemon-set took: 12.355799ms
  Jul 29 16:53:34.277: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.893214ms
  Jul 29 16:53:36.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 16:53:36.484: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 16:53:36.488: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"33728"},"items":null}

  Jul 29 16:53:36.494: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"33728"},"items":null}

  Jul 29 16:53:36.522: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6244" for this suite. @ 07/29/23 16:53:36.536
• [8.703 seconds]
------------------------------
[sig-network] Networking Granular Checks: Pods should function for intra-pod communication: http [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:84
  STEP: Creating a kubernetes client @ 07/29/23 16:53:36.551
  Jul 29 16:53:36.552: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 16:53:36.556
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:53:36.579
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:53:36.583
  STEP: Performing setup for networking test in namespace pod-network-test-616 @ 07/29/23 16:53:36.587
  STEP: creating a selector @ 07/29/23 16:53:36.587
  STEP: Creating the service pods in kubernetes @ 07/29/23 16:53:36.587
  Jul 29 16:53:36.588: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 16:53:58.828
  Jul 29 16:54:00.859: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 16:54:00.859: INFO: Breadth first check of 10.233.64.248 on host 192.168.121.39...
  Jul 29 16:54:00.864: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.175:9080/dial?request=hostname&protocol=http&host=10.233.64.248&port=8083&tries=1'] Namespace:pod-network-test-616 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:54:00.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:54:00.865: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:54:00.866: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-616/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.175%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.64.248%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:54:01.029: INFO: Waiting for responses: map[]
  Jul 29 16:54:01.029: INFO: reached 10.233.64.248 after 0/1 tries
  Jul 29 16:54:01.029: INFO: Breadth first check of 10.233.65.86 on host 192.168.121.49...
  Jul 29 16:54:01.036: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.175:9080/dial?request=hostname&protocol=http&host=10.233.65.86&port=8083&tries=1'] Namespace:pod-network-test-616 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:54:01.036: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:54:01.038: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:54:01.038: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-616/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.175%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.65.86%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:54:01.162: INFO: Waiting for responses: map[]
  Jul 29 16:54:01.162: INFO: reached 10.233.65.86 after 0/1 tries
  Jul 29 16:54:01.162: INFO: Breadth first check of 10.233.66.42 on host 192.168.121.144...
  Jul 29 16:54:01.170: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s 'http://10.233.66.175:9080/dial?request=hostname&protocol=http&host=10.233.66.42&port=8083&tries=1'] Namespace:pod-network-test-616 PodName:test-container-pod ContainerName:webserver Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 16:54:01.171: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 16:54:01.172: INFO: ExecWithOptions: Clientset creation
  Jul 29 16:54:01.172: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-616/pods/test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+%27http%3A%2F%2F10.233.66.175%3A9080%2Fdial%3Frequest%3Dhostname%26protocol%3Dhttp%26host%3D10.233.66.42%26port%3D8083%26tries%3D1%27&container=webserver&container=webserver&stderr=true&stdout=true)
  Jul 29 16:54:01.282: INFO: Waiting for responses: map[]
  Jul 29 16:54:01.282: INFO: reached 10.233.66.42 after 0/1 tries
  Jul 29 16:54:01.283: INFO: Going to retry 0 out of 3 pods....
  Jul 29 16:54:01.284: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-616" for this suite. @ 07/29/23 16:54:01.301
• [24.760 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's cpu request [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:222
  STEP: Creating a kubernetes client @ 07/29/23 16:54:01.314
  Jul 29 16:54:01.314: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:54:01.317
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:01.351
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:01.355
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:54:01.359
  STEP: Saw pod success @ 07/29/23 16:54:05.408
  Jul 29 16:54:05.413: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-b3c6f9ba-08af-4063-b9d1-e42ebb8b27b6 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:54:05.424
  Jul 29 16:54:05.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7552" for this suite. @ 07/29/23 16:54:05.455
• [4.150 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide container's memory limit [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:208
  STEP: Creating a kubernetes client @ 07/29/23 16:54:05.467
  Jul 29 16:54:05.467: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 16:54:05.469
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:05.495
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:05.499
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:54:05.504
  STEP: Saw pod success @ 07/29/23 16:54:09.555
  Jul 29 16:54:09.561: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-bd1a42d3-01a2-46e4-810d-f31586ce7a47 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:54:09.575
  Jul 29 16:54:09.605: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-2283" for this suite. @ 07/29/23 16:54:09.617
• [4.165 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (root,0644,default) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:167
  STEP: Creating a kubernetes client @ 07/29/23 16:54:09.638
  Jul 29 16:54:09.638: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 16:54:09.64
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:09.665
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:09.669
  STEP: Creating a pod to test emptydir 0644 on node default medium @ 07/29/23 16:54:09.674
  STEP: Saw pod success @ 07/29/23 16:54:13.711
  Jul 29 16:54:13.717: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-0669e58f-7ba7-41e1-bb7e-707511602c49 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 16:54:13.73
  Jul 29 16:54:13.753: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7181" for this suite. @ 07/29/23 16:54:13.76
• [4.132 seconds]
------------------------------
SSS
------------------------------
[sig-apps] ReplicaSet Replace and Patch tests [Conformance]
test/e2e/apps/replica_set.go:154
  STEP: Creating a kubernetes client @ 07/29/23 16:54:13.77
  Jul 29 16:54:13.770: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 16:54:13.772
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:13.796
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:13.8
  Jul 29 16:54:13.829: INFO: Pod name sample-pod: Found 0 pods out of 1
  Jul 29 16:54:18.839: INFO: Pod name sample-pod: Found 1 pods out of 1
  STEP: ensuring each pod is running @ 07/29/23 16:54:18.839
  STEP: Scaling up "test-rs" replicaset  @ 07/29/23 16:54:18.84
  Jul 29 16:54:18.861: INFO: Updating replica set "test-rs"
  STEP: patching the ReplicaSet @ 07/29/23 16:54:18.862
  W0729 16:54:18.903927      13 warnings.go:70] unknown field "spec.template.spec.TerminationGracePeriodSeconds"
  Jul 29 16:54:18.909: INFO: observed ReplicaSet test-rs in namespace replicaset-4383 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 16:54:18.940: INFO: observed ReplicaSet test-rs in namespace replicaset-4383 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 16:54:18.979: INFO: observed ReplicaSet test-rs in namespace replicaset-4383 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 16:54:19.004: INFO: observed ReplicaSet test-rs in namespace replicaset-4383 with ReadyReplicas 1, AvailableReplicas 1
  Jul 29 16:54:21.002: INFO: observed ReplicaSet test-rs in namespace replicaset-4383 with ReadyReplicas 2, AvailableReplicas 2
  Jul 29 16:54:21.623: INFO: observed Replicaset test-rs in namespace replicaset-4383 with ReadyReplicas 3 found true
  Jul 29 16:54:21.623: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-4383" for this suite. @ 07/29/23 16:54:21.631
• [7.871 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl cluster-info should check if Kubernetes control plane services is included in cluster-info  [Conformance]
test/e2e/kubectl/kubectl.go:1315
  STEP: Creating a kubernetes client @ 07/29/23 16:54:21.646
  Jul 29 16:54:21.646: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 16:54:21.648
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:21.672
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:21.679
  STEP: validating cluster-info @ 07/29/23 16:54:21.685
  Jul 29 16:54:21.686: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6417 cluster-info'
  Jul 29 16:54:21.845: INFO: stderr: ""
  Jul 29 16:54:21.845: INFO: stdout: "\x1b[0;32mKubernetes control plane\x1b[0m is running at \x1b[0;33mhttps://10.233.0.1:443\x1b[0m\n\nTo further debug and diagnose cluster problems, use 'kubectl cluster-info dump'.\n"
  Jul 29 16:54:21.845: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6417" for this suite. @ 07/29/23 16:54:21.852
• [0.220 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicaSet should list and delete a collection of ReplicaSets [Conformance]
test/e2e/apps/replica_set.go:165
  STEP: Creating a kubernetes client @ 07/29/23 16:54:21.868
  Jul 29 16:54:21.868: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 16:54:21.871
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:21.896
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:21.9
  STEP: Create a ReplicaSet @ 07/29/23 16:54:21.905
  STEP: Verify that the required pods have come up @ 07/29/23 16:54:21.915
  Jul 29 16:54:21.924: INFO: Pod name sample-pod: Found 0 pods out of 3
  Jul 29 16:54:26.936: INFO: Pod name sample-pod: Found 3 pods out of 3
  STEP: ensuring each pod is running @ 07/29/23 16:54:26.936
  Jul 29 16:54:26.948: INFO: Replica Status: {Replicas:3 FullyLabeledReplicas:3 ReadyReplicas:3 AvailableReplicas:3 ObservedGeneration:1 Conditions:[]}
  STEP: Listing all ReplicaSets @ 07/29/23 16:54:26.949
  STEP: DeleteCollection of the ReplicaSets @ 07/29/23 16:54:26.963
  STEP: After DeleteCollection verify that ReplicaSets have been deleted @ 07/29/23 16:54:27.009
  Jul 29 16:54:27.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-1690" for this suite. @ 07/29/23 16:54:27.09
• [5.270 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's cpu limit [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:194
  STEP: Creating a kubernetes client @ 07/29/23 16:54:27.142
  Jul 29 16:54:27.144: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:54:27.147
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:27.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:27.288
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:54:27.296
  STEP: Saw pod success @ 07/29/23 16:54:31.464
  Jul 29 16:54:31.469: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-03649a2e-7405-4eed-99c3-3ce5cb5383da container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:54:31.485
  Jul 29 16:54:31.515: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6368" for this suite. @ 07/29/23 16:54:31.525
• [4.401 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields at the schema root [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:194
  STEP: Creating a kubernetes client @ 07/29/23 16:54:31.545
  Jul 29 16:54:31.545: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 16:54:31.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:31.592
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:31.598
  Jul 29 16:54:31.604: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 16:54:33.762
  Jul 29 16:54:33.763: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7384 --namespace=crd-publish-openapi-7384 create -f -'
  Jul 29 16:54:35.422: INFO: stderr: ""
  Jul 29 16:54:35.422: INFO: stdout: "e2e-test-crd-publish-openapi-5233-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 29 16:54:35.423: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7384 --namespace=crd-publish-openapi-7384 delete e2e-test-crd-publish-openapi-5233-crds test-cr'
  Jul 29 16:54:35.625: INFO: stderr: ""
  Jul 29 16:54:35.625: INFO: stdout: "e2e-test-crd-publish-openapi-5233-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  Jul 29 16:54:35.625: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7384 --namespace=crd-publish-openapi-7384 apply -f -'
  Jul 29 16:54:36.896: INFO: stderr: ""
  Jul 29 16:54:36.896: INFO: stdout: "e2e-test-crd-publish-openapi-5233-crd.crd-publish-openapi-test-unknown-at-root.example.com/test-cr created\n"
  Jul 29 16:54:36.896: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7384 --namespace=crd-publish-openapi-7384 delete e2e-test-crd-publish-openapi-5233-crds test-cr'
  Jul 29 16:54:37.089: INFO: stderr: ""
  Jul 29 16:54:37.089: INFO: stdout: "e2e-test-crd-publish-openapi-5233-crd.crd-publish-openapi-test-unknown-at-root.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/29/23 16:54:37.089
  Jul 29 16:54:37.089: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7384 explain e2e-test-crd-publish-openapi-5233-crds'
  Jul 29 16:54:37.600: INFO: stderr: ""
  Jul 29 16:54:37.600: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-at-root.example.com\nKIND:       e2e-test-crd-publish-openapi-5233-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties at root for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jul 29 16:54:39.567: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7384" for this suite. @ 07/29/23 16:54:39.609
• [8.078 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:57
  STEP: Creating a kubernetes client @ 07/29/23 16:54:39.628
  Jul 29 16:54:39.628: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:54:39.63
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:39.663
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:39.669
  STEP: Creating secret with name secret-test-31d7ec80-6634-40d5-a418-eada54f177b9 @ 07/29/23 16:54:39.675
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:54:39.688
  STEP: Saw pod success @ 07/29/23 16:54:43.73
  Jul 29 16:54:43.737: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-secrets-06a193d0-86cf-4748-a9d0-0206397c104a container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:54:43.769
  Jul 29 16:54:43.798: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-8123" for this suite. @ 07/29/23 16:54:43.808
• [4.194 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment RecreateDeployment should delete old pods and create new ones [Conformance]
test/e2e/apps/deployment.go:113
  STEP: Creating a kubernetes client @ 07/29/23 16:54:43.828
  Jul 29 16:54:43.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename deployment @ 07/29/23 16:54:43.83
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:43.863
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:43.87
  Jul 29 16:54:43.876: INFO: Creating deployment "test-recreate-deployment"
  Jul 29 16:54:43.889: INFO: Waiting deployment "test-recreate-deployment" to be updated to revision 1
  Jul 29 16:54:43.899: INFO: new replicaset for deployment "test-recreate-deployment" is yet to be created
  Jul 29 16:54:45.923: INFO: Waiting deployment "test-recreate-deployment" to complete
  Jul 29 16:54:45.931: INFO: Triggering a new rollout for deployment "test-recreate-deployment"
  Jul 29 16:54:45.948: INFO: Updating deployment test-recreate-deployment
  Jul 29 16:54:45.948: INFO: Watching deployment "test-recreate-deployment" to verify that new pods will not run with olds pods
  Jul 29 16:54:46.133: INFO: Deployment "test-recreate-deployment":
  &Deployment{ObjectMeta:{test-recreate-deployment  deployment-6823  76836d7b-243f-4e88-8029-7c4179b8ffa7 34435 2 2023-07-29 16:54:43 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 16:54:45 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:54:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00405e498 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:Recreate,RollingUpdate:nil,},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:2,Replicas:1,UpdatedReplicas:1,AvailableReplicas:0,UnavailableReplicas:1,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-29 16:54:46 +0000 UTC,LastTransitionTime:2023-07-29 16:54:46 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "test-recreate-deployment-54757ffd6c" is progressing.,LastUpdateTime:2023-07-29 16:54:46 +0000 UTC,LastTransitionTime:2023-07-29 16:54:43 +0000 UTC,},},ReadyReplicas:0,CollisionCount:nil,},}

  Jul 29 16:54:46.148: INFO: New ReplicaSet "test-recreate-deployment-54757ffd6c" of Deployment "test-recreate-deployment":
  &ReplicaSet{ObjectMeta:{test-recreate-deployment-54757ffd6c  deployment-6823  dfc5b65b-3b12-47d5-aa8c-cdecc27f8f52 34433 1 2023-07-29 16:54:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment test-recreate-deployment 76836d7b-243f-4e88-8029-7c4179b8ffa7 0xc00405e887 0xc00405e888}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:54:46 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76836d7b-243f-4e88-8029-7c4179b8ffa7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:54:46 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 54757ffd6c,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00405e928 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:54:46.148: INFO: All old ReplicaSets of Deployment "test-recreate-deployment":
  Jul 29 16:54:46.148: INFO: &ReplicaSet{ObjectMeta:{test-recreate-deployment-6c99bf8bf6  deployment-6823  31fddc8a-0a1f-4518-ba24-021e286bfbd5 34422 2 2023-07-29 16:54:43 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:1 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-recreate-deployment 76836d7b-243f-4e88-8029-7c4179b8ffa7 0xc00405e9b7 0xc00405e9b8}] [] [{kube-controller-manager Update apps/v1 2023-07-29 16:54:45 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"76836d7b-243f-4e88-8029-7c4179b8ffa7\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"agnhost\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 16:54:46 +0000 UTC FieldsV1 {"f:status":{"f:observedGeneration":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*0,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: sample-pod-3,pod-template-hash: 6c99bf8bf6,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:6c99bf8bf6] map[] [] [] []} {[] [] [{agnhost registry.k8s.io/e2e-test-images/agnhost:2.43 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00405ecf8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:0,FullyLabeledReplicas:0,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 16:54:46.156: INFO: Pod "test-recreate-deployment-54757ffd6c-ndf87" is not available:
  &Pod{ObjectMeta:{test-recreate-deployment-54757ffd6c-ndf87 test-recreate-deployment-54757ffd6c- deployment-6823  ecc0254f-4b62-48dd-9445-26f094bec180 34434 0 2023-07-29 16:54:46 +0000 UTC <nil> <nil> map[name:sample-pod-3 pod-template-hash:54757ffd6c] map[] [{apps/v1 ReplicaSet test-recreate-deployment-54757ffd6c dfc5b65b-3b12-47d5-aa8c-cdecc27f8f52 0xc00405f227 0xc00405f228}] [] [{kube-controller-manager Update v1 2023-07-29 16:54:46 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"dfc5b65b-3b12-47d5-aa8c-cdecc27f8f52\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 16:54:46 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-p2h9d,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-p2h9d,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:54:46 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:54:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:54:46 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 16:54:46 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:,StartTime:2023-07-29 16:54:46 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 16:54:46.157: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-6823" for this suite. @ 07/29/23 16:54:46.166
• [2.356 seconds]
------------------------------
[sig-storage] Projected downwardAPI should set mode on item file [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:85
  STEP: Creating a kubernetes client @ 07/29/23 16:54:46.184
  Jul 29 16:54:46.184: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 16:54:46.187
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:46.226
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:46.231
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 16:54:46.237
  STEP: Saw pod success @ 07/29/23 16:54:50.274
  Jul 29 16:54:50.281: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-58c432fe-d553-4a2d-b716-6f21059831f1 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 16:54:50.295
  Jul 29 16:54:50.321: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-6374" for this suite. @ 07/29/23 16:54:50.332
• [4.159 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Runtime blackbox test on terminated container should report termination message as empty when pod succeeds and TerminationMessagePolicy FallbackToLogsOnError is set [NodeConformance] [Conformance]
test/e2e/common/node/runtime.go:232
  STEP: Creating a kubernetes client @ 07/29/23 16:54:50.345
  Jul 29 16:54:50.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-runtime @ 07/29/23 16:54:50.346
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:50.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:50.38
  STEP: create the container @ 07/29/23 16:54:50.389
  W0729 16:54:50.403580      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: wait for the container to reach Succeeded @ 07/29/23 16:54:50.404
  STEP: get the container status @ 07/29/23 16:54:53.435
  STEP: the container should be terminated @ 07/29/23 16:54:53.443
  STEP: the termination message should be set @ 07/29/23 16:54:53.443
  Jul 29 16:54:53.443: INFO: Expected: &{} to match Container's Termination Message:  --
  STEP: delete the container @ 07/29/23 16:54:53.443
  Jul 29 16:54:53.467: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-runtime-4330" for this suite. @ 07/29/23 16:54:53.487
• [3.152 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] listing validating webhooks should work [Conformance]
test/e2e/apimachinery/webhook.go:571
  STEP: Creating a kubernetes client @ 07/29/23 16:54:53.499
  Jul 29 16:54:53.499: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 16:54:53.503
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:53.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:53.546
  STEP: Setting up server cert @ 07/29/23 16:54:53.586
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 16:54:54.654
  STEP: Deploying the webhook pod @ 07/29/23 16:54:54.668
  STEP: Wait for the deployment to be ready @ 07/29/23 16:54:54.687
  Jul 29 16:54:54.701: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 16:54:56.723
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 16:54:56.74
  Jul 29 16:54:57.740: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Listing all of the created validation webhooks @ 07/29/23 16:54:57.838
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 16:54:57.906
  STEP: Deleting the collection of validation webhooks @ 07/29/23 16:54:57.957
  STEP: Creating a configMap that does not comply to the validation webhook rules @ 07/29/23 16:54:58.056
  Jul 29 16:54:58.075: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8414" for this suite. @ 07/29/23 16:54:58.193
  STEP: Destroying namespace "webhook-markers-9831" for this suite. @ 07/29/23 16:54:58.215
• [4.731 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:47
  STEP: Creating a kubernetes client @ 07/29/23 16:54:58.234
  Jul 29 16:54:58.234: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 16:54:58.236
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:54:58.266
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:54:58.273
  STEP: Creating secret with name secret-test-6d17b162-3b85-420e-9035-86e86e062b04 @ 07/29/23 16:54:58.28
  STEP: Creating a pod to test consume secrets @ 07/29/23 16:54:58.288
  STEP: Saw pod success @ 07/29/23 16:55:02.337
  Jul 29 16:55:02.345: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-secrets-f21895a1-90c2-4e9e-8f0d-683973f9afc7 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 16:55:02.371
  Jul 29 16:55:02.396: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-1010" for this suite. @ 07/29/23 16:55:02.408
• [4.189 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates that there exists conflict between pods with same hostPort and protocol but one using 0.0.0.0 hostIP [Conformance]
test/e2e/scheduling/predicates.go:705
  STEP: Creating a kubernetes client @ 07/29/23 16:55:02.424
  Jul 29 16:55:02.424: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 16:55:02.427
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 16:55:02.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 16:55:02.532
  Jul 29 16:55:02.540: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 16:55:02.557: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 16:55:02.563: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-1 before test
  Jul 29 16:55:02.580: INFO: cilium-6928s from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.580: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:55:02.581: INFO: cilium-node-init-m4qsv from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.581: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:55:02.582: INFO: coredns-5d78c9869d-xxmxx from kube-system started at 2023-07-29 15:24:09 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.582: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:55:02.582: INFO: kube-addon-manager-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.583: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:55:02.583: INFO: kube-apiserver-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.584: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:55:02.584: INFO: kube-controller-manager-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.584: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:55:02.585: INFO: kube-proxy-dscp7 from kube-system started at 2023-07-29 15:11:26 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.585: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:55:02.585: INFO: kube-scheduler-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.586: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:55:02.586: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-7bqrz from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:55:02.586: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:55:02.587: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:55:02.587: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-2 before test
  Jul 29 16:55:02.603: INFO: cilium-node-init-c52xz from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: cilium-operator-64cdf5fc9d-9t2m7 from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container cilium-operator ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: cilium-pr7vs from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: coredns-5d78c9869d-7fz8w from kube-system started at 2023-07-29 15:24:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: kube-addon-manager-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: kube-apiserver-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: kube-controller-manager-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: kube-proxy-bgt8j from kube-system started at 2023-07-29 15:11:44 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: kube-scheduler-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-568gm from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:55:02.603: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 16:55:02.603: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-3 before test
  Jul 29 16:55:02.620: INFO: cilium-jsbkd from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.621: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 16:55:02.621: INFO: cilium-node-init-fs2dr from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.622: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 16:55:02.623: INFO: kube-proxy-bhtmh from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.623: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 16:55:02.624: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:29:07 +0000 UTC (1 container statuses recorded)
  Jul 29 16:55:02.624: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 16:55:02.624: INFO: sonobuoy-e2e-job-9fe8eb0b75ee4e08 from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:55:02.625: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 16:55:02.625: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:55:02.626: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-92k89 from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 16:55:02.626: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 16:55:02.626: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: Trying to launch a pod without a label to get a node which can launch it. @ 07/29/23 16:55:02.627
  STEP: Explicitly delete pod here to free the resource it takes. @ 07/29/23 16:55:04.665
  STEP: Trying to apply a random label on the found node. @ 07/29/23 16:55:04.693
  STEP: verifying the node has the label kubernetes.io/e2e-afbd042a-e1ef-4b18-aafb-a20c4660726b 95 @ 07/29/23 16:55:04.71
  STEP: Trying to create a pod(pod4) with hostport 54322 and hostIP 0.0.0.0(empty string here) and expect scheduled @ 07/29/23 16:55:04.718
  STEP: Trying to create another pod(pod5) with hostport 54322 but hostIP 192.168.121.144 on the node which pod4 resides and expect not scheduled @ 07/29/23 16:55:06.768
  STEP: removing the label kubernetes.io/e2e-afbd042a-e1ef-4b18-aafb-a20c4660726b off the node ci9axai7aiv7-3 @ 07/29/23 17:00:06.788
  STEP: verifying the node doesn't have the label kubernetes.io/e2e-afbd042a-e1ef-4b18-aafb-a20c4660726b @ 07/29/23 17:00:06.816
  Jul 29 17:00:06.830: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-438" for this suite. @ 07/29/23 17:00:06.84
• [304.427 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with terminating scopes. [Conformance]
test/e2e/apimachinery/resource_quota.go:693
  STEP: Creating a kubernetes client @ 07/29/23 17:00:06.863
  Jul 29 17:00:06.864: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:00:06.866
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:00:06.894
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:00:06.899
  STEP: Creating a ResourceQuota with terminating scope @ 07/29/23 17:00:06.904
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 17:00:06.913
  STEP: Creating a ResourceQuota with not terminating scope @ 07/29/23 17:00:08.92
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 17:00:08.93
  STEP: Creating a long running pod @ 07/29/23 17:00:10.94
  STEP: Ensuring resource quota with not terminating scope captures the pod usage @ 07/29/23 17:00:10.981
  STEP: Ensuring resource quota with terminating scope ignored the pod usage @ 07/29/23 17:00:12.989
  STEP: Deleting the pod @ 07/29/23 17:00:14.998
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 17:00:15.024
  STEP: Creating a terminating pod @ 07/29/23 17:00:17.036
  STEP: Ensuring resource quota with terminating scope captures the pod usage @ 07/29/23 17:00:17.069
  STEP: Ensuring resource quota with not terminating scope ignored the pod usage @ 07/29/23 17:00:19.083
  STEP: Deleting the pod @ 07/29/23 17:00:21.092
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 17:00:21.112
  Jul 29 17:00:23.119: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-7976" for this suite. @ 07/29/23 17:00:23.13
• [16.292 seconds]
------------------------------
S
------------------------------
[sig-node] Probing container should have monotonically increasing restart count [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:198
  STEP: Creating a kubernetes client @ 07/29/23 17:00:23.156
  Jul 29 17:00:23.156: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 17:00:23.157
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:00:23.231
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:00:23.237
  STEP: Creating pod liveness-a84577f9-b09d-475c-8d76-fb57b469faed in namespace container-probe-2559 @ 07/29/23 17:00:23.243
  Jul 29 17:00:25.316: INFO: Started pod liveness-a84577f9-b09d-475c-8d76-fb57b469faed in namespace container-probe-2559
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 17:00:25.316
  Jul 29 17:00:25.324: INFO: Initial restart count of pod liveness-a84577f9-b09d-475c-8d76-fb57b469faed is 0
  Jul 29 17:00:45.426: INFO: Restart count of pod container-probe-2559/liveness-a84577f9-b09d-475c-8d76-fb57b469faed is now 1 (20.100979414s elapsed)
  Jul 29 17:01:05.546: INFO: Restart count of pod container-probe-2559/liveness-a84577f9-b09d-475c-8d76-fb57b469faed is now 2 (40.221036102s elapsed)
  Jul 29 17:01:25.692: INFO: Restart count of pod container-probe-2559/liveness-a84577f9-b09d-475c-8d76-fb57b469faed is now 3 (1m0.367690928s elapsed)
  Jul 29 17:01:45.786: INFO: Restart count of pod container-probe-2559/liveness-a84577f9-b09d-475c-8d76-fb57b469faed is now 4 (1m20.460850542s elapsed)
  Jul 29 17:02:58.186: INFO: Restart count of pod container-probe-2559/liveness-a84577f9-b09d-475c-8d76-fb57b469faed is now 5 (2m32.861452466s elapsed)
  Jul 29 17:02:58.187: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 17:02:58.196
  STEP: Destroying namespace "container-probe-2559" for this suite. @ 07/29/23 17:02:58.218
• [155.095 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable via the environment [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:95
  STEP: Creating a kubernetes client @ 07/29/23 17:02:58.255
  Jul 29 17:02:58.255: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 17:02:58.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:02:58.301
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:02:58.305
  STEP: creating secret secrets-9881/secret-test-4a4dcfa4-8873-4333-a529-07b0418ada36 @ 07/29/23 17:02:58.309
  STEP: Creating a pod to test consume secrets @ 07/29/23 17:02:58.318
  STEP: Saw pod success @ 07/29/23 17:03:02.363
  Jul 29 17:03:02.370: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-29828a0e-ab52-4d73-85d4-d9752669bbf6 container env-test: <nil>
  STEP: delete the pod @ 07/29/23 17:03:02.418
  Jul 29 17:03:02.445: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-9881" for this suite. @ 07/29/23 17:03:02.455
• [4.229 seconds]
------------------------------
[sig-network] Services should delete a collection of services [Conformance]
test/e2e/network/service.go:3548
  STEP: Creating a kubernetes client @ 07/29/23 17:03:02.484
  Jul 29 17:03:02.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 17:03:02.486
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:02.543
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:02.571
  STEP: creating a collection of services @ 07/29/23 17:03:02.584
  Jul 29 17:03:02.585: INFO: Creating e2e-svc-a-glsgz
  Jul 29 17:03:02.615: INFO: Creating e2e-svc-b-h5qmq
  Jul 29 17:03:02.656: INFO: Creating e2e-svc-c-m8mvm
  STEP: deleting service collection @ 07/29/23 17:03:02.701
  Jul 29 17:03:02.769: INFO: Collection of services has been deleted
  Jul 29 17:03:02.769: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-3585" for this suite. @ 07/29/23 17:03:02.779
• [0.310 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Update Demo should scale a replication controller  [Conformance]
test/e2e/kubectl/kubectl.go:354
  STEP: Creating a kubernetes client @ 07/29/23 17:03:02.799
  Jul 29 17:03:02.799: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:03:02.801
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:02.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:02.836
  STEP: creating a replication controller @ 07/29/23 17:03:02.841
  Jul 29 17:03:02.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 create -f -'
  Jul 29 17:03:03.733: INFO: stderr: ""
  Jul 29 17:03:03.733: INFO: stdout: "replicationcontroller/update-demo-nautilus created\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 17:03:03.733
  Jul 29 17:03:03.734: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 17:03:03.934: INFO: stderr: ""
  Jul 29 17:03:03.934: INFO: stdout: "update-demo-nautilus-f4chh update-demo-nautilus-gzbhz "
  Jul 29 17:03:03.934: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-f4chh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:03:04.101: INFO: stderr: ""
  Jul 29 17:03:04.101: INFO: stdout: ""
  Jul 29 17:03:04.101: INFO: update-demo-nautilus-f4chh is created but not running
  Jul 29 17:03:09.102: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 17:03:09.248: INFO: stderr: ""
  Jul 29 17:03:09.248: INFO: stdout: "update-demo-nautilus-f4chh update-demo-nautilus-gzbhz "
  Jul 29 17:03:09.249: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-f4chh -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:03:09.388: INFO: stderr: ""
  Jul 29 17:03:09.388: INFO: stdout: "true"
  Jul 29 17:03:09.389: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-f4chh -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 17:03:09.526: INFO: stderr: ""
  Jul 29 17:03:09.526: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 17:03:09.526: INFO: validating pod update-demo-nautilus-f4chh
  Jul 29 17:03:09.549: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 17:03:09.550: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 17:03:09.550: INFO: update-demo-nautilus-f4chh is verified up and running
  Jul 29 17:03:09.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-gzbhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:03:09.699: INFO: stderr: ""
  Jul 29 17:03:09.699: INFO: stdout: "true"
  Jul 29 17:03:09.699: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-gzbhz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 17:03:09.829: INFO: stderr: ""
  Jul 29 17:03:09.829: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 17:03:09.829: INFO: validating pod update-demo-nautilus-gzbhz
  Jul 29 17:03:09.844: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 17:03:09.844: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 17:03:09.844: INFO: update-demo-nautilus-gzbhz is verified up and running
  STEP: scaling down the replication controller @ 07/29/23 17:03:09.844
  Jul 29 17:03:09.859: INFO: scanned /root for discovery docs: <nil>
  Jul 29 17:03:09.859: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 scale rc update-demo-nautilus --replicas=1 --timeout=5m'
  Jul 29 17:03:11.053: INFO: stderr: ""
  Jul 29 17:03:11.054: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 17:03:11.054
  Jul 29 17:03:11.054: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 17:03:11.184: INFO: stderr: ""
  Jul 29 17:03:11.184: INFO: stdout: "update-demo-nautilus-gzbhz "
  Jul 29 17:03:11.184: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-gzbhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:03:11.326: INFO: stderr: ""
  Jul 29 17:03:11.326: INFO: stdout: "true"
  Jul 29 17:03:11.326: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-gzbhz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 17:03:11.476: INFO: stderr: ""
  Jul 29 17:03:11.476: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 17:03:11.476: INFO: validating pod update-demo-nautilus-gzbhz
  Jul 29 17:03:11.490: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 17:03:11.491: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 17:03:11.491: INFO: update-demo-nautilus-gzbhz is verified up and running
  STEP: scaling up the replication controller @ 07/29/23 17:03:11.491
  Jul 29 17:03:11.501: INFO: scanned /root for discovery docs: <nil>
  Jul 29 17:03:11.501: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 scale rc update-demo-nautilus --replicas=2 --timeout=5m'
  Jul 29 17:03:12.699: INFO: stderr: ""
  Jul 29 17:03:12.699: INFO: stdout: "replicationcontroller/update-demo-nautilus scaled\n"
  STEP: waiting for all containers in name=update-demo pods to come up. @ 07/29/23 17:03:12.699
  Jul 29 17:03:12.700: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods -o template --template={{range.items}}{{.metadata.name}} {{end}} -l name=update-demo'
  Jul 29 17:03:12.861: INFO: stderr: ""
  Jul 29 17:03:12.861: INFO: stdout: "update-demo-nautilus-6x2nl update-demo-nautilus-gzbhz "
  Jul 29 17:03:12.861: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-6x2nl -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:03:13.042: INFO: stderr: ""
  Jul 29 17:03:13.042: INFO: stdout: "true"
  Jul 29 17:03:13.043: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-6x2nl -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 17:03:13.164: INFO: stderr: ""
  Jul 29 17:03:13.164: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 17:03:13.164: INFO: validating pod update-demo-nautilus-6x2nl
  Jul 29 17:03:13.175: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 17:03:13.175: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 17:03:13.175: INFO: update-demo-nautilus-6x2nl is verified up and running
  Jul 29 17:03:13.175: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-gzbhz -o template --template={{if (exists . "status" "containerStatuses")}}{{range .status.containerStatuses}}{{if (and (eq .name "update-demo") (exists . "state" "running"))}}true{{end}}{{end}}{{end}}'
  Jul 29 17:03:13.308: INFO: stderr: ""
  Jul 29 17:03:13.308: INFO: stdout: "true"
  Jul 29 17:03:13.308: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods update-demo-nautilus-gzbhz -o template --template={{if (exists . "spec" "containers")}}{{range .spec.containers}}{{if eq .name "update-demo"}}{{.image}}{{end}}{{end}}{{end}}'
  Jul 29 17:03:13.447: INFO: stderr: ""
  Jul 29 17:03:13.447: INFO: stdout: "registry.k8s.io/e2e-test-images/nautilus:1.7"
  Jul 29 17:03:13.447: INFO: validating pod update-demo-nautilus-gzbhz
  Jul 29 17:03:13.459: INFO: got data: {
    "image": "nautilus.jpg"
  }

  Jul 29 17:03:13.459: INFO: Unmarshalled json jpg/img => {nautilus.jpg} , expecting nautilus.jpg .
  Jul 29 17:03:13.459: INFO: update-demo-nautilus-gzbhz is verified up and running
  STEP: using delete to clean up resources @ 07/29/23 17:03:13.459
  Jul 29 17:03:13.459: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 delete --grace-period=0 --force -f -'
  Jul 29 17:03:13.614: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 17:03:13.614: INFO: stdout: "replicationcontroller \"update-demo-nautilus\" force deleted\n"
  Jul 29 17:03:13.615: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get rc,svc -l name=update-demo --no-headers'
  Jul 29 17:03:13.846: INFO: stderr: "No resources found in kubectl-460 namespace.\n"
  Jul 29 17:03:13.846: INFO: stdout: ""
  Jul 29 17:03:13.847: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-460 get pods -l name=update-demo -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 17:03:14.046: INFO: stderr: ""
  Jul 29 17:03:14.046: INFO: stdout: ""
  Jul 29 17:03:14.046: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-460" for this suite. @ 07/29/23 17:03:14.066
• [11.286 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:47
  STEP: Creating a kubernetes client @ 07/29/23 17:03:14.085
  Jul 29 17:03:14.085: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:03:14.087
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:14.14
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:14.156
  STEP: Creating configMap with name configmap-test-volume-fb7343bc-ffcc-4939-8dd2-94919cd97423 @ 07/29/23 17:03:14.171
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:03:14.178
  STEP: Saw pod success @ 07/29/23 17:03:18.249
  Jul 29 17:03:18.256: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-286ece46-2600-44d6-bc4d-9b42a7b8d6ec container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 17:03:18.27
  Jul 29 17:03:18.292: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7408" for this suite. @ 07/29/23 17:03:18.3
• [4.223 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:57
  STEP: Creating a kubernetes client @ 07/29/23 17:03:18.314
  Jul 29 17:03:18.315: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:03:18.318
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:18.344
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:18.349
  STEP: Creating configMap with name configmap-test-volume-1d46c7f7-b8a2-4a76-8859-494994c50b37 @ 07/29/23 17:03:18.353
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:03:18.361
  STEP: Saw pod success @ 07/29/23 17:03:22.407
  Jul 29 17:03:22.420: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-495ac920-272d-4819-b824-1e9762af2aad container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 17:03:22.436
  Jul 29 17:03:22.464: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7106" for this suite. @ 07/29/23 17:03:22.476
• [4.172 seconds]
------------------------------
SSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:262
  STEP: Creating a kubernetes client @ 07/29/23 17:03:22.488
  Jul 29 17:03:22.488: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:03:22.491
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:22.524
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:22.53
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:03:22.537
  STEP: Saw pod success @ 07/29/23 17:03:26.586
  Jul 29 17:03:26.594: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-23a1ab02-f9be-43f2-8a94-d613f32d9777 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:03:26.608
  Jul 29 17:03:26.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2751" for this suite. @ 07/29/23 17:03:26.647
• [4.172 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-node] Pods should run through the lifecycle of Pods and PodStatus [Conformance]
test/e2e/common/node/pods.go:897
  STEP: Creating a kubernetes client @ 07/29/23 17:03:26.664
  Jul 29 17:03:26.665: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:03:26.667
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:26.693
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:26.699
  STEP: creating a Pod with a static label @ 07/29/23 17:03:26.717
  STEP: watching for Pod to be ready @ 07/29/23 17:03:26.737
  Jul 29 17:03:26.741: INFO: observed Pod pod-test in namespace pods-3570 in phase Pending with labels: map[test-pod-static:true] & conditions []
  Jul 29 17:03:26.752: INFO: observed Pod pod-test in namespace pods-3570 in phase Pending with labels: map[test-pod-static:true] & conditions [{PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:26 +0000 UTC  }]
  Jul 29 17:03:26.776: INFO: observed Pod pod-test in namespace pods-3570 in phase Pending with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:26 +0000 UTC  } {Ready False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {ContainersReady False 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:26 +0000 UTC ContainersNotReady containers with unready status: [pod-test]} {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:26 +0000 UTC  }]
  Jul 29 17:03:28.681: INFO: Found Pod pod-test in namespace pods-3570 in phase Running with labels: map[test-pod-static:true] & conditions [{Initialized True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:26 +0000 UTC  } {Ready True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:28 +0000 UTC  } {ContainersReady True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:28 +0000 UTC  } {PodScheduled True 0001-01-01 00:00:00 +0000 UTC 2023-07-29 17:03:26 +0000 UTC  }]
  STEP: patching the Pod with a new Label and updated data @ 07/29/23 17:03:28.687
  STEP: getting the Pod and ensuring that it's patched @ 07/29/23 17:03:28.707
  STEP: replacing the Pod's status Ready condition to False @ 07/29/23 17:03:28.722
  STEP: check the Pod again to ensure its Ready conditions are False @ 07/29/23 17:03:28.744
  STEP: deleting the Pod via a Collection with a LabelSelector @ 07/29/23 17:03:28.745
  STEP: watching for the Pod to be deleted @ 07/29/23 17:03:28.766
  Jul 29 17:03:28.771: INFO: observed event type MODIFIED
  Jul 29 17:03:30.719: INFO: observed event type MODIFIED
  Jul 29 17:03:30.880: INFO: observed event type MODIFIED
  Jul 29 17:03:31.706: INFO: observed event type MODIFIED
  Jul 29 17:03:31.732: INFO: observed event type MODIFIED
  Jul 29 17:03:31.751: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-3570" for this suite. @ 07/29/23 17:03:31.763
• [5.112 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] ReplicationController should get and update a ReplicationController scale [Conformance]
test/e2e/apps/rc.go:424
  STEP: Creating a kubernetes client @ 07/29/23 17:03:31.787
  Jul 29 17:03:31.787: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 17:03:31.791
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:31.836
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:31.841
  STEP: Creating ReplicationController "e2e-rc-849lb" @ 07/29/23 17:03:31.846
  Jul 29 17:03:31.859: INFO: Get Replication Controller "e2e-rc-849lb" to confirm replicas
  Jul 29 17:03:32.869: INFO: Get Replication Controller "e2e-rc-849lb" to confirm replicas
  Jul 29 17:03:32.878: INFO: Found 1 replicas for "e2e-rc-849lb" replication controller
  STEP: Getting scale subresource for ReplicationController "e2e-rc-849lb" @ 07/29/23 17:03:32.878
  STEP: Updating a scale subresource @ 07/29/23 17:03:32.885
  STEP: Verifying replicas where modified for replication controller "e2e-rc-849lb" @ 07/29/23 17:03:32.897
  Jul 29 17:03:32.897: INFO: Get Replication Controller "e2e-rc-849lb" to confirm replicas
  Jul 29 17:03:33.908: INFO: Get Replication Controller "e2e-rc-849lb" to confirm replicas
  Jul 29 17:03:33.915: INFO: Found 2 replicas for "e2e-rc-849lb" replication controller
  Jul 29 17:03:33.915: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-8749" for this suite. @ 07/29/23 17:03:33.923
• [2.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] PodTemplates should delete a collection of pod templates [Conformance]
test/e2e/common/node/podtemplates.go:122
  STEP: Creating a kubernetes client @ 07/29/23 17:03:33.94
  Jul 29 17:03:33.940: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename podtemplate @ 07/29/23 17:03:33.943
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:33.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:33.973
  STEP: Create set of pod templates @ 07/29/23 17:03:33.977
  Jul 29 17:03:33.986: INFO: created test-podtemplate-1
  Jul 29 17:03:33.997: INFO: created test-podtemplate-2
  Jul 29 17:03:34.005: INFO: created test-podtemplate-3
  STEP: get a list of pod templates with a label in the current namespace @ 07/29/23 17:03:34.005
  STEP: delete collection of pod templates @ 07/29/23 17:03:34.014
  Jul 29 17:03:34.015: INFO: requesting DeleteCollection of pod templates
  STEP: check that the list of pod templates matches the requested quantity @ 07/29/23 17:03:34.043
  Jul 29 17:03:34.044: INFO: requesting list of pod templates to confirm quantity
  Jul 29 17:03:34.049: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "podtemplate-7962" for this suite. @ 07/29/23 17:03:34.059
• [0.132 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Subpath Atomic writer volumes should support subpaths with downward pod [Conformance]
test/e2e/storage/subpath.go:92
  STEP: Creating a kubernetes client @ 07/29/23 17:03:34.079
  Jul 29 17:03:34.080: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename subpath @ 07/29/23 17:03:34.082
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:34.123
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:34.127
  STEP: Setting up data @ 07/29/23 17:03:34.131
  STEP: Creating pod pod-subpath-test-downwardapi-bjhl @ 07/29/23 17:03:34.15
  STEP: Creating a pod to test atomic-volume-subpath @ 07/29/23 17:03:34.15
  STEP: Saw pod success @ 07/29/23 17:03:58.342
  Jul 29 17:03:58.355: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-subpath-test-downwardapi-bjhl container test-container-subpath-downwardapi-bjhl: <nil>
  STEP: delete the pod @ 07/29/23 17:03:58.377
  STEP: Deleting pod pod-subpath-test-downwardapi-bjhl @ 07/29/23 17:03:58.407
  Jul 29 17:03:58.408: INFO: Deleting pod "pod-subpath-test-downwardapi-bjhl" in namespace "subpath-4960"
  Jul 29 17:03:58.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "subpath-4960" for this suite. @ 07/29/23 17:03:58.427
• [24.359 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should resolve DNS of partial qualified names for services [LinuxOnly] [Conformance]
test/e2e/network/dns.go:191
  STEP: Creating a kubernetes client @ 07/29/23 17:03:58.44
  Jul 29 17:03:58.440: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename dns @ 07/29/23 17:03:58.444
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:03:58.471
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:03:58.475
  STEP: Creating a test headless service @ 07/29/23 17:03:58.479
  STEP: Running these commands on wheezy: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7047 A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7047;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7047 A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7047;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7047.svc A)" && test -n "$$check" && echo OK > /results/wheezy_udp@dns-test-service.dns-7047.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7047.svc A)" && test -n "$$check" && echo OK > /results/wheezy_tcp@dns-test-service.dns-7047.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7047.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7047.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7047.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_udp@_http._tcp.test-service-2.dns-7047.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7047.svc SRV)" && test -n "$$check" && echo OK > /results/wheezy_tcp@_http._tcp.test-service-2.dns-7047.svc;check="$$(dig +notcp +noall +answer +search 84.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.84_udp@PTR;check="$$(dig +tcp +noall +answer +search 84.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.84_tcp@PTR;sleep 1; done
   @ 07/29/23 17:03:58.514
  STEP: Running these commands on jessie: for i in `seq 1 600`; do check="$$(dig +notcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service;check="$$(dig +tcp +noall +answer +search dns-test-service A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7047 A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7047;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7047 A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7047;check="$$(dig +notcp +noall +answer +search dns-test-service.dns-7047.svc A)" && test -n "$$check" && echo OK > /results/jessie_udp@dns-test-service.dns-7047.svc;check="$$(dig +tcp +noall +answer +search dns-test-service.dns-7047.svc A)" && test -n "$$check" && echo OK > /results/jessie_tcp@dns-test-service.dns-7047.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.dns-test-service.dns-7047.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.dns-test-service.dns-7047.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.dns-test-service.dns-7047.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc;check="$$(dig +notcp +noall +answer +search _http._tcp.test-service-2.dns-7047.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_udp@_http._tcp.test-service-2.dns-7047.svc;check="$$(dig +tcp +noall +answer +search _http._tcp.test-service-2.dns-7047.svc SRV)" && test -n "$$check" && echo OK > /results/jessie_tcp@_http._tcp.test-service-2.dns-7047.svc;check="$$(dig +notcp +noall +answer +search 84.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.84_udp@PTR;check="$$(dig +tcp +noall +answer +search 84.39.233.10.in-addr.arpa. PTR)" && test -n "$$check" && echo OK > /results/10.233.39.84_tcp@PTR;sleep 1; done
   @ 07/29/23 17:03:58.514
  STEP: creating a pod to probe DNS @ 07/29/23 17:03:58.514
  STEP: submitting the pod to kubernetes @ 07/29/23 17:03:58.515
  STEP: retrieving the pod @ 07/29/23 17:04:02.577
  STEP: looking for the results for each expected name from probers @ 07/29/23 17:04:02.585
  Jul 29 17:04:02.597: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.605: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.612: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.619: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.624: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.631: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.637: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.644: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.672: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.679: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.686: INFO: Unable to read jessie_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.691: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.698: INFO: Unable to read jessie_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.703: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.709: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.715: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:02.734: INFO: Lookups using dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7047 wheezy_tcp@dns-test-service.dns-7047 wheezy_udp@dns-test-service.dns-7047.svc wheezy_tcp@dns-test-service.dns-7047.svc wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7047 jessie_tcp@dns-test-service.dns-7047 jessie_udp@dns-test-service.dns-7047.svc jessie_tcp@dns-test-service.dns-7047.svc jessie_udp@_http._tcp.dns-test-service.dns-7047.svc jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc]

  Jul 29 17:04:07.744: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.752: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.761: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.769: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.778: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.786: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.795: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.804: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.847: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.855: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.865: INFO: Unable to read jessie_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.873: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.882: INFO: Unable to read jessie_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.891: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.900: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.908: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:07.939: INFO: Lookups using dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7047 wheezy_tcp@dns-test-service.dns-7047 wheezy_udp@dns-test-service.dns-7047.svc wheezy_tcp@dns-test-service.dns-7047.svc wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7047 jessie_tcp@dns-test-service.dns-7047 jessie_udp@dns-test-service.dns-7047.svc jessie_tcp@dns-test-service.dns-7047.svc jessie_udp@_http._tcp.dns-test-service.dns-7047.svc jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc]

  Jul 29 17:04:12.744: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.751: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.760: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.766: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.773: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.782: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.789: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.797: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.838: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.847: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.855: INFO: Unable to read jessie_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.871: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.881: INFO: Unable to read jessie_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.898: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.906: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:12.941: INFO: Lookups using dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7047 wheezy_tcp@dns-test-service.dns-7047 wheezy_udp@dns-test-service.dns-7047.svc wheezy_tcp@dns-test-service.dns-7047.svc wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7047 jessie_tcp@dns-test-service.dns-7047 jessie_udp@dns-test-service.dns-7047.svc jessie_tcp@dns-test-service.dns-7047.svc jessie_udp@_http._tcp.dns-test-service.dns-7047.svc jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc]

  Jul 29 17:04:17.745: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.752: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.768: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.776: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.783: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.790: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.801: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.813: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.853: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.862: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.867: INFO: Unable to read jessie_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.873: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.877: INFO: Unable to read jessie_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.882: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.891: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.900: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:17.934: INFO: Lookups using dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7047 wheezy_tcp@dns-test-service.dns-7047 wheezy_udp@dns-test-service.dns-7047.svc wheezy_tcp@dns-test-service.dns-7047.svc wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7047 jessie_tcp@dns-test-service.dns-7047 jessie_udp@dns-test-service.dns-7047.svc jessie_tcp@dns-test-service.dns-7047.svc jessie_udp@_http._tcp.dns-test-service.dns-7047.svc jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc]

  Jul 29 17:04:22.747: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.756: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.764: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.773: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.783: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.791: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.800: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.808: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.849: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.856: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.864: INFO: Unable to read jessie_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.872: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.881: INFO: Unable to read jessie_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.889: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.899: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.909: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:22.940: INFO: Lookups using dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7047 wheezy_tcp@dns-test-service.dns-7047 wheezy_udp@dns-test-service.dns-7047.svc wheezy_tcp@dns-test-service.dns-7047.svc wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7047 jessie_tcp@dns-test-service.dns-7047 jessie_udp@dns-test-service.dns-7047.svc jessie_tcp@dns-test-service.dns-7047.svc jessie_udp@_http._tcp.dns-test-service.dns-7047.svc jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc]

  Jul 29 17:04:27.752: INFO: Unable to read wheezy_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.762: INFO: Unable to read wheezy_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.775: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.785: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.799: INFO: Unable to read wheezy_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.809: INFO: Unable to read wheezy_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.821: INFO: Unable to read wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.830: INFO: Unable to read wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.873: INFO: Unable to read jessie_udp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.882: INFO: Unable to read jessie_tcp@dns-test-service from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.896: INFO: Unable to read jessie_udp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.914: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047 from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.930: INFO: Unable to read jessie_udp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.940: INFO: Unable to read jessie_tcp@dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.951: INFO: Unable to read jessie_udp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.962: INFO: Unable to read jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc from pod dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a: the server could not find the requested resource (get pods dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a)
  Jul 29 17:04:27.999: INFO: Lookups using dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a failed for: [wheezy_udp@dns-test-service wheezy_tcp@dns-test-service wheezy_udp@dns-test-service.dns-7047 wheezy_tcp@dns-test-service.dns-7047 wheezy_udp@dns-test-service.dns-7047.svc wheezy_tcp@dns-test-service.dns-7047.svc wheezy_udp@_http._tcp.dns-test-service.dns-7047.svc wheezy_tcp@_http._tcp.dns-test-service.dns-7047.svc jessie_udp@dns-test-service jessie_tcp@dns-test-service jessie_udp@dns-test-service.dns-7047 jessie_tcp@dns-test-service.dns-7047 jessie_udp@dns-test-service.dns-7047.svc jessie_tcp@dns-test-service.dns-7047.svc jessie_udp@_http._tcp.dns-test-service.dns-7047.svc jessie_tcp@_http._tcp.dns-test-service.dns-7047.svc]

  Jul 29 17:04:32.890: INFO: DNS probes using dns-7047/dns-test-0ae3e7a3-b3de-4216-bedc-4410cf4fcb4a succeeded

  Jul 29 17:04:32.890: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 17:04:32.9
  STEP: deleting the test service @ 07/29/23 17:04:32.933
  STEP: deleting the test headless service @ 07/29/23 17:04:33.023
  STEP: Destroying namespace "dns-7047" for this suite. @ 07/29/23 17:04:33.054
• [34.629 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected configMap should be consumable from pods in volume with defaultMode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:57
  STEP: Creating a kubernetes client @ 07/29/23 17:04:33.07
  Jul 29 17:04:33.070: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:04:33.077
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:33.119
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:33.123
  STEP: Creating configMap with name projected-configmap-test-volume-40984109-b26f-46e7-b2aa-81b5f1845348 @ 07/29/23 17:04:33.128
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:04:33.136
  STEP: Saw pod success @ 07/29/23 17:04:37.189
  Jul 29 17:04:37.198: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-configmaps-b72ce406-e0db-4f62-aacb-c83b434393ce container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 17:04:37.215
  Jul 29 17:04:37.247: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-8263" for this suite. @ 07/29/23 17:04:37.257
• [4.201 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown metadata fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:117
  STEP: Creating a kubernetes client @ 07/29/23 17:04:37.273
  Jul 29 17:04:37.273: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 17:04:37.276
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:37.32
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:37.327
  STEP: apply creating a deployment @ 07/29/23 17:04:37.336
  Jul 29 17:04:37.341: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-6156" for this suite. @ 07/29/23 17:04:37.384
• [0.126 seconds]
------------------------------
SSS
------------------------------
[sig-apps] Deployment Deployment should have a working scale subresource [Conformance]
test/e2e/apps/deployment.go:150
  STEP: Creating a kubernetes client @ 07/29/23 17:04:37.402
  Jul 29 17:04:37.402: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename deployment @ 07/29/23 17:04:37.404
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:37.436
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:37.444
  Jul 29 17:04:37.449: INFO: Creating simple deployment test-new-deployment
  Jul 29 17:04:37.484: INFO: deployment "test-new-deployment" doesn't have the required revision set
  STEP: getting scale subresource @ 07/29/23 17:04:39.517
  STEP: updating a scale subresource @ 07/29/23 17:04:39.525
  STEP: verifying the deployment Spec.Replicas was modified @ 07/29/23 17:04:39.549
  STEP: Patch a scale subresource @ 07/29/23 17:04:39.555
  Jul 29 17:04:39.598: INFO: Deployment "test-new-deployment":
  &Deployment{ObjectMeta:{test-new-deployment  deployment-4016  33d2aefe-be43-4466-b1b8-38f841e27146 36554 3 2023-07-29 17:04:37 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 <nil> FieldsV1 {"f:spec":{"f:replicas":{}}} scale} {e2e.test Update apps/v1 2023-07-29 17:04:37 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:04:38 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*4,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00383e638 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Available,Status:True,Reason:MinimumReplicasAvailable,Message:Deployment has minimum availability.,LastUpdateTime:2023-07-29 17:04:38 +0000 UTC,LastTransitionTime:2023-07-29 17:04:38 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:NewReplicaSetAvailable,Message:ReplicaSet "test-new-deployment-67bd4bf6dc" has successfully progressed.,LastUpdateTime:2023-07-29 17:04:38 +0000 UTC,LastTransitionTime:2023-07-29 17:04:37 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 17:04:39.612: INFO: New ReplicaSet "test-new-deployment-67bd4bf6dc" of Deployment "test-new-deployment":
  &ReplicaSet{ObjectMeta:{test-new-deployment-67bd4bf6dc  deployment-4016  3b20fc63-2875-4fa1-96ef-9737107469dd 36553 2 2023-07-29 17:04:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:2 deployment.kubernetes.io/max-replicas:3 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-new-deployment 33d2aefe-be43-4466-b1b8-38f841e27146 0xc0058c9c47 0xc0058c9c48}] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:04:38 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 17:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"33d2aefe-be43-4466-b1b8-38f841e27146\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*2,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0058c9ce8 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 17:04:39.638: INFO: Pod "test-new-deployment-67bd4bf6dc-77kfn" is available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-77kfn test-new-deployment-67bd4bf6dc- deployment-4016  42f5e2e7-9949-4d52-acb7-196f4f004216 36547 0 2023-07-29 17:04:37 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 3b20fc63-2875-4fa1-96ef-9737107469dd 0xc00383ea17 0xc00383ea18}] [] [{kube-controller-manager Update v1 2023-07-29 17:04:37 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b20fc63-2875-4fa1-96ef-9737107469dd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:04:38 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.135\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-mlmxf,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-mlmxf,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:37 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:38 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:37 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.135,StartTime:2023-07-29 17:04:37 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:04:38 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://5ea9ce2c4c6c21774338c0d07bf78bb61c4880a509c56ad098e6003da9b2bef6,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.135,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:04:39.639: INFO: Pod "test-new-deployment-67bd4bf6dc-t8m49" is not available:
  &Pod{ObjectMeta:{test-new-deployment-67bd4bf6dc-t8m49 test-new-deployment-67bd4bf6dc- deployment-4016  433aec3e-726a-4be0-a525-33c1e3e62451 36557 0 2023-07-29 17:04:39 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet test-new-deployment-67bd4bf6dc 3b20fc63-2875-4fa1-96ef-9737107469dd 0xc00383ec07 0xc00383ec08}] [] [{kube-controller-manager Update v1 2023-07-29 17:04:39 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"3b20fc63-2875-4fa1-96ef-9737107469dd\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-q56jk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-q56jk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:04:39 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:04:39.639: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4016" for this suite. @ 07/29/23 17:04:39.653
• [2.268 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition creating/deleting custom resource definition objects works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:58
  STEP: Creating a kubernetes client @ 07/29/23 17:04:39.676
  Jul 29 17:04:39.676: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 17:04:39.679
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:39.716
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:39.722
  Jul 29 17:04:39.730: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:04:40.788: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-1013" for this suite. @ 07/29/23 17:04:40.799
• [1.140 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] Security Context When creating a pod with privileged should run the container as unprivileged when false [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/security_context.go:528
  STEP: Creating a kubernetes client @ 07/29/23 17:04:40.827
  Jul 29 17:04:40.828: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename security-context-test @ 07/29/23 17:04:40.831
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:40.867
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:40.874
  Jul 29 17:04:44.960: INFO: Got logs for pod "busybox-privileged-false-2f3ff50c-d87e-4332-a053-1dae372266de": "ip: RTNETLINK answers: Operation not permitted\n"
  Jul 29 17:04:44.961: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "security-context-test-4296" for this suite. @ 07/29/23 17:04:44.981
• [4.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should allow activeDeadlineSeconds to be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:399
  STEP: Creating a kubernetes client @ 07/29/23 17:04:45.009
  Jul 29 17:04:45.009: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:04:45.014
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:45.061
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:45.067
  STEP: creating the pod @ 07/29/23 17:04:45.071
  STEP: submitting the pod to kubernetes @ 07/29/23 17:04:45.072
  W0729 17:04:45.090978      13 warnings.go:70] metadata.name: this is used in the Pod's hostname, which can result in surprising behavior; a DNS label is recommended: [must be no more than 63 characters]
  STEP: verifying the pod is in kubernetes @ 07/29/23 17:04:47.12
  STEP: updating the pod @ 07/29/23 17:04:47.128
  Jul 29 17:04:47.653: INFO: Successfully updated pod "pod-update-activedeadlineseconds-0fdff37e-1b95-431c-8d08-591ddc4d6487"
  Jul 29 17:04:51.695: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-2109" for this suite. @ 07/29/23 17:04:51.71
• [6.720 seconds]
------------------------------
SSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0666,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:137
  STEP: Creating a kubernetes client @ 07/29/23 17:04:51.731
  Jul 29 17:04:51.731: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:04:51.735
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:51.764
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:51.769
  STEP: Creating a pod to test emptydir 0666 on tmpfs @ 07/29/23 17:04:51.774
  STEP: Saw pod success @ 07/29/23 17:04:55.822
  Jul 29 17:04:55.829: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-e60218bb-aa16-40ac-8a87-c9b27d1e7506 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:04:55.844
  Jul 29 17:04:55.873: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6406" for this suite. @ 07/29/23 17:04:55.888
• [4.173 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be updated [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:345
  STEP: Creating a kubernetes client @ 07/29/23 17:04:55.909
  Jul 29 17:04:55.909: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:04:55.912
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:55.946
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:55.952
  STEP: creating the pod @ 07/29/23 17:04:55.958
  STEP: submitting the pod to kubernetes @ 07/29/23 17:04:55.958
  STEP: verifying the pod is in kubernetes @ 07/29/23 17:04:58.003
  STEP: updating the pod @ 07/29/23 17:04:58.011
  Jul 29 17:04:58.529: INFO: Successfully updated pod "pod-update-942639d0-6b89-48fe-8575-c75d0a60721f"
  STEP: verifying the updated pod is in kubernetes @ 07/29/23 17:04:58.539
  Jul 29 17:04:58.554: INFO: Pod update OK
  Jul 29 17:04:58.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-686" for this suite. @ 07/29/23 17:04:58.564
• [2.669 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-network] DNS should provide DNS for ExternalName services [Conformance]
test/e2e/network/dns.go:329
  STEP: Creating a kubernetes client @ 07/29/23 17:04:58.582
  Jul 29 17:04:58.583: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename dns @ 07/29/23 17:04:58.586
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:04:58.62
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:04:58.626
  STEP: Creating a test externalName service @ 07/29/23 17:04:58.631
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1361.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local; sleep 1; done
   @ 07/29/23 17:04:58.642
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1361.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local; sleep 1; done
   @ 07/29/23 17:04:58.643
  STEP: creating a pod to probe DNS @ 07/29/23 17:04:58.643
  STEP: submitting the pod to kubernetes @ 07/29/23 17:04:58.645
  STEP: retrieving the pod @ 07/29/23 17:05:00.677
  STEP: looking for the results for each expected name from probers @ 07/29/23 17:05:00.684
  Jul 29 17:05:00.706: INFO: DNS probes using dns-test-999a8f64-cef4-42df-ba81-80daa05f08fb succeeded

  STEP: changing the externalName to bar.example.com @ 07/29/23 17:05:00.707
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1361.svc.cluster.local CNAME > /results/wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local; sleep 1; done
   @ 07/29/23 17:05:00.726
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1361.svc.cluster.local CNAME > /results/jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local; sleep 1; done
   @ 07/29/23 17:05:00.726
  STEP: creating a second pod to probe DNS @ 07/29/23 17:05:00.726
  STEP: submitting the pod to kubernetes @ 07/29/23 17:05:00.726
  STEP: retrieving the pod @ 07/29/23 17:05:02.755
  STEP: looking for the results for each expected name from probers @ 07/29/23 17:05:02.763
  Jul 29 17:05:02.775: INFO: File wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:02.782: INFO: File jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:02.782: INFO: Lookups using dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 failed for: [wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local]

  Jul 29 17:05:07.793: INFO: File wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:07.800: INFO: File jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:07.800: INFO: Lookups using dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 failed for: [wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local]

  Jul 29 17:05:12.791: INFO: File wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:12.798: INFO: File jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:12.798: INFO: Lookups using dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 failed for: [wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local]

  Jul 29 17:05:17.795: INFO: File wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:17.805: INFO: File jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:17.806: INFO: Lookups using dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 failed for: [wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local]

  Jul 29 17:05:22.793: INFO: File wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:22.803: INFO: File jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:22.804: INFO: Lookups using dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 failed for: [wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local]

  Jul 29 17:05:27.796: INFO: File wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:27.807: INFO: File jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local from pod  dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 contains 'foo.example.com.
  ' instead of 'bar.example.com.'
  Jul 29 17:05:27.807: INFO: Lookups using dns-1361/dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 failed for: [wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local]

  Jul 29 17:05:32.809: INFO: DNS probes using dns-test-5d159e74-cc3c-4ff6-ab00-a6c396dcfca0 succeeded

  STEP: changing the service to type=ClusterIP @ 07/29/23 17:05:32.809
  STEP: Running these commands on wheezy: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1361.svc.cluster.local A > /results/wheezy_udp@dns-test-service-3.dns-1361.svc.cluster.local; sleep 1; done
   @ 07/29/23 17:05:32.86
  STEP: Running these commands on jessie: for i in `seq 1 30`; do dig +short dns-test-service-3.dns-1361.svc.cluster.local A > /results/jessie_udp@dns-test-service-3.dns-1361.svc.cluster.local; sleep 1; done
   @ 07/29/23 17:05:32.861
  STEP: creating a third pod to probe DNS @ 07/29/23 17:05:32.861
  STEP: submitting the pod to kubernetes @ 07/29/23 17:05:32.871
  STEP: retrieving the pod @ 07/29/23 17:05:36.945
  STEP: looking for the results for each expected name from probers @ 07/29/23 17:05:36.954
  Jul 29 17:05:36.978: INFO: DNS probes using dns-test-5184ea2f-2c3e-47d1-8f7f-e33e39c23ddc succeeded

  Jul 29 17:05:36.979: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 17:05:36.99
  STEP: deleting the pod @ 07/29/23 17:05:37.029
  STEP: deleting the pod @ 07/29/23 17:05:37.108
  STEP: deleting the test externalName service @ 07/29/23 17:05:37.15
  STEP: Destroying namespace "dns-1361" for this suite. @ 07/29/23 17:05:37.215
• [38.652 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Probing container should be restarted with a /healthz http liveness probe [NodeConformance] [Conformance]
test/e2e/common/node/container_probe.go:168
  STEP: Creating a kubernetes client @ 07/29/23 17:05:37.237
  Jul 29 17:05:37.237: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-probe @ 07/29/23 17:05:37.241
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:05:37.288
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:05:37.297
  STEP: Creating pod liveness-046f6a74-0599-4410-b1b7-afb19f08c9dc in namespace container-probe-7919 @ 07/29/23 17:05:37.315
  Jul 29 17:05:41.387: INFO: Started pod liveness-046f6a74-0599-4410-b1b7-afb19f08c9dc in namespace container-probe-7919
  STEP: checking the pod's current state and verifying that restartCount is present @ 07/29/23 17:05:41.387
  Jul 29 17:05:41.395: INFO: Initial restart count of pod liveness-046f6a74-0599-4410-b1b7-afb19f08c9dc is 0
  Jul 29 17:05:59.496: INFO: Restart count of pod container-probe-7919/liveness-046f6a74-0599-4410-b1b7-afb19f08c9dc is now 1 (18.101010134s elapsed)
  Jul 29 17:05:59.497: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting the pod @ 07/29/23 17:05:59.507
  STEP: Destroying namespace "container-probe-7919" for this suite. @ 07/29/23 17:05:59.529
• [22.305 seconds]
------------------------------
SSSS
------------------------------
[sig-network] Proxy version v1 A set of valid responses are returned for both pod and service ProxyWithPath [Conformance]
test/e2e/network/proxy.go:286
  STEP: Creating a kubernetes client @ 07/29/23 17:05:59.543
  Jul 29 17:05:59.544: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename proxy @ 07/29/23 17:05:59.548
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:05:59.625
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:05:59.631
  Jul 29 17:05:59.637: INFO: Creating pod...
  Jul 29 17:06:01.684: INFO: Creating service...
  Jul 29 17:06:01.707: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/pods/agnhost/proxy/some/path/with/DELETE
  Jul 29 17:06:01.729: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 17:06:01.729: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/pods/agnhost/proxy/some/path/with/GET
  Jul 29 17:06:01.752: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 29 17:06:01.753: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/pods/agnhost/proxy/some/path/with/HEAD
  Jul 29 17:06:01.762: INFO: http.Client request:HEAD | StatusCode:200
  Jul 29 17:06:01.763: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/pods/agnhost/proxy/some/path/with/OPTIONS
  Jul 29 17:06:01.771: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 17:06:01.772: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/pods/agnhost/proxy/some/path/with/PATCH
  Jul 29 17:06:01.780: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 17:06:01.780: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/pods/agnhost/proxy/some/path/with/POST
  Jul 29 17:06:01.786: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 17:06:01.786: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/pods/agnhost/proxy/some/path/with/PUT
  Jul 29 17:06:01.793: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 17:06:01.793: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/services/test-service/proxy/some/path/with/DELETE
  Jul 29 17:06:01.803: INFO: http.Client request:DELETE | StatusCode:200 | Response:foo | Method:DELETE
  Jul 29 17:06:01.803: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/services/test-service/proxy/some/path/with/GET
  Jul 29 17:06:01.810: INFO: http.Client request:GET | StatusCode:200 | Response:foo | Method:GET
  Jul 29 17:06:01.810: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/services/test-service/proxy/some/path/with/HEAD
  Jul 29 17:06:01.819: INFO: http.Client request:HEAD | StatusCode:200
  Jul 29 17:06:01.819: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/services/test-service/proxy/some/path/with/OPTIONS
  Jul 29 17:06:01.827: INFO: http.Client request:OPTIONS | StatusCode:200 | Response:foo | Method:OPTIONS
  Jul 29 17:06:01.827: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/services/test-service/proxy/some/path/with/PATCH
  Jul 29 17:06:01.835: INFO: http.Client request:PATCH | StatusCode:200 | Response:foo | Method:PATCH
  Jul 29 17:06:01.835: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/services/test-service/proxy/some/path/with/POST
  Jul 29 17:06:01.846: INFO: http.Client request:POST | StatusCode:200 | Response:foo | Method:POST
  Jul 29 17:06:01.846: INFO: Starting http.Client for https://10.233.0.1:443/api/v1/namespaces/proxy-9012/services/test-service/proxy/some/path/with/PUT
  Jul 29 17:06:01.855: INFO: http.Client request:PUT | StatusCode:200 | Response:foo | Method:PUT
  Jul 29 17:06:01.856: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "proxy-9012" for this suite. @ 07/29/23 17:06:01.867
• [2.335 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a service. [Conformance]
test/e2e/apimachinery/resource_quota.go:101
  STEP: Creating a kubernetes client @ 07/29/23 17:06:01.889
  Jul 29 17:06:01.889: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:06:01.891
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:01.932
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:01.937
  STEP: Counting existing ResourceQuota @ 07/29/23 17:06:01.942
  STEP: Creating a ResourceQuota @ 07/29/23 17:06:06.951
  STEP: Ensuring resource quota status is calculated @ 07/29/23 17:06:06.963
  STEP: Creating a Service @ 07/29/23 17:06:08.971
  STEP: Creating a NodePort Service @ 07/29/23 17:06:08.998
  STEP: Not allowing a LoadBalancer Service with NodePort to be created that exceeds remaining quota @ 07/29/23 17:06:09.039
  STEP: Ensuring resource quota status captures service creation @ 07/29/23 17:06:09.08
  STEP: Deleting Services @ 07/29/23 17:06:11.091
  STEP: Ensuring resource quota status released usage @ 07/29/23 17:06:11.215
  Jul 29 17:06:13.225: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-4551" for this suite. @ 07/29/23 17:06:13.237
• [11.360 seconds]
------------------------------
SSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate configmap [Conformance]
test/e2e/apimachinery/webhook.go:249
  STEP: Creating a kubernetes client @ 07/29/23 17:06:13.25
  Jul 29 17:06:13.250: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:06:13.252
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:13.283
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:13.288
  STEP: Setting up server cert @ 07/29/23 17:06:13.331
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:06:14.43
  STEP: Deploying the webhook pod @ 07/29/23 17:06:14.449
  STEP: Wait for the deployment to be ready @ 07/29/23 17:06:14.468
  Jul 29 17:06:14.485: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 17:06:16.517
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:06:16.542
  Jul 29 17:06:17.543: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating configmap webhook via the AdmissionRegistration API @ 07/29/23 17:06:17.553
  Jul 29 17:06:17.587: INFO: Waiting for webhook configuration to be ready...
  STEP: create a configmap that should be updated by the webhook @ 07/29/23 17:06:17.705
  Jul 29 17:06:17.731: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-8644" for this suite. @ 07/29/23 17:06:17.85
  STEP: Destroying namespace "webhook-markers-837" for this suite. @ 07/29/23 17:06:17.862
• [4.630 seconds]
------------------------------
[sig-storage] ConfigMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:423
  STEP: Creating a kubernetes client @ 07/29/23 17:06:17.881
  Jul 29 17:06:17.881: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:06:17.893
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:17.937
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:17.942
  STEP: Creating configMap with name configmap-test-volume-4537d713-bf82-4249-87af-5c04a5fc2bd9 @ 07/29/23 17:06:17.947
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:06:17.955
  STEP: Saw pod success @ 07/29/23 17:06:22.001
  Jul 29 17:06:22.007: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-6268f170-b0d0-469a-b2ce-3e2ca33e4e8e container configmap-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 17:06:22.019
  Jul 29 17:06:22.045: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-4164" for this suite. @ 07/29/23 17:06:22.054
• [4.183 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-apps] Job should adopt matching orphans and release non-matching pods [Conformance]
test/e2e/apps/job.go:513
  STEP: Creating a kubernetes client @ 07/29/23 17:06:22.066
  Jul 29 17:06:22.066: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename job @ 07/29/23 17:06:22.068
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:22.102
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:22.107
  STEP: Creating a job @ 07/29/23 17:06:22.111
  STEP: Ensuring active pods == parallelism @ 07/29/23 17:06:22.123
  STEP: Orphaning one of the Job's Pods @ 07/29/23 17:06:24.135
  Jul 29 17:06:24.676: INFO: Successfully updated pod "adopt-release-6l5mz"
  STEP: Checking that the Job readopts the Pod @ 07/29/23 17:06:24.677
  STEP: Removing the labels from the Job's Pod @ 07/29/23 17:06:26.694
  Jul 29 17:06:27.234: INFO: Successfully updated pod "adopt-release-6l5mz"
  STEP: Checking that the Job releases the Pod @ 07/29/23 17:06:27.234
  Jul 29 17:06:29.259: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-1791" for this suite. @ 07/29/23 17:06:29.271
• [7.218 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-network] Ingress API should support creating Ingress API operations [Conformance]
test/e2e/network/ingress.go:556
  STEP: Creating a kubernetes client @ 07/29/23 17:06:29.287
  Jul 29 17:06:29.287: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename ingress @ 07/29/23 17:06:29.289
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:29.319
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:29.324
  STEP: getting /apis @ 07/29/23 17:06:29.329
  STEP: getting /apis/networking.k8s.io @ 07/29/23 17:06:29.339
  STEP: getting /apis/networking.k8s.iov1 @ 07/29/23 17:06:29.341
  STEP: creating @ 07/29/23 17:06:29.343
  STEP: getting @ 07/29/23 17:06:29.382
  STEP: listing @ 07/29/23 17:06:29.389
  STEP: watching @ 07/29/23 17:06:29.396
  Jul 29 17:06:29.396: INFO: starting watch
  STEP: cluster-wide listing @ 07/29/23 17:06:29.397
  STEP: cluster-wide watching @ 07/29/23 17:06:29.403
  Jul 29 17:06:29.403: INFO: starting watch
  STEP: patching @ 07/29/23 17:06:29.405
  STEP: updating @ 07/29/23 17:06:29.415
  Jul 29 17:06:29.429: INFO: waiting for watch events with expected annotations
  Jul 29 17:06:29.430: INFO: saw patched and updated annotations
  STEP: patching /status @ 07/29/23 17:06:29.43
  STEP: updating /status @ 07/29/23 17:06:29.444
  STEP: get /status @ 07/29/23 17:06:29.46
  STEP: deleting @ 07/29/23 17:06:29.47
  STEP: deleting a collection @ 07/29/23 17:06:29.503
  Jul 29 17:06:29.538: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "ingress-6981" for this suite. @ 07/29/23 17:06:29.546
• [0.275 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-network] DNS should support configurable pod DNS nameservers [Conformance]
test/e2e/network/dns.go:407
  STEP: Creating a kubernetes client @ 07/29/23 17:06:29.577
  Jul 29 17:06:29.577: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename dns @ 07/29/23 17:06:29.579
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:29.603
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:29.609
  STEP: Creating a pod with dnsPolicy=None and customized dnsConfig... @ 07/29/23 17:06:29.614
  Jul 29 17:06:29.626: INFO: Created pod &Pod{ObjectMeta:{test-dns-nameservers  dns-3230  10842fab-7dc0-4938-98ae-7d81dd5c22fc 37364 0 2023-07-29 17:06:29 +0000 UTC <nil> <nil> map[] map[] [] [] [{e2e.test Update v1 2023-07-29 17:06:29 +0000 UTC FieldsV1 {"f:spec":{"f:containers":{"k:{\"name\":\"agnhost-container\"}":{".":{},"f:args":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsConfig":{".":{},"f:nameservers":{},"f:searches":{}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6n6rw,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:agnhost-container,Image:registry.k8s.io/e2e-test-images/agnhost:2.43,Command:[],Args:[pause],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6n6rw,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:None,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:&PodDNSConfig{Nameservers:[1.1.1.1],Searches:[resolv.conf.local],Options:[]PodDNSConfigOption{},},ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  STEP: Verifying customized DNS suffix list is configured on pod... @ 07/29/23 17:06:31.642
  Jul 29 17:06:31.643: INFO: ExecWithOptions {Command:[/agnhost dns-suffix] Namespace:dns-3230 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:06:31.643: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:06:31.644: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:06:31.645: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-3230/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-suffix&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  STEP: Verifying customized DNS server is configured on pod... @ 07/29/23 17:06:31.783
  Jul 29 17:06:31.784: INFO: ExecWithOptions {Command:[/agnhost dns-server-list] Namespace:dns-3230 PodName:test-dns-nameservers ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:06:31.784: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:06:31.785: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:06:31.786: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/dns-3230/pods/test-dns-nameservers/exec?command=%2Fagnhost&command=dns-server-list&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 17:06:31.922: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 17:06:31.933: INFO: Deleting pod test-dns-nameservers...
  STEP: Destroying namespace "dns-3230" for this suite. @ 07/29/23 17:06:31.962
• [2.409 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD preserving unknown fields in an embedded object [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:236
  STEP: Creating a kubernetes client @ 07/29/23 17:06:31.988
  Jul 29 17:06:31.988: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:06:31.99
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:32.032
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:32.038
  Jul 29 17:06:32.043: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 17:06:33.903
  Jul 29 17:06:33.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7550 --namespace=crd-publish-openapi-7550 create -f -'
  Jul 29 17:06:35.714: INFO: stderr: ""
  Jul 29 17:06:35.714: INFO: stdout: "e2e-test-crd-publish-openapi-9744-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 29 17:06:35.714: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7550 --namespace=crd-publish-openapi-7550 delete e2e-test-crd-publish-openapi-9744-crds test-cr'
  Jul 29 17:06:35.903: INFO: stderr: ""
  Jul 29 17:06:35.903: INFO: stdout: "e2e-test-crd-publish-openapi-9744-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  Jul 29 17:06:35.904: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7550 --namespace=crd-publish-openapi-7550 apply -f -'
  Jul 29 17:06:36.392: INFO: stderr: ""
  Jul 29 17:06:36.392: INFO: stdout: "e2e-test-crd-publish-openapi-9744-crd.crd-publish-openapi-test-unknown-in-nested.example.com/test-cr created\n"
  Jul 29 17:06:36.392: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7550 --namespace=crd-publish-openapi-7550 delete e2e-test-crd-publish-openapi-9744-crds test-cr'
  Jul 29 17:06:36.550: INFO: stderr: ""
  Jul 29 17:06:36.550: INFO: stdout: "e2e-test-crd-publish-openapi-9744-crd.crd-publish-openapi-test-unknown-in-nested.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR @ 07/29/23 17:06:36.55
  Jul 29 17:06:36.551: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-7550 explain e2e-test-crd-publish-openapi-9744-crds'
  Jul 29 17:06:36.996: INFO: stderr: ""
  Jul 29 17:06:36.996: INFO: stdout: "GROUP:      crd-publish-openapi-test-unknown-in-nested.example.com\nKIND:       e2e-test-crd-publish-openapi-9744-crd\nVERSION:    v1\n\nDESCRIPTION:\n    preserve-unknown-properties in nested field for Testing\n    \nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n  spec\t<Object>\n    Specification of Waldo\n\n  status\t<Object>\n    Status of Waldo\n\n\n"
  Jul 29 17:06:38.817: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7550" for this suite. @ 07/29/23 17:06:38.84
• [6.865 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a secret. [Conformance]
test/e2e/apimachinery/resource_quota.go:161
  STEP: Creating a kubernetes client @ 07/29/23 17:06:38.854
  Jul 29 17:06:38.854: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:06:38.856
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:38.891
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:38.896
  STEP: Discovering how many secrets are in namespace by default @ 07/29/23 17:06:38.901
  STEP: Counting existing ResourceQuota @ 07/29/23 17:06:43.908
  STEP: Creating a ResourceQuota @ 07/29/23 17:06:48.916
  STEP: Ensuring resource quota status is calculated @ 07/29/23 17:06:48.93
  STEP: Creating a Secret @ 07/29/23 17:06:50.937
  STEP: Ensuring resource quota status captures secret creation @ 07/29/23 17:06:50.957
  STEP: Deleting a secret @ 07/29/23 17:06:52.965
  STEP: Ensuring resource quota status released usage @ 07/29/23 17:06:52.977
  Jul 29 17:06:54.987: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-8906" for this suite. @ 07/29/23 17:06:54.997
• [16.158 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets should be consumable from pods in volume with mappings [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:79
  STEP: Creating a kubernetes client @ 07/29/23 17:06:55.032
  Jul 29 17:06:55.033: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 17:06:55.035
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:55.085
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:55.09
  STEP: Creating secret with name secret-test-map-aa915844-e4bf-411d-a6c6-9d01b1a674e8 @ 07/29/23 17:06:55.094
  STEP: Creating a pod to test consume secrets @ 07/29/23 17:06:55.109
  STEP: Saw pod success @ 07/29/23 17:06:59.172
  Jul 29 17:06:59.182: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-secrets-50972f24-1147-4359-9b61-6b5843e2d366 container secret-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 17:06:59.211
  Jul 29 17:06:59.239: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7698" for this suite. @ 07/29/23 17:06:59.247
• [4.225 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a busybox command that always fails in a pod should be possible to delete [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:135
  STEP: Creating a kubernetes client @ 07/29/23 17:06:59.258
  Jul 29 17:06:59.258: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 17:06:59.26
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:59.291
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:59.295
  Jul 29 17:06:59.332: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-2062" for this suite. @ 07/29/23 17:06:59.345
• [0.094 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute prestop http hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:213
  STEP: Creating a kubernetes client @ 07/29/23 17:06:59.359
  Jul 29 17:06:59.359: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 17:06:59.361
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:06:59.387
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:06:59.392
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 17:06:59.403
  STEP: create the pod with lifecycle hook @ 07/29/23 17:07:01.445
  STEP: delete the pod with lifecycle hook @ 07/29/23 17:07:03.489
  STEP: check prestop hook @ 07/29/23 17:07:05.516
  Jul 29 17:07:05.527: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-9008" for this suite. @ 07/29/23 17:07:05.538
• [6.191 seconds]
------------------------------
SSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide podname only [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:54
  STEP: Creating a kubernetes client @ 07/29/23 17:07:05.554
  Jul 29 17:07:05.554: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:07:05.557
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:05.584
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:05.588
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:07:05.594
  STEP: Saw pod success @ 07/29/23 17:07:09.634
  Jul 29 17:07:09.640: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-f738411f-7a49-4274-b737-91d416b8ac77 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:07:09.655
  Jul 29 17:07:09.699: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1103" for this suite. @ 07/29/23 17:07:09.71
• [4.173 seconds]
------------------------------
SSSSSS
------------------------------
[sig-scheduling] SchedulerPredicates [Serial] validates resource limits of pods that are allowed to run  [Conformance]
test/e2e/scheduling/predicates.go:332
  STEP: Creating a kubernetes client @ 07/29/23 17:07:09.728
  Jul 29 17:07:09.728: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sched-pred @ 07/29/23 17:07:09.73
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:09.82
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:09.827
  Jul 29 17:07:09.831: INFO: Waiting up to 1m0s for all (but 0) nodes to be ready
  Jul 29 17:07:09.849: INFO: Waiting for terminating namespaces to be deleted...
  Jul 29 17:07:09.863: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-1 before test
  Jul 29 17:07:09.883: INFO: cilium-6928s from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.884: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 17:07:09.884: INFO: cilium-node-init-m4qsv from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.885: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 17:07:09.885: INFO: coredns-5d78c9869d-xxmxx from kube-system started at 2023-07-29 15:24:09 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.885: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 17:07:09.886: INFO: kube-addon-manager-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.886: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 17:07:09.886: INFO: kube-apiserver-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.887: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 17:07:09.887: INFO: kube-controller-manager-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.888: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 17:07:09.888: INFO: kube-proxy-dscp7 from kube-system started at 2023-07-29 15:11:26 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.888: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 17:07:09.889: INFO: kube-scheduler-ci9axai7aiv7-1 from kube-system started at 2023-07-29 15:25:15 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.889: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 17:07:09.889: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-7bqrz from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:09.890: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 17:07:09.890: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 17:07:09.890: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-2 before test
  Jul 29 17:07:09.905: INFO: cilium-node-init-c52xz from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.906: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 17:07:09.906: INFO: cilium-operator-64cdf5fc9d-9t2m7 from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.906: INFO: 	Container cilium-operator ready: true, restart count 0
  Jul 29 17:07:09.907: INFO: cilium-pr7vs from kube-system started at 2023-07-29 15:23:03 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.907: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 17:07:09.908: INFO: coredns-5d78c9869d-7fz8w from kube-system started at 2023-07-29 15:24:04 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.908: INFO: 	Container coredns ready: true, restart count 0
  Jul 29 17:07:09.908: INFO: kube-addon-manager-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.909: INFO: 	Container kube-addon-manager ready: true, restart count 0
  Jul 29 17:07:09.909: INFO: kube-apiserver-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.909: INFO: 	Container kube-apiserver ready: true, restart count 0
  Jul 29 17:07:09.910: INFO: kube-controller-manager-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.911: INFO: 	Container kube-controller-manager ready: true, restart count 0
  Jul 29 17:07:09.912: INFO: kube-proxy-bgt8j from kube-system started at 2023-07-29 15:11:44 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.912: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 17:07:09.912: INFO: kube-scheduler-ci9axai7aiv7-2 from kube-system started at 2023-07-29 15:25:36 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.913: INFO: 	Container kube-scheduler ready: true, restart count 0
  Jul 29 17:07:09.913: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-568gm from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:09.913: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 17:07:09.914: INFO: 	Container systemd-logs ready: true, restart count 0
  Jul 29 17:07:09.914: INFO: 
  Logging pods the apiserver thinks is on node ci9axai7aiv7-3 before test
  Jul 29 17:07:09.927: INFO: pod-handle-http-request from container-lifecycle-hook-9008 started at 2023-07-29 17:06:59 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:09.928: INFO: 	Container container-handle-http-request ready: true, restart count 0
  Jul 29 17:07:09.928: INFO: 	Container container-handle-https-request ready: true, restart count 0
  Jul 29 17:07:09.929: INFO: cilium-jsbkd from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.929: INFO: 	Container cilium-agent ready: true, restart count 0
  Jul 29 17:07:09.929: INFO: cilium-node-init-fs2dr from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.929: INFO: 	Container node-init ready: true, restart count 0
  Jul 29 17:07:09.930: INFO: kube-proxy-bhtmh from kube-system started at 2023-07-29 15:26:04 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.930: INFO: 	Container kube-proxy ready: true, restart count 0
  Jul 29 17:07:09.930: INFO: sonobuoy from sonobuoy started at 2023-07-29 15:29:07 +0000 UTC (1 container statuses recorded)
  Jul 29 17:07:09.931: INFO: 	Container kube-sonobuoy ready: true, restart count 0
  Jul 29 17:07:09.931: INFO: sonobuoy-e2e-job-9fe8eb0b75ee4e08 from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:09.931: INFO: 	Container e2e ready: true, restart count 0
  Jul 29 17:07:09.931: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 17:07:09.932: INFO: sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-92k89 from sonobuoy started at 2023-07-29 15:29:17 +0000 UTC (2 container statuses recorded)
  Jul 29 17:07:09.932: INFO: 	Container sonobuoy-worker ready: true, restart count 0
  Jul 29 17:07:09.932: INFO: 	Container systemd-logs ready: true, restart count 0
  STEP: verifying the node has the label node ci9axai7aiv7-1 @ 07/29/23 17:07:09.981
  STEP: verifying the node has the label node ci9axai7aiv7-2 @ 07/29/23 17:07:10.023
  STEP: verifying the node has the label node ci9axai7aiv7-3 @ 07/29/23 17:07:10.069
  Jul 29 17:07:10.190: INFO: Pod pod-handle-http-request requesting resource cpu=0m on Node ci9axai7aiv7-3
  Jul 29 17:07:10.191: INFO: Pod cilium-6928s requesting resource cpu=0m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.192: INFO: Pod cilium-jsbkd requesting resource cpu=0m on Node ci9axai7aiv7-3
  Jul 29 17:07:10.193: INFO: Pod cilium-node-init-c52xz requesting resource cpu=100m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.193: INFO: Pod cilium-node-init-fs2dr requesting resource cpu=100m on Node ci9axai7aiv7-3
  Jul 29 17:07:10.194: INFO: Pod cilium-node-init-m4qsv requesting resource cpu=100m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.194: INFO: Pod cilium-operator-64cdf5fc9d-9t2m7 requesting resource cpu=0m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.195: INFO: Pod cilium-pr7vs requesting resource cpu=0m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.195: INFO: Pod coredns-5d78c9869d-7fz8w requesting resource cpu=100m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.199: INFO: Pod coredns-5d78c9869d-xxmxx requesting resource cpu=100m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.200: INFO: Pod kube-addon-manager-ci9axai7aiv7-1 requesting resource cpu=5m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.200: INFO: Pod kube-addon-manager-ci9axai7aiv7-2 requesting resource cpu=5m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.200: INFO: Pod kube-apiserver-ci9axai7aiv7-1 requesting resource cpu=250m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.203: INFO: Pod kube-apiserver-ci9axai7aiv7-2 requesting resource cpu=250m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.204: INFO: Pod kube-controller-manager-ci9axai7aiv7-1 requesting resource cpu=200m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.206: INFO: Pod kube-controller-manager-ci9axai7aiv7-2 requesting resource cpu=200m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.208: INFO: Pod kube-proxy-bgt8j requesting resource cpu=0m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.210: INFO: Pod kube-proxy-bhtmh requesting resource cpu=0m on Node ci9axai7aiv7-3
  Jul 29 17:07:10.212: INFO: Pod kube-proxy-dscp7 requesting resource cpu=0m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.213: INFO: Pod kube-scheduler-ci9axai7aiv7-1 requesting resource cpu=100m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.214: INFO: Pod kube-scheduler-ci9axai7aiv7-2 requesting resource cpu=100m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.214: INFO: Pod sonobuoy requesting resource cpu=0m on Node ci9axai7aiv7-3
  Jul 29 17:07:10.215: INFO: Pod sonobuoy-e2e-job-9fe8eb0b75ee4e08 requesting resource cpu=0m on Node ci9axai7aiv7-3
  Jul 29 17:07:10.215: INFO: Pod sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-568gm requesting resource cpu=0m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.216: INFO: Pod sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-7bqrz requesting resource cpu=0m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.217: INFO: Pod sonobuoy-systemd-logs-daemon-set-2603497ca7244dad-92k89 requesting resource cpu=0m on Node ci9axai7aiv7-3
  STEP: Starting Pods to consume most of the cluster CPU. @ 07/29/23 17:07:10.217
  Jul 29 17:07:10.217: INFO: Creating a pod which consumes cpu=591m on Node ci9axai7aiv7-1
  Jul 29 17:07:10.243: INFO: Creating a pod which consumes cpu=591m on Node ci9axai7aiv7-2
  Jul 29 17:07:10.262: INFO: Creating a pod which consumes cpu=1050m on Node ci9axai7aiv7-3
  STEP: Creating another pod that requires unavailable amount of CPU. @ 07/29/23 17:07:14.332
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7962fad3-7c46-412a-8558-56878dc73552.1776659be279819f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2873/filler-pod-7962fad3-7c46-412a-8558-56878dc73552 to ci9axai7aiv7-3] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7962fad3-7c46-412a-8558-56878dc73552.1776659c1ec62668], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7962fad3-7c46-412a-8558-56878dc73552.1776659c2a2c9d01], Reason = [Created], Message = [Created container filler-pod-7962fad3-7c46-412a-8558-56878dc73552] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-7962fad3-7c46-412a-8558-56878dc73552.1776659c2b7e1b18], Reason = [Started], Message = [Started container filler-pod-7962fad3-7c46-412a-8558-56878dc73552] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a3e8c956-e709-4f65-a10b-ecf6a8a62354.1776659be06e448f], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2873/filler-pod-a3e8c956-e709-4f65-a10b-ecf6a8a62354 to ci9axai7aiv7-1] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a3e8c956-e709-4f65-a10b-ecf6a8a62354.1776659c39222a2f], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a3e8c956-e709-4f65-a10b-ecf6a8a62354.1776659c4d317485], Reason = [Created], Message = [Created container filler-pod-a3e8c956-e709-4f65-a10b-ecf6a8a62354] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-a3e8c956-e709-4f65-a10b-ecf6a8a62354.1776659c4f54d2ee], Reason = [Started], Message = [Started container filler-pod-a3e8c956-e709-4f65-a10b-ecf6a8a62354] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b5a4772d-cfd7-4f3b-81fa-8e0c0f010d81.1776659be1d27541], Reason = [Scheduled], Message = [Successfully assigned sched-pred-2873/filler-pod-b5a4772d-cfd7-4f3b-81fa-8e0c0f010d81 to ci9axai7aiv7-2] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b5a4772d-cfd7-4f3b-81fa-8e0c0f010d81.1776659c26e2959c], Reason = [Pulled], Message = [Container image "registry.k8s.io/pause:3.9" already present on machine] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b5a4772d-cfd7-4f3b-81fa-8e0c0f010d81.1776659c3ac84091], Reason = [Created], Message = [Created container filler-pod-b5a4772d-cfd7-4f3b-81fa-8e0c0f010d81] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Normal], Name = [filler-pod-b5a4772d-cfd7-4f3b-81fa-8e0c0f010d81.1776659c3f77b280], Reason = [Started], Message = [Started container filler-pod-b5a4772d-cfd7-4f3b-81fa-8e0c0f010d81] @ 07/29/23 17:07:14.342
  STEP: Considering event: 
  Type = [Warning], Name = [additional-pod.1776659cd309e26a], Reason = [FailedScheduling], Message = [0/3 nodes are available: 3 Insufficient cpu. preemption: 0/3 nodes are available: 3 No preemption victims found for incoming pod..] @ 07/29/23 17:07:14.37
  STEP: removing the label node off the node ci9axai7aiv7-1 @ 07/29/23 17:07:15.368
  STEP: verifying the node doesn't have the label node @ 07/29/23 17:07:15.4
  STEP: removing the label node off the node ci9axai7aiv7-2 @ 07/29/23 17:07:15.417
  STEP: verifying the node doesn't have the label node @ 07/29/23 17:07:15.508
  STEP: removing the label node off the node ci9axai7aiv7-3 @ 07/29/23 17:07:15.547
  STEP: verifying the node doesn't have the label node @ 07/29/23 17:07:15.598
  Jul 29 17:07:15.655: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sched-pred-2873" for this suite. @ 07/29/23 17:07:15.673
• [5.983 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-apps] Deployment deployment should support proportional scaling [Conformance]
test/e2e/apps/deployment.go:160
  STEP: Creating a kubernetes client @ 07/29/23 17:07:15.715
  Jul 29 17:07:15.716: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename deployment @ 07/29/23 17:07:15.718
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:15.762
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:15.777
  Jul 29 17:07:15.794: INFO: Creating deployment "webserver-deployment"
  Jul 29 17:07:15.807: INFO: Waiting for observed generation 1
  Jul 29 17:07:17.872: INFO: Waiting for all required pods to come up
  Jul 29 17:07:17.917: INFO: Pod name httpd: Found 10 pods out of 10
  STEP: ensuring each pod is running @ 07/29/23 17:07:17.917
  Jul 29 17:07:20.000: INFO: Waiting for deployment "webserver-deployment" to complete
  Jul 29 17:07:20.012: INFO: Updating deployment "webserver-deployment" with a non-existent image
  Jul 29 17:07:20.030: INFO: Updating deployment webserver-deployment
  Jul 29 17:07:20.030: INFO: Waiting for observed generation 2
  Jul 29 17:07:22.054: INFO: Waiting for the first rollout's replicaset to have .status.availableReplicas = 8
  Jul 29 17:07:22.068: INFO: Waiting for the first rollout's replicaset to have .spec.replicas = 8
  Jul 29 17:07:22.075: INFO: Waiting for the first rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 17:07:22.121: INFO: Verifying that the second rollout's replicaset has .status.availableReplicas = 0
  Jul 29 17:07:22.121: INFO: Waiting for the second rollout's replicaset to have .spec.replicas = 5
  Jul 29 17:07:22.144: INFO: Waiting for the second rollout's replicaset of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 17:07:22.174: INFO: Verifying that deployment "webserver-deployment" has minimum required number of available replicas
  Jul 29 17:07:22.174: INFO: Scaling up the deployment "webserver-deployment" from 10 to 30
  Jul 29 17:07:22.215: INFO: Updating deployment webserver-deployment
  Jul 29 17:07:22.215: INFO: Waiting for the replicasets of deployment "webserver-deployment" to have desired number of replicas
  Jul 29 17:07:22.265: INFO: Verifying that first rollout's replicaset has .spec.replicas = 20
  Jul 29 17:07:22.324: INFO: Verifying that second rollout's replicaset has .spec.replicas = 13
  Jul 29 17:07:22.408: INFO: Deployment "webserver-deployment":
  &Deployment{ObjectMeta:{webserver-deployment  deployment-4065  d8fad474-d58e-40d4-80f4-04b51486bead 37987 3 2023-07-29 17:07:15 +0000 UTC <nil> <nil> map[name:httpd] map[deployment.kubernetes.io/revision:2] [] [] [{e2e.test Update apps/v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Available\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:unavailableReplicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*30,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0040c5278 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:2,MaxSurge:3,},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:3,Replicas:13,UpdatedReplicas:5,AvailableReplicas:8,UnavailableReplicas:5,Conditions:[]DeploymentCondition{DeploymentCondition{Type:Progressing,Status:True,Reason:ReplicaSetUpdated,Message:ReplicaSet "webserver-deployment-7b75d79cf5" is progressing.,LastUpdateTime:2023-07-29 17:07:20 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,},DeploymentCondition{Type:Available,Status:False,Reason:MinimumReplicasUnavailable,Message:Deployment does not have minimum availability.,LastUpdateTime:2023-07-29 17:07:22 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,},},ReadyReplicas:8,CollisionCount:nil,},}

  Jul 29 17:07:22.542: INFO: New ReplicaSet "webserver-deployment-7b75d79cf5" of Deployment "webserver-deployment":
  &ReplicaSet{ObjectMeta:{webserver-deployment-7b75d79cf5  deployment-4065  58df2087-5c2e-458a-b1f4-64f5538746d5 37983 3 2023-07-29 17:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:2] [{apps/v1 Deployment webserver-deployment d8fad474-d58e-40d4-80f4-04b51486bead 0xc00393dc87 0xc00393dc88}] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:status":{"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8fad474-d58e-40d4-80f4-04b51486bead\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*13,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 7b75d79cf5,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [] [] []} {[] [] [{httpd webserver:404 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393dd28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:5,FullyLabeledReplicas:5,ObservedGeneration:2,ReadyReplicas:0,AvailableReplicas:0,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 17:07:22.543: INFO: All old ReplicaSets of Deployment "webserver-deployment":
  Jul 29 17:07:22.544: INFO: &ReplicaSet{ObjectMeta:{webserver-deployment-67bd4bf6dc  deployment-4065  ce5c1986-9a71-4142-afa8-7117e81b33ee 37977 3 2023-07-29 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[deployment.kubernetes.io/desired-replicas:30 deployment.kubernetes.io/max-replicas:33 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment webserver-deployment d8fad474-d58e-40d4-80f4-04b51486bead 0xc00393db97 0xc00393db98}] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status} {kube-controller-manager Update apps/v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"d8fad474-d58e-40d4-80f4-04b51486bead\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} }]},Spec:ReplicaSetSpec{Replicas:*20,Selector:&v1.LabelSelector{MatchLabels:map[string]string{name: httpd,pod-template-hash: 67bd4bf6dc,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc00393dc28 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:8,FullyLabeledReplicas:8,ObservedGeneration:2,ReadyReplicas:8,AvailableReplicas:8,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 17:07:22.674: INFO: Pod "webserver-deployment-67bd4bf6dc-5bnpz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5bnpz webserver-deployment-67bd4bf6dc- deployment-4065  0a63431f-a231-41d7-b8d8-fa3a714c57b9 38022 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462c227 0xc00462c228}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bv4m8,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bv4m8,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.675: INFO: Pod "webserver-deployment-67bd4bf6dc-5xvsz" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-5xvsz webserver-deployment-67bd4bf6dc- deployment-4065  4dc8949a-b2a9-4e1d-bd57-f02a15993366 38028 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462c367 0xc00462c368}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kx6pc,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kx6pc,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:,StartTime:2023-07-29 17:07:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.676: INFO: Pod "webserver-deployment-67bd4bf6dc-65zx4" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-65zx4 webserver-deployment-67bd4bf6dc- deployment-4065  bb4babf4-2a0d-49a1-a319-34bac4097b27 37879 0 2023-07-29 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462c537 0xc00462c538}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.196\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hlw4g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hlw4g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.39,PodIP:10.233.64.196,StartTime:2023-07-29 17:07:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:07:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3316871073bdb4a719568a23e62d76e7b4bb9eac03a87ba5c792c8e535f9474c,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.196,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.677: INFO: Pod "webserver-deployment-67bd4bf6dc-68xxl" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-68xxl webserver-deployment-67bd4bf6dc- deployment-4065  f15a48b0-cadc-4ff8-b89f-d04fd933a7ca 37874 0 2023-07-29 17:07:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462c727 0xc00462c728}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.249\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwt6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwt6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:10.233.65.249,StartTime:2023-07-29 17:07:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:07:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://617b382d8ccc4c87872e7d4f4c46a895b4b86a00102ab10083eba83950516b13,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.249,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.678: INFO: Pod "webserver-deployment-67bd4bf6dc-6td8c" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-6td8c webserver-deployment-67bd4bf6dc- deployment-4065  54f9a00c-289e-43ab-b2de-2f64ec3bc05f 37856 0 2023-07-29 17:07:16 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462c917 0xc00462c918}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:16 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.213\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-56q6b,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-56q6b,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.39,PodIP:10.233.64.213,StartTime:2023-07-29 17:07:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:07:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://be4480d9fc5e19d4b780e73692ce382c4d9e99bc0e65e14c7c97c9e5f0ea2fec,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.213,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.679: INFO: Pod "webserver-deployment-67bd4bf6dc-7x6c2" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-7x6c2 webserver-deployment-67bd4bf6dc- deployment-4065  4523f602-747c-4b84-b19b-c4d3952e7043 37992 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462cb07 0xc00462cb08}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-tjrdl,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-tjrdl,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:,StartTime:2023-07-29 17:07:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.679: INFO: Pod "webserver-deployment-67bd4bf6dc-9fr8h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-9fr8h webserver-deployment-67bd4bf6dc- deployment-4065  e210fc5c-5006-44b6-bfac-6326b34ec43e 38016 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462ccd7 0xc00462ccd8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k2r6g,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k2r6g,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.680: INFO: Pod "webserver-deployment-67bd4bf6dc-d69tw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-d69tw webserver-deployment-67bd4bf6dc- deployment-4065  579216cf-5477-415c-b31a-745801cda5ca 38023 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462cff0 0xc00462cff1}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-6rttb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-6rttb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.681: INFO: Pod "webserver-deployment-67bd4bf6dc-dwr64" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-dwr64 webserver-deployment-67bd4bf6dc- deployment-4065  c0117609-7c89-4ed0-bb6d-2dbc40515fef 37842 0 2023-07-29 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462d2c7 0xc00462d2c8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.41\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qgkrm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qgkrm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:16 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:10.233.65.41,StartTime:2023-07-29 17:07:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:07:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://3e7fb8ba52f4102463fee8479fd0d9f4cd15e929b0d4d4a7d4c1d7c92707d8bb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.41,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.682: INFO: Pod "webserver-deployment-67bd4bf6dc-hsv6r" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-hsv6r webserver-deployment-67bd4bf6dc- deployment-4065  2f60d1df-e087-463f-a503-3ad1c836c551 37867 0 2023-07-29 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462d4b7 0xc00462d4b8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.140\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-9cmw4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-9cmw4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.140,StartTime:2023-07-29 17:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:07:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://53e9aa5592929c3c80d40fe9590de9897c2b797767d9b914579a7434d88a7b41,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.140,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.683: INFO: Pod "webserver-deployment-67bd4bf6dc-jbxp8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-jbxp8 webserver-deployment-67bd4bf6dc- deployment-4065  30f1dab9-c212-40f0-a84e-6c35a8299f4e 38019 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462d6a7 0xc00462d6a8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-nfjdz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-nfjdz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.684: INFO: Pod "webserver-deployment-67bd4bf6dc-lq52q" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lq52q webserver-deployment-67bd4bf6dc- deployment-4065  76c793e8-9853-4214-a7cb-b87cc8ca2c20 37869 0 2023-07-29 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462d810 0xc00462d811}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:19 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.62\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-vxq5c,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-vxq5c,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:16 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:19 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.62,StartTime:2023-07-29 17:07:16 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:07:18 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://dee3f6537827793befd2a4b0e898024c2570d847b561e778941c02c00eb1291f,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.62,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.685: INFO: Pod "webserver-deployment-67bd4bf6dc-lwt68" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-lwt68 webserver-deployment-67bd4bf6dc- deployment-4065  d3bdcea8-e5de-4508-ae6f-52c8cc09af58 38027 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462d9f7 0xc00462d9f8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xmtjs,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xmtjs,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.689: INFO: Pod "webserver-deployment-67bd4bf6dc-m2gxs" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-m2gxs webserver-deployment-67bd4bf6dc- deployment-4065  14b589c2-a441-4fbb-9ddd-19214b29a1f9 38017 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462db37 0xc00462db38}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-8gqzz,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-8gqzz,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.689: INFO: Pod "webserver-deployment-67bd4bf6dc-p5pgr" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-p5pgr webserver-deployment-67bd4bf6dc- deployment-4065  42225fd8-27a3-4bdb-b074-ceee6c4c3191 38021 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462dca0 0xc00462dca1}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-7mdlb,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-7mdlb,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.690: INFO: Pod "webserver-deployment-67bd4bf6dc-s2f6r" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-s2f6r webserver-deployment-67bd4bf6dc- deployment-4065  03fb2e1b-be7b-4d5d-8e28-c0b7ae908a53 38000 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462ddd7 0xc00462ddd8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-79bpt,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-79bpt,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.691: INFO: Pod "webserver-deployment-67bd4bf6dc-tkgrw" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-tkgrw webserver-deployment-67bd4bf6dc- deployment-4065  c0c5a759-7420-4159-a90c-17013d9b0f8c 38026 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00462df40 0xc00462df41}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-xbrst,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-xbrst,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.692: INFO: Pod "webserver-deployment-67bd4bf6dc-vc5nd" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-vc5nd webserver-deployment-67bd4bf6dc- deployment-4065  0c714f01-ac9b-497e-b7f9-9a56aa169ab1 37858 0 2023-07-29 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00536e077 0xc00536e078}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.64.11\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-kwww4,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-kwww4,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.39,PodIP:10.233.64.11,StartTime:2023-07-29 17:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:07:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://9911ac0b2723f3eec76c1819ba221efb9fa39b50f570e60cd2ee1c6e82d989ee,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.64.11,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.692: INFO: Pod "webserver-deployment-67bd4bf6dc-xjqxv" is not available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xjqxv webserver-deployment-67bd4bf6dc- deployment-4065  ddcde0ef-0751-40d5-ab6a-7dce7996f1e2 38007 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00536e267 0xc00536e268}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sk848,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sk848,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:,StartTime:2023-07-29 17:07:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.693: INFO: Pod "webserver-deployment-67bd4bf6dc-xmzgb" is available:
  &Pod{ObjectMeta:{webserver-deployment-67bd4bf6dc-xmzgb webserver-deployment-67bd4bf6dc- deployment-4065  e91b72eb-6ef5-44bd-9dd5-24174ea9e979 37843 0 2023-07-29 17:07:15 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:67bd4bf6dc] map[] [{apps/v1 ReplicaSet webserver-deployment-67bd4bf6dc ce5c1986-9a71-4142-afa8-7117e81b33ee 0xc00536e437 0xc00536e438}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:15 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"ce5c1986-9a71-4142-afa8-7117e81b33ee\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:18 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.65.52\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-qcr7l,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-qcr7l,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:18 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:15 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:10.233.65.52,StartTime:2023-07-29 17:07:15 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:07:17 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://967119d41dc617b382d4b0034572c8f55909f640765d64c8b1d858923087a4cb,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.65.52,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.694: INFO: Pod "webserver-deployment-7b75d79cf5-2k4dh" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-2k4dh webserver-deployment-7b75d79cf5- deployment-4065  1d425ca8-4f80-497b-83d4-ad32a2a4f5b5 37932 0 2023-07-29 17:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536e627 0xc00536e628}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jrfzv,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jrfzv,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.39,PodIP:,StartTime:2023-07-29 17:07:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.695: INFO: Pod "webserver-deployment-7b75d79cf5-4ls5d" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-4ls5d webserver-deployment-7b75d79cf5- deployment-4065  2a7394a3-506b-4183-a78d-5fbcf47a4d44 38009 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536e817 0xc00536e818}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bnq5z,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bnq5z,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:,StartTime:2023-07-29 17:07:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.696: INFO: Pod "webserver-deployment-7b75d79cf5-8bpkm" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8bpkm webserver-deployment-7b75d79cf5- deployment-4065  2dc69f85-232b-4b69-926c-b73b526c0140 38014 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536ea07 0xc00536ea08}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-k95ht,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-k95ht,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.697: INFO: Pod "webserver-deployment-7b75d79cf5-8jjcg" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-8jjcg webserver-deployment-7b75d79cf5- deployment-4065  c6744f92-07bb-45c8-a226-97aba41b2970 38029 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536eb57 0xc00536eb58}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-hwx2t,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-hwx2t,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.697: INFO: Pod "webserver-deployment-7b75d79cf5-gm96h" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-gm96h webserver-deployment-7b75d79cf5- deployment-4065  80822717-3d00-4fdd-9131-c14e6ae9e2a4 37894 0 2023-07-29 17:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536ecd0 0xc00536ecd1}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-sz5xm,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-sz5xm,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:,StartTime:2023-07-29 17:07:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.698: INFO: Pod "webserver-deployment-7b75d79cf5-hhtc4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-hhtc4 webserver-deployment-7b75d79cf5- deployment-4065  7941f1a4-0b2e-4e5a-ace2-eaf5814a0f8a 37922 0 2023-07-29 17:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536eeb7 0xc00536eeb8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bkbts,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bkbts,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:,StartTime:2023-07-29 17:07:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.699: INFO: Pod "webserver-deployment-7b75d79cf5-kbqqf" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-kbqqf webserver-deployment-7b75d79cf5- deployment-4065  ab94a2fd-75d3-42ac-90b1-a90574e772fc 37899 0 2023-07-29 17:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536f0a7 0xc00536f0a8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-4lf6v,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-4lf6v,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-1,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.39,PodIP:,StartTime:2023-07-29 17:07:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.700: INFO: Pod "webserver-deployment-7b75d79cf5-s7rzt" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-s7rzt webserver-deployment-7b75d79cf5- deployment-4065  03fb834b-631a-43c1-a3af-9ab72f15c156 38015 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536f297 0xc00536f298}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8mlg,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8mlg,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.700: INFO: Pod "webserver-deployment-7b75d79cf5-srz78" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-srz78 webserver-deployment-7b75d79cf5- deployment-4065  18711981-0baa-44c1-89fa-c79c4e9b9895 38012 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536f410 0xc00536f411}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-bfm99,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-bfm99,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:22 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:,StartTime:2023-07-29 17:07:22 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.702: INFO: Pod "webserver-deployment-7b75d79cf5-tjl45" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-tjl45 webserver-deployment-7b75d79cf5- deployment-4065  9da32d0a-28ff-492a-9f80-37a9cd535e58 37901 0 2023-07-29 17:07:20 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536f5f7 0xc00536f5f8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:07:20 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-h8lb9,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-h8lb9,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-2,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:ContainersReady,Status:False,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:ContainersNotReady,Message:containers with unready status: [httpd],},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:07:20 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.49,PodIP:,StartTime:2023-07-29 17:07:20 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:&ContainerStateWaiting{Reason:ContainerCreating,Message:,},Running:nil,Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:false,RestartCount:0,Image:webserver:404,ImageID:,ContainerID:,Started:*false,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.703: INFO: Pod "webserver-deployment-7b75d79cf5-wfhg4" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-wfhg4 webserver-deployment-7b75d79cf5- deployment-4065  56bf1bb8-72a6-48e9-8c9d-6b21b4936901 38018 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536f7e7 0xc00536f7e8}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-lcrkh,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-lcrkh,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.704: INFO: Pod "webserver-deployment-7b75d79cf5-zkkv8" is not available:
  &Pod{ObjectMeta:{webserver-deployment-7b75d79cf5-zkkv8 webserver-deployment-7b75d79cf5- deployment-4065  a7672d12-fe47-4514-a617-dd06a9e68646 38025 0 2023-07-29 17:07:22 +0000 UTC <nil> <nil> map[name:httpd pod-template-hash:7b75d79cf5] map[] [{apps/v1 ReplicaSet webserver-deployment-7b75d79cf5 58df2087-5c2e-458a-b1f4-64f5538746d5 0xc00536f957 0xc00536f958}] [] [{kube-controller-manager Update v1 2023-07-29 17:07:22 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"58df2087-5c2e-458a-b1f4-64f5538746d5\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} }]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-jtrmk,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:webserver:404,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-jtrmk,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Pending,Conditions:[]PodCondition{},Message:,Reason:,HostIP:,PodIP:,StartTime:<nil>,ContainerStatuses:[]ContainerStatus{},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:07:22.704: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-4065" for this suite. @ 07/29/23 17:07:22.763
• [7.252 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-network] Services should be able to create a functioning NodePort service [Conformance]
test/e2e/network/service.go:1280
  STEP: Creating a kubernetes client @ 07/29/23 17:07:22.986
  Jul 29 17:07:22.987: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 17:07:22.993
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:23.204
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:23.209
  STEP: creating service nodeport-test with type=NodePort in namespace services-5196 @ 07/29/23 17:07:23.215
  STEP: creating replication controller nodeport-test in namespace services-5196 @ 07/29/23 17:07:23.371
  I0729 17:07:23.479276      13 runners.go:194] Created replication controller with name: nodeport-test, namespace: services-5196, replica count: 2
  I0729 17:07:26.712319      13 runners.go:194] nodeport-test Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 17:07:26.712: INFO: Creating new exec pod
  Jul 29 17:07:29.793: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5196 exec execpod6nsrl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul 29 17:07:30.273: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 29 17:07:30.275: INFO: stdout: ""
  Jul 29 17:07:31.278: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5196 exec execpod6nsrl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 nodeport-test 80'
  Jul 29 17:07:31.571: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 nodeport-test 80\nConnection to nodeport-test 80 port [tcp/http] succeeded!\n"
  Jul 29 17:07:31.571: INFO: stdout: "nodeport-test-m9n77"
  Jul 29 17:07:31.573: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5196 exec execpod6nsrl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.7.204 80'
  Jul 29 17:07:31.863: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.7.204 80\nConnection to 10.233.7.204 80 port [tcp/http] succeeded!\n"
  Jul 29 17:07:31.863: INFO: stdout: "nodeport-test-mqwd2"
  Jul 29 17:07:31.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5196 exec execpod6nsrl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.39 31057'
  Jul 29 17:07:32.152: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.39 31057\nConnection to 192.168.121.39 31057 port [tcp/*] succeeded!\n"
  Jul 29 17:07:32.152: INFO: stdout: "nodeport-test-mqwd2"
  Jul 29 17:07:32.154: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5196 exec execpod6nsrl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.49 31057'
  Jul 29 17:07:32.415: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.49 31057\nConnection to 192.168.121.49 31057 port [tcp/*] succeeded!\n"
  Jul 29 17:07:32.415: INFO: stdout: ""
  Jul 29 17:07:33.416: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-5196 exec execpod6nsrl -- /bin/sh -x -c echo hostName | nc -v -t -w 2 192.168.121.49 31057'
  Jul 29 17:07:33.676: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 192.168.121.49 31057\nConnection to 192.168.121.49 31057 port [tcp/*] succeeded!\n"
  Jul 29 17:07:33.676: INFO: stdout: "nodeport-test-mqwd2"
  Jul 29 17:07:33.676: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "services-5196" for this suite. @ 07/29/23 17:07:33.691
• [10.726 seconds]
------------------------------
[sig-apps] Daemon set [Serial] should run and stop complex daemon [Conformance]
test/e2e/apps/daemon_set.go:205
  STEP: Creating a kubernetes client @ 07/29/23 17:07:33.707
  Jul 29 17:07:33.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 17:07:33.711
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:33.732
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:33.736
  Jul 29 17:07:33.787: INFO: Creating daemon "daemon-set" with a node selector
  STEP: Initially, daemon pods should not be running on any nodes. @ 07/29/23 17:07:33.796
  Jul 29 17:07:33.802: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:33.802: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Change node label to blue, check that daemon pod is launched. @ 07/29/23 17:07:33.802
  Jul 29 17:07:33.850: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:33.850: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 17:07:34.864: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:34.864: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 17:07:35.863: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:35.864: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 17:07:36.859: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 17:07:36.860: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Update the node label to green, and wait for daemons to be unscheduled @ 07/29/23 17:07:36.871
  Jul 29 17:07:36.919: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 17:07:36.920: INFO: Number of running nodes: 0, number of available pods: 1 in daemonset daemon-set
  Jul 29 17:07:37.926: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:37.926: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  STEP: Update DaemonSet node selector to green, and change its update strategy to RollingUpdate @ 07/29/23 17:07:37.926
  Jul 29 17:07:37.950: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:37.950: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 17:07:38.969: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:38.969: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 17:07:39.957: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:39.957: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 17:07:40.964: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 1
  Jul 29 17:07:40.964: INFO: Number of running nodes: 1, number of available pods: 1 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 17:07:40.985
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-5401, will wait for the garbage collector to delete the pods @ 07/29/23 17:07:40.986
  Jul 29 17:07:41.057: INFO: Deleting DaemonSet.extensions daemon-set took: 12.293296ms
  Jul 29 17:07:41.158: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.196227ms
  Jul 29 17:07:42.575: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:07:42.576: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 17:07:42.585: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"38470"},"items":null}

  Jul 29 17:07:42.590: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"38470"},"items":null}

  Jul 29 17:07:42.645: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-5401" for this suite. @ 07/29/23 17:07:42.653
• [8.958 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl diff should check if kubectl diff finds a difference for Deployments [Conformance]
test/e2e/kubectl/kubectl.go:996
  STEP: Creating a kubernetes client @ 07/29/23 17:07:42.671
  Jul 29 17:07:42.671: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:07:42.673
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:42.696
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:42.7
  STEP: create deployment with httpd image @ 07/29/23 17:07:42.705
  Jul 29 17:07:42.706: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6477 create -f -'
  Jul 29 17:07:43.422: INFO: stderr: ""
  Jul 29 17:07:43.422: INFO: stdout: "deployment.apps/httpd-deployment created\n"
  STEP: verify diff finds difference between live and declared image @ 07/29/23 17:07:43.422
  Jul 29 17:07:43.422: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6477 diff -f -'
  Jul 29 17:07:43.929: INFO: rc: 1
  Jul 29 17:07:43.929: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-6477 delete -f -'
  Jul 29 17:07:44.210: INFO: stderr: ""
  Jul 29 17:07:44.210: INFO: stdout: "deployment.apps \"httpd-deployment\" deleted\n"
  Jul 29 17:07:44.219: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-6477" for this suite. @ 07/29/23 17:07:44.232
• [1.578 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes should support (non-root,0644,tmpfs) [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:127
  STEP: Creating a kubernetes client @ 07/29/23 17:07:44.253
  Jul 29 17:07:44.254: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:07:44.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:44.312
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:44.316
  STEP: Creating a pod to test emptydir 0644 on tmpfs @ 07/29/23 17:07:44.342
  STEP: Saw pod success @ 07/29/23 17:07:48.394
  Jul 29 17:07:48.400: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-96e26a33-aeca-4f7d-acf0-2d385e537366 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:07:48.411
  Jul 29 17:07:48.436: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-4405" for this suite. @ 07/29/23 17:07:48.449
• [4.207 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl replace should update a single-container pod's image  [Conformance]
test/e2e/kubectl/kubectl.go:1735
  STEP: Creating a kubernetes client @ 07/29/23 17:07:48.469
  Jul 29 17:07:48.470: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:07:48.472
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:48.501
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:48.505
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 17:07:48.51
  Jul 29 17:07:48.511: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-9420 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 29 17:07:48.655: INFO: stderr: ""
  Jul 29 17:07:48.655: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: verifying the pod e2e-test-httpd-pod is running @ 07/29/23 17:07:48.655
  STEP: verifying the pod e2e-test-httpd-pod was created @ 07/29/23 17:07:53.706
  Jul 29 17:07:53.707: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-9420 get pod e2e-test-httpd-pod -o json'
  Jul 29 17:07:53.862: INFO: stderr: ""
  Jul 29 17:07:53.862: INFO: stdout: "{\n    \"apiVersion\": \"v1\",\n    \"kind\": \"Pod\",\n    \"metadata\": {\n        \"creationTimestamp\": \"2023-07-29T17:07:48Z\",\n        \"labels\": {\n            \"run\": \"e2e-test-httpd-pod\"\n        },\n        \"name\": \"e2e-test-httpd-pod\",\n        \"namespace\": \"kubectl-9420\",\n        \"resourceVersion\": \"38570\",\n        \"uid\": \"3be794eb-2d9c-4b15-b664-aed1c5fcb150\"\n    },\n    \"spec\": {\n        \"containers\": [\n            {\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imagePullPolicy\": \"IfNotPresent\",\n                \"name\": \"e2e-test-httpd-pod\",\n                \"resources\": {},\n                \"terminationMessagePath\": \"/dev/termination-log\",\n                \"terminationMessagePolicy\": \"File\",\n                \"volumeMounts\": [\n                    {\n                        \"mountPath\": \"/var/run/secrets/kubernetes.io/serviceaccount\",\n                        \"name\": \"kube-api-access-f2p4k\",\n                        \"readOnly\": true\n                    }\n                ]\n            }\n        ],\n        \"dnsPolicy\": \"ClusterFirst\",\n        \"enableServiceLinks\": true,\n        \"nodeName\": \"ci9axai7aiv7-3\",\n        \"preemptionPolicy\": \"PreemptLowerPriority\",\n        \"priority\": 0,\n        \"restartPolicy\": \"Always\",\n        \"schedulerName\": \"default-scheduler\",\n        \"securityContext\": {},\n        \"serviceAccount\": \"default\",\n        \"serviceAccountName\": \"default\",\n        \"terminationGracePeriodSeconds\": 30,\n        \"tolerations\": [\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/not-ready\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            },\n            {\n                \"effect\": \"NoExecute\",\n                \"key\": \"node.kubernetes.io/unreachable\",\n                \"operator\": \"Exists\",\n                \"tolerationSeconds\": 300\n            }\n        ],\n        \"volumes\": [\n            {\n                \"name\": \"kube-api-access-f2p4k\",\n                \"projected\": {\n                    \"defaultMode\": 420,\n                    \"sources\": [\n                        {\n                            \"serviceAccountToken\": {\n                                \"expirationSeconds\": 3607,\n                                \"path\": \"token\"\n                            }\n                        },\n                        {\n                            \"configMap\": {\n                                \"items\": [\n                                    {\n                                        \"key\": \"ca.crt\",\n                                        \"path\": \"ca.crt\"\n                                    }\n                                ],\n                                \"name\": \"kube-root-ca.crt\"\n                            }\n                        },\n                        {\n                            \"downwardAPI\": {\n                                \"items\": [\n                                    {\n                                        \"fieldRef\": {\n                                            \"apiVersion\": \"v1\",\n                                            \"fieldPath\": \"metadata.namespace\"\n                                        },\n                                        \"path\": \"namespace\"\n                                    }\n                                ]\n                            }\n                        }\n                    ]\n                }\n            }\n        ]\n    },\n    \"status\": {\n        \"conditions\": [\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T17:07:48Z\",\n                \"status\": \"True\",\n                \"type\": \"Initialized\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T17:07:50Z\",\n                \"status\": \"True\",\n                \"type\": \"Ready\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T17:07:50Z\",\n                \"status\": \"True\",\n                \"type\": \"ContainersReady\"\n            },\n            {\n                \"lastProbeTime\": null,\n                \"lastTransitionTime\": \"2023-07-29T17:07:48Z\",\n                \"status\": \"True\",\n                \"type\": \"PodScheduled\"\n            }\n        ],\n        \"containerStatuses\": [\n            {\n                \"containerID\": \"cri-o://90a9d6867f2ba3962c6f9015131f3d459e026d77e345c62609758db8fefd77a4\",\n                \"image\": \"registry.k8s.io/e2e-test-images/httpd:2.4.38-4\",\n                \"imageID\": \"registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22\",\n                \"lastState\": {},\n                \"name\": \"e2e-test-httpd-pod\",\n                \"ready\": true,\n                \"restartCount\": 0,\n                \"started\": true,\n                \"state\": {\n                    \"running\": {\n                        \"startedAt\": \"2023-07-29T17:07:49Z\"\n                    }\n                }\n            }\n        ],\n        \"hostIP\": \"192.168.121.144\",\n        \"phase\": \"Running\",\n        \"podIP\": \"10.233.66.105\",\n        \"podIPs\": [\n            {\n                \"ip\": \"10.233.66.105\"\n            }\n        ],\n        \"qosClass\": \"BestEffort\",\n        \"startTime\": \"2023-07-29T17:07:48Z\"\n    }\n}\n"
  STEP: replace the image in the pod @ 07/29/23 17:07:53.862
  Jul 29 17:07:53.863: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-9420 replace -f -'
  Jul 29 17:07:54.436: INFO: stderr: ""
  Jul 29 17:07:54.436: INFO: stdout: "pod/e2e-test-httpd-pod replaced\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/busybox:1.29-4 @ 07/29/23 17:07:54.436
  Jul 29 17:07:54.446: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-9420 delete pods e2e-test-httpd-pod'
  Jul 29 17:07:56.687: INFO: stderr: ""
  Jul 29 17:07:56.687: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 17:07:56.687: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-9420" for this suite. @ 07/29/23 17:07:56.696
• [8.240 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] EndpointSliceMirroring should mirror a custom Endpoints resource through create update and delete [Conformance]
test/e2e/network/endpointslicemirroring.go:55
  STEP: Creating a kubernetes client @ 07/29/23 17:07:56.711
  Jul 29 17:07:56.712: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename endpointslicemirroring @ 07/29/23 17:07:56.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:07:56.788
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:07:56.793
  STEP: mirroring a new custom Endpoint @ 07/29/23 17:07:56.813
  Jul 29 17:07:56.842: INFO: Waiting for at least 1 EndpointSlice to exist, got 0
  STEP: mirroring an update to a custom Endpoint @ 07/29/23 17:07:58.85
  Jul 29 17:07:58.865: INFO: Expected EndpointSlice to have 10.2.3.4 as address, got 10.1.2.3
  STEP: mirroring deletion of a custom Endpoint @ 07/29/23 17:08:00.873
  Jul 29 17:08:00.891: INFO: Waiting for 0 EndpointSlices to exist, got 1
  Jul 29 17:08:02.900: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "endpointslicemirroring-6554" for this suite. @ 07/29/23 17:08:02.91
• [6.215 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] DisruptionController should update/patch PodDisruptionBudget status [Conformance]
test/e2e/apps/disruption.go:164
  STEP: Creating a kubernetes client @ 07/29/23 17:08:02.934
  Jul 29 17:08:02.934: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename disruption @ 07/29/23 17:08:02.936
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:02.968
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:02.976
  STEP: Waiting for the pdb to be processed @ 07/29/23 17:08:02.995
  STEP: Updating PodDisruptionBudget status @ 07/29/23 17:08:05.023
  STEP: Waiting for all pods to be running @ 07/29/23 17:08:05.034
  Jul 29 17:08:05.041: INFO: running pods: 0 < 1
  STEP: locating a running pod @ 07/29/23 17:08:07.05
  STEP: Waiting for the pdb to be processed @ 07/29/23 17:08:07.074
  STEP: Patching PodDisruptionBudget status @ 07/29/23 17:08:07.091
  STEP: Waiting for the pdb to be processed @ 07/29/23 17:08:07.111
  Jul 29 17:08:07.127: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "disruption-8351" for this suite. @ 07/29/23 17:08:07.141
• [4.227 seconds]
------------------------------
SSSSSSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide node allocatable (cpu) as default cpu limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:250
  STEP: Creating a kubernetes client @ 07/29/23 17:08:07.163
  Jul 29 17:08:07.163: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:08:07.165
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:07.2
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:07.205
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:08:07.21
  STEP: Saw pod success @ 07/29/23 17:08:11.255
  Jul 29 17:08:11.267: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-c2515123-18f5-425a-939b-aad3c270aad8 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:08:11.285
  Jul 29 17:08:11.320: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-7865" for this suite. @ 07/29/23 17:08:11.331
• [4.180 seconds]
------------------------------
SSSS
------------------------------
[sig-apps] ReplicaSet should serve a basic image on each replica with a public image  [Conformance]
test/e2e/apps/replica_set.go:111
  STEP: Creating a kubernetes client @ 07/29/23 17:08:11.345
  Jul 29 17:08:11.345: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replicaset @ 07/29/23 17:08:11.348
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:11.376
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:11.38
  Jul 29 17:08:11.384: INFO: Creating ReplicaSet my-hostname-basic-97775983-e31f-42ec-8942-ff172901f30f
  Jul 29 17:08:11.407: INFO: Pod name my-hostname-basic-97775983-e31f-42ec-8942-ff172901f30f: Found 0 pods out of 1
  Jul 29 17:08:16.416: INFO: Pod name my-hostname-basic-97775983-e31f-42ec-8942-ff172901f30f: Found 1 pods out of 1
  Jul 29 17:08:16.418: INFO: Ensuring a pod for ReplicaSet "my-hostname-basic-97775983-e31f-42ec-8942-ff172901f30f" is running
  Jul 29 17:08:16.426: INFO: Pod "my-hostname-basic-97775983-e31f-42ec-8942-ff172901f30f-rj4ft" is running (conditions: [{Type:Initialized Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 17:08:11 +0000 UTC Reason: Message:} {Type:Ready Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 17:08:12 +0000 UTC Reason: Message:} {Type:ContainersReady Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 17:08:12 +0000 UTC Reason: Message:} {Type:PodScheduled Status:True LastProbeTime:0001-01-01 00:00:00 +0000 UTC LastTransitionTime:2023-07-29 17:08:11 +0000 UTC Reason: Message:}])
  Jul 29 17:08:16.427: INFO: Trying to dial the pod
  STEP: trying to dial each unique pod @ 07/29/23 17:08:16.427
  Jul 29 17:08:16.454: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replicaset-3787" for this suite. @ 07/29/23 17:08:16.464
• [5.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update annotations on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:163
  STEP: Creating a kubernetes client @ 07/29/23 17:08:16.485
  Jul 29 17:08:16.486: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:08:16.488
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:16.542
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:16.548
  STEP: Creating the pod @ 07/29/23 17:08:16.554
  Jul 29 17:08:19.177: INFO: Successfully updated pod "annotationupdate45435d81-a6fb-4bb4-9fe1-a88121b40cde"
  Jul 29 17:08:21.222: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-748" for this suite. @ 07/29/23 17:08:21.232
• [4.764 seconds]
------------------------------
SSSSSS
------------------------------
[sig-api-machinery] Namespaces [Serial] should apply an update to a Namespace [Conformance]
test/e2e/apimachinery/namespace.go:370
  STEP: Creating a kubernetes client @ 07/29/23 17:08:21.253
  Jul 29 17:08:21.253: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename namespaces @ 07/29/23 17:08:21.256
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:21.317
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:21.324
  STEP: Updating Namespace "namespaces-8438" @ 07/29/23 17:08:21.331
  Jul 29 17:08:21.357: INFO: Namespace "namespaces-8438" now has labels, map[string]string{"e2e-framework":"namespaces", "e2e-run":"17f5218d-1ccd-4f9b-86d8-e116b9c61702", "kubernetes.io/metadata.name":"namespaces-8438", "namespaces-8438":"updated", "pod-security.kubernetes.io/enforce":"baseline"}
  Jul 29 17:08:21.357: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "namespaces-8438" for this suite. @ 07/29/23 17:08:21.367
• [0.134 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group and version but different kinds [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:357
  STEP: Creating a kubernetes client @ 07/29/23 17:08:21.39
  Jul 29 17:08:21.390: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:08:21.392
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:21.44
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:21.452
  STEP: CRs in the same group and version but different kinds (two CRDs) show up in OpenAPI documentation @ 07/29/23 17:08:21.459
  Jul 29 17:08:21.460: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:08:23.596: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:08:32.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-3241" for this suite. @ 07/29/23 17:08:32.252
• [10.873 seconds]
------------------------------
SSSSSSS
------------------------------
[sig-node] Variable Expansion should allow substituting values in a container's args [NodeConformance] [Conformance]
test/e2e/common/node/expansion.go:95
  STEP: Creating a kubernetes client @ 07/29/23 17:08:32.264
  Jul 29 17:08:32.264: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 17:08:32.267
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:32.299
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:32.303
  STEP: Creating a pod to test substitution in container's args @ 07/29/23 17:08:32.306
  STEP: Saw pod success @ 07/29/23 17:08:36.34
  Jul 29 17:08:36.348: INFO: Trying to get logs from node ci9axai7aiv7-3 pod var-expansion-3d91a892-df07-4950-99ee-813cc28d34d2 container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 17:08:36.38
  Jul 29 17:08:36.411: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "var-expansion-5211" for this suite. @ 07/29/23 17:08:36.42
• [4.168 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-network] Networking Granular Checks: Pods should function for node-pod communication: http [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/network/networking.go:107
  STEP: Creating a kubernetes client @ 07/29/23 17:08:36.434
  Jul 29 17:08:36.434: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pod-network-test @ 07/29/23 17:08:36.437
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:08:36.472
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:08:36.478
  STEP: Performing setup for networking test in namespace pod-network-test-7831 @ 07/29/23 17:08:36.485
  STEP: creating a selector @ 07/29/23 17:08:36.485
  STEP: Creating the service pods in kubernetes @ 07/29/23 17:08:36.485
  Jul 29 17:08:36.485: INFO: Waiting up to 10m0s for all (but 0) nodes to be schedulable
  STEP: Creating test pods @ 07/29/23 17:08:58.72
  Jul 29 17:09:00.770: INFO: Setting MaxTries for pod polling to 39 for networking test based on endpoint count 3
  Jul 29 17:09:00.770: INFO: Going to poll 10.233.64.170 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 17:09:00.777: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.64.170:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7831 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:09:00.777: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:09:00.779: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:09:00.780: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7831/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.64.170%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 17:09:00.915: INFO: Found all 1 expected endpoints: [netserver-0]
  Jul 29 17:09:00.915: INFO: Going to poll 10.233.65.92 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 17:09:00.924: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.65.92:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7831 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:09:00.924: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:09:00.925: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:09:00.925: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7831/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.65.92%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 17:09:01.020: INFO: Found all 1 expected endpoints: [netserver-1]
  Jul 29 17:09:01.020: INFO: Going to poll 10.233.66.150 on port 8083 at least 0 times, with a maximum of 39 tries before failing
  Jul 29 17:09:01.027: INFO: ExecWithOptions {Command:[/bin/sh -c curl -g -q -s --max-time 15 --connect-timeout 1 http://10.233.66.150:8083/hostName | grep -v '^\s*$'] Namespace:pod-network-test-7831 PodName:host-test-container-pod ContainerName:agnhost-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:09:01.027: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:09:01.028: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:09:01.029: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/pod-network-test-7831/pods/host-test-container-pod/exec?command=%2Fbin%2Fsh&command=-c&command=curl+-g+-q+-s+--max-time+15+--connect-timeout+1+http%3A%2F%2F10.233.66.150%3A8083%2FhostName+%7C+grep+-v+%27%5E%5Cs%2A%24%27&container=agnhost-container&container=agnhost-container&stderr=true&stdout=true)
  Jul 29 17:09:01.158: INFO: Found all 1 expected endpoints: [netserver-2]
  Jul 29 17:09:01.159: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pod-network-test-7831" for this suite. @ 07/29/23 17:09:01.17
• [24.752 seconds]
------------------------------
S
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate pod and apply defaults after mutation [Conformance]
test/e2e/apimachinery/webhook.go:260
  STEP: Creating a kubernetes client @ 07/29/23 17:09:01.187
  Jul 29 17:09:01.187: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:09:01.189
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:09:01.223
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:09:01.229
  STEP: Setting up server cert @ 07/29/23 17:09:01.269
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:09:02.12
  STEP: Deploying the webhook pod @ 07/29/23 17:09:02.133
  STEP: Wait for the deployment to be ready @ 07/29/23 17:09:02.152
  Jul 29 17:09:02.168: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  STEP: Deploying the webhook service @ 07/29/23 17:09:04.197
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:09:04.225
  Jul 29 17:09:05.226: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the mutating pod webhook via the AdmissionRegistration API @ 07/29/23 17:09:05.238
  STEP: create a pod that should be updated by the webhook @ 07/29/23 17:09:05.274
  Jul 29 17:09:05.310: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3044" for this suite. @ 07/29/23 17:09:05.428
  STEP: Destroying namespace "webhook-markers-2551" for this suite. @ 07/29/23 17:09:05.444
• [4.275 seconds]
------------------------------
SSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should support remote command execution over websockets [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:537
  STEP: Creating a kubernetes client @ 07/29/23 17:09:05.463
  Jul 29 17:09:05.463: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:09:05.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:09:05.513
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:09:05.523
  Jul 29 17:09:05.530: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: creating the pod @ 07/29/23 17:09:05.532
  STEP: submitting the pod to kubernetes @ 07/29/23 17:09:05.532
  Jul 29 17:09:07.712: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-9948" for this suite. @ 07/29/23 17:09:07.732
• [2.303 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should not be able to mutate or prevent deletion of webhook configuration objects [Conformance]
test/e2e/apimachinery/webhook.go:272
  STEP: Creating a kubernetes client @ 07/29/23 17:09:07.768
  Jul 29 17:09:07.768: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:09:07.771
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:09:07.828
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:09:07.835
  STEP: Setting up server cert @ 07/29/23 17:09:07.891
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:09:08.874
  STEP: Deploying the webhook pod @ 07/29/23 17:09:08.884
  STEP: Wait for the deployment to be ready @ 07/29/23 17:09:08.909
  Jul 29 17:09:08.923: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 17:09:10.939
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:09:10.955
  Jul 29 17:09:11.958: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering a validating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/29/23 17:09:11.97
  STEP: Registering a mutating webhook on ValidatingWebhookConfiguration and MutatingWebhookConfiguration objects, via the AdmissionRegistration API @ 07/29/23 17:09:12.015
  STEP: Creating a dummy validating-webhook-configuration object @ 07/29/23 17:09:12.037
  STEP: Deleting the validating-webhook-configuration, which should be possible to remove @ 07/29/23 17:09:12.048
  STEP: Creating a dummy mutating-webhook-configuration object @ 07/29/23 17:09:12.063
  STEP: Deleting the mutating-webhook-configuration, which should be possible to remove @ 07/29/23 17:09:12.078
  Jul 29 17:09:12.089: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-1977" for this suite. @ 07/29/23 17:09:12.22
  STEP: Destroying namespace "webhook-markers-9469" for this suite. @ 07/29/23 17:09:12.235
• [4.478 seconds]
------------------------------
[sig-storage] Projected configMap should be consumable in multiple volumes in the same pod [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:375
  STEP: Creating a kubernetes client @ 07/29/23 17:09:12.248
  Jul 29 17:09:12.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:09:12.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:09:12.286
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:09:12.29
  STEP: Creating configMap with name projected-configmap-test-volume-d61f1b4a-07e5-4088-80e0-6690cc9bafed @ 07/29/23 17:09:12.294
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:09:12.304
  STEP: Saw pod success @ 07/29/23 17:09:16.348
  Jul 29 17:09:16.358: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-projected-configmaps-ee552ac8-8b32-48b4-b244-a8c7b19ba6bd container projected-configmap-volume-test: <nil>
  STEP: delete the pod @ 07/29/23 17:09:16.372
  Jul 29 17:09:16.417: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-1727" for this suite. @ 07/29/23 17:09:16.428
• [4.190 seconds]
------------------------------
SS
------------------------------
[sig-apps] CronJob should not schedule new jobs when ForbidConcurrent [Slow] [Conformance]
test/e2e/apps/cronjob.go:125
  STEP: Creating a kubernetes client @ 07/29/23 17:09:16.439
  Jul 29 17:09:16.439: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 17:09:16.441
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:09:16.469
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:09:16.474
  STEP: Creating a ForbidConcurrent cronjob @ 07/29/23 17:09:16.479
  STEP: Ensuring a job is scheduled @ 07/29/23 17:09:16.494
  STEP: Ensuring exactly one is scheduled @ 07/29/23 17:10:00.501
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/29/23 17:10:00.514
  STEP: Ensuring no more jobs are scheduled @ 07/29/23 17:10:00.52
  STEP: Removing cronjob @ 07/29/23 17:15:00.534
  Jul 29 17:15:00.547: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "cronjob-2445" for this suite. @ 07/29/23 17:15:00.558
• [344.133 seconds]
------------------------------
S
------------------------------
[sig-apps] ReplicationController should surface a failure condition on a common issue like exceeded quota [Conformance]
test/e2e/apps/rc.go:85
  STEP: Creating a kubernetes client @ 07/29/23 17:15:00.573
  Jul 29 17:15:00.573: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 17:15:00.578
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:00.631
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:00.637
  Jul 29 17:15:00.644: INFO: Creating quota "condition-test" that allows only two pods to run in the current namespace
  STEP: Creating rc "condition-test" that asks for more than the allowed pod quota @ 07/29/23 17:15:01.693
  STEP: Checking rc "condition-test" has the desired failure condition set @ 07/29/23 17:15:01.702
  STEP: Scaling down rc "condition-test" to satisfy pod quota @ 07/29/23 17:15:02.734
  Jul 29 17:15:02.753: INFO: Updating replication controller "condition-test"
  STEP: Checking rc "condition-test" has no failure condition set @ 07/29/23 17:15:02.753
  Jul 29 17:15:03.770: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "replication-controller-2526" for this suite. @ 07/29/23 17:15:03.783
• [3.224 seconds]
------------------------------
SSSSSS
------------------------------
[sig-apps] Job should run a job to completion when tasks sometimes fail and are locally restarted [Conformance]
test/e2e/apps/job.go:430
  STEP: Creating a kubernetes client @ 07/29/23 17:15:03.797
  Jul 29 17:15:03.797: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename job @ 07/29/23 17:15:03.8
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:03.831
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:03.839
  STEP: Creating a job @ 07/29/23 17:15:03.846
  STEP: Ensuring job reaches completions @ 07/29/23 17:15:03.859
  Jul 29 17:15:15.872: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "job-6006" for this suite. @ 07/29/23 17:15:15.884
• [12.103 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should honor timeout [Conformance]
test/e2e/apimachinery/webhook.go:370
  STEP: Creating a kubernetes client @ 07/29/23 17:15:15.905
  Jul 29 17:15:15.905: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:15:15.909
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:15.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:15.95
  STEP: Setting up server cert @ 07/29/23 17:15:15.996
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:15:16.648
  STEP: Deploying the webhook pod @ 07/29/23 17:15:16.663
  STEP: Wait for the deployment to be ready @ 07/29/23 17:15:16.688
  Jul 29 17:15:16.709: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 17:15:18.733
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:15:18.755
  Jul 29 17:15:19.756: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Setting timeout (1s) shorter than webhook latency (5s) @ 07/29/23 17:15:19.764
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 17:15:19.764
  STEP: Request fails when timeout (1s) is shorter than slow webhook latency (5s) @ 07/29/23 17:15:19.799
  STEP: Having no error when timeout is shorter than webhook latency and failure policy is ignore @ 07/29/23 17:15:20.818
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 17:15:20.818
  STEP: Having no error when timeout is longer than webhook latency @ 07/29/23 17:15:21.868
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 17:15:21.868
  STEP: Having no error when timeout is empty (defaulted to 10s in v1) @ 07/29/23 17:15:26.933
  STEP: Registering slow webhook via the AdmissionRegistration API @ 07/29/23 17:15:26.933
  Jul 29 17:15:31.996: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-6630" for this suite. @ 07/29/23 17:15:32.156
  STEP: Destroying namespace "webhook-markers-9804" for this suite. @ 07/29/23 17:15:32.172
• [16.286 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl label should update the label on a resource  [Conformance]
test/e2e/kubectl/kubectl.go:1574
  STEP: Creating a kubernetes client @ 07/29/23 17:15:32.197
  Jul 29 17:15:32.197: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:15:32.199
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:32.234
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:32.239
  STEP: creating the pod @ 07/29/23 17:15:32.245
  Jul 29 17:15:32.246: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1995 create -f -'
  Jul 29 17:15:33.945: INFO: stderr: ""
  Jul 29 17:15:33.946: INFO: stdout: "pod/pause created\n"
  STEP: adding the label testing-label with value testing-label-value to a pod @ 07/29/23 17:15:35.968
  Jul 29 17:15:35.968: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1995 label pods pause testing-label=testing-label-value'
  Jul 29 17:15:36.129: INFO: stderr: ""
  Jul 29 17:15:36.129: INFO: stdout: "pod/pause labeled\n"
  STEP: verifying the pod has the label testing-label with the value testing-label-value @ 07/29/23 17:15:36.129
  Jul 29 17:15:36.130: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1995 get pod pause -L testing-label'
  Jul 29 17:15:36.268: INFO: stderr: ""
  Jul 29 17:15:36.268: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    testing-label-value\n"
  STEP: removing the label testing-label of a pod @ 07/29/23 17:15:36.268
  Jul 29 17:15:36.269: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1995 label pods pause testing-label-'
  Jul 29 17:15:36.420: INFO: stderr: ""
  Jul 29 17:15:36.420: INFO: stdout: "pod/pause unlabeled\n"
  STEP: verifying the pod doesn't have the label testing-label @ 07/29/23 17:15:36.42
  Jul 29 17:15:36.421: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1995 get pod pause -L testing-label'
  Jul 29 17:15:36.549: INFO: stderr: ""
  Jul 29 17:15:36.549: INFO: stdout: "NAME    READY   STATUS    RESTARTS   AGE   TESTING-LABEL\npause   1/1     Running   0          3s    \n"
  STEP: using delete to clean up resources @ 07/29/23 17:15:36.549
  Jul 29 17:15:36.550: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1995 delete --grace-period=0 --force -f -'
  Jul 29 17:15:36.683: INFO: stderr: "Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely.\n"
  Jul 29 17:15:36.683: INFO: stdout: "pod \"pause\" force deleted\n"
  Jul 29 17:15:36.684: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1995 get rc,svc -l name=pause --no-headers'
  Jul 29 17:15:36.841: INFO: stderr: "No resources found in kubectl-1995 namespace.\n"
  Jul 29 17:15:36.841: INFO: stdout: ""
  Jul 29 17:15:36.842: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-1995 get pods -l name=pause -o go-template={{ range .items }}{{ if not .metadata.deletionTimestamp }}{{ .metadata.name }}{{ "\n" }}{{ end }}{{ end }}'
  Jul 29 17:15:36.970: INFO: stderr: ""
  Jul 29 17:15:36.970: INFO: stdout: ""
  Jul 29 17:15:36.970: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-1995" for this suite. @ 07/29/23 17:15:36.981
• [4.795 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should update labels on modification [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:131
  STEP: Creating a kubernetes client @ 07/29/23 17:15:37
  Jul 29 17:15:37.000: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:15:37.002
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:37.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:37.044
  STEP: Creating the pod @ 07/29/23 17:15:37.049
  Jul 29 17:15:39.643: INFO: Successfully updated pod "labelsupdateeac79dad-aa8c-44f1-a2e9-26e38322ae8b"
  Jul 29 17:15:41.678: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-1395" for this suite. @ 07/29/23 17:15:41.691
• [4.704 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Container Lifecycle Hook when create a pod with lifecycle hook should execute poststart exec hook properly [NodeConformance] [Conformance]
test/e2e/common/node/lifecycle_hook.go:135
  STEP: Creating a kubernetes client @ 07/29/23 17:15:41.711
  Jul 29 17:15:41.711: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename container-lifecycle-hook @ 07/29/23 17:15:41.713
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:41.745
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:41.749
  STEP: create the container to handle the HTTPGet hook request. @ 07/29/23 17:15:41.76
  STEP: create the pod with lifecycle hook @ 07/29/23 17:15:45.819
  STEP: check poststart hook @ 07/29/23 17:15:47.852
  STEP: delete the pod with lifecycle hook @ 07/29/23 17:15:47.865
  Jul 29 17:15:49.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "container-lifecycle-hook-4529" for this suite. @ 07/29/23 17:15:49.901
• [8.200 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should include webhook resources in discovery documents [Conformance]
test/e2e/apimachinery/webhook.go:118
  STEP: Creating a kubernetes client @ 07/29/23 17:15:49.919
  Jul 29 17:15:49.919: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:15:49.921
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:49.95
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:49.958
  STEP: Setting up server cert @ 07/29/23 17:15:50.013
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:15:50.427
  STEP: Deploying the webhook pod @ 07/29/23 17:15:50.445
  STEP: Wait for the deployment to be ready @ 07/29/23 17:15:50.473
  Jul 29 17:15:50.492: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  STEP: Deploying the webhook service @ 07/29/23 17:15:52.515
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:15:52.536
  Jul 29 17:15:53.536: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: fetching the /apis discovery document @ 07/29/23 17:15:53.545
  STEP: finding the admissionregistration.k8s.io API group in the /apis discovery document @ 07/29/23 17:15:53.548
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis discovery document @ 07/29/23 17:15:53.548
  STEP: fetching the /apis/admissionregistration.k8s.io discovery document @ 07/29/23 17:15:53.549
  STEP: finding the admissionregistration.k8s.io/v1 API group/version in the /apis/admissionregistration.k8s.io discovery document @ 07/29/23 17:15:53.552
  STEP: fetching the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/29/23 17:15:53.552
  STEP: finding mutatingwebhookconfigurations and validatingwebhookconfigurations resources in the /apis/admissionregistration.k8s.io/v1 discovery document @ 07/29/23 17:15:53.555
  Jul 29 17:15:53.555: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-5572" for this suite. @ 07/29/23 17:15:53.673
  STEP: Destroying namespace "webhook-markers-132" for this suite. @ 07/29/23 17:15:53.687
• [3.783 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on default medium should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:157
  STEP: Creating a kubernetes client @ 07/29/23 17:15:53.706
  Jul 29 17:15:53.706: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:15:53.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:53.759
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:53.766
  STEP: Creating a pod to test emptydir volume type on node default medium @ 07/29/23 17:15:53.771
  STEP: Saw pod success @ 07/29/23 17:15:57.818
  Jul 29 17:15:57.825: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-c53ba801-55fc-4ce9-b7ec-18245a011ca8 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:15:57.843
  Jul 29 17:15:57.874: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-6241" for this suite. @ 07/29/23 17:15:57.885
• [4.194 seconds]
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for CRD without validation schema [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:153
  STEP: Creating a kubernetes client @ 07/29/23 17:15:57.9
  Jul 29 17:15:57.900: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:15:57.903
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:15:57.933
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:15:57.937
  Jul 29 17:15:57.941: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: kubectl validation (kubectl create and apply) allows request with any unknown properties @ 07/29/23 17:15:59.935
  Jul 29 17:15:59.935: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-4834 --namespace=crd-publish-openapi-4834 create -f -'
  Jul 29 17:16:01.431: INFO: stderr: ""
  Jul 29 17:16:01.431: INFO: stdout: "e2e-test-crd-publish-openapi-7377-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 29 17:16:01.431: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-4834 --namespace=crd-publish-openapi-4834 delete e2e-test-crd-publish-openapi-7377-crds test-cr'
  Jul 29 17:16:01.585: INFO: stderr: ""
  Jul 29 17:16:01.585: INFO: stdout: "e2e-test-crd-publish-openapi-7377-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  Jul 29 17:16:01.585: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-4834 --namespace=crd-publish-openapi-4834 apply -f -'
  Jul 29 17:16:02.045: INFO: stderr: ""
  Jul 29 17:16:02.045: INFO: stdout: "e2e-test-crd-publish-openapi-7377-crd.crd-publish-openapi-test-empty.example.com/test-cr created\n"
  Jul 29 17:16:02.045: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-4834 --namespace=crd-publish-openapi-4834 delete e2e-test-crd-publish-openapi-7377-crds test-cr'
  Jul 29 17:16:02.241: INFO: stderr: ""
  Jul 29 17:16:02.241: INFO: stdout: "e2e-test-crd-publish-openapi-7377-crd.crd-publish-openapi-test-empty.example.com \"test-cr\" deleted\n"
  STEP: kubectl explain works to explain CR without validation schema @ 07/29/23 17:16:02.241
  Jul 29 17:16:02.242: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=crd-publish-openapi-4834 explain e2e-test-crd-publish-openapi-7377-crds'
  Jul 29 17:16:02.736: INFO: stderr: ""
  Jul 29 17:16:02.736: INFO: stdout: "GROUP:      crd-publish-openapi-test-empty.example.com\nKIND:       e2e-test-crd-publish-openapi-7377-crd\nVERSION:    v1\n\nDESCRIPTION:\n    <empty>\nFIELDS:\n  apiVersion\t<string>\n    APIVersion defines the versioned schema of this representation of an object.\n    Servers should convert recognized schemas to the latest internal value, and\n    may reject unrecognized values. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#resources\n\n  kind\t<string>\n    Kind is a string value representing the REST resource this object\n    represents. Servers may infer this from the endpoint the client submits\n    requests to. Cannot be updated. In CamelCase. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#types-kinds\n\n  metadata\t<ObjectMeta>\n    Standard object's metadata. More info:\n    https://git.k8s.io/community/contributors/devel/sig-architecture/api-conventions.md#metadata\n\n\n"
  Jul 29 17:16:05.271: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4834" for this suite. @ 07/29/23 17:16:05.289
• [7.405 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Daemon set [Serial] should update pod when spec was updated and update strategy is RollingUpdate [Conformance]
test/e2e/apps/daemon_set.go:385
  STEP: Creating a kubernetes client @ 07/29/23 17:16:05.307
  Jul 29 17:16:05.307: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename daemonsets @ 07/29/23 17:16:05.31
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:16:05.364
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:16:05.378
  Jul 29 17:16:05.432: INFO: Creating simple daemon set daemon-set
  STEP: Check that daemon pods launch on every node of the cluster. @ 07/29/23 17:16:05.444
  Jul 29 17:16:05.462: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:16:05.462: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 17:16:06.484: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:16:06.484: INFO: Node ci9axai7aiv7-1 is running 0 daemon pod, expected 1
  Jul 29 17:16:07.483: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:16:07.484: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Update daemon pods image. @ 07/29/23 17:16:07.51
  STEP: Check that daemon pods images are updated. @ 07/29/23 17:16:07.532
  Jul 29 17:16:07.538: INFO: Wrong image for pod: daemon-set-55rgz. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:07.538: INFO: Wrong image for pod: daemon-set-c2bmx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:07.538: INFO: Wrong image for pod: daemon-set-ljlkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:08.555: INFO: Wrong image for pod: daemon-set-c2bmx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:08.556: INFO: Wrong image for pod: daemon-set-ljlkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:09.554: INFO: Wrong image for pod: daemon-set-c2bmx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:09.554: INFO: Wrong image for pod: daemon-set-ljlkt. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:09.554: INFO: Pod daemon-set-xlp5w is not available
  Jul 29 17:16:10.555: INFO: Wrong image for pod: daemon-set-c2bmx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:11.559: INFO: Pod daemon-set-6m5nd is not available
  Jul 29 17:16:11.560: INFO: Wrong image for pod: daemon-set-c2bmx. Expected: registry.k8s.io/e2e-test-images/agnhost:2.43, got: registry.k8s.io/e2e-test-images/httpd:2.4.38-4.
  Jul 29 17:16:12.557: INFO: Pod daemon-set-xmh4f is not available
  Jul 29 17:16:13.554: INFO: Pod daemon-set-xmh4f is not available
  STEP: Check that daemon pods are still running on every node of the cluster. @ 07/29/23 17:16:13.563
  Jul 29 17:16:13.578: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 2
  Jul 29 17:16:13.579: INFO: Node ci9axai7aiv7-2 is running 0 daemon pod, expected 1
  Jul 29 17:16:14.603: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 3
  Jul 29 17:16:14.604: INFO: Number of running nodes: 3, number of available pods: 3 in daemonset daemon-set
  STEP: Deleting DaemonSet "daemon-set" @ 07/29/23 17:16:14.649
  STEP: deleting DaemonSet.extensions daemon-set in namespace daemonsets-6044, will wait for the garbage collector to delete the pods @ 07/29/23 17:16:14.649
  Jul 29 17:16:14.726: INFO: Deleting DaemonSet.extensions daemon-set took: 18.181596ms
  Jul 29 17:16:14.827: INFO: Terminating DaemonSet.extensions daemon-set pods took: 101.055146ms
  Jul 29 17:16:15.838: INFO: Number of nodes with available pods controlled by daemonset daemon-set: 0
  Jul 29 17:16:15.838: INFO: Number of running nodes: 0, number of available pods: 0 in daemonset daemon-set
  Jul 29 17:16:15.844: INFO: daemonset: {"kind":"DaemonSetList","apiVersion":"apps/v1","metadata":{"resourceVersion":"41016"},"items":null}

  Jul 29 17:16:15.850: INFO: pods: {"kind":"PodList","apiVersion":"v1","metadata":{"resourceVersion":"41016"},"items":null}

  Jul 29 17:16:15.881: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "daemonsets-6044" for this suite. @ 07/29/23 17:16:15.888
• [10.595 seconds]
------------------------------
[sig-storage] Downward API volume should set DefaultMode on files [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:69
  STEP: Creating a kubernetes client @ 07/29/23 17:16:15.903
  Jul 29 17:16:15.903: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:16:15.905
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:16:15.944
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:16:15.951
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:16:15.955
  STEP: Saw pod success @ 07/29/23 17:16:20.001
  Jul 29 17:16:20.008: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-dd1bf6b3-8bec-4a84-a20c-8109da65b7b3 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:16:20.022
  Jul 29 17:16:20.052: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-7571" for this suite. @ 07/29/23 17:16:20.061
• [4.171 seconds]
------------------------------
[sig-apps] ReplicationController should test the lifecycle of a ReplicationController [Conformance]
test/e2e/apps/rc.go:112
  STEP: Creating a kubernetes client @ 07/29/23 17:16:20.074
  Jul 29 17:16:20.074: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename replication-controller @ 07/29/23 17:16:20.076
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:16:20.111
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:16:20.117
  STEP: creating a ReplicationController @ 07/29/23 17:16:20.131
  STEP: waiting for RC to be added @ 07/29/23 17:16:20.144
  STEP: waiting for available Replicas @ 07/29/23 17:16:20.144
  STEP: patching ReplicationController @ 07/29/23 17:16:21.292
  STEP: waiting for RC to be modified @ 07/29/23 17:16:21.305
  STEP: patching ReplicationController status @ 07/29/23 17:16:21.305
  STEP: waiting for RC to be modified @ 07/29/23 17:16:21.315
  STEP: waiting for available Replicas @ 07/29/23 17:16:21.316
  STEP: fetching ReplicationController status @ 07/29/23 17:16:21.331
  STEP: patching ReplicationController scale @ 07/29/23 17:16:21.337
  STEP: waiting for RC to be modified @ 07/29/23 17:16:21.346
  STEP: waiting for ReplicationController's scale to be the max amount @ 07/29/23 17:16:21.347
  STEP: fetching ReplicationController; ensuring that it's patched @ 07/29/23 17:16:24.469
  STEP: updating ReplicationController status @ 07/29/23 17:16:24.475
  STEP: waiting for RC to be modified @ 07/29/23 17:16:24.488
  STEP: listing all ReplicationControllers @ 07/29/23 17:16:24.489
  STEP: checking that ReplicationController has expected values @ 07/29/23 17:16:24.496
  STEP: deleting ReplicationControllers by collection @ 07/29/23 17:16:24.496
  STEP: waiting for ReplicationController to have a DELETED watchEvent @ 07/29/23 17:16:24.514
  Jul 29 17:16:24.596: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 17:16:24.597618      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "replication-controller-4085" for this suite. @ 07/29/23 17:16:24.605
• [4.544 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Secrets should be consumable from pods in env vars [NodeConformance] [Conformance]
test/e2e/common/node/secrets.go:46
  STEP: Creating a kubernetes client @ 07/29/23 17:16:24.626
  Jul 29 17:16:24.626: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 17:16:24.628
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:16:24.66
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:16:24.666
  STEP: Creating secret with name secret-test-96b5c294-d5a9-449d-8310-635af5fd3d86 @ 07/29/23 17:16:24.672
  STEP: Creating a pod to test consume secrets @ 07/29/23 17:16:24.68
  E0729 17:16:25.598473      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:26.599409      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:27.599609      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:28.599766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:16:28.717
  Jul 29 17:16:28.725: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-secrets-21c0855b-26b2-45bf-bd4a-dd82c5339bf4 container secret-env-test: <nil>
  STEP: delete the pod @ 07/29/23 17:16:28.747
  Jul 29 17:16:28.783: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-179" for this suite. @ 07/29/23 17:16:28.792
• [4.185 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should be submitted and removed [NodeConformance] [Conformance]
test/e2e/common/node/pods.go:227
  STEP: Creating a kubernetes client @ 07/29/23 17:16:28.816
  Jul 29 17:16:28.816: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:16:28.818
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:16:28.858
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:16:28.865
  STEP: creating the pod @ 07/29/23 17:16:28.873
  STEP: setting up watch @ 07/29/23 17:16:28.873
  STEP: submitting the pod to kubernetes @ 07/29/23 17:16:28.983
  STEP: verifying the pod is in kubernetes @ 07/29/23 17:16:29.005
  STEP: verifying pod creation was observed @ 07/29/23 17:16:29.015
  E0729 17:16:29.600983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:30.601251      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting the pod gracefully @ 07/29/23 17:16:31.04
  STEP: verifying pod deletion was observed @ 07/29/23 17:16:31.051
  E0729 17:16:31.601527      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:32.602551      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:16:33.361: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "pods-7908" for this suite. @ 07/29/23 17:16:33.368
• [4.563 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] Sysctls [LinuxOnly] [NodeConformance] should support sysctls [MinimumKubeletVersion:1.21] [Conformance]
test/e2e/common/node/sysctl.go:77
  STEP: Creating a kubernetes client @ 07/29/23 17:16:33.379
  Jul 29 17:16:33.380: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename sysctl @ 07/29/23 17:16:33.381
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:16:33.462
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:16:33.473
  STEP: Creating a pod with the kernel.shm_rmid_forced sysctl @ 07/29/23 17:16:33.478
  STEP: Watching for error events or started pod @ 07/29/23 17:16:33.491
  E0729 17:16:33.603138      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:34.603127      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for pod completion @ 07/29/23 17:16:35.5
  E0729 17:16:35.603810      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:36.603782      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Checking that the pod succeeded @ 07/29/23 17:16:37.526
  STEP: Getting logs from the pod @ 07/29/23 17:16:37.527
  STEP: Checking that the sysctl is actually updated @ 07/29/23 17:16:37.542
  Jul 29 17:16:37.542: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "sysctl-4720" for this suite. @ 07/29/23 17:16:37.55
• [4.187 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not cause race condition when used for configmaps [Serial] [Conformance]
test/e2e/storage/empty_dir_wrapper.go:188
  STEP: Creating a kubernetes client @ 07/29/23 17:16:37.571
  Jul 29 17:16:37.571: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/29/23 17:16:37.573
  E0729 17:16:37.604876      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:16:37.605
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:16:37.61
  STEP: Creating 50 configmaps @ 07/29/23 17:16:37.616
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 17:16:37.993
  Jul 29 17:16:38.022: INFO: Pod name wrapped-volume-race-55264899-028f-4693-935c-c080cface339: Found 0 pods out of 5
  E0729 17:16:38.605943      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:39.606434      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:40.607106      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:41.607572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:42.608149      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:16:43.040: INFO: Pod name wrapped-volume-race-55264899-028f-4693-935c-c080cface339: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 17:16:43.04
  E0729 17:16:43.608793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:44.609203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 17:16:45.096
  Jul 29 17:16:45.124: INFO: Pod name wrapped-volume-race-e540acab-82a9-4760-a7b8-f1488d0fc0c3: Found 0 pods out of 5
  E0729 17:16:45.610641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:46.614381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:47.614837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:48.615241      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:49.615286      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:16:50.183: INFO: Pod name wrapped-volume-race-e540acab-82a9-4760-a7b8-f1488d0fc0c3: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 17:16:50.184
  STEP: Creating RC which spawns configmap-volume pods @ 07/29/23 17:16:50.227
  Jul 29 17:16:50.262: INFO: Pod name wrapped-volume-race-0374b898-ddb5-4e61-a1ef-8b97f5a0db86: Found 0 pods out of 5
  E0729 17:16:50.617100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:51.623248      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:52.623414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:53.623694      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:54.624019      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:16:55.285: INFO: Pod name wrapped-volume-race-0374b898-ddb5-4e61-a1ef-8b97f5a0db86: Found 5 pods out of 5
  STEP: Ensuring each pod is running @ 07/29/23 17:16:55.285
  Jul 29 17:16:55.326: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: deleting ReplicationController wrapped-volume-race-0374b898-ddb5-4e61-a1ef-8b97f5a0db86 in namespace emptydir-wrapper-912, will wait for the garbage collector to delete the pods @ 07/29/23 17:16:55.336
  Jul 29 17:16:55.407: INFO: Deleting ReplicationController wrapped-volume-race-0374b898-ddb5-4e61-a1ef-8b97f5a0db86 took: 13.192028ms
  Jul 29 17:16:55.608: INFO: Terminating ReplicationController wrapped-volume-race-0374b898-ddb5-4e61-a1ef-8b97f5a0db86 pods took: 201.708ms
  E0729 17:16:55.625161      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:56.624905      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-e540acab-82a9-4760-a7b8-f1488d0fc0c3 in namespace emptydir-wrapper-912, will wait for the garbage collector to delete the pods @ 07/29/23 17:16:57.01
  Jul 29 17:16:57.102: INFO: Deleting ReplicationController wrapped-volume-race-e540acab-82a9-4760-a7b8-f1488d0fc0c3 took: 14.141002ms
  Jul 29 17:16:57.206: INFO: Terminating ReplicationController wrapped-volume-race-e540acab-82a9-4760-a7b8-f1488d0fc0c3 pods took: 103.740042ms
  E0729 17:16:57.641191      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:16:58.641725      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: deleting ReplicationController wrapped-volume-race-55264899-028f-4693-935c-c080cface339 in namespace emptydir-wrapper-912, will wait for the garbage collector to delete the pods @ 07/29/23 17:16:59.006
  Jul 29 17:16:59.083: INFO: Deleting ReplicationController wrapped-volume-race-55264899-028f-4693-935c-c080cface339 took: 15.703608ms
  Jul 29 17:16:59.384: INFO: Terminating ReplicationController wrapped-volume-race-55264899-028f-4693-935c-c080cface339 pods took: 301.476694ms
  E0729 17:16:59.642074      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:00.642336      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Cleaning up the configMaps @ 07/29/23 17:17:01.585
  E0729 17:17:01.642949      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "emptydir-wrapper-912" for this suite. @ 07/29/23 17:17:02.056
• [24.493 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Downward API volume should provide node allocatable (memory) as default memory limit if the limit is not set [NodeConformance] [Conformance]
test/e2e/common/storage/downwardapi_volume.go:262
  STEP: Creating a kubernetes client @ 07/29/23 17:17:02.07
  Jul 29 17:17:02.071: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:17:02.073
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:17:02.103
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:17:02.107
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:17:02.113
  E0729 17:17:02.658638      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:03.644362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:04.644641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:05.646116      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:17:06.152
  Jul 29 17:17:06.177: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-efb883ee-1f18-4b59-bb90-db71929f2d43 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:17:06.195
  Jul 29 17:17:06.226: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-880" for this suite. @ 07/29/23 17:17:06.234
• [4.173 seconds]
------------------------------
SSSSS
------------------------------
[sig-node] ConfigMap should run through a ConfigMap lifecycle [Conformance]
test/e2e/common/node/configmap.go:169
  STEP: Creating a kubernetes client @ 07/29/23 17:17:06.248
  Jul 29 17:17:06.248: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:17:06.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:17:06.28
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:17:06.284
  STEP: creating a ConfigMap @ 07/29/23 17:17:06.29
  STEP: fetching the ConfigMap @ 07/29/23 17:17:06.305
  STEP: patching the ConfigMap @ 07/29/23 17:17:06.31
  STEP: listing all ConfigMaps in all namespaces with a label selector @ 07/29/23 17:17:06.316
  STEP: deleting the ConfigMap by collection with a label selector @ 07/29/23 17:17:06.323
  STEP: listing all ConfigMaps in test namespace @ 07/29/23 17:17:06.337
  Jul 29 17:17:06.344: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-7774" for this suite. @ 07/29/23 17:17:06.352
• [0.113 seconds]
------------------------------
SS
------------------------------
[sig-storage] EmptyDir wrapper volumes should not conflict [Conformance]
test/e2e/storage/empty_dir_wrapper.go:67
  STEP: Creating a kubernetes client @ 07/29/23 17:17:06.362
  Jul 29 17:17:06.362: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir-wrapper @ 07/29/23 17:17:06.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:17:06.391
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:17:06.395
  E0729 17:17:06.646408      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:07.646984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:17:08.440: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Cleaning up the secret @ 07/29/23 17:17:08.448
  STEP: Cleaning up the configmap @ 07/29/23 17:17:08.458
  STEP: Cleaning up the pod @ 07/29/23 17:17:08.467
  STEP: Destroying namespace "emptydir-wrapper-868" for this suite. @ 07/29/23 17:17:08.484
• [2.133 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-apps] Deployment should validate Deployment Status endpoints [Conformance]
test/e2e/apps/deployment.go:485
  STEP: Creating a kubernetes client @ 07/29/23 17:17:08.5
  Jul 29 17:17:08.500: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename deployment @ 07/29/23 17:17:08.502
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:17:08.54
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:17:08.544
  STEP: creating a Deployment @ 07/29/23 17:17:08.553
  Jul 29 17:17:08.554: INFO: Creating simple deployment test-deployment-l28h7
  Jul 29 17:17:08.580: INFO: deployment "test-deployment-l28h7" doesn't have the required revision set
  E0729 17:17:08.648109      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:09.648743      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Getting /status @ 07/29/23 17:17:10.607
  Jul 29 17:17:10.616: INFO: Deployment test-deployment-l28h7 has Conditions: [{Available True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.} {Progressing True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l28h7-5994cf9475" has successfully progressed.}]
  STEP: updating Deployment Status @ 07/29/23 17:17:10.617
  Jul 29 17:17:10.632: INFO: updatedStatus.Conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"Available", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 17, 17, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 17, 9, 0, time.Local), Reason:"MinimumReplicasAvailable", Message:"Deployment has minimum availability."}, v1.DeploymentCondition{Type:"Progressing", Status:"True", LastUpdateTime:time.Date(2023, time.July, 29, 17, 17, 9, 0, time.Local), LastTransitionTime:time.Date(2023, time.July, 29, 17, 17, 8, 0, time.Local), Reason:"NewReplicaSetAvailable", Message:"ReplicaSet \"test-deployment-l28h7-5994cf9475\" has successfully progressed."}, v1.DeploymentCondition{Type:"StatusUpdate", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"E2E", Message:"Set from e2e test"}}
  STEP: watching for the Deployment status to be updated @ 07/29/23 17:17:10.632
  Jul 29 17:17:10.637: INFO: Observed &Deployment event: ADDED
  Jul 29 17:17:10.637: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l28h7-5994cf9475"}
  Jul 29 17:17:10.638: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.639: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l28h7-5994cf9475"}
  Jul 29 17:17:10.639: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 17:17:10.640: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.640: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 17:17:10.641: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-l28h7-5994cf9475" is progressing.}
  Jul 29 17:17:10.641: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.642: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 17:17:10.642: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l28h7-5994cf9475" has successfully progressed.}
  Jul 29 17:17:10.643: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.643: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 17:17:10.643: INFO: Observed Deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l28h7-5994cf9475" has successfully progressed.}
  Jul 29 17:17:10.644: INFO: Found Deployment test-deployment-l28h7 in namespace deployment-9969 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 17:17:10.644: INFO: Deployment test-deployment-l28h7 has an updated status
  STEP: patching the Statefulset Status @ 07/29/23 17:17:10.645
  Jul 29 17:17:10.645: INFO: Patch payload: {"status":{"conditions":[{"type":"StatusPatched","status":"True"}]}}
  E0729 17:17:10.649774      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:17:10.658: INFO: Patched status conditions: []v1.DeploymentCondition{v1.DeploymentCondition{Type:"StatusPatched", Status:"True", LastUpdateTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), LastTransitionTime:time.Date(1, time.January, 1, 0, 0, 0, 0, time.UTC), Reason:"", Message:""}}
  STEP: watching for the Deployment status to be patched @ 07/29/23 17:17:10.658
  Jul 29 17:17:10.661: INFO: Observed &Deployment event: ADDED
  Jul 29 17:17:10.661: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l28h7-5994cf9475"}
  Jul 29 17:17:10.661: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.662: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetCreated Created new replica set "test-deployment-l28h7-5994cf9475"}
  Jul 29 17:17:10.662: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 17:17:10.662: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.662: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available False 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC MinimumReplicasUnavailable Deployment does not have minimum availability.}
  Jul 29 17:17:10.662: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:08 +0000 UTC 2023-07-29 17:17:08 +0000 UTC ReplicaSetUpdated ReplicaSet "test-deployment-l28h7-5994cf9475" is progressing.}
  Jul 29 17:17:10.663: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.663: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 17:17:10.663: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l28h7-5994cf9475" has successfully progressed.}
  Jul 29 17:17:10.663: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.663: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Available True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:09 +0000 UTC MinimumReplicasAvailable Deployment has minimum availability.}
  Jul 29 17:17:10.663: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {Progressing True 2023-07-29 17:17:09 +0000 UTC 2023-07-29 17:17:08 +0000 UTC NewReplicaSetAvailable ReplicaSet "test-deployment-l28h7-5994cf9475" has successfully progressed.}
  Jul 29 17:17:10.663: INFO: Observed deployment test-deployment-l28h7 in namespace deployment-9969 with annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusUpdate True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC E2E Set from e2e test}
  Jul 29 17:17:10.663: INFO: Observed &Deployment event: MODIFIED
  Jul 29 17:17:10.663: INFO: Found deployment test-deployment-l28h7 in namespace deployment-9969 with labels: map[e2e:testing name:httpd] annotations: map[deployment.kubernetes.io/revision:1] & Conditions: {StatusPatched True 0001-01-01 00:00:00 +0000 UTC 0001-01-01 00:00:00 +0000 UTC  }
  Jul 29 17:17:10.664: INFO: Deployment test-deployment-l28h7 has a patched status
  Jul 29 17:17:10.671: INFO: Deployment "test-deployment-l28h7":
  &Deployment{ObjectMeta:{test-deployment-l28h7  deployment-9969  1c25aa65-56ed-4128-a43d-2d72a4c3e0d8 41930 1 2023-07-29 17:17:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[deployment.kubernetes.io/revision:1] [] [] [{e2e.test Update apps/v1 2023-07-29 17:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:progressDeadlineSeconds":{},"f:replicas":{},"f:revisionHistoryLimit":{},"f:selector":{},"f:strategy":{"f:rollingUpdate":{".":{},"f:maxSurge":{},"f:maxUnavailable":{}},"f:type":{}},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {e2e.test Update apps/v1 2023-07-29 17:17:10 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"StatusPatched\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:status":{},"f:type":{}}}}} status} {kube-controller-manager Update apps/v1 2023-07-29 17:17:10 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/revision":{}}},"f:status":{"f:availableReplicas":{},"f:conditions":{".":{},"k:{\"type\":\"Progressing\"}":{".":{},"f:lastTransitionTime":{},"f:lastUpdateTime":{},"f:message":{},"f:reason":{},"f:status":{},"f:type":{}}},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{},"f:updatedReplicas":{}}} status}]},Spec:DeploymentSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073ed508 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},Strategy:DeploymentStrategy{Type:RollingUpdate,RollingUpdate:&RollingUpdateDeployment{MaxUnavailable:25%!,(MISSING)MaxSurge:25%!,(MISSING)},},MinReadySeconds:0,RevisionHistoryLimit:*10,Paused:false,ProgressDeadlineSeconds:*600,},Status:DeploymentStatus{ObservedGeneration:1,Replicas:1,UpdatedReplicas:1,AvailableReplicas:1,UnavailableReplicas:0,Conditions:[]DeploymentCondition{DeploymentCondition{Type:StatusPatched,Status:True,Reason:,Message:,LastUpdateTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:0001-01-01 00:00:00 +0000 UTC,},DeploymentCondition{Type:Progressing,Status:True,Reason:FoundNewReplicaSet,Message:Found new replica set "test-deployment-l28h7-5994cf9475",LastUpdateTime:2023-07-29 17:17:10 +0000 UTC,LastTransitionTime:2023-07-29 17:17:10 +0000 UTC,},},ReadyReplicas:1,CollisionCount:nil,},}

  Jul 29 17:17:10.679: INFO: New ReplicaSet "test-deployment-l28h7-5994cf9475" of Deployment "test-deployment-l28h7":
  &ReplicaSet{ObjectMeta:{test-deployment-l28h7-5994cf9475  deployment-9969  4a2d2bba-d056-481a-9bfb-194772c0e04d 41925 1 2023-07-29 17:17:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[deployment.kubernetes.io/desired-replicas:1 deployment.kubernetes.io/max-replicas:2 deployment.kubernetes.io/revision:1] [{apps/v1 Deployment test-deployment-l28h7 1c25aa65-56ed-4128-a43d-2d72a4c3e0d8 0xc0073ed8d0 0xc0073ed8d1}] [] [{kube-controller-manager Update apps/v1 2023-07-29 17:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:annotations":{".":{},"f:deployment.kubernetes.io/desired-replicas":{},"f:deployment.kubernetes.io/max-replicas":{},"f:deployment.kubernetes.io/revision":{}},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"1c25aa65-56ed-4128-a43d-2d72a4c3e0d8\"}":{}}},"f:spec":{"f:replicas":{},"f:selector":{},"f:template":{"f:metadata":{"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}}}} } {kube-controller-manager Update apps/v1 2023-07-29 17:17:09 +0000 UTC FieldsV1 {"f:status":{"f:availableReplicas":{},"f:fullyLabeledReplicas":{},"f:observedGeneration":{},"f:readyReplicas":{},"f:replicas":{}}} status}]},Spec:ReplicaSetSpec{Replicas:*1,Selector:&v1.LabelSelector{MatchLabels:map[string]string{e2e: testing,name: httpd,pod-template-hash: 5994cf9475,},MatchExpressions:[]LabelSelectorRequirement{},},Template:{{      0 0001-01-01 00:00:00 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [] [] []} {[] [] [{httpd registry.k8s.io/e2e-test-images/httpd:2.4.38-4 [] []  [] [] [] {map[] map[] []} [] [] [] nil nil nil nil /dev/termination-log File IfNotPresent SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,} false false false}] [] Always 0xc0073ed978 <nil> ClusterFirst map[]   <nil>  false false false <nil> &PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,} []   nil default-scheduler [] []  <nil> nil [] <nil> <nil> <nil> map[] [] <nil> nil <nil> [] []}},MinReadySeconds:0,},Status:ReplicaSetStatus{Replicas:1,FullyLabeledReplicas:1,ObservedGeneration:1,ReadyReplicas:1,AvailableReplicas:1,Conditions:[]ReplicaSetCondition{},},}
  Jul 29 17:17:10.685: INFO: Pod "test-deployment-l28h7-5994cf9475-d4s4h" is available:
  &Pod{ObjectMeta:{test-deployment-l28h7-5994cf9475-d4s4h test-deployment-l28h7-5994cf9475- deployment-9969  d38af654-9152-42f1-8664-9aec57a81b05 41924 0 2023-07-29 17:17:08 +0000 UTC <nil> <nil> map[e2e:testing name:httpd pod-template-hash:5994cf9475] map[] [{apps/v1 ReplicaSet test-deployment-l28h7-5994cf9475 4a2d2bba-d056-481a-9bfb-194772c0e04d 0xc0073edd10 0xc0073edd11}] [] [{kube-controller-manager Update v1 2023-07-29 17:17:08 +0000 UTC FieldsV1 {"f:metadata":{"f:generateName":{},"f:labels":{".":{},"f:e2e":{},"f:name":{},"f:pod-template-hash":{}},"f:ownerReferences":{".":{},"k:{\"uid\":\"4a2d2bba-d056-481a-9bfb-194772c0e04d\"}":{}}},"f:spec":{"f:containers":{"k:{\"name\":\"httpd\"}":{".":{},"f:image":{},"f:imagePullPolicy":{},"f:name":{},"f:resources":{},"f:securityContext":{},"f:terminationMessagePath":{},"f:terminationMessagePolicy":{}}},"f:dnsPolicy":{},"f:enableServiceLinks":{},"f:restartPolicy":{},"f:schedulerName":{},"f:securityContext":{},"f:terminationGracePeriodSeconds":{}}} } {kubelet Update v1 2023-07-29 17:17:09 +0000 UTC FieldsV1 {"f:status":{"f:conditions":{"k:{\"type\":\"ContainersReady\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Initialized\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}},"k:{\"type\":\"Ready\"}":{".":{},"f:lastProbeTime":{},"f:lastTransitionTime":{},"f:status":{},"f:type":{}}},"f:containerStatuses":{},"f:hostIP":{},"f:phase":{},"f:podIP":{},"f:podIPs":{".":{},"k:{\"ip\":\"10.233.66.83\"}":{".":{},"f:ip":{}}},"f:startTime":{}}} status}]},Spec:PodSpec{Volumes:[]Volume{Volume{Name:kube-api-access-b82ns,VolumeSource:VolumeSource{HostPath:nil,EmptyDir:nil,GCEPersistentDisk:nil,AWSElasticBlockStore:nil,GitRepo:nil,Secret:nil,NFS:nil,ISCSI:nil,Glusterfs:nil,PersistentVolumeClaim:nil,RBD:nil,FlexVolume:nil,Cinder:nil,CephFS:nil,Flocker:nil,DownwardAPI:nil,FC:nil,AzureFile:nil,ConfigMap:nil,VsphereVolume:nil,Quobyte:nil,AzureDisk:nil,PhotonPersistentDisk:nil,PortworxVolume:nil,ScaleIO:nil,Projected:&ProjectedVolumeSource{Sources:[]VolumeProjection{VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:nil,ServiceAccountToken:&ServiceAccountTokenProjection{Audience:,ExpirationSeconds:*3607,Path:token,},},VolumeProjection{Secret:nil,DownwardAPI:nil,ConfigMap:&ConfigMapProjection{LocalObjectReference:LocalObjectReference{Name:kube-root-ca.crt,},Items:[]KeyToPath{KeyToPath{Key:ca.crt,Path:ca.crt,Mode:nil,},},Optional:nil,},ServiceAccountToken:nil,},VolumeProjection{Secret:nil,DownwardAPI:&DownwardAPIProjection{Items:[]DownwardAPIVolumeFile{DownwardAPIVolumeFile{Path:namespace,FieldRef:&ObjectFieldSelector{APIVersion:v1,FieldPath:metadata.namespace,},ResourceFieldRef:nil,Mode:nil,},},},ConfigMap:nil,ServiceAccountToken:nil,},},DefaultMode:*420,},StorageOS:nil,CSI:nil,Ephemeral:nil,},},},Containers:[]Container{Container{Name:httpd,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,Command:[],Args:[],WorkingDir:,Ports:[]ContainerPort{},Env:[]EnvVar{},Resources:ResourceRequirements{Limits:ResourceList{},Requests:ResourceList{},Claims:[]ResourceClaim{},},VolumeMounts:[]VolumeMount{VolumeMount{Name:kube-api-access-b82ns,ReadOnly:true,MountPath:/var/run/secrets/kubernetes.io/serviceaccount,SubPath:,MountPropagation:nil,SubPathExpr:,},},LivenessProbe:nil,ReadinessProbe:nil,Lifecycle:nil,TerminationMessagePath:/dev/termination-log,ImagePullPolicy:IfNotPresent,SecurityContext:&SecurityContext{Capabilities:nil,Privileged:nil,SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,ReadOnlyRootFilesystem:nil,AllowPrivilegeEscalation:nil,RunAsGroup:nil,ProcMount:nil,WindowsOptions:nil,SeccompProfile:nil,},Stdin:false,StdinOnce:false,TTY:false,EnvFrom:[]EnvFromSource{},TerminationMessagePolicy:File,VolumeDevices:[]VolumeDevice{},StartupProbe:nil,ResizePolicy:[]ContainerResizePolicy{},},},RestartPolicy:Always,TerminationGracePeriodSeconds:*0,ActiveDeadlineSeconds:nil,DNSPolicy:ClusterFirst,NodeSelector:map[string]string{},ServiceAccountName:default,DeprecatedServiceAccount:default,NodeName:ci9axai7aiv7-3,HostNetwork:false,HostPID:false,HostIPC:false,SecurityContext:&PodSecurityContext{SELinuxOptions:nil,RunAsUser:nil,RunAsNonRoot:nil,SupplementalGroups:[],FSGroup:nil,RunAsGroup:nil,Sysctls:[]Sysctl{},WindowsOptions:nil,FSGroupChangePolicy:nil,SeccompProfile:nil,},ImagePullSecrets:[]LocalObjectReference{},Hostname:,Subdomain:,Affinity:nil,SchedulerName:default-scheduler,InitContainers:[]Container{},AutomountServiceAccountToken:nil,Tolerations:[]Toleration{Toleration{Key:node.kubernetes.io/not-ready,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},Toleration{Key:node.kubernetes.io/unreachable,Operator:Exists,Value:,Effect:NoExecute,TolerationSeconds:*300,},},HostAliases:[]HostAlias{},PriorityClassName:,Priority:*0,DNSConfig:nil,ShareProcessNamespace:nil,ReadinessGates:[]PodReadinessGate{},RuntimeClassName:nil,EnableServiceLinks:*true,PreemptionPolicy:*PreemptLowerPriority,Overhead:ResourceList{},TopologySpreadConstraints:[]TopologySpreadConstraint{},EphemeralContainers:[]EphemeralContainer{},SetHostnameAsFQDN:nil,OS:nil,HostUsers:nil,SchedulingGates:[]PodSchedulingGate{},ResourceClaims:[]PodResourceClaim{},},Status:PodStatus{Phase:Running,Conditions:[]PodCondition{PodCondition{Type:Initialized,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:17:08 +0000 UTC,Reason:,Message:,},PodCondition{Type:Ready,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:ContainersReady,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:17:09 +0000 UTC,Reason:,Message:,},PodCondition{Type:PodScheduled,Status:True,LastProbeTime:0001-01-01 00:00:00 +0000 UTC,LastTransitionTime:2023-07-29 17:17:08 +0000 UTC,Reason:,Message:,},},Message:,Reason:,HostIP:192.168.121.144,PodIP:10.233.66.83,StartTime:2023-07-29 17:17:08 +0000 UTC,ContainerStatuses:[]ContainerStatus{ContainerStatus{Name:httpd,State:ContainerState{Waiting:nil,Running:&ContainerStateRunning{StartedAt:2023-07-29 17:17:09 +0000 UTC,},Terminated:nil,},LastTerminationState:ContainerState{Waiting:nil,Running:nil,Terminated:nil,},Ready:true,RestartCount:0,Image:registry.k8s.io/e2e-test-images/httpd:2.4.38-4,ImageID:registry.k8s.io/e2e-test-images/httpd@sha256:148b022f5c5da426fc2f3c14b5c0867e58ef05961510c84749ac1fddcb0fef22,ContainerID:cri-o://c23e81e34d8ad856bcf117faf979a2c7f435170e5d84e1aa9d34738258ab084e,Started:*true,AllocatedResources:ResourceList{},Resources:nil,},},QOSClass:BestEffort,InitContainerStatuses:[]ContainerStatus{},NominatedNodeName:,PodIPs:[]PodIP{PodIP{IP:10.233.66.83,},},EphemeralContainerStatuses:[]ContainerStatus{},Resize:,},}
  Jul 29 17:17:10.686: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "deployment-9969" for this suite. @ 07/29/23 17:17:10.695
• [2.204 seconds]
------------------------------
SSSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount an API token into pods  [Conformance]
test/e2e/auth/service_accounts.go:78
  STEP: Creating a kubernetes client @ 07/29/23 17:17:10.707
  Jul 29 17:17:10.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 17:17:10.709
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:17:10.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:17:10.747
  E0729 17:17:11.650499      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:12.650927      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: reading a file in the container @ 07/29/23 17:17:12.79
  Jul 29 17:17:12.791: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9994 pod-service-account-229c0be2-b625-4b72-9fb9-7cb0ea1c23aa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/token'
  STEP: reading a file in the container @ 07/29/23 17:17:13.062
  Jul 29 17:17:13.063: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9994 pod-service-account-229c0be2-b625-4b72-9fb9-7cb0ea1c23aa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/ca.crt'
  STEP: reading a file in the container @ 07/29/23 17:17:13.31
  Jul 29 17:17:13.311: INFO: Running '/usr/local/bin/kubectl exec --namespace=svcaccounts-9994 pod-service-account-229c0be2-b625-4b72-9fb9-7cb0ea1c23aa -c=test -- cat /var/run/secrets/kubernetes.io/serviceaccount/namespace'
  Jul 29 17:17:13.581: INFO: Got root ca configmap in namespace "svcaccounts-9994"
  Jul 29 17:17:13.585: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-9994" for this suite. @ 07/29/23 17:17:13.592
• [2.894 seconds]
------------------------------
SSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Projected configMap updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/projected_configmap.go:124
  STEP: Creating a kubernetes client @ 07/29/23 17:17:13.605
  Jul 29 17:17:13.605: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:17:13.609
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:17:13.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:17:13.646
  E0729 17:17:13.650995      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating projection with configMap that has name projected-configmap-test-upd-6997f3ed-027b-492f-8127-8d06847e5ab3 @ 07/29/23 17:17:13.661
  STEP: Creating the pod @ 07/29/23 17:17:13.67
  E0729 17:17:14.651607      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:15.652213      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Updating configmap projected-configmap-test-upd-6997f3ed-027b-492f-8127-8d06847e5ab3 @ 07/29/23 17:17:15.717
  STEP: waiting to observe update in volume @ 07/29/23 17:17:15.726
  E0729 17:17:16.652641      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:17.653586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:18.653665      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:19.653902      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:20.654271      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:21.655317      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:22.655816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:23.656198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:24.656129      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:25.656350      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:26.657395      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:27.658363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:28.658266      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:29.658599      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:30.658896      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:31.659215      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:32.659460      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:33.659580      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:34.660181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:35.660345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:36.661452      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:37.662206      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:38.662589      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:39.662913      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:40.663931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:41.664500      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:42.664644      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:43.665132      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:44.666214      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:45.666361      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:46.666930      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:47.666859      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:48.679651      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:49.678006      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:50.679359      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:51.679472      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:52.680351      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:53.680917      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:54.681513      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:55.682235      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:56.683075      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:57.683174      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:58.683777      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:17:59.684453      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:00.684579      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:01.685221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:02.686181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:03.686272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:04.686381      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:05.686586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:06.687287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:07.687365      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:08.688100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:09.688363      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:10.689531      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:11.689482      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:12.689647      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:13.690497      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:14.691427      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:15.692318      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:16.693763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:17.693737      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:18.694766      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:19.695811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:20.696769      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:21.697416      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:22.709131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:23.701786      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:24.702189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:25.702855      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:26.706813      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:27.703297      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:28.704362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:29.705273      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:18:30.438: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-9347" for this suite. @ 07/29/23 17:18:30.446
• [76.851 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should update a ServiceAccount [Conformance]
test/e2e/auth/service_accounts.go:808
  STEP: Creating a kubernetes client @ 07/29/23 17:18:30.457
  Jul 29 17:18:30.457: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 17:18:30.46
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:18:30.487
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:18:30.491
  STEP: Creating ServiceAccount "e2e-sa-gdd7v"  @ 07/29/23 17:18:30.494
  Jul 29 17:18:30.500: INFO: AutomountServiceAccountToken: false
  STEP: Updating ServiceAccount "e2e-sa-gdd7v"  @ 07/29/23 17:18:30.5
  Jul 29 17:18:30.512: INFO: AutomountServiceAccountToken: true
  Jul 29 17:18:30.512: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-8139" for this suite. @ 07/29/23 17:18:30.519
• [0.070 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] Secrets optional updates should be reflected in volume [NodeConformance] [Conformance]
test/e2e/common/storage/secrets_volume.go:205
  STEP: Creating a kubernetes client @ 07/29/23 17:18:30.531
  Jul 29 17:18:30.531: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename secrets @ 07/29/23 17:18:30.532
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:18:30.557
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:18:30.561
  STEP: Creating secret with name s-test-opt-del-1e9f1bbd-1fa7-4ca0-b1da-4bf7e96bbd74 @ 07/29/23 17:18:30.577
  STEP: Creating secret with name s-test-opt-upd-be3511cc-4df2-4264-ba20-f7db018798ba @ 07/29/23 17:18:30.591
  STEP: Creating the pod @ 07/29/23 17:18:30.598
  E0729 17:18:30.706338      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:31.707100      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting secret s-test-opt-del-1e9f1bbd-1fa7-4ca0-b1da-4bf7e96bbd74 @ 07/29/23 17:18:32.673
  STEP: Updating secret s-test-opt-upd-be3511cc-4df2-4264-ba20-f7db018798ba @ 07/29/23 17:18:32.687
  STEP: Creating secret with name s-test-opt-create-4cd0395c-8d01-4b6b-9227-1442d5f44e78 @ 07/29/23 17:18:32.694
  STEP: waiting to observe update in volume @ 07/29/23 17:18:32.7
  E0729 17:18:32.707860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:33.712390      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:34.709819      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:35.716175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:36.717529      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:37.717142      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:38.718016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:39.718811      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:40.718987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:41.720056      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:42.720229      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:43.720370      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:44.720539      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:45.720726      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:46.720862      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:47.720955      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:48.721265      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:49.721518      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:50.722130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:51.722586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:52.722953      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:53.723145      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:54.723789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:55.724535      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:56.725663      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:57.725935      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:58.726018      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:18:59.726201      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:00.726493      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:01.727221      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:02.727313      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:03.728189      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:04.728415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:05.729173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:06.729883      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:07.730012      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:08.730374      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:09.730846      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:10.731131      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:11.731874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:12.733288      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:13.732781      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:14.733293      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:15.733226      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:16.733856      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:17.734912      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:18.735158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:19.735969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:20.737034      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:21.737627      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:22.738614      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:23.739068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:24.739274      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:25.739731      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:26.740429      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:27.740524      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:28.740920      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:29.740979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:30.741377      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:31.742153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:32.742232      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:33.742553      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:34.742735      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:35.743219      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:36.743689      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:37.744176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:38.744707      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:39.745547      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:40.745734      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:41.746479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:42.746870      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:43.747005      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:44.747155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:45.747577      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:46.747852      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:47.748512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:48.749422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:49.749466      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:50.750000      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:51.750393      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:52.750575      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:19:53.529: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "secrets-7249" for this suite. @ 07/29/23 17:19:53.539
• [83.017 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] FieldValidation should detect unknown and duplicate fields of a typed object [Conformance]
test/e2e/apimachinery/field_validation.go:64
  STEP: Creating a kubernetes client @ 07/29/23 17:19:53.553
  Jul 29 17:19:53.553: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename field-validation @ 07/29/23 17:19:53.555
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:19:53.597
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:19:53.601
  STEP: apply creating a deployment @ 07/29/23 17:19:53.605
  Jul 29 17:19:53.608: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "field-validation-5891" for this suite. @ 07/29/23 17:19:53.636
• [0.093 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Pods should patch a pod status [Conformance]
test/e2e/common/node/pods.go:1084
  STEP: Creating a kubernetes client @ 07/29/23 17:19:53.652
  Jul 29 17:19:53.652: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename pods @ 07/29/23 17:19:53.654
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:19:53.688
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:19:53.694
  STEP: Create a pod @ 07/29/23 17:19:53.698
  E0729 17:19:53.751153      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:54.751488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: patching /status @ 07/29/23 17:19:55.728
  Jul 29 17:19:55.744: INFO: Status Message: "Patched by e2e test" and Reason: "E2E"
  Jul 29 17:19:55.746: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 17:19:55.752190      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "pods-8156" for this suite. @ 07/29/23 17:19:55.754
• [2.112 seconds]
------------------------------
SSSSS
------------------------------
[sig-storage] Projected downwardAPI should provide container's memory request [NodeConformance] [Conformance]
test/e2e/common/storage/projected_downwardapi.go:236
  STEP: Creating a kubernetes client @ 07/29/23 17:19:55.764
  Jul 29 17:19:55.764: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename projected @ 07/29/23 17:19:55.766
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:19:55.797
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:19:55.802
  STEP: Creating a pod to test downward API volume plugin @ 07/29/23 17:19:55.806
  E0729 17:19:56.753494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:57.753664      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:58.753794      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:19:59.754016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:19:59.848
  Jul 29 17:19:59.857: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downwardapi-volume-e50ab89d-c3bc-46e7-931f-1e082930fbf7 container client-container: <nil>
  STEP: delete the pod @ 07/29/23 17:19:59.871
  Jul 29 17:19:59.894: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "projected-2055" for this suite. @ 07/29/23 17:19:59.901
• [4.149 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] patching/updating a mutating webhook should work [Conformance]
test/e2e/apimachinery/webhook.go:497
  STEP: Creating a kubernetes client @ 07/29/23 17:19:59.915
  Jul 29 17:19:59.915: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:19:59.917
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:19:59.943
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:19:59.948
  STEP: Setting up server cert @ 07/29/23 17:19:59.989
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:20:00.732
  STEP: Deploying the webhook pod @ 07/29/23 17:20:00.75
  E0729 17:20:00.755175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Wait for the deployment to be ready @ 07/29/23 17:20:00.768
  Jul 29 17:20:00.777: INFO: new replicaset for deployment "sample-webhook-deployment" is yet to be created
  E0729 17:20:01.755339      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:02.756181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:20:02.798
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:20:02.816
  E0729 17:20:03.756307      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:03.816: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Creating a mutating webhook configuration @ 07/29/23 17:20:03.822
  STEP: Updating a mutating webhook configuration's rules to not include the create operation @ 07/29/23 17:20:03.856
  STEP: Creating a configMap that should not be mutated @ 07/29/23 17:20:03.874
  STEP: Patching a mutating webhook configuration's rules to include the create operation @ 07/29/23 17:20:03.903
  STEP: Creating a configMap that should be mutated @ 07/29/23 17:20:03.916
  Jul 29 17:20:03.962: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-3212" for this suite. @ 07/29/23 17:20:04.056
  STEP: Destroying namespace "webhook-markers-2711" for this suite. @ 07/29/23 17:20:04.069
• [4.166 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of different groups [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:276
  STEP: Creating a kubernetes client @ 07/29/23 17:20:04.083
  Jul 29 17:20:04.083: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:20:04.085
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:04.121
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:04.126
  STEP: CRs in different groups (two CRDs) show up in OpenAPI documentation @ 07/29/23 17:20:04.13
  Jul 29 17:20:04.131: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  E0729 17:20:04.756687      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:05.756639      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:05.851: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  E0729 17:20:06.757816      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:07.758624      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:08.759793      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:09.760130      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:10.760827      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:11.761586      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:12.762243      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:13.283: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-7223" for this suite. @ 07/29/23 17:20:13.303
• [9.231 seconds]
------------------------------
SSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] CustomResourcePublishOpenAPI [Privileged:ClusterAdmin] works for multiple CRDs of same group but different versions [Conformance]
test/e2e/apimachinery/crd_publish_openapi.go:309
  STEP: Creating a kubernetes client @ 07/29/23 17:20:13.317
  Jul 29 17:20:13.317: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename crd-publish-openapi @ 07/29/23 17:20:13.32
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:13.345
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:13.348
  STEP: CRs in the same group but different versions (one multiversion CRD) show up in OpenAPI documentation @ 07/29/23 17:20:13.352
  Jul 29 17:20:13.353: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  E0729 17:20:13.763467      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:14.763795      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:15.764366      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:16.785121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:17.785062      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:18.786540      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:19.787283      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:20.788015      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: CRs in the same group but different versions (two CRDs) show up in OpenAPI documentation @ 07/29/23 17:20:21.298
  Jul 29 17:20:21.299: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  E0729 17:20:21.788548      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:22.788552      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:23.017: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  E0729 17:20:23.789717      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:24.790458      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:25.791299      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:26.792169      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:27.793188      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:28.792954      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:29.793538      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:30.666: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "crd-publish-openapi-4312" for this suite. @ 07/29/23 17:20:30.69
• [17.382 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a replica set. [Conformance]
test/e2e/apimachinery/resource_quota.go:451
  STEP: Creating a kubernetes client @ 07/29/23 17:20:30.707
  Jul 29 17:20:30.707: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:20:30.71
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:30.738
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:30.743
  STEP: Counting existing ResourceQuota @ 07/29/23 17:20:30.751
  E0729 17:20:30.794352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:31.794918      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:32.796306      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:33.796964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:34.797242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 17:20:35.757
  STEP: Ensuring resource quota status is calculated @ 07/29/23 17:20:35.772
  E0729 17:20:35.798345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:36.799193      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ReplicaSet @ 07/29/23 17:20:37.781
  E0729 17:20:37.799525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota status captures replicaset creation @ 07/29/23 17:20:37.799
  E0729 17:20:38.799980      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:39.800384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting a ReplicaSet @ 07/29/23 17:20:39.809
  STEP: Ensuring resource quota status released usage @ 07/29/23 17:20:39.818
  E0729 17:20:40.801181      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:41.801946      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:41.826: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-2554" for this suite. @ 07/29/23 17:20:41.834
• [11.137 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-auth] ServiceAccounts should mount projected service account token [Conformance]
test/e2e/auth/service_accounts.go:275
  STEP: Creating a kubernetes client @ 07/29/23 17:20:41.848
  Jul 29 17:20:41.848: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename svcaccounts @ 07/29/23 17:20:41.85
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:41.875
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:41.881
  STEP: Creating a pod to test service account token:  @ 07/29/23 17:20:41.885
  E0729 17:20:42.802989      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:43.803530      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:44.803941      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:45.804152      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:20:45.925
  Jul 29 17:20:45.930: INFO: Trying to get logs from node ci9axai7aiv7-3 pod test-pod-6b83f7cd-cbbc-40c8-9dda-d08e37590ae0 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 17:20:45.959
  Jul 29 17:20:45.984: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "svcaccounts-3732" for this suite. @ 07/29/23 17:20:45.992
• [4.152 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl server-side dry-run should check if kubectl can dry-run update Pods [Conformance]
test/e2e/kubectl/kubectl.go:1027
  STEP: Creating a kubernetes client @ 07/29/23 17:20:46.006
  Jul 29 17:20:46.006: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:20:46.008
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:46.039
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:46.043
  STEP: running the image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 17:20:46.048
  Jul 29 17:20:46.048: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-8843 run e2e-test-httpd-pod --image=registry.k8s.io/e2e-test-images/httpd:2.4.38-4 --pod-running-timeout=2m0s --labels=run=e2e-test-httpd-pod'
  Jul 29 17:20:46.253: INFO: stderr: ""
  Jul 29 17:20:46.253: INFO: stdout: "pod/e2e-test-httpd-pod created\n"
  STEP: replace the image in the pod with server-side dry-run @ 07/29/23 17:20:46.253
  Jul 29 17:20:46.254: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-8843 patch pod e2e-test-httpd-pod -p {"spec":{"containers":[{"name": "e2e-test-httpd-pod","image": "registry.k8s.io/e2e-test-images/busybox:1.29-4"}]}} --dry-run=server'
  Jul 29 17:20:46.434: INFO: stderr: ""
  Jul 29 17:20:46.435: INFO: stdout: "pod/e2e-test-httpd-pod patched\n"
  STEP: verifying the pod e2e-test-httpd-pod has the right image registry.k8s.io/e2e-test-images/httpd:2.4.38-4 @ 07/29/23 17:20:46.435
  Jul 29 17:20:46.442: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-8843 delete pods e2e-test-httpd-pod'
  E0729 17:20:46.804753      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:47.804975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:48.805772      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:48.891: INFO: stderr: ""
  Jul 29 17:20:48.891: INFO: stdout: "pod \"e2e-test-httpd-pod\" deleted\n"
  Jul 29 17:20:48.891: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-8843" for this suite. @ 07/29/23 17:20:48.903
• [2.907 seconds]
------------------------------
[sig-api-machinery] Garbage collector should orphan RS created by deployment when deleteOptions.PropagationPolicy is Orphan [Conformance]
test/e2e/apimachinery/garbage_collector.go:538
  STEP: Creating a kubernetes client @ 07/29/23 17:20:48.913
  Jul 29 17:20:48.913: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename gc @ 07/29/23 17:20:48.915
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:48.942
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:48.95
  STEP: create the deployment @ 07/29/23 17:20:48.955
  W0729 17:20:48.963730      13 warnings.go:70] metadata.name: this is used in Pod names and hostnames, which can result in surprising behavior; a DNS label is recommended: [must not contain dots]
  STEP: Wait for the Deployment to create new ReplicaSet @ 07/29/23 17:20:48.964
  STEP: delete the deployment @ 07/29/23 17:20:49.491
  STEP: wait for deployment deletion to see if the garbage collector mistakenly deletes the rs @ 07/29/23 17:20:49.503
  E0729 17:20:49.806362      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Gathering metrics @ 07/29/23 17:20:50.056
  Jul 29 17:20:50.236: INFO: For apiserver_request_total:
  For apiserver_request_latency_seconds:
  For apiserver_init_events_total:
  For garbage_collector_attempt_to_delete_queue_latency:
  For garbage_collector_attempt_to_delete_work_duration:
  For garbage_collector_attempt_to_orphan_queue_latency:
  For garbage_collector_attempt_to_orphan_work_duration:
  For garbage_collector_dirty_processing_latency_microseconds:
  For garbage_collector_event_processing_latency_microseconds:
  For garbage_collector_graph_changes_queue_latency:
  For garbage_collector_graph_changes_work_duration:
  For garbage_collector_orphan_processing_latency_microseconds:
  For namespace_queue_latency:
  For namespace_queue_latency_sum:
  For namespace_queue_latency_count:
  For namespace_retries:
  For namespace_work_duration:
  For namespace_work_duration_sum:
  For namespace_work_duration_count:
  For function_duration_seconds:
  For errors_total:
  For evicted_pods_total:

  Jul 29 17:20:50.236: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "gc-5706" for this suite. @ 07/29/23 17:20:50.248
• [1.356 seconds]
------------------------------
SS
------------------------------
[sig-storage] CSIInlineVolumes should support ephemeral VolumeLifecycleMode in CSIDriver API [Conformance]
test/e2e/storage/csi_inline.go:46
  STEP: Creating a kubernetes client @ 07/29/23 17:20:50.271
  Jul 29 17:20:50.271: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename csiinlinevolumes @ 07/29/23 17:20:50.273
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:50.298
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:50.303
  STEP: creating @ 07/29/23 17:20:50.307
  STEP: getting @ 07/29/23 17:20:50.347
  STEP: listing @ 07/29/23 17:20:50.36
  STEP: deleting @ 07/29/23 17:20:50.365
  Jul 29 17:20:50.394: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "csiinlinevolumes-6230" for this suite. @ 07/29/23 17:20:50.402
• [0.143 seconds]
------------------------------
SSS
------------------------------
[sig-network] Services should be able to change the type from ExternalName to ClusterIP [Conformance]
test/e2e/network/service.go:1416
  STEP: Creating a kubernetes client @ 07/29/23 17:20:50.414
  Jul 29 17:20:50.414: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename services @ 07/29/23 17:20:50.416
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:50.475
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:50.48
  STEP: creating a service externalname-service with the type=ExternalName in namespace services-1075 @ 07/29/23 17:20:50.487
  STEP: changing the ExternalName service to type=ClusterIP @ 07/29/23 17:20:50.498
  STEP: creating replication controller externalname-service in namespace services-1075 @ 07/29/23 17:20:50.523
  I0729 17:20:50.555192      13 runners.go:194] Created replication controller with name: externalname-service, namespace: services-1075, replica count: 2
  E0729 17:20:50.806944      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:51.807916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:52.808057      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  I0729 17:20:53.606654      13 runners.go:194] externalname-service Pods: 2 out of 2 created, 2 running, 0 pending, 0 waiting, 0 inactive, 0 terminating, 0 unknown, 0 runningButNotReady 
  Jul 29 17:20:53.606: INFO: Creating new exec pod
  E0729 17:20:53.808372      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:54.808931      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:55.809742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:56.638: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-1075 exec execpodxblvz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  E0729 17:20:56.810837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:56.902: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 29 17:20:56.902: INFO: stdout: ""
  E0729 17:20:57.811016      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:20:57.905: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-1075 exec execpodxblvz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 externalname-service 80'
  Jul 29 17:20:58.176: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 externalname-service 80\nConnection to externalname-service 80 port [tcp/http] succeeded!\n"
  Jul 29 17:20:58.176: INFO: stdout: "externalname-service-f592r"
  Jul 29 17:20:58.176: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=services-1075 exec execpodxblvz -- /bin/sh -x -c echo hostName | nc -v -t -w 2 10.233.15.32 80'
  Jul 29 17:20:58.409: INFO: stderr: "+ echo hostName\n+ nc -v -t -w 2 10.233.15.32 80\nConnection to 10.233.15.32 80 port [tcp/http] succeeded!\n"
  Jul 29 17:20:58.409: INFO: stdout: "externalname-service-f592r"
  Jul 29 17:20:58.410: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 17:20:58.417: INFO: Cleaning up the ExternalName to ClusterIP test service
  STEP: Destroying namespace "services-1075" for this suite. @ 07/29/23 17:20:58.445
• [8.048 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Kubelet when scheduling a read only busybox container should not write to root filesystem [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/node/kubelet.go:184
  STEP: Creating a kubernetes client @ 07/29/23 17:20:58.464
  Jul 29 17:20:58.464: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubelet-test @ 07/29/23 17:20:58.466
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:20:58.493
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:20:58.498
  E0729 17:20:58.812230      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:20:59.813491      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:00.566: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubelet-test-803" for this suite. @ 07/29/23 17:21:00.58
• [2.135 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should mutate custom resource [Conformance]
test/e2e/apimachinery/webhook.go:284
  STEP: Creating a kubernetes client @ 07/29/23 17:21:00.603
  Jul 29 17:21:00.603: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:21:00.606
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:00.639
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:00.645
  STEP: Setting up server cert @ 07/29/23 17:21:00.684
  E0729 17:21:00.813352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:01.813928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:21:02.095
  STEP: Deploying the webhook pod @ 07/29/23 17:21:02.138
  STEP: Wait for the deployment to be ready @ 07/29/23 17:21:02.185
  Jul 29 17:21:02.202: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 17:21:02.817608      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:03.820355      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:21:04.228
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:21:04.249
  E0729 17:21:04.817994      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:05.250: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  Jul 29 17:21:05.272: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Registering the mutating webhook for custom resource e2e-test-webhook-7328-crds.webhook.example.com via the AdmissionRegistration API @ 07/29/23 17:21:05.795
  E0729 17:21:05.818032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a custom resource that should be mutated by the webhook @ 07/29/23 17:21:05.841
  E0729 17:21:06.819133      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:07.819461      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:08.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "webhook-951" for this suite. @ 07/29/23 17:21:08.68
  STEP: Destroying namespace "webhook-markers-9361" for this suite. @ 07/29/23 17:21:08.689
• [8.096 seconds]
------------------------------
SS
------------------------------
[sig-node] Variable Expansion should fail substituting values in a volume subpath with absolute path [Slow] [Conformance]
test/e2e/common/node/expansion.go:189
  STEP: Creating a kubernetes client @ 07/29/23 17:21:08.701
  Jul 29 17:21:08.701: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename var-expansion @ 07/29/23 17:21:08.703
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:08.726
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:08.731
  E0729 17:21:08.820304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:09.820546      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:10.767: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  Jul 29 17:21:10.779: INFO: Deleting pod "var-expansion-4993edb2-538c-45bf-ac89-440815b1a20d" in namespace "var-expansion-8918"
  Jul 29 17:21:10.792: INFO: Wait up to 5m0s for pod "var-expansion-4993edb2-538c-45bf-ac89-440815b1a20d" to be fully deleted
  E0729 17:21:10.821175      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:11.821778      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "var-expansion-8918" for this suite. @ 07/29/23 17:21:12.807
• [4.117 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Watchers should observe an object deletion if it stops meeting the requirements of the selector [Conformance]
test/e2e/apimachinery/watch.go:257
  STEP: Creating a kubernetes client @ 07/29/23 17:21:12.825
  Jul 29 17:21:12.826: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename watch @ 07/29/23 17:21:12.828
  E0729 17:21:12.828993      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:12.852
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:12.855
  STEP: creating a watch on configmaps with a certain label @ 07/29/23 17:21:12.86
  STEP: creating a new configmap @ 07/29/23 17:21:12.861
  STEP: modifying the configmap once @ 07/29/23 17:21:12.869
  STEP: changing the label value of the configmap @ 07/29/23 17:21:12.882
  STEP: Expecting to observe a delete notification for the watched object @ 07/29/23 17:21:12.893
  Jul 29 17:21:12.894: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9450  6b5008c3-66e9-48d4-a317-8038832da9f7 43160 0 2023-07-29 17:21:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 17:21:12 +0000 UTC FieldsV1 {"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:21:12.895: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9450  6b5008c3-66e9-48d4-a317-8038832da9f7 43161 0 2023-07-29 17:21:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 17:21:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:21:12.895: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9450  6b5008c3-66e9-48d4-a317-8038832da9f7 43162 0 2023-07-29 17:21:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 17:21:12 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 1,},BinaryData:map[string][]byte{},Immutable:nil,}
  STEP: modifying the configmap a second time @ 07/29/23 17:21:12.895
  STEP: Expecting not to observe a notification because the object no longer meets the selector's requirements @ 07/29/23 17:21:12.907
  E0729 17:21:13.829277      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:14.829959      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:15.830983      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:16.831833      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:17.831956      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:18.832525      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:19.832685      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:20.832875      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:21.833742      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:22.833869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: changing the label value of the configmap back @ 07/29/23 17:21:22.908
  STEP: modifying the configmap a third time @ 07/29/23 17:21:22.926
  STEP: deleting the configmap @ 07/29/23 17:21:22.939
  STEP: Expecting to observe an add notification for the watched object when the label value was restored @ 07/29/23 17:21:22.951
  Jul 29 17:21:22.951: INFO: Got : ADDED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9450  6b5008c3-66e9-48d4-a317-8038832da9f7 43208 0 2023-07-29 17:21:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 17:21:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 2,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:21:22.951: INFO: Got : MODIFIED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9450  6b5008c3-66e9-48d4-a317-8038832da9f7 43209 0 2023-07-29 17:21:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 17:21:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:21:22.952: INFO: Got : DELETED &ConfigMap{ObjectMeta:{e2e-watch-test-label-changed  watch-9450  6b5008c3-66e9-48d4-a317-8038832da9f7 43210 0 2023-07-29 17:21:12 +0000 UTC <nil> <nil> map[watch-this-configmap:label-changed-and-restored] map[] [] [] [{e2e.test Update v1 2023-07-29 17:21:22 +0000 UTC FieldsV1 {"f:data":{".":{},"f:mutation":{}},"f:metadata":{"f:labels":{".":{},"f:watch-this-configmap":{}}}} }]},Data:map[string]string{mutation: 3,},BinaryData:map[string][]byte{},Immutable:nil,}
  Jul 29 17:21:22.953: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "watch-9450" for this suite. @ 07/29/23 17:21:22.962
• [10.148 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes pod should support shared volumes between containers [Conformance]
test/e2e/common/storage/empty_dir.go:227
  STEP: Creating a kubernetes client @ 07/29/23 17:21:22.98
  Jul 29 17:21:22.980: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:21:22.983
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:23.06
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:23.07
  STEP: Creating Pod @ 07/29/23 17:21:23.075
  E0729 17:21:23.834615      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:24.835422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Reading file content from the nginx-container @ 07/29/23 17:21:25.12
  Jul 29 17:21:25.120: INFO: ExecWithOptions {Command:[/bin/sh -c cat /usr/share/volumeshare/shareddata.txt] Namespace:emptydir-9602 PodName:pod-sharedvolume-34850cdc-f91f-46bb-9e97-c60147df7b7e ContainerName:busybox-main-container Stdin:<nil> CaptureStdout:true CaptureStderr:true PreserveWhitespace:false Quiet:false}
  Jul 29 17:21:25.120: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:21:25.122: INFO: ExecWithOptions: Clientset creation
  Jul 29 17:21:25.122: INFO: ExecWithOptions: execute(POST https://10.233.0.1:443/api/v1/namespaces/emptydir-9602/pods/pod-sharedvolume-34850cdc-f91f-46bb-9e97-c60147df7b7e/exec?command=%2Fbin%2Fsh&command=-c&command=cat+%2Fusr%2Fshare%2Fvolumeshare%2Fshareddata.txt&container=busybox-main-container&container=busybox-main-container&stderr=true&stdout=true)
  Jul 29 17:21:25.213: INFO: Exec stderr: ""
  Jul 29 17:21:25.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-9602" for this suite. @ 07/29/23 17:21:25.221
• [2.252 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-cli] Kubectl client Kubectl version should check is all data is printed  [Conformance]
test/e2e/kubectl/kubectl.go:1673
  STEP: Creating a kubernetes client @ 07/29/23 17:21:25.24
  Jul 29 17:21:25.240: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename kubectl @ 07/29/23 17:21:25.244
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:25.271
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:25.276
  Jul 29 17:21:25.281: INFO: Running '/usr/local/bin/kubectl --kubeconfig=/tmp/kubeconfig-3894441350 --namespace=kubectl-7966 version'
  Jul 29 17:21:25.411: INFO: stderr: "WARNING: This version information is deprecated and will be replaced with the output from kubectl version --short.  Use --output=yaml|json to get the full version.\n"
  Jul 29 17:21:25.411: INFO: stdout: "Client Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:20:54Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\nKustomize Version: v5.0.1\nServer Version: version.Info{Major:\"1\", Minor:\"27\", GitVersion:\"v1.27.4\", GitCommit:\"fa3d7990104d7c1f16943a67f11b154b71f6a132\", GitTreeState:\"clean\", BuildDate:\"2023-07-19T12:14:49Z\", GoVersion:\"go1.20.6\", Compiler:\"gc\", Platform:\"linux/amd64\"}\n"
  Jul 29 17:21:25.412: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "kubectl-7966" for this suite. @ 07/29/23 17:21:25.421
• [0.190 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] Downward API should provide container's limits.cpu/memory and requests.cpu/memory as env vars [NodeConformance] [Conformance]
test/e2e/common/node/downwardapi.go:167
  STEP: Creating a kubernetes client @ 07/29/23 17:21:25.437
  Jul 29 17:21:25.437: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename downward-api @ 07/29/23 17:21:25.439
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:25.464
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:25.468
  STEP: Creating a pod to test downward api env vars @ 07/29/23 17:21:25.472
  E0729 17:21:25.835494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:26.836511      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:27.836573      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:28.836848      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:21:29.525
  Jul 29 17:21:29.531: INFO: Trying to get logs from node ci9axai7aiv7-3 pod downward-api-ef841b5d-9453-40c9-a2ef-279032e2aa3e container dapi-container: <nil>
  STEP: delete the pod @ 07/29/23 17:21:29.543
  Jul 29 17:21:29.563: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "downward-api-4892" for this suite. @ 07/29/23 17:21:29.57
• [4.142 seconds]
------------------------------
SSSS
------------------------------
[sig-api-machinery] AdmissionWebhook [Privileged:ClusterAdmin] should deny crd creation [Conformance]
test/e2e/apimachinery/webhook.go:300
  STEP: Creating a kubernetes client @ 07/29/23 17:21:29.581
  Jul 29 17:21:29.581: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename webhook @ 07/29/23 17:21:29.583
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:29.602
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:29.607
  STEP: Setting up server cert @ 07/29/23 17:21:29.639
  E0729 17:21:29.837710      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Create role binding to let webhook read extension-apiserver-authentication @ 07/29/23 17:21:30.586
  STEP: Deploying the webhook pod @ 07/29/23 17:21:30.6
  STEP: Wait for the deployment to be ready @ 07/29/23 17:21:30.626
  Jul 29 17:21:30.640: INFO: deployment "sample-webhook-deployment" doesn't have the required revision set
  E0729 17:21:30.839031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:31.839038      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deploying the webhook service @ 07/29/23 17:21:32.662
  STEP: Verifying the service has paired with the endpoint @ 07/29/23 17:21:32.682
  E0729 17:21:32.840158      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:33.682: INFO: Waiting for amount of service:e2e-test-webhook endpoints to be 1
  STEP: Registering the crd webhook via the AdmissionRegistration API @ 07/29/23 17:21:33.694
  STEP: Creating a custom resource definition that should be denied by the webhook @ 07/29/23 17:21:33.724
  Jul 29 17:21:33.725: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  Jul 29 17:21:33.745: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 17:21:33.840916      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "webhook-6119" for this suite. @ 07/29/23 17:21:33.855
  STEP: Destroying namespace "webhook-markers-4592" for this suite. @ 07/29/23 17:21:33.891
• [4.331 seconds]
------------------------------
SSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should create a ResourceQuota and capture the life of a pod. [Conformance]
test/e2e/apimachinery/resource_quota.go:232
  STEP: Creating a kubernetes client @ 07/29/23 17:21:33.917
  Jul 29 17:21:33.917: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:21:33.919
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:33.951
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:33.96
  STEP: Counting existing ResourceQuota @ 07/29/23 17:21:33.966
  E0729 17:21:34.841055      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:35.841975      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:36.843198      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:37.843151      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:38.843817      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota @ 07/29/23 17:21:38.975
  STEP: Ensuring resource quota status is calculated @ 07/29/23 17:21:38.996
  E0729 17:21:39.844352      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:40.845334      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a Pod that fits quota @ 07/29/23 17:21:41.004
  STEP: Ensuring ResourceQuota status captures the pod usage @ 07/29/23 17:21:41.025
  E0729 17:21:41.845901      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:42.846512      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Not allowing a pod to be created that exceeds remaining quota @ 07/29/23 17:21:43.034
  STEP: Not allowing a pod to be created that exceeds remaining quota(validation on extended resources) @ 07/29/23 17:21:43.038
  STEP: Ensuring a pod cannot update its resource requirements @ 07/29/23 17:21:43.042
  STEP: Ensuring attempts to update pod resource requirements did not change quota usage @ 07/29/23 17:21:43.049
  E0729 17:21:43.846952      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:44.847874      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/29/23 17:21:45.058
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 17:21:45.079
  E0729 17:21:45.847923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:46.848039      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:47.091: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-883" for this suite. @ 07/29/23 17:21:47.103
• [13.202 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-node] InitContainer [NodeConformance] should invoke init containers on a RestartNever pod [Conformance]
test/e2e/common/node/init_container.go:177
  STEP: Creating a kubernetes client @ 07/29/23 17:21:47.122
  Jul 29 17:21:47.122: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename init-container @ 07/29/23 17:21:47.125
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:47.197
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:47.201
  STEP: creating the pod @ 07/29/23 17:21:47.206
  Jul 29 17:21:47.206: INFO: PodSpec: initContainers in spec.initContainers
  E0729 17:21:47.851317      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:48.851667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:49.851693      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:50.851908      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:51.853060      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:52.337: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "init-container-9419" for this suite. @ 07/29/23 17:21:52.351
• [5.238 seconds]
------------------------------
SSS
------------------------------
[sig-api-machinery] CustomResourceDefinition resources [Privileged:ClusterAdmin] Simple CustomResourceDefinition getting/updating/patching custom resource definition status sub-resource works  [Conformance]
test/e2e/apimachinery/custom_resource_definition.go:145
  STEP: Creating a kubernetes client @ 07/29/23 17:21:52.361
  Jul 29 17:21:52.361: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename custom-resource-definition @ 07/29/23 17:21:52.364
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:52.39
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:52.398
  Jul 29 17:21:52.406: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  E0729 17:21:52.853612      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:52.975: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "custom-resource-definition-9109" for this suite. @ 07/29/23 17:21:52.985
• [0.634 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Servers with support for Table transformation should return a 406 for a backend which does not implement metadata [Conformance]
test/e2e/apimachinery/table_conversion.go:154
  STEP: Creating a kubernetes client @ 07/29/23 17:21:53.001
  Jul 29 17:21:53.001: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename tables @ 07/29/23 17:21:53.004
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:53.025
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:53.032
  Jul 29 17:21:53.041: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "tables-5368" for this suite. @ 07/29/23 17:21:53.05
• [0.060 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[sig-storage] ConfigMap should be consumable from pods in volume with mappings and Item mode set [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/configmap_volume.go:99
  STEP: Creating a kubernetes client @ 07/29/23 17:21:53.064
  Jul 29 17:21:53.064: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:21:53.067
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:53.093
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:53.1
  STEP: Creating configMap with name configmap-test-volume-map-4cd47c56-91b2-47e9-900e-7526088b8a26 @ 07/29/23 17:21:53.107
  STEP: Creating a pod to test consume configMaps @ 07/29/23 17:21:53.114
  E0729 17:21:53.853945      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:54.854557      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:55.855242      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:56.856059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:21:57.168
  Jul 29 17:21:57.173: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-configmaps-32c08f29-1387-4f01-b035-a15f148c5075 container agnhost-container: <nil>
  STEP: delete the pod @ 07/29/23 17:21:57.185
  Jul 29 17:21:57.214: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5823" for this suite. @ 07/29/23 17:21:57.223
• [4.175 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] Discovery should validate PreferredVersion for each APIGroup [Conformance]
test/e2e/apimachinery/discovery.go:122
  STEP: Creating a kubernetes client @ 07/29/23 17:21:57.249
  Jul 29 17:21:57.249: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename discovery @ 07/29/23 17:21:57.251
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:57.278
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:57.283
  STEP: Setting up server cert @ 07/29/23 17:21:57.29
  E0729 17:21:57.856904      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:21:58.401: INFO: Checking APIGroup: apiregistration.k8s.io
  Jul 29 17:21:58.403: INFO: PreferredVersion.GroupVersion: apiregistration.k8s.io/v1
  Jul 29 17:21:58.403: INFO: Versions found [{apiregistration.k8s.io/v1 v1}]
  Jul 29 17:21:58.404: INFO: apiregistration.k8s.io/v1 matches apiregistration.k8s.io/v1
  Jul 29 17:21:58.404: INFO: Checking APIGroup: apps
  Jul 29 17:21:58.405: INFO: PreferredVersion.GroupVersion: apps/v1
  Jul 29 17:21:58.406: INFO: Versions found [{apps/v1 v1}]
  Jul 29 17:21:58.406: INFO: apps/v1 matches apps/v1
  Jul 29 17:21:58.406: INFO: Checking APIGroup: events.k8s.io
  Jul 29 17:21:58.407: INFO: PreferredVersion.GroupVersion: events.k8s.io/v1
  Jul 29 17:21:58.407: INFO: Versions found [{events.k8s.io/v1 v1}]
  Jul 29 17:21:58.407: INFO: events.k8s.io/v1 matches events.k8s.io/v1
  Jul 29 17:21:58.407: INFO: Checking APIGroup: authentication.k8s.io
  Jul 29 17:21:58.410: INFO: PreferredVersion.GroupVersion: authentication.k8s.io/v1
  Jul 29 17:21:58.411: INFO: Versions found [{authentication.k8s.io/v1 v1}]
  Jul 29 17:21:58.411: INFO: authentication.k8s.io/v1 matches authentication.k8s.io/v1
  Jul 29 17:21:58.411: INFO: Checking APIGroup: authorization.k8s.io
  Jul 29 17:21:58.412: INFO: PreferredVersion.GroupVersion: authorization.k8s.io/v1
  Jul 29 17:21:58.412: INFO: Versions found [{authorization.k8s.io/v1 v1}]
  Jul 29 17:21:58.412: INFO: authorization.k8s.io/v1 matches authorization.k8s.io/v1
  Jul 29 17:21:58.412: INFO: Checking APIGroup: autoscaling
  Jul 29 17:21:58.414: INFO: PreferredVersion.GroupVersion: autoscaling/v2
  Jul 29 17:21:58.414: INFO: Versions found [{autoscaling/v2 v2} {autoscaling/v1 v1}]
  Jul 29 17:21:58.414: INFO: autoscaling/v2 matches autoscaling/v2
  Jul 29 17:21:58.414: INFO: Checking APIGroup: batch
  Jul 29 17:21:58.415: INFO: PreferredVersion.GroupVersion: batch/v1
  Jul 29 17:21:58.415: INFO: Versions found [{batch/v1 v1}]
  Jul 29 17:21:58.415: INFO: batch/v1 matches batch/v1
  Jul 29 17:21:58.415: INFO: Checking APIGroup: certificates.k8s.io
  Jul 29 17:21:58.418: INFO: PreferredVersion.GroupVersion: certificates.k8s.io/v1
  Jul 29 17:21:58.418: INFO: Versions found [{certificates.k8s.io/v1 v1}]
  Jul 29 17:21:58.418: INFO: certificates.k8s.io/v1 matches certificates.k8s.io/v1
  Jul 29 17:21:58.418: INFO: Checking APIGroup: networking.k8s.io
  Jul 29 17:21:58.420: INFO: PreferredVersion.GroupVersion: networking.k8s.io/v1
  Jul 29 17:21:58.420: INFO: Versions found [{networking.k8s.io/v1 v1}]
  Jul 29 17:21:58.420: INFO: networking.k8s.io/v1 matches networking.k8s.io/v1
  Jul 29 17:21:58.420: INFO: Checking APIGroup: policy
  Jul 29 17:21:58.424: INFO: PreferredVersion.GroupVersion: policy/v1
  Jul 29 17:21:58.425: INFO: Versions found [{policy/v1 v1}]
  Jul 29 17:21:58.425: INFO: policy/v1 matches policy/v1
  Jul 29 17:21:58.425: INFO: Checking APIGroup: rbac.authorization.k8s.io
  Jul 29 17:21:58.428: INFO: PreferredVersion.GroupVersion: rbac.authorization.k8s.io/v1
  Jul 29 17:21:58.428: INFO: Versions found [{rbac.authorization.k8s.io/v1 v1}]
  Jul 29 17:21:58.428: INFO: rbac.authorization.k8s.io/v1 matches rbac.authorization.k8s.io/v1
  Jul 29 17:21:58.428: INFO: Checking APIGroup: storage.k8s.io
  Jul 29 17:21:58.430: INFO: PreferredVersion.GroupVersion: storage.k8s.io/v1
  Jul 29 17:21:58.430: INFO: Versions found [{storage.k8s.io/v1 v1}]
  Jul 29 17:21:58.430: INFO: storage.k8s.io/v1 matches storage.k8s.io/v1
  Jul 29 17:21:58.430: INFO: Checking APIGroup: admissionregistration.k8s.io
  Jul 29 17:21:58.432: INFO: PreferredVersion.GroupVersion: admissionregistration.k8s.io/v1
  Jul 29 17:21:58.432: INFO: Versions found [{admissionregistration.k8s.io/v1 v1}]
  Jul 29 17:21:58.432: INFO: admissionregistration.k8s.io/v1 matches admissionregistration.k8s.io/v1
  Jul 29 17:21:58.432: INFO: Checking APIGroup: apiextensions.k8s.io
  Jul 29 17:21:58.433: INFO: PreferredVersion.GroupVersion: apiextensions.k8s.io/v1
  Jul 29 17:21:58.434: INFO: Versions found [{apiextensions.k8s.io/v1 v1}]
  Jul 29 17:21:58.434: INFO: apiextensions.k8s.io/v1 matches apiextensions.k8s.io/v1
  Jul 29 17:21:58.434: INFO: Checking APIGroup: scheduling.k8s.io
  Jul 29 17:21:58.435: INFO: PreferredVersion.GroupVersion: scheduling.k8s.io/v1
  Jul 29 17:21:58.436: INFO: Versions found [{scheduling.k8s.io/v1 v1}]
  Jul 29 17:21:58.436: INFO: scheduling.k8s.io/v1 matches scheduling.k8s.io/v1
  Jul 29 17:21:58.436: INFO: Checking APIGroup: coordination.k8s.io
  Jul 29 17:21:58.438: INFO: PreferredVersion.GroupVersion: coordination.k8s.io/v1
  Jul 29 17:21:58.439: INFO: Versions found [{coordination.k8s.io/v1 v1}]
  Jul 29 17:21:58.439: INFO: coordination.k8s.io/v1 matches coordination.k8s.io/v1
  Jul 29 17:21:58.439: INFO: Checking APIGroup: node.k8s.io
  Jul 29 17:21:58.440: INFO: PreferredVersion.GroupVersion: node.k8s.io/v1
  Jul 29 17:21:58.440: INFO: Versions found [{node.k8s.io/v1 v1}]
  Jul 29 17:21:58.441: INFO: node.k8s.io/v1 matches node.k8s.io/v1
  Jul 29 17:21:58.441: INFO: Checking APIGroup: discovery.k8s.io
  Jul 29 17:21:58.443: INFO: PreferredVersion.GroupVersion: discovery.k8s.io/v1
  Jul 29 17:21:58.443: INFO: Versions found [{discovery.k8s.io/v1 v1}]
  Jul 29 17:21:58.443: INFO: discovery.k8s.io/v1 matches discovery.k8s.io/v1
  Jul 29 17:21:58.443: INFO: Checking APIGroup: flowcontrol.apiserver.k8s.io
  Jul 29 17:21:58.444: INFO: PreferredVersion.GroupVersion: flowcontrol.apiserver.k8s.io/v1beta3
  Jul 29 17:21:58.445: INFO: Versions found [{flowcontrol.apiserver.k8s.io/v1beta3 v1beta3} {flowcontrol.apiserver.k8s.io/v1beta2 v1beta2}]
  Jul 29 17:21:58.445: INFO: flowcontrol.apiserver.k8s.io/v1beta3 matches flowcontrol.apiserver.k8s.io/v1beta3
  Jul 29 17:21:58.445: INFO: Checking APIGroup: cilium.io
  Jul 29 17:21:58.446: INFO: PreferredVersion.GroupVersion: cilium.io/v2
  Jul 29 17:21:58.446: INFO: Versions found [{cilium.io/v2 v2} {cilium.io/v2alpha1 v2alpha1}]
  Jul 29 17:21:58.447: INFO: cilium.io/v2 matches cilium.io/v2
  Jul 29 17:21:58.447: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "discovery-9786" for this suite. @ 07/29/23 17:21:58.455
• [1.220 seconds]
------------------------------
SSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSS
------------------------------
[sig-api-machinery] ResourceQuota should verify ResourceQuota with best effort scope. [Conformance]
test/e2e/apimachinery/resource_quota.go:806
  STEP: Creating a kubernetes client @ 07/29/23 17:21:58.484
  Jul 29 17:21:58.484: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename resourcequota @ 07/29/23 17:21:58.487
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:21:58.528
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:21:58.536
  STEP: Creating a ResourceQuota with best effort scope @ 07/29/23 17:21:58.542
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 17:21:58.555
  E0729 17:21:58.857863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:21:59.857820      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a ResourceQuota with not best effort scope @ 07/29/23 17:22:00.563
  STEP: Ensuring ResourceQuota status is calculated @ 07/29/23 17:22:00.58
  E0729 17:22:00.858964      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:01.860330      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a best-effort pod @ 07/29/23 17:22:02.589
  STEP: Ensuring resource quota with best effort scope captures the pod usage @ 07/29/23 17:22:02.629
  E0729 17:22:02.860863      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:03.860885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with not best effort ignored the pod usage @ 07/29/23 17:22:04.637
  E0729 17:22:04.861984      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:05.863167      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/29/23 17:22:06.653
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 17:22:06.683
  E0729 17:22:06.863143      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:07.864264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Creating a not best-effort pod @ 07/29/23 17:22:08.689
  STEP: Ensuring resource quota with not best effort scope captures the pod usage @ 07/29/23 17:22:08.704
  E0729 17:22:08.865014      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:09.865682      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring resource quota with best effort scope ignored the pod usage @ 07/29/23 17:22:10.714
  E0729 17:22:10.865697      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:11.866966      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Deleting the pod @ 07/29/23 17:22:12.723
  STEP: Ensuring resource quota status released the pod usage @ 07/29/23 17:22:12.744
  E0729 17:22:12.867345      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:13.867446      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  Jul 29 17:22:14.754: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "resourcequota-603" for this suite. @ 07/29/23 17:22:14.767
• [16.295 seconds]
------------------------------
SSSSSSSSSSSSS
------------------------------
[sig-apps] CronJob should replace jobs when ReplaceConcurrent [Conformance]
test/e2e/apps/cronjob.go:161
  STEP: Creating a kubernetes client @ 07/29/23 17:22:14.784
  Jul 29 17:22:14.785: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename cronjob @ 07/29/23 17:22:14.788
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:22:14.818
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:22:14.824
  STEP: Creating a ReplaceConcurrent cronjob @ 07/29/23 17:22:14.829
  STEP: Ensuring a job is scheduled @ 07/29/23 17:22:14.84
  E0729 17:22:14.867828      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:15.868789      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:16.869344      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:17.869479      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:18.870561      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:19.870632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:20.870679      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:21.871670      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:22.872414      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:23.873135      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:24.873247      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:25.874068      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:26.874661      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:27.875264      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:28.875205      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:29.875706      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:30.876501      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:31.876928      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:32.877468      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:33.877939      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:34.878444      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:35.878885      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:36.879907      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:37.880098      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:38.880217      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:39.880530      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:40.881563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:41.882021      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:42.882987      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:43.883400      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:44.884371      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:45.884667      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:46.885225      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:47.885578      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:48.886415      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:49.886569      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:50.886867      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:51.887979      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:52.888702      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:53.888837      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:54.889923      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:55.890155      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:56.890563      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:57.891304      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:58.891684      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:22:59.892991      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Ensuring exactly one is scheduled @ 07/29/23 17:23:00.847
  STEP: Ensuring exactly one running job exists by listing jobs explicitly @ 07/29/23 17:23:00.852
  STEP: Ensuring the job is replaced with a new one @ 07/29/23 17:23:00.857
  E0729 17:23:00.893495      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:01.894432      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:02.894566      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:03.894823      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:04.895632      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:05.896763      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:06.897105      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:07.897253      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:08.898203      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:09.898752      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:10.899422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:11.900532      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:12.901294      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:13.901572      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:14.901683      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:15.902059      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:16.902071      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:17.902215      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:18.902825      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:19.903032      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:20.903625      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:21.903826      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:22.904391      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:23.904464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:24.904494      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:25.904831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:26.905121      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:27.905233      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:28.905860      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:29.906031      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:30.906610      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:31.906643      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:32.907176      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:33.907754      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:34.908384      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:35.908890      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:36.909246      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:37.910287      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:38.910545      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:39.910869      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:40.911517      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:41.912969      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:42.913488      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:43.914103      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:44.914692      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:45.915719      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:46.916903      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:47.917583      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:48.918598      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:49.919276      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:50.919824      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:51.920422      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:52.921476      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:53.921657      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:54.921936      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:55.924128      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:56.924831      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:57.925202      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:58.925910      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:23:59.926110      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Removing cronjob @ 07/29/23 17:24:00.869
  Jul 29 17:24:00.884: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  E0729 17:24:00.947464      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Destroying namespace "cronjob-5511" for this suite. @ 07/29/23 17:24:00.951
• [106.182 seconds]
------------------------------
SSSSSSSSSSSSSSS
------------------------------
[sig-storage] EmptyDir volumes volume on tmpfs should have the correct mode [LinuxOnly] [NodeConformance] [Conformance]
test/e2e/common/storage/empty_dir.go:87
  STEP: Creating a kubernetes client @ 07/29/23 17:24:00.968
  Jul 29 17:24:00.968: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename emptydir @ 07/29/23 17:24:00.97
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:24:01.003
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:24:01.009
  STEP: Creating a pod to test emptydir volume type on tmpfs @ 07/29/23 17:24:01.014
  E0729 17:24:01.934443      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:24:02.935272      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:24:03.935173      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  E0729 17:24:04.935228      13 retrywatcher.go:130] "Watch failed" err="context canceled"
  STEP: Saw pod success @ 07/29/23 17:24:05.088
  Jul 29 17:24:05.095: INFO: Trying to get logs from node ci9axai7aiv7-3 pod pod-81827772-f71a-4adc-953e-bf6641162bd9 container test-container: <nil>
  STEP: delete the pod @ 07/29/23 17:24:05.133
  Jul 29 17:24:05.155: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "emptydir-7696" for this suite. @ 07/29/23 17:24:05.164
• [4.206 seconds]
------------------------------
SSSSSSSSSSS
------------------------------
[sig-node] ConfigMap should fail to create ConfigMap with empty key [Conformance]
test/e2e/common/node/configmap.go:138
  STEP: Creating a kubernetes client @ 07/29/23 17:24:05.176
  Jul 29 17:24:05.176: INFO: >>> kubeConfig: /tmp/kubeconfig-3894441350
  STEP: Building a namespace api object, basename configmap @ 07/29/23 17:24:05.178
  STEP: Waiting for a default service account to be provisioned in namespace @ 07/29/23 17:24:05.213
  STEP: Waiting for kube-root-ca.crt to be provisioned in namespace @ 07/29/23 17:24:05.219
  STEP: Creating configMap that has name configmap-test-emptyKey-54256a32-7083-4d16-b9ac-d8c972eafb6f @ 07/29/23 17:24:05.224
  Jul 29 17:24:05.228: INFO: Waiting up to 3m0s for all (but 0) nodes to be ready
  STEP: Destroying namespace "configmap-5486" for this suite. @ 07/29/23 17:24:05.237
• [0.076 seconds]
------------------------------
SSSSSSSSSS
------------------------------
[SynchronizedAfterSuite] 
test/e2e/e2e.go:88
  Jul 29 17:24:05.258: INFO: Running AfterSuite actions on node 1
  Jul 29 17:24:05.258: INFO: Skipping dumping logs from cluster
[SynchronizedAfterSuite] PASSED [0.001 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e suite report
test/e2e/e2e_test.go:152
[ReportAfterSuite] PASSED [0.000 seconds]
------------------------------
[ReportAfterSuite] Kubernetes e2e JUnit report
test/e2e/framework/test_context.go:593
[ReportAfterSuite] PASSED [0.100 seconds]
------------------------------

Ran 378 of 7207 Specs in 6833.095 seconds
SUCCESS! -- 378 Passed | 0 Failed | 0 Pending | 6829 Skipped
PASS

Ginkgo ran 1 suite in 1h53m54.162102152s
Test Suite Passed
[38;5;228mYou're using deprecated Ginkgo functionality:[0m
[38;5;228m=============================================[0m
  [38;5;11m--noColor is deprecated, use --no-color instead[0m
  [1mLearn more at:[0m [38;5;14m[4mhttps://onsi.github.io/ginkgo/MIGRATING_TO_V2#changed-command-line-flags[0m

[38;5;243mTo silence deprecations that can be silenced set the following environment variable:[0m
  [38;5;243mACK_GINKGO_DEPRECATIONS=2.9.1[0m

